{
  "data": [
    {
      "id": "0c176d19-ff82-42fe-96be-8e3be073c1e1",
      "source": "firecrawl/docs/api-reference-endpoint-crawl-post.md",
      "content": "---\ntitle: Firecrawl Docs\nurl: https://docs.firecrawl.dev/api-reference/endpoint/crawl-post\n---  \n[Firecrawl Docs home page![light logo](https://mintlify.s3.us-west-1.amazonaws.com/firecrawl/logo/light.svg)![dark logo](https://mintlify.s3.us-west-1.amazonaws.com/firecrawl/logo/dark.svg)](https://firecrawl.dev/)  \nv1  \nSearch or ask...  \nCtrl K  \nSearch...  \nNavigation  \nCrawl Endpoints  \nCrawl  \n[Documentation](https://docs.firecrawl.dev/introduction) [SDKs](https://docs.firecrawl.dev/sdks/overview) [Learn](https://www.firecrawl.dev/blog/category/tutorials) [API Reference](https://docs.firecrawl.dev/api-reference/introduction)  \nPOST  \n/  \ncrawl  \nTry it  \ncURL  \nPython  \nJavaScript  \nPHP  \nGo  \nJava  \nCopy  \n```\ncurl --request POST \\\n--url https://api.firecrawl.dev/v1/crawl \\\n--header 'Authorization: Bearer <token>' \\\n--header 'Content-Type: application/json' \\\n--data '{\n\"url\": \"<string>\",\n\"excludePaths\": [\\\n\"<string>\"\\\n],\n\"includePaths\": [\\\n\"<string>\"\\\n],\n\"maxDepth\": 2,\n\"ignoreSitemap\": false,\n\"ignoreQueryParameters\": false,\n\"limit\": 10000,\n\"allowBackwardLinks\": false,\n\"allowExternalLinks\": false,\n\"webhook\": \"<string>\",\n\"scrapeOptions\": {\n\"formats\": [\\\n\"markdown\"\\\n],\n\"headers\": {},\n\"includeTags\": [\\\n\"<string>\"\\\n],\n\"excludeTags\": [\\\n\"<string>\"\\\n],\n\"onlyMainContent\": true,\n\"removeBase64Images\": true,\n\"blockAds\": true,\n\"mobile\": false,\n\"waitFor\": 123\n}\n}'\n```  \n200  \n402  \n429  \n500  \nCopy  \n```\n{\n\"success\": true,\n\"id\": \"<string>\",\n\"url\": \"<string>\"\n}\n```",
      "metadata": {
        "title": "Firecrawl Docs",
        "url": "https://docs.firecrawl.dev/api-reference/endpoint/crawl-post"
      }
    },
    {
      "id": "d88af744-2ebf-43bd-83b0-8099a4aa91f5",
      "source": "firecrawl/docs/api-reference-endpoint-crawl-post.md",
      "content": "Authorizations\n\n[](https://docs.firecrawl.dev/api-reference/endpoint/crawl-post#authorization-authorization)  \nAuthorization  \nstring  \nheader  \nrequired  \nBearer authentication header of the form `Bearer <token>`, where `<token>` is your auth token.",
      "metadata": {
        "title": "Firecrawl Docs",
        "url": "https://docs.firecrawl.dev/api-reference/endpoint/crawl-post"
      }
    },
    {
      "id": "f46ecbd7-baa6-46c8-bf5c-39403a2878dc",
      "source": "firecrawl/docs/api-reference-endpoint-crawl-post.md",
      "content": "Body\n\napplication/json  \n[](https://docs.firecrawl.dev/api-reference/endpoint/crawl-post#body-url)  \nurl  \nstring  \nrequired  \nThe base URL to start crawling from  \n[](https://docs.firecrawl.dev/api-reference/endpoint/crawl-post#body-exclude-paths)  \nexcludePaths  \nstring[]  \nURL pathname regex patterns that exclude matching URLs from the crawl. For example, if you set \"excludePaths\": [\"blog/.*\"] for the base URL firecrawl.dev, any results matching that pattern will be excluded, such as https://www.firecrawl.dev/blog/firecrawl-launch-week-1-recap.  \n[](https://docs.firecrawl.dev/api-reference/endpoint/crawl-post#body-include-paths)  \nincludePaths  \nstring[]  \nURL pathname regex patterns that include matching URLs in the crawl. Only the paths that match the specified patterns will be included in the response. For example, if you set \"includePaths\": [\"blog/.*\"] for the base URL firecrawl.dev, only results matching that pattern will be included, such as https://www.firecrawl.dev/blog/firecrawl-launch-week-1-recap.  \n[](https://docs.firecrawl.dev/api-reference/endpoint/crawl-post#body-max-depth)  \nmaxDepth  \ninteger  \ndefault:  \n2  \nMaximum depth to crawl relative to the entered URL.  \n[](https://docs.firecrawl.dev/api-reference/endpoint/crawl-post#body-ignore-sitemap)  \nignoreSitemap  \nboolean  \ndefault:  \nfalse  \nIgnore the website sitemap when crawling  \n[](https://docs.firecrawl.dev/api-reference/endpoint/crawl-post#body-ignore-query-parameters)  \nignoreQueryParameters  \nboolean  \ndefault:  \nfalse  \nDo not re-scrape the same path with different (or none) query parameters  \n[](https://docs.firecrawl.dev/api-reference/endpoint/crawl-post#body-limit)  \nlimit  \ninteger  \ndefault:  \n10000  \nMaximum number of pages to crawl. Default limit is 10000.  \n[](https://docs.firecrawl.dev/api-reference/endpoint/crawl-post#body-allow-backward-links)  \nallowBackwardLinks  \nboolean  \ndefault:  \nfalse  \nEnables the crawler to navigate from a specific URL to previously linked pages.  \n[](https://docs.firecrawl.dev/api-reference/endpoint/crawl-post#body-allow-external-links)  \nallowExternalLinks  \nboolean  \ndefault:  \nfalse  \nAllows the crawler to follow links to external websites.  \n[](https://docs.firecrawl.dev/api-reference/endpoint/crawl-post#body-webhook)  \nwebhook  \nstringobject  \nThe URL to send the webhook to. This will trigger for crawl started (crawl.started) ,every page crawled (crawl.page) and when the crawl is completed (crawl.completed or crawl.failed). The response will be the same as the `/scrape` endpoint.  \n[](https://docs.firecrawl.dev/api-reference/endpoint/crawl-post#body-scrape-options)  \nscrapeOptions  \nobject  \nShowchild attributes  \n[](https://docs.firecrawl.dev/api-reference/endpoint/crawl-post#body-scrape-options-formats)  \nscrapeOptions.formats  \nenum<string>[]  \nFormats to include in the output.  \nAvailable options:  \n`markdown`,  \n`html`,  \n`rawHtml`,  \n`links`,  \n`screenshot`  \n[](https://docs.firecrawl.dev/api-reference/endpoint/crawl-post#body-scrape-options-headers)  \nscrapeOptions.headers  \nobject  \nHeaders to send with the request. Can be used to send cookies, user-agent, etc.  \n[](https://docs.firecrawl.dev/api-reference/endpoint/crawl-post#body-scrape-options-include-tags)  \nscrapeOptions.includeTags  \nstring[]  \nTags to include in the output.  \n[](https://docs.firecrawl.dev/api-reference/endpoint/crawl-post#body-scrape-options-exclude-tags)  \nscrapeOptions.excludeTags  \nstring[]  \nTags to exclude from the output.  \n[](https://docs.firecrawl.dev/api-reference/endpoint/crawl-post#body-scrape-options-only-main-content)  \nscrapeOptions.onlyMainContent  \nboolean  \ndefault:  \ntrue  \nOnly return the main content of the page excluding headers, navs, footers, etc.  \n[](https://docs.firecrawl.dev/api-reference/endpoint/crawl-post#body-scrape-options-remove-base64-images)  \nscrapeOptions.removeBase64Images  \nboolean  \ndefault:  \ntrue  \nRemove base64 encoded images from the output  \n[](https://docs.firecrawl.dev/api-reference/endpoint/crawl-post#body-scrape-options-block-ads)  \nscrapeOptions.blockAds  \nboolean  \ndefault:  \ntrue  \nEnables ad-blocking and cookie popup blocking.  \n[](https://docs.firecrawl.dev/api-reference/endpoint/crawl-post#body-scrape-options-mobile)  \nscrapeOptions.mobile  \nboolean  \ndefault:  \nfalse  \nSet to true if you want to emulate scraping from a mobile device. Useful for testing responsive pages and taking mobile screenshots.  \n[](https://docs.firecrawl.dev/api-reference/endpoint/crawl-post#body-scrape-options-wait-for)  \nscrapeOptions.waitFor  \ninteger  \ndefault:  \n123  \nWait x amount of milliseconds for the page to load to fetch content",
      "metadata": {
        "title": "Firecrawl Docs",
        "url": "https://docs.firecrawl.dev/api-reference/endpoint/crawl-post"
      }
    },
    {
      "id": "741d9ad9-f734-4778-8985-3c21bec3282a",
      "source": "firecrawl/docs/api-reference-endpoint-crawl-post.md",
      "content": "Response\n\n200 - application/json  \n[](https://docs.firecrawl.dev/api-reference/endpoint/crawl-post#response-success)  \nsuccess  \nboolean  \n[](https://docs.firecrawl.dev/api-reference/endpoint/crawl-post#response-id)  \nid  \nstring  \n[](https://docs.firecrawl.dev/api-reference/endpoint/crawl-post#response-url)  \nurl  \nstring  \n[Suggest edits](https://github.com/hellofirecrawl/docs/edit/main/api-reference/endpoint/crawl-post.mdx) [Raise issue](https://github.com/hellofirecrawl/docs/issues/new?title=Issue%20on%20docs&body=Path:%20/api-reference/endpoint/crawl-post)  \n[Get Batch Scrape Errors](https://docs.firecrawl.dev/api-reference/endpoint/batch-scrape-get-errors) [Get Crawl Status](https://docs.firecrawl.dev/api-reference/endpoint/crawl-get)  \ncURL  \nPython  \nJavaScript  \nPHP  \nGo  \nJava  \nCopy  \n```\ncurl --request POST \\\n--url https://api.firecrawl.dev/v1/crawl \\\n--header 'Authorization: Bearer <token>' \\\n--header 'Content-Type: application/json' \\\n--data '{\n\"url\": \"<string>\",\n\"excludePaths\": [\\\n\"<string>\"\\\n],\n\"includePaths\": [\\\n\"<string>\"\\\n],\n\"maxDepth\": 2,\n\"ignoreSitemap\": false,\n\"ignoreQueryParameters\": false,\n\"limit\": 10000,\n\"allowBackwardLinks\": false,\n\"allowExternalLinks\": false,\n\"webhook\": \"<string>\",\n\"scrapeOptions\": {\n\"formats\": [\\\n\"markdown\"\\\n],\n\"headers\": {},\n\"includeTags\": [\\\n\"<string>\"\\\n],\n\"excludeTags\": [\\\n\"<string>\"\\\n],\n\"onlyMainContent\": true,\n\"removeBase64Images\": true,\n\"blockAds\": true,\n\"mobile\": false,\n\"waitFor\": 123\n}\n}'\n```  \n200  \n402  \n429  \n500  \nCopy  \n```\n{\n\"success\": true,\n\"id\": \"<string>\",\n\"url\": \"<string>\"\n}\n```",
      "metadata": {
        "title": "Firecrawl Docs",
        "url": "https://docs.firecrawl.dev/api-reference/endpoint/crawl-post"
      }
    },
    {
      "id": "c5beb36b-53d8-4e2b-87b1-b769bb397c2a",
      "source": "firecrawl/docs/v0-sdks-python.md",
      "content": "---\ntitle: Python SDK | Firecrawl\nurl: https://docs.firecrawl.dev/v0/sdks/python\n---  \n[Firecrawl Docs home page![light logo](https://mintlify.s3.us-west-1.amazonaws.com/firecrawl/logo/light.svg)![dark logo](https://mintlify.s3.us-west-1.amazonaws.com/firecrawl/logo/dark.svg)](https://firecrawl.dev/)  \nv0  \nSearch or ask...  \nCtrl K  \nSearch...  \nNavigation  \nSDKs  \nPython  \n[Documentation](https://docs.firecrawl.dev/v0/introduction) [SDKs](https://docs.firecrawl.dev/v0/sdks/python) [Learn](https://www.firecrawl.dev/blog/category/tutorials) [API Reference](https://docs.firecrawl.dev/v0/api-reference/introduction)  \n> Note: this is using [v0 version of the Firecrawl API](https://docs.firecrawl.dev/v0/introduction) which is being deprecated. We recommend switching to [v1](https://docs.firecrawl.dev/sdks/python).",
      "metadata": {
        "title": "Python SDK | Firecrawl",
        "url": "https://docs.firecrawl.dev/v0/sdks/python"
      }
    },
    {
      "id": "32cbdeeb-b80c-42cc-9415-0999ea1145f8",
      "source": "firecrawl/docs/v0-sdks-python.md",
      "content": "[](https://docs.firecrawl.dev/v0/sdks/python#installation) Installation\n\nTo install the Firecrawl Python SDK, you can use pip:  \nCopy  \n```bash\npip install firecrawl-py==0.0.16\n\n```",
      "metadata": {
        "title": "Python SDK | Firecrawl",
        "url": "https://docs.firecrawl.dev/v0/sdks/python"
      }
    },
    {
      "id": "e813fba1-63df-4697-b6aa-829edd82c61c",
      "source": "firecrawl/docs/v0-sdks-python.md",
      "content": "[](https://docs.firecrawl.dev/v0/sdks/python#usage) Usage\n\n1. Get an API key from [firecrawl.dev](https://firecrawl.dev/)\n2. Set the API key as an environment variable named `FIRECRAWL_API_KEY` or pass it as a parameter to the `FirecrawlApp` class.  \nHereâ€™s an example of how to use the SDK:  \nCopy  \n```python\nfrom firecrawl import FirecrawlApp\n\n# Initialize the FirecrawlApp with your API key\napp = FirecrawlApp(api_key='your_api_key')\n\n# Scrape a single URL\nurl = 'https://docs.firecrawl.dev'\nscraped_data = app.scrape_url(url)\n\n# Crawl a website\ncrawl_url = 'https://docs.firecrawl.dev'\nparams = {\n'pageOptions': {\n'onlyMainContent': True\n}\n}\ncrawl_result = app.crawl_url(crawl_url, params=params)\n\n```",
      "metadata": {
        "title": "Python SDK | Firecrawl",
        "url": "https://docs.firecrawl.dev/v0/sdks/python"
      }
    },
    {
      "id": "5014e1f9-2275-4722-9b79-c145919ad1d2",
      "source": "firecrawl/docs/v0-sdks-python.md",
      "content": "[](https://docs.firecrawl.dev/v0/sdks/python#usage) Usage > [](https://docs.firecrawl.dev/v0/sdks/python#scraping-a-url) Scraping a URL\n\nTo scrape a single URL, use the `scrape_url` method. It takes the URL as a parameter and returns the scraped data as a dictionary.  \nCopy  \n```python\nurl = 'https://example.com'\nscraped_data = app.scrape_url(url)\n\n```",
      "metadata": {
        "title": "Python SDK | Firecrawl",
        "url": "https://docs.firecrawl.dev/v0/sdks/python"
      }
    },
    {
      "id": "89e48d56-9fa9-4943-a267-23f4eec673f1",
      "source": "firecrawl/docs/v0-sdks-python.md",
      "content": "[](https://docs.firecrawl.dev/v0/sdks/python#usage) Usage > [](https://docs.firecrawl.dev/v0/sdks/python#extracting-structured-data-from-a-url) Extracting structured data from a URL\n\nWith LLM extraction, you can easily extract structured data from any URL. We support pydantic schemas to make it easier for you too. Here is how you to use it:  \nCopy  \n```python\nclass ArticleSchema(BaseModel):\ntitle: str\npoints: int\nby: str\ncommentsURL: str\n\nclass TopArticlesSchema(BaseModel):\ntop: List[ArticleSchema] = Field(..., max_items=5, description=\"Top 5 stories\")\n\ndata = app.scrape_url('https://news.ycombinator.com', {\n'extractorOptions': {\n'extractionSchema': TopArticlesSchema.model_json_schema(),\n'mode': 'llm-extraction'\n},\n'pageOptions':{\n'onlyMainContent': True\n}\n})\nprint(data[\"llm_extraction\"])\n\n```",
      "metadata": {
        "title": "Python SDK | Firecrawl",
        "url": "https://docs.firecrawl.dev/v0/sdks/python"
      }
    },
    {
      "id": "be713f6d-d20a-40b6-b316-5f3610138197",
      "source": "firecrawl/docs/v0-sdks-python.md",
      "content": "[](https://docs.firecrawl.dev/v0/sdks/python#usage) Usage > [](https://docs.firecrawl.dev/v0/sdks/python#crawling-a-website) Crawling a Website\n\nTo crawl a website, use the `crawl_url` method. It takes the starting URL and optional parameters as arguments. The `params` argument allows you to specify additional options for the crawl job, such as the maximum number of pages to crawl, allowed domains, and the output format.  \nThe `wait_until_done` parameter determines whether the method should wait for the crawl job to complete before returning the result. If set to `True`, the method will periodically check the status of the crawl job until it is completed or the specified `timeout` (in seconds) is reached. If set to `False`, the method will return immediately with the job ID, and you can manually check the status of the crawl job using the `check_crawl_status` method.  \nCopy  \n```python\ncrawl_url = 'https://example.com'\nparams = {\n'crawlerOptions': {\n'excludes': ['blog/*'],\n'includes': [], # leave empty for all pages\n'limit': 1000,\n},\n'pageOptions': {\n'onlyMainContent': True\n}\n}\ncrawl_result = app.crawl_url(crawl_url, params=params, wait_until_done=True, timeout=5)\n\n```  \nIf `wait_until_done` is set to `True`, the `crawl_url` method will return the crawl result once the job is completed. If the job fails or is stopped, an exception will be raised.",
      "metadata": {
        "title": "Python SDK | Firecrawl",
        "url": "https://docs.firecrawl.dev/v0/sdks/python"
      }
    },
    {
      "id": "ccdcb7b6-59fd-4291-8358-5ab49792f1fa",
      "source": "firecrawl/docs/v0-sdks-python.md",
      "content": "[](https://docs.firecrawl.dev/v0/sdks/python#usage) Usage > [](https://docs.firecrawl.dev/v0/sdks/python#checking-crawl-status) Checking Crawl Status\n\nTo check the status of a crawl job, use the `check_crawl_status` method. It takes the job ID as a parameter and returns the current status of the crawl job.  \nCopy  \n```python\njob_id = crawl_result['jobId']\nstatus = app.check_crawl_status(job_id)\n\n```",
      "metadata": {
        "title": "Python SDK | Firecrawl",
        "url": "https://docs.firecrawl.dev/v0/sdks/python"
      }
    },
    {
      "id": "e5159f15-4f4c-4a8a-b3a2-a6e8d9e2deb3",
      "source": "firecrawl/docs/v0-sdks-python.md",
      "content": "[](https://docs.firecrawl.dev/v0/sdks/python#usage) Usage > [](https://docs.firecrawl.dev/v0/sdks/python#search-for-a-query) Search for a query\n\nUsed to search the web, get the most relevant results, scrap each page and return the markdown.  \nCopy  \n```python\nquery = 'what is mendable?'\nsearch_result = app.search(query)\n\n```",
      "metadata": {
        "title": "Python SDK | Firecrawl",
        "url": "https://docs.firecrawl.dev/v0/sdks/python"
      }
    },
    {
      "id": "10d46e15-5543-4711-88ff-e7ed679edf8d",
      "source": "firecrawl/docs/v0-sdks-python.md",
      "content": "[](https://docs.firecrawl.dev/v0/sdks/python#error-handling) Error Handling\n\nThe SDK handles errors returned by the Firecrawl API and raises appropriate exceptions. If an error occurs during a request, an exception will be raised with a descriptive error message.  \n[Suggest edits](https://github.com/hellofirecrawl/docs/edit/main/v0/sdks/python.mdx) [Raise issue](https://github.com/hellofirecrawl/docs/issues/new?title=Issue%20on%20docs&body=Path:%20/v0/sdks/python)  \n[Node](https://docs.firecrawl.dev/v0/sdks/node)  \nOn this page  \n- [Installation](https://docs.firecrawl.dev/v0/sdks/python#installation)\n- [Usage](https://docs.firecrawl.dev/v0/sdks/python#usage)\n- [Scraping a URL](https://docs.firecrawl.dev/v0/sdks/python#scraping-a-url)\n- [Extracting structured data from a URL](https://docs.firecrawl.dev/v0/sdks/python#extracting-structured-data-from-a-url)\n- [Crawling a Website](https://docs.firecrawl.dev/v0/sdks/python#crawling-a-website)\n- [Checking Crawl Status](https://docs.firecrawl.dev/v0/sdks/python#checking-crawl-status)\n- [Search for a query](https://docs.firecrawl.dev/v0/sdks/python#search-for-a-query)\n- [Error Handling](https://docs.firecrawl.dev/v0/sdks/python#error-handling)",
      "metadata": {
        "title": "Python SDK | Firecrawl",
        "url": "https://docs.firecrawl.dev/v0/sdks/python"
      }
    },
    {
      "id": "5c706d32-f93b-4c12-9671-600d09ddeddf",
      "source": "firecrawl/docs/features-batch-scrape.md",
      "content": "---\ntitle: Batch Scrape | Firecrawl\nurl: https://docs.firecrawl.dev/features/batch-scrape\n---  \n[Firecrawl Docs home page![light logo](https://mintlify.s3.us-west-1.amazonaws.com/firecrawl/logo/light.svg)![dark logo](https://mintlify.s3.us-west-1.amazonaws.com/firecrawl/logo/dark.svg)](https://firecrawl.dev/)  \nv1  \nSearch or ask...  \nCtrl K  \nSearch...  \nNavigation  \nScrape  \nBatch Scrape  \n[Documentation](https://docs.firecrawl.dev/introduction) [SDKs](https://docs.firecrawl.dev/sdks/overview) [Learn](https://www.firecrawl.dev/blog/category/tutorials) [API Reference](https://docs.firecrawl.dev/api-reference/introduction)",
      "metadata": {
        "title": "Batch Scrape | Firecrawl",
        "url": "https://docs.firecrawl.dev/features/batch-scrape"
      }
    },
    {
      "id": "9b9359f9-0ffa-4404-8f3c-edc2088020ba",
      "source": "firecrawl/docs/features-batch-scrape.md",
      "content": "[](https://docs.firecrawl.dev/features/batch-scrape#batch-scraping-multiple-urls) Batch scraping multiple URLs\n\nYou can now batch scrape multiple URLs at the same time. It takes the starting URLs and optional parameters as arguments. The params argument allows you to specify additional options for the batch scrape job, such as the output formats.",
      "metadata": {
        "title": "Batch Scrape | Firecrawl",
        "url": "https://docs.firecrawl.dev/features/batch-scrape"
      }
    },
    {
      "id": "28d2d2ff-9afd-4d8a-aee1-6452d5f9e64a",
      "source": "firecrawl/docs/features-batch-scrape.md",
      "content": "[](https://docs.firecrawl.dev/features/batch-scrape#batch-scraping-multiple-urls) Batch scraping multiple URLs > [](https://docs.firecrawl.dev/features/batch-scrape#how-it-works) How it works\n\nIt is very similar to how the `/crawl` endpoint works. It submits a batch scrape job and returns a job ID to check the status of the batch scrape.  \nThe sdk provides 2 methods, synchronous and asynchronous. The synchronous method will return the results of the batch scrape job, while the asynchronous method will return a job ID that you can use to check the status of the batch scrape.",
      "metadata": {
        "title": "Batch Scrape | Firecrawl",
        "url": "https://docs.firecrawl.dev/features/batch-scrape"
      }
    },
    {
      "id": "46fd42e6-7395-44b5-9c05-e7d4c820ae2c",
      "source": "firecrawl/docs/features-batch-scrape.md",
      "content": "[](https://docs.firecrawl.dev/features/batch-scrape#batch-scraping-multiple-urls) Batch scraping multiple URLs > [](https://docs.firecrawl.dev/features/batch-scrape#usage) Usage\n\nPython  \nNode  \ncURL  \nCopy  \n```python\nfrom firecrawl import FirecrawlApp\n\napp = FirecrawlApp(api_key=\"fc-YOUR_API_KEY\")\n\n# Scrape multiple websites:\nbatch_scrape_result = app.batch_scrape_urls(['firecrawl.dev', 'mendable.ai'], {'formats': ['markdown', 'html']})\nprint(batch_scrape_result)\n\n# Or, you can use the asynchronous method:\nbatch_scrape_job = app.async_batch_scrape_urls(['firecrawl.dev', 'mendable.ai'], {'formats': ['markdown', 'html']})\nprint(batch_scrape_job)\n\n# (async) You can then use the job ID to check the status of the batch scrape:\nbatch_scrape_status = app.check_batch_scrape_status(batch_scrape_job['id'])\nprint(batch_scrape_status)\n\n```",
      "metadata": {
        "title": "Batch Scrape | Firecrawl",
        "url": "https://docs.firecrawl.dev/features/batch-scrape"
      }
    },
    {
      "id": "f64fe274-0bf5-4c3f-8abb-5856f59246bd",
      "source": "firecrawl/docs/features-batch-scrape.md",
      "content": "[](https://docs.firecrawl.dev/features/batch-scrape#batch-scraping-multiple-urls) Batch scraping multiple URLs > [](https://docs.firecrawl.dev/features/batch-scrape#response) Response\n\nIf youâ€™re using the sync methods from the SDKs, it will return the results of the batch scrape job. Otherwise, it will return a job ID that you can use to check the status of the batch scrape.",
      "metadata": {
        "title": "Batch Scrape | Firecrawl",
        "url": "https://docs.firecrawl.dev/features/batch-scrape"
      }
    },
    {
      "id": "21044d2d-26df-44c4-a272-5ade515294ee",
      "source": "firecrawl/docs/features-batch-scrape.md",
      "content": "[](https://docs.firecrawl.dev/features/batch-scrape#batch-scraping-multiple-urls) Batch scraping multiple URLs > [](https://docs.firecrawl.dev/features/batch-scrape#response) Response > [](https://docs.firecrawl.dev/features/batch-scrape#synchronous) Synchronous\n\nCompleted  \nCopy  \n```json\n{\n\"status\": \"completed\",\n\"total\": 36,\n\"completed\": 36,\n\"creditsUsed\": 36,\n\"expiresAt\": \"2024-00-00T00:00:00.000Z\",\n\"next\": \"https://api.firecrawl.dev/v1/batch/scrape/123-456-789?skip=26\",\n\"data\": [\\\n{\\\n\"markdown\": \"[Firecrawl Docs home page![light logo](https://mintlify.s3-us-west-1.amazonaws.com/firecrawl/logo/light.svg)!...\",\\\n\"html\": \"<!DOCTYPE html><html lang=\"en\" class=\"js-focus-visible lg:[--scroll-mt:9.5rem]\" data-js-focus-visible=\"\">...\",\\\n\"metadata\": {\\\n\"title\": \"Build a 'Chat with website' using Groq Llama 3 | Firecrawl\",\\\n\"language\": \"en\",\\\n\"sourceURL\": \"https://docs.firecrawl.dev/learn/rag-llama3\",\\\n\"description\": \"Learn how to use Firecrawl, Groq Llama 3, and Langchain to build a 'Chat with your website' bot.\",\\\n\"ogLocaleAlternate\": [],\\\n\"statusCode\": 200\\\n}\\\n},\\\n...\\\n]\\\n}\\\n\\\n```\\\n\\",
      "metadata": {
        "title": "Batch Scrape | Firecrawl",
        "url": "https://docs.firecrawl.dev/features/batch-scrape"
      }
    },
    {
      "id": "174f996f-7c12-437d-acce-4e91c26bfa1d",
      "source": "firecrawl/docs/features-batch-scrape.md",
      "content": "[](https://docs.firecrawl.dev/features/batch-scrape#batch-scraping-multiple-urls) Batch scraping multiple URLs > [](https://docs.firecrawl.dev/features/batch-scrape#response) Response > [](https://docs.firecrawl.dev/features/batch-scrape#asynchronous) Asynchronous\\\n\n\\\nYou can then use the job ID to check the status of the batch scrape by calling the `/batch/scrape/{id}` endpoint. This endpoint is meant to be used while the job is still running or right after it has completed **as batch scrape jobs expire after 24 hours**.\\\n\\\nCopy\\\n\\\n```json\\\n{\\\n\"success\": true,\\\n\"id\": \"123-456-789\",\\\n\"url\": \"https://api.firecrawl.dev/v1/batch/scrape/123-456-789\"\\\n}\\\n\\\n```\\\n\\",
      "metadata": {
        "title": "Batch Scrape | Firecrawl",
        "url": "https://docs.firecrawl.dev/features/batch-scrape"
      }
    },
    {
      "id": "71baf9ac-c592-4e1c-a83f-df3959fad837",
      "source": "firecrawl/docs/features-batch-scrape.md",
      "content": "[](https://docs.firecrawl.dev/features/batch-scrape#batch-scrape-with-extraction) Batch scrape with extraction\\\n\n\\\nYou can also use the batch scrape endpoint to extract structured data from the pages. This is useful if you want to get the same structured data from a list of URLs.\\\n\\\nPython\\\n\\\nNode\\\n\\\ncURL\\\n\\\nCopy\\\n\\\n```python\\\nfrom firecrawl import FirecrawlApp\\\n\\\napp = FirecrawlApp(api_key=\"fc-YOUR_API_KEY\")\\\n\\\n# Scrape multiple websites:\\\nbatch_scrape_result = app.batch_scrape_urls(\\\n['https://docs.firecrawl.dev', 'https://docs.firecrawl.dev/sdks/overview'],\\\n{\\\n'formats': ['extract'],\\\n'extract': {\\\n'prompt': 'Extract the title and description from the page.',\\\n'schema': {\\\n'type': 'object',\\\n'properties': {\\\n'title': {'type': 'string'},\\\n'description': {'type': 'string'}\\\n},\\\n'required': ['title', 'description']\\\n}\\\n}\\\n}\\\n)\\\nprint(batch_scrape_result)\\\n\\\n# Or, you can use the asynchronous method:\\\nbatch_scrape_job = app.async_batch_scrape_urls(\\\n['https://docs.firecrawl.dev', 'https://docs.firecrawl.dev/sdks/overview'],\\\n{\\\n'formats': ['extract'],\\\n'extract': {\\\n'prompt': 'Extract the title and description from the page.',\\\n'schema': {\\\n'type': 'object',\\\n'properties': {\\\n'title': {'type': 'string'},\\\n'description': {'type': 'string'}\\\n},\\\n'required': ['title', 'description']\\\n}\\\n}\\\n}\\\n)\\\nprint(batch_scrape_job)\\\n\\\n# (async) You can then use the job ID to check the status of the batch scrape:\\\nbatch_scrape_status = app.check_batch_scrape_status(batch_scrape_job['id'])\\\nprint(batch_scrape_status)\\\n\\\n```\\\n\\",
      "metadata": {
        "title": "Batch Scrape | Firecrawl",
        "url": "https://docs.firecrawl.dev/features/batch-scrape"
      }
    },
    {
      "id": "0e089fbf-e332-4838-ba42-1caf5f450ae7",
      "source": "firecrawl/docs/features-batch-scrape.md",
      "content": "[](https://docs.firecrawl.dev/features/batch-scrape#batch-scrape-with-extraction) Batch scrape with extraction\\ > [](https://docs.firecrawl.dev/features/batch-scrape#response-2) Response\\\n\n\\",
      "metadata": {
        "title": "Batch Scrape | Firecrawl",
        "url": "https://docs.firecrawl.dev/features/batch-scrape"
      }
    },
    {
      "id": "a75b1654-426c-498b-9203-5d4a200ab30c",
      "source": "firecrawl/docs/features-batch-scrape.md",
      "content": "[](https://docs.firecrawl.dev/features/batch-scrape#batch-scrape-with-extraction) Batch scrape with extraction\\ > [](https://docs.firecrawl.dev/features/batch-scrape#response-2) Response\\ > [](https://docs.firecrawl.dev/features/batch-scrape#synchronous-2) Synchronous\\\n\n\\\nCompleted\\\n\\\nCopy\\\n\\\n```json\\\n{\\\n\"status\": \"completed\",\\\n\"total\": 36,\\\n\"completed\": 36,\\\n\"creditsUsed\": 36,\\\n\"expiresAt\": \"2024-00-00T00:00:00.000Z\",\\\n\"next\": \"https://api.firecrawl.dev/v1/batch/scrape/123-456-789?skip=26\",\\\n\"data\": [\\\n{\\\n\"extract\": {\\\n\"title\": \"Build a 'Chat with website' using Groq Llama 3 | Firecrawl\",\\\n\"description\": \"Learn how to use Firecrawl, Groq Llama 3, and Langchain to build a 'Chat with your website' bot.\"\\\n}\\\n},\\\n...\\\n]\\\n}\\\n\\\n```\\\n\\",
      "metadata": {
        "title": "Batch Scrape | Firecrawl",
        "url": "https://docs.firecrawl.dev/features/batch-scrape"
      }
    },
    {
      "id": "4d1ea41a-aef2-4be6-b487-0059557499de",
      "source": "firecrawl/docs/features-batch-scrape.md",
      "content": "[](https://docs.firecrawl.dev/features/batch-scrape#batch-scrape-with-extraction) Batch scrape with extraction\\ > [](https://docs.firecrawl.dev/features/batch-scrape#response-2) Response\\ > [](https://docs.firecrawl.dev/features/batch-scrape#asynchronous-2) Asynchronous\\\n\n\\\nCopy\\\n\\\n```json\\\n{\\\n\"success\": true,\\\n\"id\": \"123-456-789\",\\\n\"url\": \"https://api.firecrawl.dev/v1/batch/scrape/123-456-789\"\\\n}\\\n\\\n```\\\n\\\n[Suggest edits](https://github.com/hellofirecrawl/docs/edit/main/features/batch-scrape.mdx) [Raise issue](https://github.com/hellofirecrawl/docs/issues/new?title=Issue%20on%20docs&body=Path:%20/features/batch-scrape)\\\n\\\n[Scrape](https://docs.firecrawl.dev/features/scrape) [LLM Extract](https://docs.firecrawl.dev/features/llm-extract)\\\n\\\nOn this page\\\n\\\n- [Batch scraping multiple URLs](https://docs.firecrawl.dev/features/batch-scrape#batch-scraping-multiple-urls)\\\n- [How it works](https://docs.firecrawl.dev/features/batch-scrape#how-it-works)\\\n- [Usage](https://docs.firecrawl.dev/features/batch-scrape#usage)\\\n- [Response](https://docs.firecrawl.dev/features/batch-scrape#response)\\\n- [Synchronous](https://docs.firecrawl.dev/features/batch-scrape#synchronous)\\\n- [Asynchronous](https://docs.firecrawl.dev/features/batch-scrape#asynchronous)\\\n- [Batch scrape with extraction](https://docs.firecrawl.dev/features/batch-scrape#batch-scrape-with-extraction)\\\n- [Response](https://docs.firecrawl.dev/features/batch-scrape#response-2)\\\n- [Synchronous](https://docs.firecrawl.dev/features/batch-scrape#synchronous-2)\\\n- [Asynchronous](https://docs.firecrawl.dev/features/batch-scrape#asynchronous-2)",
      "metadata": {
        "title": "Batch Scrape | Firecrawl",
        "url": "https://docs.firecrawl.dev/features/batch-scrape"
      }
    },
    {
      "id": "8a4cbf47-1447-43ed-8457-fdbf16169923",
      "source": "firecrawl/docs/v1-welcome.md",
      "content": "---\ntitle: Welcome to V1 | Firecrawl\nurl: https://docs.firecrawl.dev/v1-welcome\n---  \n[Firecrawl Docs home page![light logo](https://mintlify.s3.us-west-1.amazonaws.com/firecrawl/logo/light.svg)![dark logo](https://mintlify.s3.us-west-1.amazonaws.com/firecrawl/logo/dark.svg)](https://firecrawl.dev/)  \nv1  \nSearch or ask...  \nCtrl K  \nSearch...  \nNavigation  \nGet Started  \nWelcome to V1  \n[Documentation](https://docs.firecrawl.dev/introduction) [SDKs](https://docs.firecrawl.dev/sdks/overview) [Learn](https://www.firecrawl.dev/blog/category/tutorials) [API Reference](https://docs.firecrawl.dev/api-reference/introduction)  \nFirecrawl V1 is here! With that we introduce a more reliable and developer friendly API.  \nHere is whatâ€™s new:  \n- Output Formats for `/scrape`. Choose what formats you want your output in.\n- New [`/map` endpoint](https://docs.firecrawl.dev/features/map) for getting most of the URLs of a webpage.\n- Developer friendly API for `/crawl/{id}` status.\n- 2x Rate Limits for all plans.\n- [Go SDK](https://docs.firecrawl.dev/sdks/go) and [Rust SDK](https://docs.firecrawl.dev/sdks/rust)\n- Teams support\n- API Key Management in the dashboard.\n- `onlyMainContent` is now default to `true`.\n- `/crawl` webhooks and websocket support.",
      "metadata": {
        "title": "Welcome to V1 | Firecrawl",
        "url": "https://docs.firecrawl.dev/v1-welcome"
      }
    },
    {
      "id": "d3007b57-a0ad-4096-b962-247b7d502875",
      "source": "firecrawl/docs/v1-welcome.md",
      "content": "[](https://docs.firecrawl.dev/v1-welcome#scrape-formats) Scrape Formats\n\nYou can now choose what formats you want your output in. You can specify multiple output formats. Supported formats are:  \n- Markdown (markdown)\n- HTML (html)\n- Raw HTML (rawHtml) (with no modifications)\n- Screenshot (screenshot or screenshot@fullPage)\n- Links (links)\n- Extract (extract) - structured output  \nOutput keys will match the format you choose.  \nPython  \nNode  \nGo  \nRust  \ncURL  \nCopy  \n```python\nfrom firecrawl import FirecrawlApp\n\napp = FirecrawlApp(api_key=\"fc-YOUR_API_KEY\")\n\n# Scrape a website:\nscrape_result = app.scrape_url('firecrawl.dev', params={'formats': ['markdown', 'html']})\nprint(scrape_result)\n\n```",
      "metadata": {
        "title": "Welcome to V1 | Firecrawl",
        "url": "https://docs.firecrawl.dev/v1-welcome"
      }
    },
    {
      "id": "096f8cff-7af0-4679-9ce0-6495401030c8",
      "source": "firecrawl/docs/v1-welcome.md",
      "content": "[](https://docs.firecrawl.dev/v1-welcome#scrape-formats) Scrape Formats > [](https://docs.firecrawl.dev/v1-welcome#response) Response\n\nSDKs will return the data object directly. cURL will return the payload exactly as shown below.  \nCopy  \n```json\n{\n\"success\": true,\n\"data\" : {\n\"markdown\": \"Launch Week I is here! [See our Day 2 Release ðŸš€](https://www.firecrawl.dev/blog/launch-week-i-day-2-doubled-rate-limits)[ðŸ’¥ Get 2 months free...\",\\\n\"html\": \"<!DOCTYPE html><html lang=\"en\" class=\"light\" style=\"color-scheme: light;\"><body class=\"__variable_36bd41 __variable_d7dc5d font-inter ...\",\\\n\"metadata\": {\\\n\"title\": \"Home - Firecrawl\",\\\n\"description\": \"Firecrawl crawls and converts any website into clean markdown.\",\\\n\"language\": \"en\",\\\n\"keywords\": \"Firecrawl,Markdown,Data,Mendable,Langchain\",\\\n\"robots\": \"follow, index\",\\\n\"ogTitle\": \"Firecrawl\",\\\n\"ogDescription\": \"Turn any website into LLM-ready data.\",\\\n\"ogUrl\": \"https://www.firecrawl.dev/\",\\\n\"ogImage\": \"https://www.firecrawl.dev/og.png?123\",\\\n\"ogLocaleAlternate\": [],\\\n\"ogSiteName\": \"Firecrawl\",\\\n\"sourceURL\": \"https://firecrawl.dev\",\\\n\"statusCode\": 200\\\n}\\\n}\\\n}\\\n\\\n```\\\n\\",
      "metadata": {
        "title": "Welcome to V1 | Firecrawl",
        "url": "https://docs.firecrawl.dev/v1-welcome"
      }
    },
    {
      "id": "6b4e6fe1-f1be-42a2-a53b-167337d83ead",
      "source": "firecrawl/docs/v1-welcome.md",
      "content": "[](https://docs.firecrawl.dev/v1-welcome#introducing-map-alpha) Introducing /map (Alpha)\\\n\n\\\nThe easiest way to go from a single url to a map of the entire website.\\\n\\",
      "metadata": {
        "title": "Welcome to V1 | Firecrawl",
        "url": "https://docs.firecrawl.dev/v1-welcome"
      }
    },
    {
      "id": "29dc1b8c-a96e-4626-bd84-c4216947a9ac",
      "source": "firecrawl/docs/v1-welcome.md",
      "content": "[](https://docs.firecrawl.dev/v1-welcome#introducing-map-alpha) Introducing /map (Alpha)\\ > [](https://docs.firecrawl.dev/v1-welcome#usage) Usage\\\n\n\\\nPython\\\n\\\nNode\\\n\\\nGo\\\n\\\nRust\\\n\\\ncURL\\\n\\\nCopy\\\n\\\n```python\\\nfrom firecrawl import FirecrawlApp\\\n\\\napp = FirecrawlApp(api_key=\"fc-YOUR_API_KEY\")\\\n\\\n# Map a website:\\\nmap_result = app.map_url('https://firecrawl.dev')\\\nprint(map_result)\\\n\\\n```\\\n\\",
      "metadata": {
        "title": "Welcome to V1 | Firecrawl",
        "url": "https://docs.firecrawl.dev/v1-welcome"
      }
    },
    {
      "id": "87d69a2b-025d-48bd-8787-50d539fb818f",
      "source": "firecrawl/docs/v1-welcome.md",
      "content": "[](https://docs.firecrawl.dev/v1-welcome#introducing-map-alpha) Introducing /map (Alpha)\\ > [](https://docs.firecrawl.dev/v1-welcome#response-2) Response\\\n\n\\\nSDKs will return the data object directly. cURL will return the payload exactly as shown below.\\\n\\\nCopy\\\n\\\n```json\\\n{\\\n\"status\": \"success\",\\\n\"links\": [\\\n\"https://firecrawl.dev\",\\\n\"https://www.firecrawl.dev/pricing\",\\\n\"https://www.firecrawl.dev/blog\",\\\n\"https://www.firecrawl.dev/playground\",\\\n\"https://www.firecrawl.dev/smart-crawl\",\\\n...\\\n]\\\n}\\\n\\\n```\\\n\\",
      "metadata": {
        "title": "Welcome to V1 | Firecrawl",
        "url": "https://docs.firecrawl.dev/v1-welcome"
      }
    },
    {
      "id": "e673d9c8-d68f-42b6-9814-fc553f38c1e1",
      "source": "firecrawl/docs/v1-welcome.md",
      "content": "[](https://docs.firecrawl.dev/v1-welcome#websockets) WebSockets\\\n\n\\\nTo crawl a website with WebSockets, use the `Crawl URL and Watch` method.\\\n\\\nPython\\\n\\\nNode\\\n\\\nCopy\\\n\\\n```python\\\n# inside an async function...\\\nnest_asyncio.apply()\\\n\\\n# Define event handlers\\\ndef on_document(detail):\\\nprint(\"DOC\", detail)\\\n\\\ndef on_error(detail):\\\nprint(\"ERR\", detail['error'])\\\n\\\ndef on_done(detail):\\\nprint(\"DONE\", detail['status'])\\\n\\\n# Function to start the crawl and watch process\\\nasync def start_crawl_and_watch():\\\n# Initiate the crawl job and get the watcher\\\nwatcher = app.crawl_url_and_watch('firecrawl.dev', { 'excludePaths': ['blog/*'], 'limit': 5 })\\\n\\\n# Add event listeners\\\nwatcher.add_event_listener(\"document\", on_document)\\\nwatcher.add_event_listener(\"error\", on_error)\\\nwatcher.add_event_listener(\"done\", on_done)\\\n\\\n# Start the watcher\\\nawait watcher.connect()\\\n\\\n# Run the event loop\\\nawait start_crawl_and_watch()\\\n\\\n```\\\n\\",
      "metadata": {
        "title": "Welcome to V1 | Firecrawl",
        "url": "https://docs.firecrawl.dev/v1-welcome"
      }
    },
    {
      "id": "a2de709b-21e6-4b7d-a88b-b72476a50b0f",
      "source": "firecrawl/docs/v1-welcome.md",
      "content": "[](https://docs.firecrawl.dev/v1-welcome#extract-format) Extract format\\\n\n\\\nLLM extraction is now available in v1 under the `extract` format. To extract structured from a page, you can pass a schema to the endpoint or just provide a prompt.\\\n\\\nPython\\\n\\\nNode\\\n\\\ncURL\\\n\\\nCopy\\\n\\\n```python\\\nfrom firecrawl import FirecrawlApp\\\nfrom pydantic import BaseModel, Field\\\n\\\n# Initialize the FirecrawlApp with your API key\\\napp = FirecrawlApp(api_key='your_api_key')\\\n\\\nclass ExtractSchema(BaseModel):\\\ncompany_mission: str\\\nsupports_sso: bool\\\nis_open_source: bool\\\nis_in_yc: bool\\\n\\\ndata = app.scrape_url('https://docs.firecrawl.dev/', {\\\n'formats': ['json'],\\\n'jsonOptions': {\\\n'schema': ExtractSchema.model_json_schema(),\\\n}\\\n})\\\nprint(data[\"json\"])\\\n\\\n```\\\n\\\nOutput:\\\n\\\nJSON\\\n\\\nCopy\\\n\\\n```json\\\n{\\\n\"success\": true,\\\n\"data\": {\\\n\"json\": {\\\n\"company_mission\": \"Train a secure AI on your technical resources that answers customer and employee questions so your team doesn't have to\",\\\n\"supports_sso\": true,\\\n\"is_open_source\": false,\\\n\"is_in_yc\": true\\\n},\\\n\"metadata\": {\\\n\"title\": \"Mendable\",\\\n\"description\": \"Mendable allows you to easily build AI chat applications. Ingest, customize, then deploy with one line of code anywhere you want. Brought to you by SideGuide\",\\\n\"robots\": \"follow, index\",\\\n\"ogTitle\": \"Mendable\",\\\n\"ogDescription\": \"Mendable allows you to easily build AI chat applications. Ingest, customize, then deploy with one line of code anywhere you want. Brought to you by SideGuide\",\\\n\"ogUrl\": \"https://docs.firecrawl.dev/\",\\\n\"ogImage\": \"https://docs.firecrawl.dev/mendable_new_og1.png\",\\\n\"ogLocaleAlternate\": [],\\\n\"ogSiteName\": \"Mendable\",\\\n\"sourceURL\": \"https://docs.firecrawl.dev/\"\\\n},\\\n}\\\n}\\\n\\\n```\\\n\\",
      "metadata": {
        "title": "Welcome to V1 | Firecrawl",
        "url": "https://docs.firecrawl.dev/v1-welcome"
      }
    },
    {
      "id": "10f944c3-bf66-4955-853f-c457b7a1b2e7",
      "source": "firecrawl/docs/v1-welcome.md",
      "content": "[](https://docs.firecrawl.dev/v1-welcome#extract-format) Extract format\\ > [](https://docs.firecrawl.dev/v1-welcome#extracting-without-schema-new) Extracting without schema (New)\\\n\n\\\nYou can now extract without a schema by just passing a `prompt` to the endpoint. The llm chooses the structure of the data.\\\n\\\ncURL\\\n\\\nCopy\\\n\\\n```bash\\\ncurl -X POST https://api.firecrawl.dev/v1/scrape \\\n-H 'Content-Type: application/json' \\\n-H 'Authorization: Bearer YOUR_API_KEY' \\\n-d '{\\\n\"url\": \"https://docs.firecrawl.dev/\",\\\n\"formats\": [\"json\"],\\\n\"jsonOptions\": {\\\n\"prompt\": \"Extract the company mission from the page.\"\\\n}\\\n}'\\\n\\\n```\\\n\\\nOutput:\\\n\\\nJSON\\\n\\\nCopy\\\n\\\n```json\\\n{\\\n\"success\": true,\\\n\"data\": {\\\n\"json\": {\\\n\"company_mission\": \"Train a secure AI on your technical resources that answers customer and employee questions so your team doesn't have to\",\\\n},\\\n\"metadata\": {\\\n\"title\": \"Mendable\",\\\n\"description\": \"Mendable allows you to easily build AI chat applications. Ingest, customize, then deploy with one line of code anywhere you want. Brought to you by SideGuide\",\\\n\"robots\": \"follow, index\",\\\n\"ogTitle\": \"Mendable\",\\\n\"ogDescription\": \"Mendable allows you to easily build AI chat applications. Ingest, customize, then deploy with one line of code anywhere you want. Brought to you by SideGuide\",\\\n\"ogUrl\": \"https://docs.firecrawl.dev/\",\\\n\"ogImage\": \"https://docs.firecrawl.dev/mendable_new_og1.png\",\\\n\"ogLocaleAlternate\": [],\\\n\"ogSiteName\": \"Mendable\",\\\n\"sourceURL\": \"https://docs.firecrawl.dev/\"\\\n},\\\n}\\\n}\\\n\\\n```\\\n\\",
      "metadata": {
        "title": "Welcome to V1 | Firecrawl",
        "url": "https://docs.firecrawl.dev/v1-welcome"
      }
    },
    {
      "id": "9ef4e7a6-6a20-4869-b316-ff2c6c01c5eb",
      "source": "firecrawl/docs/v1-welcome.md",
      "content": "[](https://docs.firecrawl.dev/v1-welcome#new-crawl-webhook) New Crawl Webhook\\\n\n\\\nYou can now pass a `webhook` parameter to the `/crawl` endpoint. This will send a POST request to the URL you specify when the crawl is started, updated and completed.\\\n\\\nThe webhook will now trigger for every page crawled and not just the whole result at the end.\\\n\\\ncURL\\\n\\\nCopy\\\n\\\n```bash\\\ncurl -X POST https://api.firecrawl.dev/v1/crawl \\\n-H 'Content-Type: application/json' \\\n-H 'Authorization: Bearer YOUR_API_KEY' \\\n-d '{\\\n\"url\": \"https://docs.firecrawl.dev\",\\\n\"limit\": 100,\\\n\"webhook\": \"https://example.com/webhook\"\\\n}'\\\n\\\n```\\\n\\",
      "metadata": {
        "title": "Welcome to V1 | Firecrawl",
        "url": "https://docs.firecrawl.dev/v1-welcome"
      }
    },
    {
      "id": "4a4ce3d0-be27-408c-83c1-4d9e10fe5c9f",
      "source": "firecrawl/docs/v1-welcome.md",
      "content": "[](https://docs.firecrawl.dev/v1-welcome#new-crawl-webhook) New Crawl Webhook\\ > [](https://docs.firecrawl.dev/v1-welcome#webhook-events) Webhook Events\\\n\n\\\nThere are now 4 types of events:\\\n\\\n- `crawl.started` - Triggered when the crawl is started.\\\n- `crawl.page` - Triggered for every page crawled.\\\n- `crawl.completed` - Triggered when the crawl is completed to let you know itâ€™s done.\\\n- `crawl.failed` - Triggered when the crawl fails.\\\n\\",
      "metadata": {
        "title": "Welcome to V1 | Firecrawl",
        "url": "https://docs.firecrawl.dev/v1-welcome"
      }
    },
    {
      "id": "76c23103-4c6d-4b9f-a454-36ba20d999f7",
      "source": "firecrawl/docs/v1-welcome.md",
      "content": "[](https://docs.firecrawl.dev/v1-welcome#new-crawl-webhook) New Crawl Webhook\\ > [](https://docs.firecrawl.dev/v1-welcome#webhook-response) Webhook Response\\\n\n\\\n- `success` - If the webhook was successful in crawling the page correctly.\\\n- `type` - The type of event that occurred.\\\n- `id` - The ID of the crawl.\\\n- `data` - The data that was scraped (Array). This will only be non empty on `crawl.page` and will contain 1 item if the page was scraped successfully. The response is the same as the `/scrape` endpoint.\\\n- `error` - If the webhook failed, this will contain the error message.\\\n\\",
      "metadata": {
        "title": "Welcome to V1 | Firecrawl",
        "url": "https://docs.firecrawl.dev/v1-welcome"
      }
    },
    {
      "id": "898dbd28-6301-4dab-a7dd-0a579d524774",
      "source": "firecrawl/docs/v1-welcome.md",
      "content": "[](https://docs.firecrawl.dev/v1-welcome#migrating-from-v0) Migrating from V0\\\n\n\\",
      "metadata": {
        "title": "Welcome to V1 | Firecrawl",
        "url": "https://docs.firecrawl.dev/v1-welcome"
      }
    },
    {
      "id": "5aa4b4de-e171-45f3-80cd-0c24038d9977",
      "source": "firecrawl/docs/v1-welcome.md",
      "content": "[](https://docs.firecrawl.dev/v1-welcome#scrape-endpoint) /scrape endpoint\\\n\n\\\nThe updated `/scrape` endpoint has been redesigned for enhanced reliability and ease of use. The structure of the new `/scrape` request body is as follows:\\\n\\\nCopy\\\n\\\n```json\\\n{\\\n\"url\": \"<string>\",\\\n\"formats\": [\"markdown\", \"html\", \"rawHtml\", \"links\", \"screenshot\", \"json\"],\\\n\"includeTags\": [\"<string>\"],\\\n\"excludeTags\": [\"<string>\"],\\\n\"headers\": { \"<key>\": \"<value>\" },\\\n\"waitFor\": 123,\\\n\"timeout\": 123\\\n}\\\n\\\n```\\\n\\",
      "metadata": {
        "title": "Welcome to V1 | Firecrawl",
        "url": "https://docs.firecrawl.dev/v1-welcome"
      }
    },
    {
      "id": "83daf39f-efad-43eb-9cc6-e3354fe71601",
      "source": "firecrawl/docs/v1-welcome.md",
      "content": "[](https://docs.firecrawl.dev/v1-welcome#scrape-endpoint) /scrape endpoint\\ > [](https://docs.firecrawl.dev/v1-welcome#formats) Formats\\\n\n\\\nYou can now choose what formats you want your output in. You can specify multiple output formats. Supported formats are:\\\n\\\n- Markdown (markdown)\\\n- HTML (html)\\\n- Raw HTML (rawHtml) (with no modifications)\\\n- Screenshot (screenshot or screenshot@fullPage)\\\n- Links (links)\\\n- JSON (json)\\\n\\\nBy default, the output will be include only the markdown format.\\\n\\",
      "metadata": {
        "title": "Welcome to V1 | Firecrawl",
        "url": "https://docs.firecrawl.dev/v1-welcome"
      }
    },
    {
      "id": "e0872b53-e7d2-4df7-887e-97cfc249cba3",
      "source": "firecrawl/docs/v1-welcome.md",
      "content": "[](https://docs.firecrawl.dev/v1-welcome#scrape-endpoint) /scrape endpoint\\ > [](https://docs.firecrawl.dev/v1-welcome#details-on-the-new-request-body) Details on the new request body\\\n\n\\\nThe table below outlines the changes to the request body parameters for the `/scrape` endpoint in V1.\\\n\\\n| Parameter | Change | Description |\\\n| --- | --- | --- |\\\n| `onlyIncludeTags` | Moved and Renamed | Moved to root level. And renamed to `includeTags`. |\\\n| `removeTags` | Moved and Renamed | Moved to root level. And renamed to `excludeTags`. |\\\n| `onlyMainContent` | Moved | Moved to root level. `true` by default. |\\\n| `waitFor` | Moved | Moved to root level. |\\\n| `headers` | Moved | Moved to root level. |\\\n| `parsePDF` | Moved | Moved to root level. |\\\n| `extractorOptions` | No Change | |\\\n| `timeout` | No Change | |\\\n| `pageOptions` | Removed | No need for `pageOptions` parameter. The scrape options were moved to root level. |\\\n| `replaceAllPathsWithAbsolutePaths` | Removed | `replaceAllPathsWithAbsolutePaths` is not needed anymore. Every path is now default to absolute path. |\\\n| `includeHtml` | Removed | add `\"html\"` to `formats` instead. |\\\n| `includeRawHtml` | Removed | add `\"rawHtml\"` to `formats` instead. |\\\n| `screenshot` | Removed | add `\"screenshot\"` to `formats` instead. |\\\n| `fullPageScreenshot` | Removed | add `\"screenshot@fullPage\"` to `formats` instead. |\\\n| `extractorOptions` | Removed | Use `\"extract\"` format instead with `extract` object. |\\\n\\\nThe new `extract` format is described in the [llm-extract](https://docs.firecrawl.dev/features/extract) section.\\\n\\",
      "metadata": {
        "title": "Welcome to V1 | Firecrawl",
        "url": "https://docs.firecrawl.dev/v1-welcome"
      }
    },
    {
      "id": "7214ee40-4ff1-4b4b-9197-773b77fd7ecc",
      "source": "firecrawl/docs/v1-welcome.md",
      "content": "[](https://docs.firecrawl.dev/v1-welcome#crawl-endpoint) /crawl endpoint\\\n\n\\\nWeâ€™ve also updated the `/crawl` endpoint on `v1`. Check out the improved body request below:\\\n\\\nCopy\\\n\\\n```json\\\n{\\\n\"url\": \"<string>\",\\\n\"excludePaths\": [\"<string>\"],\\\n\"includePaths\": [\"<string>\"],\\\n\"maxDepth\": 2,\\\n\"ignoreSitemap\": true,\\\n\"limit\": 10,\\\n\"allowBackwardLinks\": true,\\\n\"allowExternalLinks\": true,\\\n\"scrapeOptions\": {\\\n// same options as in /scrape\\\n\"formats\": [\"markdown\", \"html\", \"rawHtml\", \"screenshot\", \"links\"],\\\n\"headers\": { \"<key>\": \"<value>\" },\\\n\"includeTags\": [\"<string>\"],\\\n\"excludeTags\": [\"<string>\"],\\\n\"onlyMainContent\": true,\\\n\"waitFor\": 123\\\n}\\\n}\\\n\\\n```\\\n\\",
      "metadata": {
        "title": "Welcome to V1 | Firecrawl",
        "url": "https://docs.firecrawl.dev/v1-welcome"
      }
    },
    {
      "id": "aaac9de3-af21-44f3-bb87-6641de929af3",
      "source": "firecrawl/docs/v1-welcome.md",
      "content": "[](https://docs.firecrawl.dev/v1-welcome#crawl-endpoint) /crawl endpoint\\ > [](https://docs.firecrawl.dev/v1-welcome#details-on-the-new-request-body-2) Details on the new request body\\\n\n\\\nThe table below outlines the changes to the request body parameters for the `/crawl` endpoint in V1.\\\n\\\n| Parameter | Change | Description |\\\n| --- | --- | --- |\\\n| `pageOptions` | Renamed | Renamed to `scrapeOptions`. |\\\n| `includes` | Moved and Renamed | Moved to root level. Renamed to `includePaths`. |\\\n| `excludes` | Moved and Renamed | Moved to root level. Renamed to `excludePaths`. |\\\n| `allowBackwardCrawling` | Moved and Renamed | Moved to root level. Renamed to `allowBackwardLinks`. |\\\n| `allowExternalLinks` | Moved | Moved to root level. |\\\n| `maxDepth` | Moved | Moved to root level. |\\\n| `ignoreSitemap` | Moved | Moved to root level. |\\\n| `limit` | Moved | Moved to root level. |\\\n| `crawlerOptions` | Removed | No need for `crawlerOptions` parameter. The crawl options were moved to root level. |\\\n| `timeout` | Removed | Use `timeout` in `scrapeOptions` instead. |\\\n\\\n[Suggest edits](https://github.com/hellofirecrawl/docs/edit/main/v1-welcome.mdx) [Raise issue](https://github.com/hellofirecrawl/docs/issues/new?title=Issue%20on%20docs&body=Path:%20/v1-welcome)\\\n\\\n[Launch Week II (New)](https://docs.firecrawl.dev/launch-week) [Rate Limits](https://docs.firecrawl.dev/rate-limits)\\\n\\\nOn this page\\\n\\\n- [Scrape Formats](https://docs.firecrawl.dev/v1-welcome#scrape-formats)\\\n- [Response](https://docs.firecrawl.dev/v1-welcome#response)\\\n- [Introducing /map (Alpha)](https://docs.firecrawl.dev/v1-welcome#introducing-map-alpha)\\\n- [Usage](https://docs.firecrawl.dev/v1-welcome#usage)\\\n- [Response](https://docs.firecrawl.dev/v1-welcome#response-2)\\\n- [WebSockets](https://docs.firecrawl.dev/v1-welcome#websockets)\\\n- [Extract format](https://docs.firecrawl.dev/v1-welcome#extract-format)\\\n- [Extracting without schema (New)](https://docs.firecrawl.dev/v1-welcome#extracting-without-schema-new)\\\n- [New Crawl Webhook](https://docs.firecrawl.dev/v1-welcome#new-crawl-webhook)\\\n- [Webhook Events](https://docs.firecrawl.dev/v1-welcome#webhook-events)\\\n- [Webhook Response](https://docs.firecrawl.dev/v1-welcome#webhook-response)\\\n- [Migrating from V0](https://docs.firecrawl.dev/v1-welcome#migrating-from-v0)\\\n- [/scrape endpoint](https://docs.firecrawl.dev/v1-welcome#scrape-endpoint)\\\n- [Formats](https://docs.firecrawl.dev/v1-welcome#formats)\\\n- [Details on the new request body](https://docs.firecrawl.dev/v1-welcome#details-on-the-new-request-body)\\\n- [/crawl endpoint](https://docs.firecrawl.dev/v1-welcome#crawl-endpoint)\\\n- [Details on the new request body](https://docs.firecrawl.dev/v1-welcome#details-on-the-new-request-body-2)",
      "metadata": {
        "title": "Welcome to V1 | Firecrawl",
        "url": "https://docs.firecrawl.dev/v1-welcome"
      }
    },
    {
      "id": "666f0f38-c09e-4a15-bd48-c6ae4897d130",
      "source": "firecrawl/docs/features-map.md",
      "content": "---\ntitle: Map | Firecrawl\nurl: https://docs.firecrawl.dev/features/map\n---  \n[Firecrawl Docs home page![light logo](https://mintlify.s3.us-west-1.amazonaws.com/firecrawl/logo/light.svg)![dark logo](https://mintlify.s3.us-west-1.amazonaws.com/firecrawl/logo/dark.svg)](https://firecrawl.dev/)  \nv1  \nSearch or ask...  \nCtrl K  \nSearch...  \nNavigation  \nFeatures  \nMap  \n[Documentation](https://docs.firecrawl.dev/introduction) [SDKs](https://docs.firecrawl.dev/sdks/overview) [Learn](https://www.firecrawl.dev/blog/category/tutorials) [API Reference](https://docs.firecrawl.dev/api-reference/introduction)",
      "metadata": {
        "title": "Map | Firecrawl",
        "url": "https://docs.firecrawl.dev/features/map"
      }
    },
    {
      "id": "4acebb5e-0a92-4415-b9ff-fe179b7543f9",
      "source": "firecrawl/docs/features-map.md",
      "content": "[](https://docs.firecrawl.dev/features/map#introducing-map) Introducing /map\n\nThe easiest way to go from a single url to a map of the entire website. This is extremely useful for:  \n- When you need to prompt the end-user to choose which links to scrape\n- Need to quickly know the links on a website\n- Need to scrape pages of a website that are related to a specific topic (use the `search` parameter)\n- Only need to scrape specific pages of a website",
      "metadata": {
        "title": "Map | Firecrawl",
        "url": "https://docs.firecrawl.dev/features/map"
      }
    },
    {
      "id": "3c852aca-fc00-4f1f-8e22-87fec622d7e8",
      "source": "firecrawl/docs/features-map.md",
      "content": "[](https://docs.firecrawl.dev/features/map#alpha-considerations) Alpha Considerations\n\nThis endpoint prioritizes speed, so it may not capture all website links. We are working on improvements. Feedback and suggestions are very welcome.",
      "metadata": {
        "title": "Map | Firecrawl",
        "url": "https://docs.firecrawl.dev/features/map"
      }
    },
    {
      "id": "cef2e52b-acc4-4da9-bdf1-ed112a78294c",
      "source": "firecrawl/docs/features-map.md",
      "content": "[](https://docs.firecrawl.dev/features/map#mapping) Mapping > [](https://docs.firecrawl.dev/features/map#map-endpoint) /map endpoint\n\nUsed to map a URL and get urls of the website. This returns most links present on the website.",
      "metadata": {
        "title": "Map | Firecrawl",
        "url": "https://docs.firecrawl.dev/features/map"
      }
    },
    {
      "id": "cd3b9ab7-4ba8-43a5-b8fe-f42bf61e98d5",
      "source": "firecrawl/docs/features-map.md",
      "content": "[](https://docs.firecrawl.dev/features/map#mapping) Mapping > [](https://docs.firecrawl.dev/features/map#installation) Installation\n\nPython  \nNode  \nGo  \nRust  \nCopy  \n```bash\npip install firecrawl-py\n\n```",
      "metadata": {
        "title": "Map | Firecrawl",
        "url": "https://docs.firecrawl.dev/features/map"
      }
    },
    {
      "id": "55c33437-4f6f-4309-a3b6-d680870f6150",
      "source": "firecrawl/docs/features-map.md",
      "content": "[](https://docs.firecrawl.dev/features/map#mapping) Mapping > [](https://docs.firecrawl.dev/features/map#usage) Usage\n\nPython  \nNode  \nGo  \nRust  \ncURL  \nCopy  \n```python\nfrom firecrawl import FirecrawlApp\n\napp = FirecrawlApp(api_key=\"fc-YOUR_API_KEY\")\n\n# Map a website:\nmap_result = app.map_url('https://firecrawl.dev')\nprint(map_result)\n\n```",
      "metadata": {
        "title": "Map | Firecrawl",
        "url": "https://docs.firecrawl.dev/features/map"
      }
    },
    {
      "id": "6ed07cac-54d5-4021-9395-c5c96920749e",
      "source": "firecrawl/docs/features-map.md",
      "content": "[](https://docs.firecrawl.dev/features/map#mapping) Mapping > [](https://docs.firecrawl.dev/features/map#response) Response\n\nSDKs will return the data object directly. cURL will return the payload exactly as shown below.  \nCopy  \n```json\n{\n\"status\": \"success\",\n\"links\": [\\\n\"https://firecrawl.dev\",\\\n\"https://www.firecrawl.dev/pricing\",\\\n\"https://www.firecrawl.dev/blog\",\\\n\"https://www.firecrawl.dev/playground\",\\\n\"https://www.firecrawl.dev/smart-crawl\",\\\n...\\\n]\n}\n\n```",
      "metadata": {
        "title": "Map | Firecrawl",
        "url": "https://docs.firecrawl.dev/features/map"
      }
    },
    {
      "id": "cfe9ffc1-eacf-42d8-95ae-e089d5f34bb9",
      "source": "firecrawl/docs/features-map.md",
      "content": "[](https://docs.firecrawl.dev/features/map#mapping) Mapping > [](https://docs.firecrawl.dev/features/map#response) Response > [](https://docs.firecrawl.dev/features/map#map-with-search) Map with search\n\nMap with `search` param allows you to search for specific urls inside a website.  \ncURL  \nCopy  \n```bash\ncurl -X POST https://api.firecrawl.dev/v1/map \\\n-H 'Content-Type: application/json' \\\n-H 'Authorization: Bearer YOUR_API_KEY' \\\n-d '{\n\"url\": \"https://firecrawl.dev\",\n\"search\": \"docs\"\n}'\n\n```  \nResponse will be an ordered list from the most relevant to the least relevant.  \nCopy  \n```json\n{\n\"status\": \"success\",\n\"links\": [\\\n\"https://docs.firecrawl.dev\",\\\n\"https://docs.firecrawl.dev/sdks/python\",\\\n\"https://docs.firecrawl.dev/learn/rag-llama3\",\\\n]\n}\n\n```  \n[Suggest edits](https://github.com/hellofirecrawl/docs/edit/main/features/map.mdx) [Raise issue](https://github.com/hellofirecrawl/docs/issues/new?title=Issue%20on%20docs&body=Path:%20/features/map)  \n[Crawl](https://docs.firecrawl.dev/features/crawl) [Extract (New)](https://docs.firecrawl.dev/features/extract)  \nOn this page  \n- [Introducing /map](https://docs.firecrawl.dev/features/map#introducing-map)\n- [Alpha Considerations](https://docs.firecrawl.dev/features/map#alpha-considerations)\n- [Mapping](https://docs.firecrawl.dev/features/map#mapping)\n- [/map endpoint](https://docs.firecrawl.dev/features/map#map-endpoint)\n- [Installation](https://docs.firecrawl.dev/features/map#installation)\n- [Usage](https://docs.firecrawl.dev/features/map#usage)\n- [Response](https://docs.firecrawl.dev/features/map#response)\n- [Map with search](https://docs.firecrawl.dev/features/map#map-with-search)",
      "metadata": {
        "title": "Map | Firecrawl",
        "url": "https://docs.firecrawl.dev/features/map"
      }
    },
    {
      "id": "0659871a-f651-415e-b0ec-b5b5abe1d671",
      "source": "firecrawl/docs/api-reference-endpoint-batch-scrape-get.md",
      "content": "---\ntitle: Firecrawl Docs\nurl: https://docs.firecrawl.dev/api-reference/endpoint/batch-scrape-get\n---  \n[Firecrawl Docs home page![light logo](https://mintlify.s3.us-west-1.amazonaws.com/firecrawl/logo/light.svg)![dark logo](https://mintlify.s3.us-west-1.amazonaws.com/firecrawl/logo/dark.svg)](https://firecrawl.dev/)  \nv1  \nSearch or ask...  \nCtrl K  \nSearch...  \nNavigation  \nScrape Endpoints  \nGet Batch Scrape Status  \n[Documentation](https://docs.firecrawl.dev/introduction) [SDKs](https://docs.firecrawl.dev/sdks/overview) [Learn](https://www.firecrawl.dev/blog/category/tutorials) [API Reference](https://docs.firecrawl.dev/api-reference/introduction)  \nGET  \n/  \nbatch  \n/  \nscrape  \n/  \n{id}  \nTry it  \ncURL  \nPython  \nJavaScript  \nPHP  \nGo  \nJava  \nCopy  \n```\ncurl --request GET \\\n--url https://api.firecrawl.dev/v1/batch/scrape/{id} \\\n--header 'Authorization: Bearer <token>'\n```  \n200  \n402  \n429  \n500  \nCopy  \n```\n{\n\"status\": \"<string>\",\n\"total\": 123,\n\"completed\": 123,\n\"creditsUsed\": 123,\n\"expiresAt\": \"2023-11-07T05:31:56Z\",\n\"next\": \"<string>\",\n\"data\": [\\\n{\\\n\"markdown\": \"<string>\",\\\n\"html\": \"<string>\",\\\n\"rawHtml\": \"<string>\",\\\n\"links\": [\\\n\"<string>\"\\\n],\\\n\"screenshot\": \"<string>\",\\\n\"metadata\": {\\\n\"title\": \"<string>\",\\\n\"description\": \"<string>\",\\\n\"language\": \"<string>\",\\\n\"sourceURL\": \"<string>\",\\\n\"<any other metadata> \": \"<string>\",\\\n\"statusCode\": 123,\\\n\"error\": \"<string>\"\\\n}\\\n}\\\n]\n}\n```",
      "metadata": {
        "title": "Firecrawl Docs",
        "url": "https://docs.firecrawl.dev/api-reference/endpoint/batch-scrape-get"
      }
    },
    {
      "id": "212fc15a-0a27-42f0-b576-2d79c991d57d",
      "source": "firecrawl/docs/api-reference-endpoint-batch-scrape-get.md",
      "content": "Authorizations\n\n[](https://docs.firecrawl.dev/api-reference/endpoint/batch-scrape-get#authorization-authorization)  \nAuthorization  \nstring  \nheader  \nrequired  \nBearer authentication header of the form `Bearer <token>`, where `<token>` is your auth token.",
      "metadata": {
        "title": "Firecrawl Docs",
        "url": "https://docs.firecrawl.dev/api-reference/endpoint/batch-scrape-get"
      }
    },
    {
      "id": "4410315e-ec3e-4a4d-a440-66f58330b38c",
      "source": "firecrawl/docs/api-reference-endpoint-batch-scrape-get.md",
      "content": "Path Parameters\n\n[](https://docs.firecrawl.dev/api-reference/endpoint/batch-scrape-get#parameter-id)  \nid  \nstring  \nrequired  \nThe ID of the batch scrape job",
      "metadata": {
        "title": "Firecrawl Docs",
        "url": "https://docs.firecrawl.dev/api-reference/endpoint/batch-scrape-get"
      }
    },
    {
      "id": "90c965a4-fae9-479e-9e68-f23b1b2c6761",
      "source": "firecrawl/docs/api-reference-endpoint-batch-scrape-get.md",
      "content": "Response\n\n200 - application/json  \n[](https://docs.firecrawl.dev/api-reference/endpoint/batch-scrape-get#response-status)  \nstatus  \nstring  \nThe current status of the batch scrape. Can be `scraping`, `completed`, or `failed`.  \n[](https://docs.firecrawl.dev/api-reference/endpoint/batch-scrape-get#response-total)  \ntotal  \ninteger  \nThe total number of pages that were attempted to be scraped.  \n[](https://docs.firecrawl.dev/api-reference/endpoint/batch-scrape-get#response-completed)  \ncompleted  \ninteger  \nThe number of pages that have been successfully scraped.  \n[](https://docs.firecrawl.dev/api-reference/endpoint/batch-scrape-get#response-credits-used)  \ncreditsUsed  \ninteger  \nThe number of credits used for the batch scrape.  \n[](https://docs.firecrawl.dev/api-reference/endpoint/batch-scrape-get#response-expires-at)  \nexpiresAt  \nstring  \nThe date and time when the batch scrape will expire.  \n[](https://docs.firecrawl.dev/api-reference/endpoint/batch-scrape-get#response-next)  \nnext  \nstring | null  \nThe URL to retrieve the next 10MB of data. Returned if the batch scrape is not completed or if the response is larger than 10MB.  \n[](https://docs.firecrawl.dev/api-reference/endpoint/batch-scrape-get#response-data)  \ndata  \nobject[]  \nThe data of the batch scrape.  \nShowchild attributes  \n[](https://docs.firecrawl.dev/api-reference/endpoint/batch-scrape-get#response-data-markdown)  \ndata.markdown  \nstring  \n[](https://docs.firecrawl.dev/api-reference/endpoint/batch-scrape-get#response-data-html)  \ndata.html  \nstring | null  \nHTML version of the content on page if `includeHtml` is true  \n[](https://docs.firecrawl.dev/api-reference/endpoint/batch-scrape-get#response-data-raw-html)  \ndata.rawHtml  \nstring | null  \nRaw HTML content of the page if `includeRawHtml` is true  \n[](https://docs.firecrawl.dev/api-reference/endpoint/batch-scrape-get#response-data-links)  \ndata.links  \nstring[]  \nList of links on the page if `includeLinks` is true  \n[](https://docs.firecrawl.dev/api-reference/endpoint/batch-scrape-get#response-data-screenshot)  \ndata.screenshot  \nstring | null  \nScreenshot of the page if `includeScreenshot` is true  \n[](https://docs.firecrawl.dev/api-reference/endpoint/batch-scrape-get#response-data-metadata)  \ndata.metadata  \nobject  \nShowchild attributes  \n[](https://docs.firecrawl.dev/api-reference/endpoint/batch-scrape-get#response-data-metadata-title)  \ndata.metadata.title  \nstring  \n[](https://docs.firecrawl.dev/api-reference/endpoint/batch-scrape-get#response-data-metadata-description)  \ndata.metadata.description  \nstring  \n[](https://docs.firecrawl.dev/api-reference/endpoint/batch-scrape-get#response-data-metadata-language)  \ndata.metadata.language  \nstring | null  \n[](https://docs.firecrawl.dev/api-reference/endpoint/batch-scrape-get#response-data-metadata-source-url)  \ndata.metadata.sourceURL  \nstring  \n[](https://docs.firecrawl.dev/api-reference/endpoint/batch-scrape-get#response-data-metadata-any-other-metadata)  \ndata.metadata.<any other metadata>  \nstring  \n[](https://docs.firecrawl.dev/api-reference/endpoint/batch-scrape-get#response-data-metadata-status-code)  \ndata.metadata.statusCode  \ninteger  \nThe status code of the page  \n[](https://docs.firecrawl.dev/api-reference/endpoint/batch-scrape-get#response-data-metadata-error)  \ndata.metadata.error  \nstring | null  \nThe error message of the page  \n[Suggest edits](https://github.com/hellofirecrawl/docs/edit/main/api-reference/endpoint/batch-scrape-get.mdx) [Raise issue](https://github.com/hellofirecrawl/docs/issues/new?title=Issue%20on%20docs&body=Path:%20/api-reference/endpoint/batch-scrape-get)  \n[Batch Scrape](https://docs.firecrawl.dev/api-reference/endpoint/batch-scrape) [Get Batch Scrape Errors](https://docs.firecrawl.dev/api-reference/endpoint/batch-scrape-get-errors)  \ncURL  \nPython  \nJavaScript  \nPHP  \nGo  \nJava  \nCopy  \n```\ncurl --request GET \\\n--url https://api.firecrawl.dev/v1/batch/scrape/{id} \\\n--header 'Authorization: Bearer <token>'\n```  \n200  \n402  \n429  \n500  \nCopy  \n```\n{\n\"status\": \"<string>\",\n\"total\": 123,\n\"completed\": 123,\n\"creditsUsed\": 123,\n\"expiresAt\": \"2023-11-07T05:31:56Z\",\n\"next\": \"<string>\",\n\"data\": [\\\n{\\\n\"markdown\": \"<string>\",\\\n\"html\": \"<string>\",\\\n\"rawHtml\": \"<string>\",\\\n\"links\": [\\\n\"<string>\"\\\n],\\\n\"screenshot\": \"<string>\",\\\n\"metadata\": {\\\n\"title\": \"<string>\",\\\n\"description\": \"<string>\",\\\n\"language\": \"<string>\",\\\n\"sourceURL\": \"<string>\",\\\n\"<any other metadata> \": \"<string>\",\\\n\"statusCode\": 123,\\\n\"error\": \"<string>\"\\\n}\\\n}\\\n]\n}\n```",
      "metadata": {
        "title": "Firecrawl Docs",
        "url": "https://docs.firecrawl.dev/api-reference/endpoint/batch-scrape-get"
      }
    },
    {
      "id": "b5b41679-1a57-4d89-b832-b06fd220a1d2",
      "source": "firecrawl/docs/api-reference-introduction.md",
      "content": "---\ntitle: Firecrawl Docs\nurl: https://docs.firecrawl.dev/api-reference/introduction\n---  \n[Firecrawl Docs home page![light logo](https://mintlify.s3.us-west-1.amazonaws.com/firecrawl/logo/light.svg)![dark logo](https://mintlify.s3.us-west-1.amazonaws.com/firecrawl/logo/dark.svg)](https://firecrawl.dev/)  \nv1  \nSearch or ask...  \nCtrl K  \nSearch...  \nNavigation  \nUsing the API  \nIntroduction  \n[Documentation](https://docs.firecrawl.dev/introduction) [SDKs](https://docs.firecrawl.dev/sdks/overview) [Learn](https://www.firecrawl.dev/blog/category/tutorials) [API Reference](https://docs.firecrawl.dev/api-reference/introduction)",
      "metadata": {
        "title": "Firecrawl Docs",
        "url": "https://docs.firecrawl.dev/api-reference/introduction"
      }
    },
    {
      "id": "3642f02c-1af6-4fe3-a85e-86490045e82c",
      "source": "firecrawl/docs/api-reference-introduction.md",
      "content": "[](https://docs.firecrawl.dev/api-reference/introduction#base-url) Base URL\n\nAll requests contain the following base URL:  \nCopy  \n```bash\nhttps://api.firecrawl.dev\n\n```",
      "metadata": {
        "title": "Firecrawl Docs",
        "url": "https://docs.firecrawl.dev/api-reference/introduction"
      }
    },
    {
      "id": "4732200c-ee79-4cd8-85f2-20c8df3a6627",
      "source": "firecrawl/docs/api-reference-introduction.md",
      "content": "[](https://docs.firecrawl.dev/api-reference/introduction#authentication) Authentication\n\nFor authentication, itâ€™s required to include an Authorization header. The header should contain `Bearer fc-123456789`, where `fc-123456789` represents your API Key.  \nCopy  \n```bash\nAuthorization: Bearer fc-123456789\n\n```",
      "metadata": {
        "title": "Firecrawl Docs",
        "url": "https://docs.firecrawl.dev/api-reference/introduction"
      }
    },
    {
      "id": "d0697e0c-42a5-4465-a4ba-75367c3206c9",
      "source": "firecrawl/docs/api-reference-introduction.md",
      "content": "[](https://docs.firecrawl.dev/api-reference/introduction#response-codes) Response codes\n\nFirecrawl employs conventional HTTP status codes to signify the outcome of your requests.  \nTypically, 2xx HTTP status codes denote success, 4xx codes represent failures related to the user, and 5xx codes signal infrastructure problems.  \n| Status | Description |\n| --- | --- |\n| 200 | Request was successful. |\n| 400 | Verify the correctness of the parameters. |\n| 401 | The API key was not provided. |\n| 402 | Payment required |\n| 404 | The requested resource could not be located. |\n| 429 | The rate limit has been surpassed. |\n| 5xx | Signifies a server error with Firecrawl. |  \nRefer to the Error Codes section for a detailed explanation of all potential API errors.",
      "metadata": {
        "title": "Firecrawl Docs",
        "url": "https://docs.firecrawl.dev/api-reference/introduction"
      }
    },
    {
      "id": "52e2bbff-902c-4aa8-b7a1-55167ac9596c",
      "source": "firecrawl/docs/api-reference-introduction.md",
      "content": "[](https://docs.firecrawl.dev/api-reference/introduction#rate-limit) Rate limit\n\nThe Firecrawl API has a rate limit to ensure the stability and reliability of the service. The rate limit is applied to all endpoints and is based on the number of requests made within a specific time frame.  \nWhen you exceed the rate limit, you will receive a 429 response code.  \n[Suggest edits](https://github.com/hellofirecrawl/docs/edit/main/api-reference/introduction.mdx) [Raise issue](https://github.com/hellofirecrawl/docs/issues/new?title=Issue%20on%20docs&body=Path:%20/api-reference/introduction)  \n[Scrape](https://docs.firecrawl.dev/api-reference/endpoint/scrape)  \nOn this page  \n- [Base URL](https://docs.firecrawl.dev/api-reference/introduction#base-url)\n- [Authentication](https://docs.firecrawl.dev/api-reference/introduction#authentication)\n- [Response codes](https://docs.firecrawl.dev/api-reference/introduction#response-codes)\n- [Rate limit](https://docs.firecrawl.dev/api-reference/introduction#rate-limit)",
      "metadata": {
        "title": "Firecrawl Docs",
        "url": "https://docs.firecrawl.dev/api-reference/introduction"
      }
    },
    {
      "id": "8beb02b0-ee8f-4688-aa58-f05032a58b04",
      "source": "firecrawl/docs/integrations-dify.md",
      "content": "---\ntitle: Dify | Firecrawl\nurl: https://docs.firecrawl.dev/integrations/dify\n---  \n[Firecrawl Docs home page![light logo](https://mintlify.s3.us-west-1.amazonaws.com/firecrawl/logo/light.svg)![dark logo](https://mintlify.s3.us-west-1.amazonaws.com/firecrawl/logo/dark.svg)](https://firecrawl.dev/)  \nv1  \nSearch or ask...  \nCtrl K  \nSearch...  \nNavigation  \nIntegrations  \nDify  \n[Documentation](https://docs.firecrawl.dev/introduction) [SDKs](https://docs.firecrawl.dev/sdks/overview) [Learn](https://www.firecrawl.dev/blog/category/tutorials) [API Reference](https://docs.firecrawl.dev/api-reference/introduction)",
      "metadata": {
        "title": "Dify | Firecrawl",
        "url": "https://docs.firecrawl.dev/integrations/dify"
      }
    },
    {
      "id": "686ae263-775f-43b2-81a1-e48390f9c9e3",
      "source": "firecrawl/docs/integrations-dify.md",
      "content": "[](https://docs.firecrawl.dev/integrations/dify#sync-data-from-websites-for-dify-workflows) Sync Data from Websites for Dify workflows\n\nFirecrawl can be used inside of [Dify the LLM workflow builder](https://cloud.dify.ai/). This page introduces how to scrape data from a web page, parse it into Markdown, and import it into the Dify knowledge base using their Firecrawl integration.",
      "metadata": {
        "title": "Dify | Firecrawl",
        "url": "https://docs.firecrawl.dev/integrations/dify"
      }
    },
    {
      "id": "73be723d-c973-4bd8-a522-77b865c59340",
      "source": "firecrawl/docs/integrations-dify.md",
      "content": "[](https://docs.firecrawl.dev/integrations/dify#sync-data-from-websites-for-dify-workflows) Sync Data from Websites for Dify workflows > [](https://docs.firecrawl.dev/integrations/dify#configuring-firecrawl) Configuring Firecrawl\n\nFirst, you need to configure Firecrawl credentials in the Data Source section of the Settings page.  \n![Configure Firecrawl key](https://mintlify.s3.us-west-1.amazonaws.com/firecrawl/images/fc_dify_config.avif)  \nLog in to your Firecrawl account and get your API Key, and then enter and save it in Dify.  \n![Save Firecrawl key](https://mintlify.s3.us-west-1.amazonaws.com/firecrawl/images/fc_dify_savekey.png)",
      "metadata": {
        "title": "Dify | Firecrawl",
        "url": "https://docs.firecrawl.dev/integrations/dify"
      }
    },
    {
      "id": "0b7a2fbc-c56c-4855-8903-827b489fcb9a",
      "source": "firecrawl/docs/integrations-dify.md",
      "content": "[](https://docs.firecrawl.dev/integrations/dify#sync-data-from-websites-for-dify-workflows) Sync Data from Websites for Dify workflows > [](https://docs.firecrawl.dev/integrations/dify#scrape-target-webpage) Scrape target webpage\n\nNow comes the fun part, scraping and crawling. On the knowledge base creation page, select Sync from website and enter the URL to be scraped.  \n![Scraping setup](https://mintlify.s3.us-west-1.amazonaws.com/firecrawl/images/fc_dify_webscrape.webp)  \nThe configuration options include: Whether to crawl sub-pages, Page crawling limit, Page scraping max depth, Excluded paths, Include only paths, and Content extraction scope. After completing the configuration, click Run to preview the parsed pages.  \n![Set Firecrawl configuration](https://mintlify.s3.us-west-1.amazonaws.com/firecrawl/images/fc_dify_fcoptions.webp)",
      "metadata": {
        "title": "Dify | Firecrawl",
        "url": "https://docs.firecrawl.dev/integrations/dify"
      }
    },
    {
      "id": "b445bfbe-54d1-4685-bb18-5564ec2d7291",
      "source": "firecrawl/docs/integrations-dify.md",
      "content": "[](https://docs.firecrawl.dev/integrations/dify#sync-data-from-websites-for-dify-workflows) Sync Data from Websites for Dify workflows > [](https://docs.firecrawl.dev/integrations/dify#review-import-results) Review import results\n\nAfter importing the parsed text from the webpage, it is stored in the knowledge base documents. View the import results and click Add URL to continue importing new web pages.  \n![See results of the Firecrawl scrape](https://mintlify.s3.us-west-1.amazonaws.com/firecrawl/images/fc_dify_results.webp)  \n[Suggest edits](https://github.com/hellofirecrawl/docs/edit/main/integrations/dify.mdx) [Raise issue](https://github.com/hellofirecrawl/docs/issues/new?title=Issue%20on%20docs&body=Path:%20/integrations/dify)  \n[CrewAI](https://docs.firecrawl.dev/integrations/crewai) [Flowise](https://docs.firecrawl.dev/integrations/flowise)  \nOn this page  \n- [Sync Data from Websites for Dify workflows](https://docs.firecrawl.dev/integrations/dify#sync-data-from-websites-for-dify-workflows)\n- [Configuring Firecrawl](https://docs.firecrawl.dev/integrations/dify#configuring-firecrawl)\n- [Scrape target webpage](https://docs.firecrawl.dev/integrations/dify#scrape-target-webpage)\n- [Review import results](https://docs.firecrawl.dev/integrations/dify#review-import-results)  \n![Save Firecrawl key](https://docs.firecrawl.dev/integrations/dify)  \n![Configure Firecrawl key](https://docs.firecrawl.dev/integrations/dify)",
      "metadata": {
        "title": "Dify | Firecrawl",
        "url": "https://docs.firecrawl.dev/integrations/dify"
      }
    },
    {
      "id": "01d9a378-61a5-4143-be80-b635528e7a11",
      "source": "firecrawl/docs/rate-limits.md",
      "content": "---\ntitle: Rate Limits | Firecrawl\nurl: https://docs.firecrawl.dev/rate-limits\n---  \n[Firecrawl Docs home page![light logo](https://mintlify.s3.us-west-1.amazonaws.com/firecrawl/logo/light.svg)![dark logo](https://mintlify.s3.us-west-1.amazonaws.com/firecrawl/logo/dark.svg)](https://firecrawl.dev/)  \nv1  \nSearch or ask...  \nCtrl K  \nSearch...  \nNavigation  \nGet Started  \nRate Limits  \n[Documentation](https://docs.firecrawl.dev/introduction) [SDKs](https://docs.firecrawl.dev/sdks/overview) [Learn](https://www.firecrawl.dev/blog/category/tutorials) [API Reference](https://docs.firecrawl.dev/api-reference/introduction)",
      "metadata": {
        "title": "Rate Limits | Firecrawl",
        "url": "https://docs.firecrawl.dev/rate-limits"
      }
    },
    {
      "id": "b9f28293-90e1-433d-9c24-318c8027a3ed",
      "source": "firecrawl/docs/rate-limits.md",
      "content": "[](https://docs.firecrawl.dev/rate-limits#standard-plans) Standard Plans\n\n| Plan | /scrape (requests/min) | /crawl (requests/min) | /search (requests/min) |\n| --- | --- | --- | --- |\n| Free | 10 | 1 | 5 |\n| Hobby | 20 | 3 | 10 |\n| Standard | 100 | 10 | 50 |\n| Growth | 1000 | 50 | 500 |  \n| | /crawl/status (requests/min) |\n| --- | --- |\n| Default | 150 |  \nThese rate limits are enforced to ensure fair usage and availability of the API for all users. If you require higher limits, please contact us at [hello@firecrawl.com](mailto:hello@firecrawl.com) to discuss custom plans.",
      "metadata": {
        "title": "Rate Limits | Firecrawl",
        "url": "https://docs.firecrawl.dev/rate-limits"
      }
    },
    {
      "id": "9ea49a41-4da2-4fa0-849e-cba3afae5216",
      "source": "firecrawl/docs/rate-limits.md",
      "content": "[](https://docs.firecrawl.dev/rate-limits#standard-plans) Standard Plans > [](https://docs.firecrawl.dev/rate-limits#batch-endpoints) Batch Endpoints\n\nBatch endpoints follow the /crawl rate limit.",
      "metadata": {
        "title": "Rate Limits | Firecrawl",
        "url": "https://docs.firecrawl.dev/rate-limits"
      }
    },
    {
      "id": "0b644e92-88f7-4ab2-92fc-3d0c804e0592",
      "source": "firecrawl/docs/rate-limits.md",
      "content": "[](https://docs.firecrawl.dev/rate-limits#extract) Extract\n\n| Plan | /extract (requests/min) |\n| --- | --- |\n| Free | 10 |\n| Hobby | 20 |\n| Standard | 100 |\n| Growth | 1000 |\n| Enterprise | Custom |  \n| | /extract/status (requests/min) |\n| --- | --- |\n| Free | 500 |",
      "metadata": {
        "title": "Rate Limits | Firecrawl",
        "url": "https://docs.firecrawl.dev/rate-limits"
      }
    },
    {
      "id": "cca76917-6677-4589-b7d6-da2f331532e1",
      "source": "firecrawl/docs/rate-limits.md",
      "content": "[](https://docs.firecrawl.dev/rate-limits#legacy-plans) Legacy Plans\n\n| Plan | /scrape (requests/min) | /crawl (concurrent req) | /search (requests/min) |\n| --- | --- | --- | --- |\n| Starter | 20 | 3 | 20 |\n| Standard Legacy | 40 | 40 | 40 |\n| Scaled Legacy | 50 | 20 | 50 |  \nIf you require higher limits, please contact us at [hello@firecrawl.com](mailto:hello@firecrawl.com) to discuss custom plans.  \n[Suggest edits](https://github.com/hellofirecrawl/docs/edit/main/rate-limits.mdx) [Raise issue](https://github.com/hellofirecrawl/docs/issues/new?title=Issue%20on%20docs&body=Path:%20/rate-limits)  \n[Welcome to V1](https://docs.firecrawl.dev/v1-welcome) [Integrations](https://docs.firecrawl.dev/integrations)  \nOn this page  \n- [Standard Plans](https://docs.firecrawl.dev/rate-limits#standard-plans)\n- [Batch Endpoints](https://docs.firecrawl.dev/rate-limits#batch-endpoints)\n- [Extract](https://docs.firecrawl.dev/rate-limits#extract)\n- [Legacy Plans](https://docs.firecrawl.dev/rate-limits#legacy-plans)",
      "metadata": {
        "title": "Rate Limits | Firecrawl",
        "url": "https://docs.firecrawl.dev/rate-limits"
      }
    },
    {
      "id": "b35af1c2-3a7a-40ef-a182-18c27e92e906",
      "source": "firecrawl/docs/features-llm-extract.md",
      "content": "---\ntitle: Extract | Firecrawl\nurl: https://docs.firecrawl.dev/features/llm-extract\n---  \n[Firecrawl Docs home page![light logo](https://mintlify.s3.us-west-1.amazonaws.com/firecrawl/logo/light.svg)![dark logo](https://mintlify.s3.us-west-1.amazonaws.com/firecrawl/logo/dark.svg)](https://firecrawl.dev/)  \nv1  \nSearch or ask...  \nCtrl K  \nSearch...  \nNavigation  \nScrape  \nLLM Extract  \n[Documentation](https://docs.firecrawl.dev/introduction) [SDKs](https://docs.firecrawl.dev/sdks/overview) [Learn](https://www.firecrawl.dev/blog/category/tutorials) [API Reference](https://docs.firecrawl.dev/api-reference/introduction)",
      "metadata": {
        "title": "Extract | Firecrawl",
        "url": "https://docs.firecrawl.dev/features/llm-extract"
      }
    },
    {
      "id": "e495aae9-b377-4deb-adad-c4a51eb6be6a",
      "source": "firecrawl/docs/features-llm-extract.md",
      "content": "[](https://docs.firecrawl.dev/features/llm-extract#scrape-and-extract-structured-data-with-firecrawl) Scrape and extract structured data with Firecrawl\n\nFirecrawl leverages Large Language Models (LLMs) to efficiently extract structured data from web pages. Hereâ€™s how:  \n1. **Schema Definition:**\nDefine the URL to scrape and the desired data schema using JSON Schema (following OpenAI tool schema). This schema specifies the data structure you expect to extract from the page.  \n2. **Scrape Endpoint:**\nPass the URL and the schema to the scrape endpoint. Documentation for this endpoint can be found here:\n[Scrape Endpoint Documentation](https://docs.firecrawl.dev/api-reference/endpoint/scrape)  \n3. **Structured Data Retrieval:**\nReceive the scraped data in the structured format defined by your schema. You can then use this data as needed in your application or for further processing.  \nThis method streamlines data extraction, reducing manual handling and enhancing efficiency.",
      "metadata": {
        "title": "Extract | Firecrawl",
        "url": "https://docs.firecrawl.dev/features/llm-extract"
      }
    },
    {
      "id": "503f17ed-033a-4cad-be18-ce192a2ce311",
      "source": "firecrawl/docs/features-llm-extract.md",
      "content": "[](https://docs.firecrawl.dev/features/llm-extract#extract-structured-data) Extract structured data > [](https://docs.firecrawl.dev/features/llm-extract#scrape-with-extract-endpoint) /scrape (with extract) endpoint\n\nUsed to extract structured data from scraped pages.  \nPython  \nNode  \ncURL  \nCopy  \n```python\nfrom firecrawl import FirecrawlApp\nfrom pydantic import BaseModel, Field\n\n# Initialize the FirecrawlApp with your API key\napp = FirecrawlApp(api_key='your_api_key')\n\nclass ExtractSchema(BaseModel):\ncompany_mission: str\nsupports_sso: bool\nis_open_source: bool\nis_in_yc: bool\n\ndata = app.scrape_url('https://docs.firecrawl.dev/', {\n'formats': ['json'],\n'jsonOptions': {\n'schema': ExtractSchema.model_json_schema(),\n}\n})\nprint(data[\"json\"])\n\n```  \nOutput:  \nJSON  \nCopy  \n```json\n{\n\"success\": true,\n\"data\": {\n\"json\": {\n\"company_mission\": \"Train a secure AI on your technical resources that answers customer and employee questions so your team doesn't have to\",\n\"supports_sso\": true,\n\"is_open_source\": false,\n\"is_in_yc\": true\n},\n\"metadata\": {\n\"title\": \"Mendable\",\n\"description\": \"Mendable allows you to easily build AI chat applications. Ingest, customize, then deploy with one line of code anywhere you want. Brought to you by SideGuide\",\n\"robots\": \"follow, index\",\n\"ogTitle\": \"Mendable\",\n\"ogDescription\": \"Mendable allows you to easily build AI chat applications. Ingest, customize, then deploy with one line of code anywhere you want. Brought to you by SideGuide\",\n\"ogUrl\": \"https://docs.firecrawl.dev/\",\n\"ogImage\": \"https://docs.firecrawl.dev/mendable_new_og1.png\",\n\"ogLocaleAlternate\": [],\n\"ogSiteName\": \"Mendable\",\n\"sourceURL\": \"https://docs.firecrawl.dev/\"\n},\n}\n}\n\n```",
      "metadata": {
        "title": "Extract | Firecrawl",
        "url": "https://docs.firecrawl.dev/features/llm-extract"
      }
    },
    {
      "id": "ee1692d6-069b-4db6-8ffb-f6d0b5901cde",
      "source": "firecrawl/docs/features-llm-extract.md",
      "content": "[](https://docs.firecrawl.dev/features/llm-extract#extract-structured-data) Extract structured data > [](https://docs.firecrawl.dev/features/llm-extract#extracting-without-schema-new) Extracting without schema (New)\n\nYou can now extract without a schema by just passing a `prompt` to the endpoint. The llm chooses the structure of the data.  \ncURL  \nCopy  \n```bash\ncurl -X POST https://api.firecrawl.dev/v1/scrape \\\n-H 'Content-Type: application/json' \\\n-H 'Authorization: Bearer YOUR_API_KEY' \\\n-d '{\n\"url\": \"https://docs.firecrawl.dev/\",\n\"formats\": [\"json\"],\n\"jsonOptions\": {\n\"prompt\": \"Extract the company mission from the page.\"\n}\n}'\n\n```  \nOutput:  \nJSON  \nCopy  \n```json\n{\n\"success\": true,\n\"data\": {\n\"json\": {\n\"company_mission\": \"Train a secure AI on your technical resources that answers customer and employee questions so your team doesn't have to\",\n},\n\"metadata\": {\n\"title\": \"Mendable\",\n\"description\": \"Mendable allows you to easily build AI chat applications. Ingest, customize, then deploy with one line of code anywhere you want. Brought to you by SideGuide\",\n\"robots\": \"follow, index\",\n\"ogTitle\": \"Mendable\",\n\"ogDescription\": \"Mendable allows you to easily build AI chat applications. Ingest, customize, then deploy with one line of code anywhere you want. Brought to you by SideGuide\",\n\"ogUrl\": \"https://docs.firecrawl.dev/\",\n\"ogImage\": \"https://docs.firecrawl.dev/mendable_new_og1.png\",\n\"ogLocaleAlternate\": [],\n\"ogSiteName\": \"Mendable\",\n\"sourceURL\": \"https://docs.firecrawl.dev/\"\n},\n}\n}\n\n```",
      "metadata": {
        "title": "Extract | Firecrawl",
        "url": "https://docs.firecrawl.dev/features/llm-extract"
      }
    },
    {
      "id": "4cc84b48-7d93-41a4-8058-3171f5ba44c1",
      "source": "firecrawl/docs/features-llm-extract.md",
      "content": "[](https://docs.firecrawl.dev/features/llm-extract#extract-structured-data) Extract structured data > [](https://docs.firecrawl.dev/features/llm-extract#extract-object) Extract object\n\nThe `extract` object accepts the following parameters:  \n- `schema`: The schema to use for the extraction.\n- `systemPrompt`: The system prompt to use for the extraction.\n- `prompt`: The prompt to use for the extraction without a schema.  \n[Suggest edits](https://github.com/hellofirecrawl/docs/edit/main/features/llm-extract.mdx) [Raise issue](https://github.com/hellofirecrawl/docs/issues/new?title=Issue%20on%20docs&body=Path:%20/features/llm-extract)  \n[Batch Scrape](https://docs.firecrawl.dev/features/batch-scrape) [Crawl](https://docs.firecrawl.dev/features/crawl)  \nOn this page  \n- [Scrape and extract structured data with Firecrawl](https://docs.firecrawl.dev/features/llm-extract#scrape-and-extract-structured-data-with-firecrawl)\n- [Extract structured data](https://docs.firecrawl.dev/features/llm-extract#extract-structured-data)\n- [/scrape (with extract) endpoint](https://docs.firecrawl.dev/features/llm-extract#scrape-with-extract-endpoint)\n- [Extracting without schema (New)](https://docs.firecrawl.dev/features/llm-extract#extracting-without-schema-new)\n- [Extract object](https://docs.firecrawl.dev/features/llm-extract#extract-object)",
      "metadata": {
        "title": "Extract | Firecrawl",
        "url": "https://docs.firecrawl.dev/features/llm-extract"
      }
    },
    {
      "id": "de24239d-8315-4a13-b4c0-0de8a6fec822",
      "source": "firecrawl/docs/sdks-go.md",
      "content": "---\ntitle: Go SDK | Firecrawl\nurl: https://docs.firecrawl.dev/sdks/go\n---  \n[Firecrawl Docs home page![light logo](https://mintlify.s3.us-west-1.amazonaws.com/firecrawl/logo/light.svg)![dark logo](https://mintlify.s3.us-west-1.amazonaws.com/firecrawl/logo/dark.svg)](https://firecrawl.dev/)  \nv1  \nSearch or ask...  \nCtrl K  \nSearch...  \nNavigation  \nSDKs  \nGo  \n[Documentation](https://docs.firecrawl.dev/introduction) [SDKs](https://docs.firecrawl.dev/sdks/overview) [Learn](https://www.firecrawl.dev/blog/category/tutorials) [API Reference](https://docs.firecrawl.dev/api-reference/introduction)",
      "metadata": {
        "title": "Go SDK | Firecrawl",
        "url": "https://docs.firecrawl.dev/sdks/go"
      }
    },
    {
      "id": "3b6ac75a-af9b-42da-a3fc-077a114a2b60",
      "source": "firecrawl/docs/sdks-go.md",
      "content": "[](https://docs.firecrawl.dev/sdks/go#installation) Installation\n\nTo install the Firecrawl Go SDK, you can use go get:  \nGo  \nCopy  \n```bash\ngo get github.com/mendableai/firecrawl-go\n\n```",
      "metadata": {
        "title": "Go SDK | Firecrawl",
        "url": "https://docs.firecrawl.dev/sdks/go"
      }
    },
    {
      "id": "2db3becb-67de-4473-8327-45752c345326",
      "source": "firecrawl/docs/sdks-go.md",
      "content": "[](https://docs.firecrawl.dev/sdks/go#usage) Usage\n\n1. Get an API key from [firecrawl.dev](https://firecrawl.dev/)\n2. Set the `API key` as a parameter to the `FirecrawlApp` struct.\n3. Set the `API URL` and/or pass it as a parameter to the `FirecrawlApp` struct. Defaults to `https://api.firecrawl.dev`.\n4. Set the `version` and/or pass it as a parameter to the `FirecrawlApp` struct. Defaults to `v1`.  \nHereâ€™s an example of how to use the SDK with error handling:  \nGo  \nCopy  \n```go\nimport (\n\"fmt\"\n\"log\"\n\"github.com/google/uuid\"\n\"github.com/mendableai/firecrawl-go\"\n)\n\nfunc ptr[T any](v T) *T {\nreturn &v\n}\n\nfunc main() {\n// Initialize the FirecrawlApp with your API key\napiKey := \"fc-YOUR_API_KEY\"\napiUrl := \"https://api.firecrawl.dev\"\nversion := \"v1\"\n\napp, err := firecrawl.NewFirecrawlApp(apiKey, apiUrl, version)\nif err != nil {\nlog.Fatalf(\"Failed to initialize FirecrawlApp: %v\", err)\n}\n\n// Scrape a website\nscrapeStatus, err := app.ScrapeUrl(\"https://firecrawl.dev\", firecrawl.ScrapeParams{\nFormats: []string{\"markdown\", \"html\"},\n})\nif err != nil {\nlog.Fatalf(\"Failed to send scrape request: %v\", err)\n}\n\nfmt.Println(scrapeStatus)\n\n// Crawl a website\nidempotencyKey := uuid.New().String() // optional idempotency key\ncrawlParams := &firecrawl.CrawlParams{\nExcludePaths: []string{\"blog/*\"},\nMaxDepth: ptr(2),\n}\n\ncrawlStatus, err := app.CrawlUrl(\"https://firecrawl.dev\", crawlParams, &idempotencyKey)\nif err != nil {\nlog.Fatalf(\"Failed to send crawl request: %v\", err)\n}\n\nfmt.Println(crawlStatus)\n}\n\n```",
      "metadata": {
        "title": "Go SDK | Firecrawl",
        "url": "https://docs.firecrawl.dev/sdks/go"
      }
    },
    {
      "id": "f455537e-9a28-40ba-8614-3d399f3ceeff",
      "source": "firecrawl/docs/sdks-go.md",
      "content": "[](https://docs.firecrawl.dev/sdks/go#usage) Usage > [](https://docs.firecrawl.dev/sdks/go#scraping-a-url) Scraping a URL\n\nTo scrape a single URL with error handling, use the `ScrapeURL` method. It takes the URL as a parameter and returns the scraped data as a dictionary.  \nGo  \nCopy  \n```go\n// Scrape a website\nscrapeResult, err := app.ScrapeUrl(\"https://firecrawl.dev\", map[string]any{\n\"formats\": []string{\"markdown\", \"html\"},\n})\nif err != nil {\nlog.Fatalf(\"Failed to scrape URL: %v\", err)\n}\n\nfmt.Println(scrapeResult)\n\n```",
      "metadata": {
        "title": "Go SDK | Firecrawl",
        "url": "https://docs.firecrawl.dev/sdks/go"
      }
    },
    {
      "id": "9a417bb0-703e-47d3-9adc-2be9aa84deda",
      "source": "firecrawl/docs/sdks-go.md",
      "content": "[](https://docs.firecrawl.dev/sdks/go#usage) Usage > [](https://docs.firecrawl.dev/sdks/go#crawling-a-website) Crawling a Website\n\nTo crawl a website, use the `CrawlUrl` method. It takes the starting URL and optional parameters as arguments. The `params` argument allows you to specify additional options for the crawl job, such as the maximum number of pages to crawl, allowed domains, and the output format.  \nGo  \nCopy  \n```go\ncrawlStatus, err := app.CrawlUrl(\"https://firecrawl.dev\", map[string]any{\n\"limit\": 100,\n\"scrapeOptions\": map[string]any{\n\"formats\": []string{\"markdown\", \"html\"},\n},\n})\nif err != nil {\nlog.Fatalf(\"Failed to send crawl request: %v\", err)\n}\n\nfmt.Println(crawlStatus)\n\n```",
      "metadata": {
        "title": "Go SDK | Firecrawl",
        "url": "https://docs.firecrawl.dev/sdks/go"
      }
    },
    {
      "id": "e9da84aa-d81f-43d6-81a7-43e9ab3da9b9",
      "source": "firecrawl/docs/sdks-go.md",
      "content": "[](https://docs.firecrawl.dev/sdks/go#usage) Usage > [](https://docs.firecrawl.dev/sdks/go#checking-crawl-status) Checking Crawl Status\n\nTo check the status of a crawl job, use the `CheckCrawlStatus` method. It takes the job ID as a parameter and returns the current status of the crawl job.  \nGo  \nCopy  \n```go\n// Get crawl status\ncrawlStatus, err := app.CheckCrawlStatus(\"<crawl_id>\")\n\nif err != nil {\nlog.Fatalf(\"Failed to get crawl status: %v\", err)\n}\n\nfmt.Println(crawlStatus)\n\n```",
      "metadata": {
        "title": "Go SDK | Firecrawl",
        "url": "https://docs.firecrawl.dev/sdks/go"
      }
    },
    {
      "id": "bc66ccff-0239-44d3-81ae-c44bd287e4e0",
      "source": "firecrawl/docs/sdks-go.md",
      "content": "[](https://docs.firecrawl.dev/sdks/go#usage) Usage > [](https://docs.firecrawl.dev/sdks/go#map-a-website) Map a Website\n\nUse `MapUrl` to generate a list of URLs from a website. The `params` argument let you customize the mapping process, including options to exclude subdomains or to utilize the sitemap.  \nGo  \nCopy  \n```go\n// Map a website\nmapResult, err := app.MapUrl(\"https://firecrawl.dev\", nil)\nif err != nil {\nlog.Fatalf(\"Failed to map URL: %v\", err)\n}\n\nfmt.Println(mapResult)\n\n```",
      "metadata": {
        "title": "Go SDK | Firecrawl",
        "url": "https://docs.firecrawl.dev/sdks/go"
      }
    },
    {
      "id": "2850e507-31c2-465c-9e95-00f883cba5f2",
      "source": "firecrawl/docs/sdks-go.md",
      "content": "[](https://docs.firecrawl.dev/sdks/go#error-handling) Error Handling\n\nThe SDK handles errors returned by the Firecrawl API and raises appropriate exceptions. If an error occurs during a request, an exception will be raised with a descriptive error message.  \n[Suggest edits](https://github.com/hellofirecrawl/docs/edit/main/sdks/go.mdx) [Raise issue](https://github.com/hellofirecrawl/docs/issues/new?title=Issue%20on%20docs&body=Path:%20/sdks/go)  \n[Node](https://docs.firecrawl.dev/sdks/node) [Rust](https://docs.firecrawl.dev/sdks/rust)  \nOn this page  \n- [Installation](https://docs.firecrawl.dev/sdks/go#installation)\n- [Usage](https://docs.firecrawl.dev/sdks/go#usage)\n- [Scraping a URL](https://docs.firecrawl.dev/sdks/go#scraping-a-url)\n- [Crawling a Website](https://docs.firecrawl.dev/sdks/go#crawling-a-website)\n- [Checking Crawl Status](https://docs.firecrawl.dev/sdks/go#checking-crawl-status)\n- [Map a Website](https://docs.firecrawl.dev/sdks/go#map-a-website)\n- [Error Handling](https://docs.firecrawl.dev/sdks/go#error-handling)",
      "metadata": {
        "title": "Go SDK | Firecrawl",
        "url": "https://docs.firecrawl.dev/sdks/go"
      }
    },
    {
      "id": "fdd5aa37-4db3-4906-8c5e-fc9e3f9eb13b",
      "source": "firecrawl/docs/contributing-self-host.md",
      "content": "---\ntitle: Self-hosting | Firecrawl\nurl: https://docs.firecrawl.dev/contributing/self-host\n---  \n[Firecrawl Docs home page![light logo](https://mintlify.s3.us-west-1.amazonaws.com/firecrawl/logo/light.svg)![dark logo](https://mintlify.s3.us-west-1.amazonaws.com/firecrawl/logo/dark.svg)](https://firecrawl.dev/)  \nv1  \nSearch or ask...  \nCtrl K  \nSearch...  \nNavigation  \nContributing  \nSelf-hosting  \n[Documentation](https://docs.firecrawl.dev/introduction) [SDKs](https://docs.firecrawl.dev/sdks/overview) [Learn](https://www.firecrawl.dev/blog/category/tutorials) [API Reference](https://docs.firecrawl.dev/api-reference/introduction)",
      "metadata": {
        "title": "Self-hosting | Firecrawl",
        "url": "https://docs.firecrawl.dev/contributing/self-host"
      }
    },
    {
      "id": "ecb0ef08-02e4-4f77-b5b4-a26b843c142a",
      "source": "firecrawl/docs/contributing-self-host.md",
      "content": "[](https://docs.firecrawl.dev/contributing/self-host#contributor) Contributor?\n\nWelcome to [Firecrawl](https://firecrawl.dev/) ðŸ”¥! Here are some instructions on how to get the project locally so you can run it on your own and contribute.  \nIf youâ€™re contributing, note that the process is similar to other open-source repos, i.e., fork Firecrawl, make changes, run tests, PR.  \nIf you have any questions or would like help getting on board, join our Discord community [here](https://discord.gg/gSmWdAkdwd) for more information or submit an issue on Github [here](https://github.com/mendableai/firecrawl/issues/new/choose)!",
      "metadata": {
        "title": "Self-hosting | Firecrawl",
        "url": "https://docs.firecrawl.dev/contributing/self-host"
      }
    },
    {
      "id": "c0dc2818-64fc-4c49-b97c-9729d0972e05",
      "source": "firecrawl/docs/contributing-self-host.md",
      "content": "[](https://docs.firecrawl.dev/contributing/self-host#self-hosting-firecrawl) Self-hosting Firecrawl\n\nRefer to [SELF_HOST.md](https://github.com/mendableai/firecrawl/blob/main/SELF_HOST.md) for instructions on how to run it locally.",
      "metadata": {
        "title": "Self-hosting | Firecrawl",
        "url": "https://docs.firecrawl.dev/contributing/self-host"
      }
    },
    {
      "id": "5c72c994-05b5-42bb-9ef7-2f3bb67db37c",
      "source": "firecrawl/docs/contributing-self-host.md",
      "content": "[](https://docs.firecrawl.dev/contributing/self-host#why) Why?\n\nSelf-hosting Firecrawl is particularly beneficial for organizations with stringent security policies that require data to remain within controlled environments. Here are some key reasons to consider self-hosting:  \n- **Enhanced Security and Compliance:** By self-hosting, you ensure that all data handling and processing complies with internal and external regulations, keeping sensitive information within your secure infrastructure. Note that Firecrawl is a Mendable product and relies on SOC2 Type2 certification, which means that the platform adheres to high industry standards for managing data security.\n- **Customizable Services:** Self-hosting allows you to tailor the services, such as the Playwright service, to meet specific needs or handle particular use cases that may not be supported by the standard cloud offering.\n- **Learning and Community Contribution:** By setting up and maintaining your own instance, you gain a deeper understanding of how Firecrawl works, which can also lead to more meaningful contributions to the project.",
      "metadata": {
        "title": "Self-hosting | Firecrawl",
        "url": "https://docs.firecrawl.dev/contributing/self-host"
      }
    },
    {
      "id": "6e8ab263-b78d-435b-9987-b6e9d6c28893",
      "source": "firecrawl/docs/contributing-self-host.md",
      "content": "[](https://docs.firecrawl.dev/contributing/self-host#why) Why? > [](https://docs.firecrawl.dev/contributing/self-host#considerations) Considerations\n\nHowever, there are some limitations and additional responsibilities to be aware of:  \n1. **Limited Access to Fire-engine:** Currently, self-hosted instances of Firecrawl do not have access to Fire-engine, which includes advanced features for handling IP blocks, robot detection mechanisms, and more. This means that while you can manage basic scraping tasks, more complex scenarios might require additional configuration or might not be supported.\n2. **Manual Configuration Required:** If you need to use scraping methods beyond the basic fetch and Playwright options, you will need to manually configure these in the `.env` file. This requires a deeper understanding of the technologies and might involve more setup time.  \nSelf-hosting Firecrawl is ideal for those who need full control over their scraping and data processing environments but comes with the trade-off of additional maintenance and configuration efforts.",
      "metadata": {
        "title": "Self-hosting | Firecrawl",
        "url": "https://docs.firecrawl.dev/contributing/self-host"
      }
    },
    {
      "id": "bc4413e0-702b-47e6-af35-7313f50821f6",
      "source": "firecrawl/docs/contributing-self-host.md",
      "content": "[](https://docs.firecrawl.dev/contributing/self-host#steps) Steps\n\n1. First, start by installing the dependencies  \n- Docker [instructions](https://docs.docker.com/get-docker/)  \n2. Set environment variables  \nCreate an `.env` in the root directory you can copy over the template in `apps/api/.env.example`  \nTo start, we wont set up authentication, or any optional sub services (pdf parsing, JS blocking support, AI features)  \nCopy  \n```\n# .env\n\n# ===== Required ENVS ======\nNUM_WORKERS_PER_QUEUE=8\nPORT=3002\nHOST=0.0.0.0\n\n#for self-hosting using docker, use redis://redis:6379. For running locally, use redis://localhost:6379\nREDIS_URL=redis://redis:6379\n\n#for self-hosting using docker, use redis://redis:6379. For running locally, use redis://localhost:6379\nREDIS_RATE_LIMIT_URL=redis://redis:6379\nPLAYWRIGHT_MICROSERVICE_URL=http://playwright-service:3000/html\n\n## To turn on DB authentication, you need to set up supabase.\nUSE_DB_AUTHENTICATION=false\n\n# ===== Optional ENVS ======\n\n# Supabase Setup (used to support DB authentication, advanced logging, etc.)\nSUPABASE_ANON_TOKEN=\nSUPABASE_URL=\nSUPABASE_SERVICE_TOKEN=\n\n# Other Optionals\n# use if you've set up authentication and want to test with a real API key\nTEST_API_KEY=\n# set if you'd like to test the scraping rate limit\nRATE_LIMIT_TEST_API_KEY_SCRAPE=\n# set if you'd like to test the crawling rate limit\nRATE_LIMIT_TEST_API_KEY_CRAWL=\n# set if you'd like to use scraping Be to handle JS blocking\nSCRAPING_BEE_API_KEY=\n# add for LLM dependednt features (image alt generation, etc.)\nOPENAI_API_KEY=\nBULL_AUTH_KEY=@\n# use if you're configuring basic logging with logtail\nLOGTAIL_KEY=\n# set if you have a llamaparse key you'd like to use to parse pdfs\nLLAMAPARSE_API_KEY=\n# set if you'd like to send slack server health status messages\nSLACK_WEBHOOK_URL=\n# set if you'd like to send posthog events like job logs\nPOSTHOG_API_KEY=\n# set if you'd like to send posthog events like job logs\nPOSTHOG_HOST=\n\n# set if you'd like to use the fire engine closed beta\nFIRE_ENGINE_BETA_URL=\n\n# Proxy Settings for Playwright (Alternative you can can use a proxy service like oxylabs, which rotates IPs for you on every request)\nPROXY_SERVER=\nPROXY_USERNAME=\nPROXY_PASSWORD=\n# set if you'd like to block media requests to save proxy bandwidth\nBLOCK_MEDIA=\n\n# Set this to the URL of your webhook when using the self-hosted version of FireCrawl\nSELF_HOSTED_WEBHOOK_URL=\n\n# Resend API Key for transactional emails\nRESEND_API_KEY=\n\n# LOGGING_LEVEL determines the verbosity of logs that the system will output.\n# Available levels are:\n# NONE - No logs will be output.\n# ERROR - For logging error messages that indicate a failure in a specific operation.\n# WARN - For logging potentially harmful situations that are not necessarily errors.\n# INFO - For logging informational messages that highlight the progress of the application.\n# DEBUG - For logging detailed information on the flow through the system, primarily used for debugging.\n# TRACE - For logging more detailed information than the DEBUG level.\n# Set LOGGING_LEVEL to one of the above options to control logging output.\nLOGGING_LEVEL=INFO\n\n```  \n3. _(Optional) Running with TypeScript Playwright Service_\n- Update the `docker-compose.yml` file to change the Playwright service:  \nCopy  \n```plaintext\nbuild: apps/playwright-service\n\n```  \nTO  \nCopy  \n```plaintext\nbuild: apps/playwright-service-ts\n\n```  \n- Set the `PLAYWRIGHT_MICROSERVICE_URL` in your `.env` file:  \nCopy  \n```plaintext\nPLAYWRIGHT_MICROSERVICE_URL=http://localhost:3000/scrape\n\n```  \n- Donâ€™t forget to set the proxy server in your `.env` file as needed.\n4. Build and run the Docker containers:  \nCopy  \n```bash\ndocker compose build\ndocker compose up\n\n```  \nThis will run a local instance of Firecrawl which can be accessed at `http://localhost:3002`.  \nYou should be able to see the Bull Queue Manager UI on `http://localhost:3002/admin/@/queues`.  \n5. _(Optional)_ Test the API  \nIf youâ€™d like to test the crawl endpoint, you can run this:  \nCopy  \n```bash\ncurl -X POST http://localhost:3002/v0/crawl \\\n-H 'Content-Type: application/json' \\\n-d '{\n\"url\": \"https://docs.firecrawl.dev\"\n}'\n\n```",
      "metadata": {
        "title": "Self-hosting | Firecrawl",
        "url": "https://docs.firecrawl.dev/contributing/self-host"
      }
    },
    {
      "id": "98f738fc-a414-403d-a070-3fddb1414e36",
      "source": "firecrawl/docs/contributing-self-host.md",
      "content": "[](https://docs.firecrawl.dev/contributing/self-host#troubleshooting) Troubleshooting\n\nThis section provides solutions to common issues you might encounter while setting up or running your self-hosted instance of Firecrawl.",
      "metadata": {
        "title": "Self-hosting | Firecrawl",
        "url": "https://docs.firecrawl.dev/contributing/self-host"
      }
    },
    {
      "id": "67961c23-8786-4d15-81ac-a8d94649b5c0",
      "source": "firecrawl/docs/contributing-self-host.md",
      "content": "[](https://docs.firecrawl.dev/contributing/self-host#troubleshooting) Troubleshooting > [](https://docs.firecrawl.dev/contributing/self-host#supabase-client-is-not-configured) Supabase client is not configured\n\n**Symptom:**  \nCopy  \n```bash\n[YYYY-MM-DDTHH:MM:SS.SSSz]ERROR - Attempted to access Supabase client when it's not configured.\n[YYYY-MM-DDTHH:MM:SS.SSSz]ERROR - Error inserting scrape event: Error: Supabase client is not configured.\n\n```  \n**Explanation:**\nThis error occurs because the Supabase client setup is not completed. You should be able to scrape and crawl with no problems. Right now itâ€™s not possible to configure Supabase in self-hosted instances.",
      "metadata": {
        "title": "Self-hosting | Firecrawl",
        "url": "https://docs.firecrawl.dev/contributing/self-host"
      }
    },
    {
      "id": "aa8a7c3c-e45a-4ec2-b5d6-d7cf2f2ba6ea",
      "source": "firecrawl/docs/contributing-self-host.md",
      "content": "[](https://docs.firecrawl.dev/contributing/self-host#troubleshooting) Troubleshooting > [](https://docs.firecrawl.dev/contributing/self-host#you-re-bypassing-authentication) Youâ€™re bypassing authentication\n\n**Symptom:**  \nCopy  \n```bash\n[YYYY-MM-DDTHH:MM:SS.SSSz]WARN - You're bypassing authentication\n\n```  \n**Explanation:**\nThis error occurs because the Supabase client setup is not completed. You should be able to scrape and crawl with no problems. Right now itâ€™s not possible to configure Supabase in self-hosted instances.",
      "metadata": {
        "title": "Self-hosting | Firecrawl",
        "url": "https://docs.firecrawl.dev/contributing/self-host"
      }
    },
    {
      "id": "6af92623-5fc4-4ce1-a27a-983a9ee81804",
      "source": "firecrawl/docs/contributing-self-host.md",
      "content": "[](https://docs.firecrawl.dev/contributing/self-host#troubleshooting) Troubleshooting > [](https://docs.firecrawl.dev/contributing/self-host#docker-containers-fail-to-start) Docker containers fail to start\n\n**Symptom:**\nDocker containers exit unexpectedly or fail to start.  \n**Solution:**\nCheck the Docker logs for any error messages using the command:  \nCopy  \n```bash\ndocker logs [container_name]\n\n```  \n- Ensure all required environment variables are set correctly in the .env file.\n- Verify that all Docker services defined in docker-compose.yml are correctly configured and the necessary images are available.",
      "metadata": {
        "title": "Self-hosting | Firecrawl",
        "url": "https://docs.firecrawl.dev/contributing/self-host"
      }
    },
    {
      "id": "d21dd90b-0927-4310-87f6-80e5d508da65",
      "source": "firecrawl/docs/contributing-self-host.md",
      "content": "[](https://docs.firecrawl.dev/contributing/self-host#troubleshooting) Troubleshooting > [](https://docs.firecrawl.dev/contributing/self-host#connection-issues-with-redis) Connection issues with Redis\n\n**Symptom:**\nErrors related to connecting to Redis, such as timeouts or â€œConnection refusedâ€.  \n**Solution:**  \n- Ensure that the Redis service is up and running in your Docker environment.\n- Verify that the REDIS_URL and REDIS_RATE_LIMIT_URL in your .env file point to the correct Redis instance.\n- Check network settings and firewall rules that may block the connection to the Redis port.",
      "metadata": {
        "title": "Self-hosting | Firecrawl",
        "url": "https://docs.firecrawl.dev/contributing/self-host"
      }
    },
    {
      "id": "e4a8e5ef-c11b-4aa9-bd3a-83ed4ea4ff02",
      "source": "firecrawl/docs/contributing-self-host.md",
      "content": "[](https://docs.firecrawl.dev/contributing/self-host#troubleshooting) Troubleshooting > [](https://docs.firecrawl.dev/contributing/self-host#api-endpoint-does-not-respond) API endpoint does not respond\n\n**Symptom:**\nAPI requests to the Firecrawl instance timeout or return no response.  \n**Solution:**  \n- Ensure that the Firecrawl service is running by checking the Docker container status.\n- Verify that the PORT and HOST settings in your .env file are correct and that no other service is using the same port.\n- Check the network configuration to ensure that the host is accessible from the client making the API request.  \nBy addressing these common issues, you can ensure a smoother setup and operation of your self-hosted Firecrawl instance.",
      "metadata": {
        "title": "Self-hosting | Firecrawl",
        "url": "https://docs.firecrawl.dev/contributing/self-host"
      }
    },
    {
      "id": "9c6e4a19-04b4-4ad0-99bc-c71027e891f5",
      "source": "firecrawl/docs/contributing-self-host.md",
      "content": "[](https://docs.firecrawl.dev/contributing/self-host#install-firecrawl-on-a-kubernetes-cluster-simple-version) Install Firecrawl on a Kubernetes Cluster (Simple Version)\n\nRead the [examples/kubernetes-cluster-install/README.md](https://github.com/mendableai/firecrawl/blob/main/examples/kubernetes-cluster-install/README.md) for instructions on how to install Firecrawl on a Kubernetes Cluster.  \n[Suggest edits](https://github.com/hellofirecrawl/docs/edit/main/contributing/self-host.mdx) [Raise issue](https://github.com/hellofirecrawl/docs/issues/new?title=Issue%20on%20docs&body=Path:%20/contributing/self-host)  \n[Running locally](https://docs.firecrawl.dev/contributing/guide)  \nOn this page  \n- [Contributor?](https://docs.firecrawl.dev/contributing/self-host#contributor)\n- [Self-hosting Firecrawl](https://docs.firecrawl.dev/contributing/self-host#self-hosting-firecrawl)\n- [Why?](https://docs.firecrawl.dev/contributing/self-host#why)\n- [Considerations](https://docs.firecrawl.dev/contributing/self-host#considerations)\n- [Steps](https://docs.firecrawl.dev/contributing/self-host#steps)\n- [Troubleshooting](https://docs.firecrawl.dev/contributing/self-host#troubleshooting)\n- [Supabase client is not configured](https://docs.firecrawl.dev/contributing/self-host#supabase-client-is-not-configured)\n- [Youâ€™re bypassing authentication](https://docs.firecrawl.dev/contributing/self-host#you-re-bypassing-authentication)\n- [Docker containers fail to start](https://docs.firecrawl.dev/contributing/self-host#docker-containers-fail-to-start)\n- [Connection issues with Redis](https://docs.firecrawl.dev/contributing/self-host#connection-issues-with-redis)\n- [API endpoint does not respond](https://docs.firecrawl.dev/contributing/self-host#api-endpoint-does-not-respond)\n- [Install Firecrawl on a Kubernetes Cluster (Simple Version)](https://docs.firecrawl.dev/contributing/self-host#install-firecrawl-on-a-kubernetes-cluster-simple-version)",
      "metadata": {
        "title": "Self-hosting | Firecrawl",
        "url": "https://docs.firecrawl.dev/contributing/self-host"
      }
    },
    {
      "id": "84baa711-15dc-425c-bf3c-d84eef4b48b6",
      "source": "firecrawl/docs/integrations.md",
      "content": "---\ntitle: Integrations | Firecrawl\nurl: https://docs.firecrawl.dev/integrations\n---  \n[Firecrawl Docs home page![light logo](https://mintlify.s3.us-west-1.amazonaws.com/firecrawl/logo/light.svg)![dark logo](https://mintlify.s3.us-west-1.amazonaws.com/firecrawl/logo/dark.svg)](https://firecrawl.dev/)  \nv1  \nSearch or ask...  \nCtrl K  \nSearch...  \nNavigation  \nGet Started  \nIntegrations  \n[Documentation](https://docs.firecrawl.dev/introduction) [SDKs](https://docs.firecrawl.dev/sdks/overview) [Learn](https://www.firecrawl.dev/blog/category/tutorials) [API Reference](https://docs.firecrawl.dev/api-reference/introduction)  \n[![Firecrawl Document Loader](https://raw.githubusercontent.com/hellofirecrawl/docs/main/images/integrations/langchain.png)\\\n\\\n**Langchain** \\\n\\\nCheck out Firecrawl Document Loader](https://docs.firecrawl.dev/integrations/langchain) [![Firecrawl Reader](https://raw.githubusercontent.com/hellofirecrawl/docs/main/images/integrations/llamaindex.jpeg)\\\n\\\n**LlamaIndex** \\\n\\\nCheck out Firecrawl Reader](https://docs.firecrawl.dev/integrations/llamaindex) [![Dify](https://raw.githubusercontent.com/hellofirecrawl/docs/main/images/integrations/dify.jpeg)\\\n\\\n**Dify** \\\n\\\nExtract structured data from web pages](https://docs.firecrawl.dev/integrations/dify) [![Flowise](https://raw.githubusercontent.com/hellofirecrawl/docs/main/images/integrations/flowise.png)\\\n\\\n**Flowise** \\\n\\\nSync data directly from websites](https://docs.firecrawl.dev/integrations/flowise) [![Crew AI](https://raw.githubusercontent.com/hellofirecrawl/docs/main/images/integrations/crewai.png)\\\n\\\n**CrewAI** \\\n\\\nCoordinate AI agents for web scraping tasks](https://docs.firecrawl.dev/integrations/crewai) [![Langflow](https://raw.githubusercontent.com/hellofirecrawl/docs/main/images/integrations/langflow.webp)\\\n\\\n**Langflow** \\\n\\\nDesign visual web data pipelines](https://docs.firecrawl.dev/integrations/langflow) [![CamelAI](https://raw.githubusercontent.com/hellofirecrawl/docs/main/images/integrations/camelai.jpg)\\\n\\\n**Camel AI** \\\n\\\nDesign visual web data pipelines](https://docs.firecrawl.dev/integrations/camelai) [![RAGaaS](https://raw.githubusercontent.com/hellofirecrawl/docs/main/images/integrations/ragaas.png)\\\n\\\n**RAGaaS** \\\n\\\nBuild RAG applications with web data](https://docs.firecrawl.dev/integrations/ragaas)  \n[Suggest edits](https://github.com/hellofirecrawl/docs/edit/main/integrations.mdx) [Raise issue](https://github.com/hellofirecrawl/docs/issues/new?title=Issue%20on%20docs&body=Path:%20/integrations)  \n[Rate Limits](https://docs.firecrawl.dev/rate-limits) [Advanced Scraping Guide](https://docs.firecrawl.dev/advanced-scraping-guide)  \n![Firecrawl Document Loader](https://docs.firecrawl.dev/integrations)  \n![Firecrawl Reader](https://docs.firecrawl.dev/integrations)  \n![Dify](https://docs.firecrawl.dev/integrations)  \n![Flowise](https://docs.firecrawl.dev/integrations)  \n![Crew AI](https://docs.firecrawl.dev/integrations)  \n![CamelAI](https://docs.firecrawl.dev/integrations)  \n![RAGaaS](https://docs.firecrawl.dev/integrations)",
      "metadata": {
        "title": "Integrations | Firecrawl",
        "url": "https://docs.firecrawl.dev/integrations"
      }
    },
    {
      "id": "8b4ba5b3-cae8-4c04-a426-7eb16dd6223c",
      "source": "firecrawl/docs/integrations-ragaas.md",
      "content": "---\ntitle: RAGaaS | Firecrawl\nurl: https://docs.firecrawl.dev/integrations/ragaas\n---  \n[Firecrawl Docs home page![light logo](https://mintlify.s3.us-west-1.amazonaws.com/firecrawl/logo/light.svg)![dark logo](https://mintlify.s3.us-west-1.amazonaws.com/firecrawl/logo/dark.svg)](https://firecrawl.dev/)  \nv1  \nSearch or ask...  \nCtrl K  \nSearch...  \nNavigation  \nIntegrations  \nRAGaaS  \n[Documentation](https://docs.firecrawl.dev/introduction) [SDKs](https://docs.firecrawl.dev/sdks/overview) [Learn](https://www.firecrawl.dev/blog/category/tutorials) [API Reference](https://docs.firecrawl.dev/api-reference/introduction)  \n[RAGaaS](https://ragaas.dev/) is a Retrieval Augmented Generation as a Service platform that helps you build AI applications with your own data. This guide explains how to use Firecrawl with RAGaaS for web scraping capabilities.",
      "metadata": {
        "title": "RAGaaS | Firecrawl",
        "url": "https://docs.firecrawl.dev/integrations/ragaas"
      }
    },
    {
      "id": "3980dd63-dfaf-4b96-b4e7-dfed5f825d90",
      "source": "firecrawl/docs/integrations-ragaas.md",
      "content": "[](https://docs.firecrawl.dev/integrations/ragaas#setup) Setup\n\n1. First, obtain your Firecrawl API key from your [Firecrawl dashboard](https://www.firecrawl.dev/app)  \n2. Configure your RAGaaS namespace to use Firecrawl as the web scraping provider:  \ncURL  \nJavaScript  \nPython  \nCopy  \n```bash\ncurl -X PATCH https://api.ragaas.dev/v1/namespaces/YOUR_NAMESPACE_ID \\\n-H \"Authorization: Bearer YOUR_RAGAAS_API_KEY\" \\\n-H \"Content-Type: application/json\" \\\n-d '{\n\"webScraperConfig\": {\n\"provider\": \"FIRECRAWL\",\n\"apiKey\": \"YOUR_FIRECRAWL_API_KEY\"\n}\n}'\n\n```",
      "metadata": {
        "title": "RAGaaS | Firecrawl",
        "url": "https://docs.firecrawl.dev/integrations/ragaas"
      }
    },
    {
      "id": "b5817200-6b51-4554-a6ea-d1d5ad9d5665",
      "source": "firecrawl/docs/integrations-ragaas.md",
      "content": "[](https://docs.firecrawl.dev/integrations/ragaas#usage) Usage\n\nOnce configured, you can use RAGaaSâ€™s web scraping endpoints with Firecrawlâ€™s capabilities. Here are the main ingestion methods:",
      "metadata": {
        "title": "RAGaaS | Firecrawl",
        "url": "https://docs.firecrawl.dev/integrations/ragaas"
      }
    },
    {
      "id": "8b69e9b0-b57e-40df-8ca4-dea5f598c3c5",
      "source": "firecrawl/docs/integrations-ragaas.md",
      "content": "[](https://docs.firecrawl.dev/integrations/ragaas#usage) Usage > [](https://docs.firecrawl.dev/integrations/ragaas#url-list-ingestion) URL List Ingestion\n\nScrape specific URLs:  \ncURL  \nJavaScript  \nPython  \nCopy  \n```bash\ncurl -X POST https://api.ragaas.dev/v1/ingest/urls \\\n-H \"Authorization: Bearer YOUR_RAGAAS_API_KEY\" \\\n-H \"Content-Type: application/json\" \\\n-d '{\n\"namespaceId\": \"YOUR_NAMESPACE_ID\",\n\"ingestConfig\": {\n\"source\": \"URLS_LIST\",\n\"config\": {\n\"urls\": [\\\n\"https://example.com/page1\",\\\n\"https://example.com/page2\"\\\n],\n\"scrapeOptions\": {\n\"includeSelectors\": [\"article\", \"main\"],\n\"excludeSelectors\": [\".navigation\", \".footer\"]\n}\n}\n}\n}'\n\n```",
      "metadata": {
        "title": "RAGaaS | Firecrawl",
        "url": "https://docs.firecrawl.dev/integrations/ragaas"
      }
    },
    {
      "id": "4eb66f8d-f5d9-4e40-9af3-40e873e21319",
      "source": "firecrawl/docs/integrations-ragaas.md",
      "content": "[](https://docs.firecrawl.dev/integrations/ragaas#usage) Usage > [](https://docs.firecrawl.dev/integrations/ragaas#website-crawling) Website Crawling\n\nCrawl an entire website with custom rules:  \ncURL  \nJavaScript  \nPython  \nCopy  \n```bash\ncurl -X POST https://api.ragaas.dev/v1/ingest/website \\\n-H \"Authorization: Bearer YOUR_RAGAAS_API_KEY\" \\\n-H \"Content-Type: application/json\" \\\n-d '{\n\"namespaceId\": \"YOUR_NAMESPACE_ID\",\n\"ingestConfig\": {\n\"source\": \"WEBSITE\",\n\"config\": {\n\"url\": \"https://example.com\",\n\"maxDepth\": 3,\n\"maxLinks\": 100,\n\"includePaths\": [\"/docs\", \"/blog\"],\n\"excludePaths\": [\"/admin\"],\n\"scrapeOptions\": {\n\"includeSelectors\": [\"article\", \"main\"],\n\"excludeSelectors\": [\".navigation\", \".footer\"]\n}\n}\n}\n}'\n\n```",
      "metadata": {
        "title": "RAGaaS | Firecrawl",
        "url": "https://docs.firecrawl.dev/integrations/ragaas"
      }
    },
    {
      "id": "6e4cfdb6-644a-4ea5-a1ca-b4cb72f57515",
      "source": "firecrawl/docs/integrations-ragaas.md",
      "content": "[](https://docs.firecrawl.dev/integrations/ragaas#usage) Usage > [](https://docs.firecrawl.dev/integrations/ragaas#sitemap-processing) Sitemap Processing\n\nProcess all URLs from a sitemap:  \ncURL  \nJavaScript  \nPython  \nCopy  \n```bash\ncurl -X POST https://api.ragaas.dev/v1/ingest/sitemap \\\n-H \"Authorization: Bearer YOUR_RAGAAS_API_KEY\" \\\n-H \"Content-Type: application/json\" \\\n-d '{\n\"namespaceId\": \"YOUR_NAMESPACE_ID\",\n\"ingestConfig\": {\n\"source\": \"SITEMAP\",\n\"config\": {\n\"url\": \"https://example.com/sitemap.xml\",\n\"scrapeOptions\": {\n\"includeSelectors\": [\"article\", \"main\"],\n\"excludeSelectors\": [\".navigation\", \".footer\"]\n}\n}\n}\n}'\n\n```",
      "metadata": {
        "title": "RAGaaS | Firecrawl",
        "url": "https://docs.firecrawl.dev/integrations/ragaas"
      }
    },
    {
      "id": "820dbe26-9a46-4508-bc6d-81f60a21cac7",
      "source": "firecrawl/docs/integrations-ragaas.md",
      "content": "[](https://docs.firecrawl.dev/integrations/ragaas#features) Features\n\nWhen using Firecrawl with RAGaaS, you get access to:  \n- JavaScript rendering support\n- Automatic rate limiting\n- CSS selector-based content extraction\n- Recursive crawling with depth control\n- Sitemap processing",
      "metadata": {
        "title": "RAGaaS | Firecrawl",
        "url": "https://docs.firecrawl.dev/integrations/ragaas"
      }
    },
    {
      "id": "b7a134d5-257a-4fe4-b061-30d956d9ac31",
      "source": "firecrawl/docs/integrations-ragaas.md",
      "content": "[](https://docs.firecrawl.dev/integrations/ragaas#resources) Resources\n\n- [RAGaaS Documentation](https://ragaas.dev/)\n- [Web Scraping Guide](https://ragaas.dev/web-scraping)\n- [API Reference](https://ragaas.dev/api-reference/data-ingestion#ingest-urls)  \nFor additional support:  \n- Email: [support@ragaas.dev](mailto:support@ragaas.dev)\n- Discord: [Join our community](https://discord.gg/Fx3GnFKnRT)  \n[Suggest edits](https://github.com/hellofirecrawl/docs/edit/main/integrations/ragaas.mdx) [Raise issue](https://github.com/hellofirecrawl/docs/issues/new?title=Issue%20on%20docs&body=Path:%20/integrations/ragaas)  \n[Camel AI](https://docs.firecrawl.dev/integrations/camelai) [Open Source vs Cloud](https://docs.firecrawl.dev/contributing/open-source-or-cloud)  \nOn this page  \n- [Setup](https://docs.firecrawl.dev/integrations/ragaas#setup)\n- [Usage](https://docs.firecrawl.dev/integrations/ragaas#usage)\n- [URL List Ingestion](https://docs.firecrawl.dev/integrations/ragaas#url-list-ingestion)\n- [Website Crawling](https://docs.firecrawl.dev/integrations/ragaas#website-crawling)\n- [Sitemap Processing](https://docs.firecrawl.dev/integrations/ragaas#sitemap-processing)\n- [Features](https://docs.firecrawl.dev/integrations/ragaas#features)\n- [Resources](https://docs.firecrawl.dev/integrations/ragaas#resources)",
      "metadata": {
        "title": "RAGaaS | Firecrawl",
        "url": "https://docs.firecrawl.dev/integrations/ragaas"
      }
    },
    {
      "id": "ef3c9c9a-2134-42fb-bc6f-6e1b243a6397",
      "source": "firecrawl/docs/v0-api-reference-endpoint-crawl.md",
      "content": "---\ntitle: Firecrawl Docs\nurl: https://docs.firecrawl.dev/v0/api-reference/endpoint/crawl\n---  \n[Firecrawl Docs home page![light logo](https://mintlify.s3.us-west-1.amazonaws.com/firecrawl/logo/light.svg)![dark logo](https://mintlify.s3.us-west-1.amazonaws.com/firecrawl/logo/dark.svg)](https://firecrawl.dev/)  \nv0  \nSearch or ask...  \nCtrl K  \nSearch...  \nNavigation  \nEndpoints  \nCrawl  \n[Documentation](https://docs.firecrawl.dev/v0/introduction) [SDKs](https://docs.firecrawl.dev/v0/sdks/python) [Learn](https://www.firecrawl.dev/blog/category/tutorials) [API Reference](https://docs.firecrawl.dev/v0/api-reference/introduction)  \nPOST  \n/  \ncrawl  \nTry it  \ncURL  \nPython  \nJavaScript  \nPHP  \nGo  \nJava  \nCopy  \n```\ncurl --request POST \\\n--url https://api.firecrawl.dev/v0/crawl \\\n--header 'Authorization: Bearer <token>' \\\n--header 'Content-Type: application/json' \\\n--data '{\n\"url\": \"<string>\",\n\"crawlerOptions\": {\n\"includes\": [\\\n\"<string>\"\\\n],\n\"excludes\": [\\\n\"<string>\"\\\n],\n\"generateImgAltText\": false,\n\"returnOnlyUrls\": false,\n\"maxDepth\": 123,\n\"mode\": \"default\",\n\"ignoreSitemap\": false,\n\"limit\": 10000,\n\"allowBackwardCrawling\": false,\n\"allowExternalContentLinks\": false\n},\n\"pageOptions\": {\n\"headers\": {},\n\"includeHtml\": false,\n\"includeRawHtml\": false,\n\"onlyIncludeTags\": [\\\n\"<string>\"\\\n],\n\"onlyMainContent\": false,\n\"removeTags\": [\\\n\"<string>\"\\\n],\n\"replaceAllPathsWithAbsolutePaths\": false,\n\"screenshot\": false,\n\"fullPageScreenshot\": false,\n\"waitFor\": 0\n}\n}'\n```  \n200  \n402  \n429  \n500  \nCopy  \n```\n{\n\"jobId\": \"<string>\"\n}\n```",
      "metadata": {
        "title": "Firecrawl Docs",
        "url": "https://docs.firecrawl.dev/v0/api-reference/endpoint/crawl"
      }
    },
    {
      "id": "c90eecde-c383-4e08-9c28-1c03bfbd4f0f",
      "source": "firecrawl/docs/v0-api-reference-endpoint-crawl.md",
      "content": "Authorizations\n\n[](https://docs.firecrawl.dev/v0/api-reference/endpoint/crawl#authorization-authorization)  \nAuthorization  \nstring  \nheader  \nrequired  \nBearer authentication header of the form `Bearer <token>`, where `<token>` is your auth token.",
      "metadata": {
        "title": "Firecrawl Docs",
        "url": "https://docs.firecrawl.dev/v0/api-reference/endpoint/crawl"
      }
    },
    {
      "id": "7ba2d891-37f2-419d-8f69-fbab914b39a7",
      "source": "firecrawl/docs/v0-api-reference-endpoint-crawl.md",
      "content": "Body\n\napplication/json  \n[](https://docs.firecrawl.dev/v0/api-reference/endpoint/crawl#body-url)  \nurl  \nstring  \nrequired  \nThe base URL to start crawling from  \n[](https://docs.firecrawl.dev/v0/api-reference/endpoint/crawl#body-crawler-options)  \ncrawlerOptions  \nobject  \nShowchild attributes  \n[](https://docs.firecrawl.dev/v0/api-reference/endpoint/crawl#body-crawler-options-includes)  \ncrawlerOptions.includes  \nstring[]  \nURL patterns to include  \n[](https://docs.firecrawl.dev/v0/api-reference/endpoint/crawl#body-crawler-options-excludes)  \ncrawlerOptions.excludes  \nstring[]  \nURL patterns to exclude  \n[](https://docs.firecrawl.dev/v0/api-reference/endpoint/crawl#body-crawler-options-generate-img-alt-text)  \ncrawlerOptions.generateImgAltText  \nboolean  \ndefault:  \nfalse  \nGenerate alt text for images using LLMs (must have a paid plan)  \n[](https://docs.firecrawl.dev/v0/api-reference/endpoint/crawl#body-crawler-options-return-only-urls)  \ncrawlerOptions.returnOnlyUrls  \nboolean  \ndefault:  \nfalse  \nIf true, returns only the URLs as a list on the crawl status. Attention: the return response will be a list of URLs inside the data, not a list of documents.  \n[](https://docs.firecrawl.dev/v0/api-reference/endpoint/crawl#body-crawler-options-max-depth)  \ncrawlerOptions.maxDepth  \ninteger  \nMaximum depth to crawl relative to the entered URL. A maxDepth of 0 scrapes only the entered URL. A maxDepth of 1 scrapes the entered URL and all pages one level deep. A maxDepth of 2 scrapes the entered URL and all pages up to two levels deep. Higher values follow the same pattern.  \n[](https://docs.firecrawl.dev/v0/api-reference/endpoint/crawl#body-crawler-options-mode)  \ncrawlerOptions.mode  \nenum<string>  \ndefault:  \ndefault  \nThe crawling mode to use. Fast mode crawls 4x faster websites without sitemap, but may not be as accurate and shouldn't be used in heavy js-rendered websites.  \nAvailable options:  \n`default`,  \n`fast`  \n[](https://docs.firecrawl.dev/v0/api-reference/endpoint/crawl#body-crawler-options-ignore-sitemap)  \ncrawlerOptions.ignoreSitemap  \nboolean  \ndefault:  \nfalse  \nIgnore the website sitemap when crawling  \n[](https://docs.firecrawl.dev/v0/api-reference/endpoint/crawl#body-crawler-options-limit)  \ncrawlerOptions.limit  \ninteger  \ndefault:  \n10000  \nMaximum number of pages to crawl  \n[](https://docs.firecrawl.dev/v0/api-reference/endpoint/crawl#body-crawler-options-allow-backward-crawling)  \ncrawlerOptions.allowBackwardCrawling  \nboolean  \ndefault:  \nfalse  \nEnables the crawler to navigate from a specific URL to previously linked pages. For instance, from 'example.com/product/123' back to 'example.com/product'  \n[](https://docs.firecrawl.dev/v0/api-reference/endpoint/crawl#body-crawler-options-allow-external-content-links)  \ncrawlerOptions.allowExternalContentLinks  \nboolean  \ndefault:  \nfalse  \nAllows the crawler to follow links to external websites.  \n[](https://docs.firecrawl.dev/v0/api-reference/endpoint/crawl#body-page-options)  \npageOptions  \nobject  \nShowchild attributes  \n[](https://docs.firecrawl.dev/v0/api-reference/endpoint/crawl#body-page-options-headers)  \npageOptions.headers  \nobject  \nHeaders to send with the request. Can be used to send cookies, user-agent, etc.  \n[](https://docs.firecrawl.dev/v0/api-reference/endpoint/crawl#body-page-options-include-html)  \npageOptions.includeHtml  \nboolean  \ndefault:  \nfalse  \nInclude the HTML version of the content on page. Will output a html key in the response.  \n[](https://docs.firecrawl.dev/v0/api-reference/endpoint/crawl#body-page-options-include-raw-html)  \npageOptions.includeRawHtml  \nboolean  \ndefault:  \nfalse  \nInclude the raw HTML content of the page. Will output a rawHtml key in the response.  \n[](https://docs.firecrawl.dev/v0/api-reference/endpoint/crawl#body-page-options-only-include-tags)  \npageOptions.onlyIncludeTags  \nstring[]  \nOnly include tags, classes and ids from the page in the final output. Use comma separated values. Example: 'script, .ad, #footer'  \n[](https://docs.firecrawl.dev/v0/api-reference/endpoint/crawl#body-page-options-only-main-content)  \npageOptions.onlyMainContent  \nboolean  \ndefault:  \nfalse  \nOnly return the main content of the page excluding headers, navs, footers, etc.  \n[](https://docs.firecrawl.dev/v0/api-reference/endpoint/crawl#body-page-options-remove-tags)  \npageOptions.removeTags  \nstring[]  \nTags, classes and ids to remove from the page. Use comma separated values. Example: 'script, .ad, #footer'  \n[](https://docs.firecrawl.dev/v0/api-reference/endpoint/crawl#body-page-options-replace-all-paths-with-absolute-paths)  \npageOptions.replaceAllPathsWithAbsolutePaths  \nboolean  \ndefault:  \nfalse  \nReplace all relative paths with absolute paths for images and links  \n[](https://docs.firecrawl.dev/v0/api-reference/endpoint/crawl#body-page-options-screenshot)  \npageOptions.screenshot  \nboolean  \ndefault:  \nfalse  \nInclude a screenshot of the top of the page that you are scraping.  \n[](https://docs.firecrawl.dev/v0/api-reference/endpoint/crawl#body-page-options-full-page-screenshot)  \npageOptions.fullPageScreenshot  \nboolean  \ndefault:  \nfalse  \nInclude a full page screenshot of the page that you are scraping.  \n[](https://docs.firecrawl.dev/v0/api-reference/endpoint/crawl#body-page-options-wait-for)  \npageOptions.waitFor  \ninteger  \ndefault:  \n0  \nWait x amount of milliseconds for the page to load to fetch content",
      "metadata": {
        "title": "Firecrawl Docs",
        "url": "https://docs.firecrawl.dev/v0/api-reference/endpoint/crawl"
      }
    },
    {
      "id": "b97139e7-5d03-45dd-bbae-3dd2749c6583",
      "source": "firecrawl/docs/v0-api-reference-endpoint-crawl.md",
      "content": "Response\n\n200 - application/json  \n[](https://docs.firecrawl.dev/v0/api-reference/endpoint/crawl#response-job-id)  \njobId  \nstring  \n[Suggest edits](https://github.com/hellofirecrawl/docs/edit/main/v0/api-reference/endpoint/crawl.mdx) [Raise issue](https://github.com/hellofirecrawl/docs/issues/new?title=Issue%20on%20docs&body=Path:%20/v0/api-reference/endpoint/crawl)  \n[Search (Beta)](https://docs.firecrawl.dev/v0/api-reference/endpoint/search) [Get Crawl Status](https://docs.firecrawl.dev/v0/api-reference/endpoint/status)  \ncURL  \nPython  \nJavaScript  \nPHP  \nGo  \nJava  \nCopy  \n```\ncurl --request POST \\\n--url https://api.firecrawl.dev/v0/crawl \\\n--header 'Authorization: Bearer <token>' \\\n--header 'Content-Type: application/json' \\\n--data '{\n\"url\": \"<string>\",\n\"crawlerOptions\": {\n\"includes\": [\\\n\"<string>\"\\\n],\n\"excludes\": [\\\n\"<string>\"\\\n],\n\"generateImgAltText\": false,\n\"returnOnlyUrls\": false,\n\"maxDepth\": 123,\n\"mode\": \"default\",\n\"ignoreSitemap\": false,\n\"limit\": 10000,\n\"allowBackwardCrawling\": false,\n\"allowExternalContentLinks\": false\n},\n\"pageOptions\": {\n\"headers\": {},\n\"includeHtml\": false,\n\"includeRawHtml\": false,\n\"onlyIncludeTags\": [\\\n\"<string>\"\\\n],\n\"onlyMainContent\": false,\n\"removeTags\": [\\\n\"<string>\"\\\n],\n\"replaceAllPathsWithAbsolutePaths\": false,\n\"screenshot\": false,\n\"fullPageScreenshot\": false,\n\"waitFor\": 0\n}\n}'\n```  \n200  \n402  \n429  \n500  \nCopy  \n```\n{\n\"jobId\": \"<string>\"\n}\n```",
      "metadata": {
        "title": "Firecrawl Docs",
        "url": "https://docs.firecrawl.dev/v0/api-reference/endpoint/crawl"
      }
    },
    {
      "id": "36fd9b64-da4f-4d41-b914-8996b35356f6",
      "source": "firecrawl/docs/api-reference-endpoint-map.md",
      "content": "---\ntitle: Firecrawl Docs\nurl: https://docs.firecrawl.dev/api-reference/endpoint/map\n---  \n[Firecrawl Docs home page![light logo](https://mintlify.s3.us-west-1.amazonaws.com/firecrawl/logo/light.svg)![dark logo](https://mintlify.s3.us-west-1.amazonaws.com/firecrawl/logo/dark.svg)](https://firecrawl.dev/)  \nv1  \nSearch or ask...  \nCtrl K  \nSearch...  \nNavigation  \nMap Endpoints  \nMap  \n[Documentation](https://docs.firecrawl.dev/introduction) [SDKs](https://docs.firecrawl.dev/sdks/overview) [Learn](https://www.firecrawl.dev/blog/category/tutorials) [API Reference](https://docs.firecrawl.dev/api-reference/introduction)  \nPOST  \n/  \nmap  \nTry it  \ncURL  \nPython  \nJavaScript  \nPHP  \nGo  \nJava  \nCopy  \n```\ncurl --request POST \\\n--url https://api.firecrawl.dev/v1/map \\\n--header 'Authorization: Bearer <token>' \\\n--header 'Content-Type: application/json' \\\n--data '{\n\"url\": \"<string>\",\n\"search\": \"<string>\",\n\"ignoreSitemap\": true,\n\"sitemapOnly\": false,\n\"includeSubdomains\": false,\n\"limit\": 5000,\n\"timeout\": 123\n}'\n```  \n200  \n402  \n429  \n500  \nCopy  \n```\n{\n\"success\": true,\n\"links\": [\\\n\"<string>\"\\\n]\n}\n```",
      "metadata": {
        "title": "Firecrawl Docs",
        "url": "https://docs.firecrawl.dev/api-reference/endpoint/map"
      }
    },
    {
      "id": "abcc352e-6641-46e2-bc73-8bed357613d8",
      "source": "firecrawl/docs/api-reference-endpoint-map.md",
      "content": "Authorizations\n\n[](https://docs.firecrawl.dev/api-reference/endpoint/map#authorization-authorization)  \nAuthorization  \nstring  \nheader  \nrequired  \nBearer authentication header of the form `Bearer <token>`, where `<token>` is your auth token.",
      "metadata": {
        "title": "Firecrawl Docs",
        "url": "https://docs.firecrawl.dev/api-reference/endpoint/map"
      }
    },
    {
      "id": "3b1eb122-4813-41fd-a082-c0995bbad457",
      "source": "firecrawl/docs/api-reference-endpoint-map.md",
      "content": "Body\n\napplication/json  \n[](https://docs.firecrawl.dev/api-reference/endpoint/map#body-url)  \nurl  \nstring  \nrequired  \nThe base URL to start crawling from  \n[](https://docs.firecrawl.dev/api-reference/endpoint/map#body-search)  \nsearch  \nstring  \nSearch query to use for mapping. During the Alpha phase, the 'smart' part of the search functionality is limited to 1000 search results. However, if map finds more results, there is no limit applied.  \n[](https://docs.firecrawl.dev/api-reference/endpoint/map#body-ignore-sitemap)  \nignoreSitemap  \nboolean  \ndefault:  \ntrue  \nIgnore the website sitemap when crawling.  \n[](https://docs.firecrawl.dev/api-reference/endpoint/map#body-sitemap-only)  \nsitemapOnly  \nboolean  \ndefault:  \nfalse  \nOnly return links found in the website sitemap  \n[](https://docs.firecrawl.dev/api-reference/endpoint/map#body-include-subdomains)  \nincludeSubdomains  \nboolean  \ndefault:  \nfalse  \nInclude subdomains of the website  \n[](https://docs.firecrawl.dev/api-reference/endpoint/map#body-limit)  \nlimit  \ninteger  \ndefault:  \n5000  \nMaximum number of links to return  \nRequired range: `x < 5000`  \n[](https://docs.firecrawl.dev/api-reference/endpoint/map#body-timeout)  \ntimeout  \ninteger  \nTimeout in milliseconds. There is no timeout by default.",
      "metadata": {
        "title": "Firecrawl Docs",
        "url": "https://docs.firecrawl.dev/api-reference/endpoint/map"
      }
    },
    {
      "id": "46f16486-cb95-4da7-9a39-e825e1c2ecbb",
      "source": "firecrawl/docs/api-reference-endpoint-map.md",
      "content": "Response\n\n200 - application/json  \n[](https://docs.firecrawl.dev/api-reference/endpoint/map#response-success)  \nsuccess  \nboolean  \n[](https://docs.firecrawl.dev/api-reference/endpoint/map#response-links)  \nlinks  \nstring[]  \n[Suggest edits](https://github.com/hellofirecrawl/docs/edit/main/api-reference/endpoint/map.mdx) [Raise issue](https://github.com/hellofirecrawl/docs/issues/new?title=Issue%20on%20docs&body=Path:%20/api-reference/endpoint/map)  \n[Get Crawl Errors](https://docs.firecrawl.dev/api-reference/endpoint/crawl-get-errors) [Extract](https://docs.firecrawl.dev/api-reference/endpoint/extract)  \ncURL  \nPython  \nJavaScript  \nPHP  \nGo  \nJava  \nCopy  \n```\ncurl --request POST \\\n--url https://api.firecrawl.dev/v1/map \\\n--header 'Authorization: Bearer <token>' \\\n--header 'Content-Type: application/json' \\\n--data '{\n\"url\": \"<string>\",\n\"search\": \"<string>\",\n\"ignoreSitemap\": true,\n\"sitemapOnly\": false,\n\"includeSubdomains\": false,\n\"limit\": 5000,\n\"timeout\": 123\n}'\n```  \n200  \n402  \n429  \n500  \nCopy  \n```\n{\n\"success\": true,\n\"links\": [\\\n\"<string>\"\\\n]\n}\n```",
      "metadata": {
        "title": "Firecrawl Docs",
        "url": "https://docs.firecrawl.dev/api-reference/endpoint/map"
      }
    },
    {
      "id": "86983993-bf43-4528-89e3-e92933d7ffea",
      "source": "firecrawl/docs/v0-api-reference-introduction.md",
      "content": "---\ntitle: Firecrawl Docs\nurl: https://docs.firecrawl.dev/v0/api-reference/introduction\n---  \n[Firecrawl Docs home page![light logo](https://mintlify.s3.us-west-1.amazonaws.com/firecrawl/logo/light.svg)![dark logo](https://mintlify.s3.us-west-1.amazonaws.com/firecrawl/logo/dark.svg)](https://firecrawl.dev/)  \nv0  \nSearch or ask...  \nCtrl K  \nSearch...  \nNavigation  \nUsing the API  \nIntroduction  \n[Documentation](https://docs.firecrawl.dev/v0/introduction) [SDKs](https://docs.firecrawl.dev/v0/sdks/python) [Learn](https://www.firecrawl.dev/blog/category/tutorials) [API Reference](https://docs.firecrawl.dev/v0/api-reference/introduction)",
      "metadata": {
        "title": "Firecrawl Docs",
        "url": "https://docs.firecrawl.dev/v0/api-reference/introduction"
      }
    },
    {
      "id": "ee0cdea3-3dfe-4289-ba99-b4256d2eda12",
      "source": "firecrawl/docs/v0-api-reference-introduction.md",
      "content": "[](https://docs.firecrawl.dev/v0/api-reference/introduction#base-url) Base URL\n\nAll requests contain the following base URL:  \nCopy  \n```bash\nhttps://api.firecrawl.dev\n\n```",
      "metadata": {
        "title": "Firecrawl Docs",
        "url": "https://docs.firecrawl.dev/v0/api-reference/introduction"
      }
    },
    {
      "id": "1ed08826-89cc-4c74-b277-3d5bac818a4a",
      "source": "firecrawl/docs/v0-api-reference-introduction.md",
      "content": "[](https://docs.firecrawl.dev/v0/api-reference/introduction#authentication) Authentication\n\nFor authentication, itâ€™s required to include an Authorization header. The header should contain `Bearer fc_123456789`, where `fc_123456789` represents your API Key.  \nCopy  \n```bash\nAuthorization: Bearer fc_123456789\n\n```",
      "metadata": {
        "title": "Firecrawl Docs",
        "url": "https://docs.firecrawl.dev/v0/api-reference/introduction"
      }
    },
    {
      "id": "e767de8e-e7f4-4dbd-a305-6794334a5fe5",
      "source": "firecrawl/docs/v0-api-reference-introduction.md",
      "content": "[](https://docs.firecrawl.dev/v0/api-reference/introduction#response-codes) Response codes\n\nFirecrawl employs conventional HTTP status codes to signify the outcome of your requests.  \nTypically, 2xx HTTP status codes denote success, 4xx codes represent failures related to the user, and 5xx codes signal infrastructure problems.  \n| Status | Description |\n| --- | --- |\n| 200 | Request was successful. |\n| 400 | Verify the correctness of the parameters. |\n| 401 | The API key was not provided. |\n| 402 | Payment required |\n| 404 | The requested resource could not be located. |\n| 429 | The rate limit has been surpassed. |\n| 5xx | Signifies a server error with Firecrawl. |  \nRefer to the Error Codes section for a detailed explanation of all potential API errors.",
      "metadata": {
        "title": "Firecrawl Docs",
        "url": "https://docs.firecrawl.dev/v0/api-reference/introduction"
      }
    },
    {
      "id": "4b95203f-f157-4ba1-9e5f-fbc98ad22596",
      "source": "firecrawl/docs/v0-api-reference-introduction.md",
      "content": "[](https://docs.firecrawl.dev/v0/api-reference/introduction#rate-limit) Rate limit\n\nThe Firecrawl API has a rate limit to ensure the stability and reliability of the service. The rate limit is applied to all endpoints and is based on the number of requests made within a specific time frame.  \nWhen you exceed the rate limit, you will receive a 429 response code.  \n[Suggest edits](https://github.com/hellofirecrawl/docs/edit/main/v0/api-reference/introduction.mdx) [Raise issue](https://github.com/hellofirecrawl/docs/issues/new?title=Issue%20on%20docs&body=Path:%20/v0/api-reference/introduction)  \n[Scrape](https://docs.firecrawl.dev/v0/api-reference/endpoint/scrape)  \nOn this page  \n- [Base URL](https://docs.firecrawl.dev/v0/api-reference/introduction#base-url)\n- [Authentication](https://docs.firecrawl.dev/v0/api-reference/introduction#authentication)\n- [Response codes](https://docs.firecrawl.dev/v0/api-reference/introduction#response-codes)\n- [Rate limit](https://docs.firecrawl.dev/v0/api-reference/introduction#rate-limit)",
      "metadata": {
        "title": "Firecrawl Docs",
        "url": "https://docs.firecrawl.dev/v0/api-reference/introduction"
      }
    },
    {
      "id": "6d4d939b-20e7-4278-b2cb-8d3c6f647116",
      "source": "firecrawl/docs/v0-sdks-node.md",
      "content": "---\ntitle: Node SDK | Firecrawl\nurl: https://docs.firecrawl.dev/v0/sdks/node\n---  \n[Firecrawl Docs home page![light logo](https://mintlify.s3.us-west-1.amazonaws.com/firecrawl/logo/light.svg)![dark logo](https://mintlify.s3.us-west-1.amazonaws.com/firecrawl/logo/dark.svg)](https://firecrawl.dev/)  \nv0  \nSearch or ask...  \nCtrl K  \nSearch...  \nNavigation  \nSDKs  \nNode  \n[Documentation](https://docs.firecrawl.dev/v0/introduction) [SDKs](https://docs.firecrawl.dev/v0/sdks/python) [Learn](https://www.firecrawl.dev/blog/category/tutorials) [API Reference](https://docs.firecrawl.dev/v0/api-reference/introduction)  \n> Note: this is using [v0 version of the Firecrawl API](https://docs.firecrawl.dev/v0/introduction) which is being deprecated. We recommend switching to [v1](https://docs.firecrawl.dev/sdks/node).",
      "metadata": {
        "title": "Node SDK | Firecrawl",
        "url": "https://docs.firecrawl.dev/v0/sdks/node"
      }
    },
    {
      "id": "af83d245-ad12-4d9d-b726-d99111dba06f",
      "source": "firecrawl/docs/v0-sdks-node.md",
      "content": "[](https://docs.firecrawl.dev/v0/sdks/node#installation) Installation\n\nTo install the Firecrawl Node SDK, you can use npm:  \nCopy  \n```bash\nnpm install @mendable/firecrawl-js@0.0.36\n\n```",
      "metadata": {
        "title": "Node SDK | Firecrawl",
        "url": "https://docs.firecrawl.dev/v0/sdks/node"
      }
    },
    {
      "id": "2e97722c-6d71-4386-abc0-0bd035d91c2a",
      "source": "firecrawl/docs/v0-sdks-node.md",
      "content": "[](https://docs.firecrawl.dev/v0/sdks/node#usage) Usage\n\n1. Get an API key from [firecrawl.dev](https://firecrawl.dev/)\n2. Set the API key as an environment variable named `FIRECRAWL_API_KEY` or pass it as a parameter to the `FirecrawlApp` class.  \nHereâ€™s an example of how to use the SDK with error handling:  \nCopy  \n```js\nimport FirecrawlApp from '@mendable/firecrawl-js';\n\n// Initialize the FirecrawlApp with your API key\nconst app = new FirecrawlApp({ apiKey: \"YOUR_API_KEY\" });\n\n// Scrape a single URL\nconst url = 'https://docs.firecrawl.dev';\nconst scrapedData = await app.scrapeUrl(url);\n\n// Crawl a website\nconst crawlUrl = 'https://docs.firecrawl.dev';\nconst params = {\ncrawlerOptions: {\nexcludes: ['blog/'],\nincludes: [], // leave empty for all pages\nlimit: 1000,\n},\npageOptions: {\nonlyMainContent: true\n}\n};\n\nconst crawlResult = await app.crawlUrl(crawlUrl, params);\n\n```",
      "metadata": {
        "title": "Node SDK | Firecrawl",
        "url": "https://docs.firecrawl.dev/v0/sdks/node"
      }
    },
    {
      "id": "e3f85ea8-8220-4981-b68d-15a99117a11b",
      "source": "firecrawl/docs/v0-sdks-node.md",
      "content": "[](https://docs.firecrawl.dev/v0/sdks/node#usage) Usage > [](https://docs.firecrawl.dev/v0/sdks/node#scraping-a-url) Scraping a URL\n\nTo scrape a single URL with error handling, use the `scrapeUrl` method. It takes the URL as a parameter and returns the scraped data as a dictionary.  \nCopy  \n```js\nconst url = 'https://example.com';\nconst scrapedData = await app.scrapeUrl(url);\n\n```",
      "metadata": {
        "title": "Node SDK | Firecrawl",
        "url": "https://docs.firecrawl.dev/v0/sdks/node"
      }
    },
    {
      "id": "863d3992-fbbb-41a1-9b8f-aa6fb17a77de",
      "source": "firecrawl/docs/v0-sdks-node.md",
      "content": "[](https://docs.firecrawl.dev/v0/sdks/node#usage) Usage > [](https://docs.firecrawl.dev/v0/sdks/node#crawling-a-website) Crawling a Website\n\nTo crawl a website with error handling, use the `crawlUrl` method. It takes the starting URL and optional parameters as arguments. The `params` argument allows you to specify additional options for the crawl job, such as the maximum number of pages to crawl, allowed domains, and the output format.  \nCopy  \n```js\nconst crawlUrl = 'https://example.com';\n\nconst params = {\ncrawlerOptions: {\nexcludes: ['blog/'],\nincludes: [], // leave empty for all pages\nlimit: 1000,\n},\npageOptions: {\nonlyMainContent: true\n}\n};\n\nconst waitUntilDone = true;\nconst pollInterval = 5;\n\nconst crawlResult = await app.crawlUrl(\ncrawlUrl,\nparams,\nwaitUntilDone,\npollInterval\n);\n\n```",
      "metadata": {
        "title": "Node SDK | Firecrawl",
        "url": "https://docs.firecrawl.dev/v0/sdks/node"
      }
    },
    {
      "id": "ec6050d7-07d2-4372-ba6e-02c62a17051f",
      "source": "firecrawl/docs/v0-sdks-node.md",
      "content": "[](https://docs.firecrawl.dev/v0/sdks/node#usage) Usage > [](https://docs.firecrawl.dev/v0/sdks/node#checking-crawl-status) Checking Crawl Status\n\nTo check the status of a crawl job with error handling, use the `checkCrawlStatus` method. It takes the job ID as a parameter and returns the current status of the crawl job.  \nCopy  \n```js\nconst status = await app.checkCrawlStatus(jobId);\n\n```",
      "metadata": {
        "title": "Node SDK | Firecrawl",
        "url": "https://docs.firecrawl.dev/v0/sdks/node"
      }
    },
    {
      "id": "d9d2bf2a-6493-4a1b-ae4e-8366e90b961b",
      "source": "firecrawl/docs/v0-sdks-node.md",
      "content": "[](https://docs.firecrawl.dev/v0/sdks/node#usage) Usage > [](https://docs.firecrawl.dev/v0/sdks/node#extracting-structured-data-from-a-url) Extracting structured data from a URL\n\nWith LLM extraction, you can easily extract structured data from any URL. We support zod schema to make it easier for you too. Here is how you to use it:  \nCopy  \n```js\nimport FirecrawlApp from \"@mendable/firecrawl-js\";\nimport { z } from \"zod\";\n\nconst app = new FirecrawlApp({\napiKey: \"fc-YOUR_API_KEY\",\n});\n\n// Define schema to extract contents into\nconst schema = z.object({\ntop: z\n.array(\nz.object({\ntitle: z.string(),\npoints: z.number(),\nby: z.string(),\ncommentsURL: z.string(),\n})\n)\n.length(5)\n.describe(\"Top 5 stories on Hacker News\"),\n});\n\nconst scrapeResult = await app.scrapeUrl(\"https://firecrawl.dev\", {\nextractorOptions: { extractionSchema: schema },\n});\n\nconsole.log(scrapeResult.data[\"llm_extraction\"]);\n\n```",
      "metadata": {
        "title": "Node SDK | Firecrawl",
        "url": "https://docs.firecrawl.dev/v0/sdks/node"
      }
    },
    {
      "id": "ea430c99-c201-47ad-a726-d12e7ca27f3f",
      "source": "firecrawl/docs/v0-sdks-node.md",
      "content": "[](https://docs.firecrawl.dev/v0/sdks/node#usage) Usage > [](https://docs.firecrawl.dev/v0/sdks/node#search-for-a-query) Search for a query\n\nWith the `search` method, you can search for a query in a search engine and get the top results along with the page content for each result. The method takes the query as a parameter and returns the search results.  \nCopy  \n```js\nconst query = 'what is mendable?';\nconst searchResults = await app.search(query, {\npageOptions: {\nfetchPageContent: true // Fetch the page content for each search result\n}\n});\n\n```",
      "metadata": {
        "title": "Node SDK | Firecrawl",
        "url": "https://docs.firecrawl.dev/v0/sdks/node"
      }
    },
    {
      "id": "1085e4ae-5de6-448b-a40b-a010ae278a45",
      "source": "firecrawl/docs/v0-sdks-node.md",
      "content": "[](https://docs.firecrawl.dev/v0/sdks/node#error-handling) Error Handling\n\nThe SDK handles errors returned by the Firecrawl API and raises appropriate exceptions. If an error occurs during a request, an exception will be raised with a descriptive error message. The examples above demonstrate how to handle these errors using `try/catch` blocks.  \n[Suggest edits](https://github.com/hellofirecrawl/docs/edit/main/v0/sdks/node.mdx) [Raise issue](https://github.com/hellofirecrawl/docs/issues/new?title=Issue%20on%20docs&body=Path:%20/v0/sdks/node)  \n[Python](https://docs.firecrawl.dev/v0/sdks/python) [Go](https://docs.firecrawl.dev/v0/sdks/go)  \nOn this page  \n- [Installation](https://docs.firecrawl.dev/v0/sdks/node#installation)\n- [Usage](https://docs.firecrawl.dev/v0/sdks/node#usage)\n- [Scraping a URL](https://docs.firecrawl.dev/v0/sdks/node#scraping-a-url)\n- [Crawling a Website](https://docs.firecrawl.dev/v0/sdks/node#crawling-a-website)\n- [Checking Crawl Status](https://docs.firecrawl.dev/v0/sdks/node#checking-crawl-status)\n- [Extracting structured data from a URL](https://docs.firecrawl.dev/v0/sdks/node#extracting-structured-data-from-a-url)\n- [Search for a query](https://docs.firecrawl.dev/v0/sdks/node#search-for-a-query)\n- [Error Handling](https://docs.firecrawl.dev/v0/sdks/node#error-handling)",
      "metadata": {
        "title": "Node SDK | Firecrawl",
        "url": "https://docs.firecrawl.dev/v0/sdks/node"
      }
    },
    {
      "id": "42adbd1c-8251-4146-ab9d-24a9b514e754",
      "source": "firecrawl/docs/v0-features-scrape.md",
      "content": "---\ntitle: Scrape | Firecrawl\nurl: https://docs.firecrawl.dev/v0/features/scrape\n---  \n[Firecrawl Docs home page![light logo](https://mintlify.s3.us-west-1.amazonaws.com/firecrawl/logo/light.svg)![dark logo](https://mintlify.s3.us-west-1.amazonaws.com/firecrawl/logo/dark.svg)](https://firecrawl.dev/)  \nv0  \nSearch or ask...  \nCtrl K  \nSearch...  \nNavigation  \nFeatures  \nScrape  \n[Documentation](https://docs.firecrawl.dev/v0/introduction) [SDKs](https://docs.firecrawl.dev/v0/sdks/python) [Learn](https://www.firecrawl.dev/blog/category/tutorials) [API Reference](https://docs.firecrawl.dev/v0/api-reference/introduction)",
      "metadata": {
        "title": "Scrape | Firecrawl",
        "url": "https://docs.firecrawl.dev/v0/features/scrape"
      }
    },
    {
      "id": "7af1033d-ddc8-4a78-9ed6-0802452307fa",
      "source": "firecrawl/docs/v0-features-scrape.md",
      "content": "[](https://docs.firecrawl.dev/v0/features/scrape#scraping-with-firecrawl) Scraping with Firecrawl\n\nFirecrawl converts web pages into markdown, ideal for LLM applications. Hereâ€™s why:  \n1. **Complexities Managed:**\nHandles proxies, caching, rate limits, and JavaScript-blocked content for smooth scraping.  \n2. **Dynamic Content:**\nGathers data from JavaScript-rendered websites, pdfs, images etc.  \n3. **Markdown or Structured data conversion:**\nConverts collected data into clean markdown or structured output, perfect for LLM processing or any other task.  \nFor more details, refer to the [Scrape Endpoint API Reference](https://docs.firecrawl.dev/api-reference/endpoint/scrape).",
      "metadata": {
        "title": "Scrape | Firecrawl",
        "url": "https://docs.firecrawl.dev/v0/features/scrape"
      }
    },
    {
      "id": "0120e3e3-f774-4a58-9d4f-7dddc18e2c52",
      "source": "firecrawl/docs/v0-features-scrape.md",
      "content": "[](https://docs.firecrawl.dev/v0/features/scrape#scrape-a-url) Scrape a URL > [](https://docs.firecrawl.dev/v0/features/scrape#scrape-endpoint) /scrape endpoint\n\nUsed to scrape a URL and get its content.",
      "metadata": {
        "title": "Scrape | Firecrawl",
        "url": "https://docs.firecrawl.dev/v0/features/scrape"
      }
    },
    {
      "id": "10fdbadd-fd86-422b-9a48-e3c17aefedf1",
      "source": "firecrawl/docs/v0-features-scrape.md",
      "content": "[](https://docs.firecrawl.dev/v0/features/scrape#scrape-a-url) Scrape a URL > [](https://docs.firecrawl.dev/v0/features/scrape#installation) Installation\n\nPython  \nJavaScript  \nGo  \nRust  \nCopy  \n```bash\npip install firecrawl-py\n\n```",
      "metadata": {
        "title": "Scrape | Firecrawl",
        "url": "https://docs.firecrawl.dev/v0/features/scrape"
      }
    },
    {
      "id": "4e911e16-e77b-4044-a78e-ca3ef573c403",
      "source": "firecrawl/docs/v0-features-scrape.md",
      "content": "[](https://docs.firecrawl.dev/v0/features/scrape#scrape-a-url) Scrape a URL > [](https://docs.firecrawl.dev/v0/features/scrape#usage) Usage\n\nPython  \nJavaScript  \nGo  \nRust  \ncURL  \nCopy  \n```python\nfrom firecrawl import FirecrawlApp\n\napp = FirecrawlApp(api_key=\"YOUR_API_KEY\")\n\ncontent = app.scrape_url(\"https://mendable.ai\")\n\n```",
      "metadata": {
        "title": "Scrape | Firecrawl",
        "url": "https://docs.firecrawl.dev/v0/features/scrape"
      }
    },
    {
      "id": "9c13291b-899f-4fa5-9130-dbf7ef43eaf4",
      "source": "firecrawl/docs/v0-features-scrape.md",
      "content": "[](https://docs.firecrawl.dev/v0/features/scrape#scrape-a-url) Scrape a URL > [](https://docs.firecrawl.dev/v0/features/scrape#response) Response\n\nSDKs will return the data object directly. cURL will return the payload exactly as shown below.  \nCopy  \n```json\n{\n\"success\": true,\n\"data\": {\n\"content\": \"Raw Content \",\n\"markdown\": \"# Markdown Content\",\n\"provider\": \"web-scraper\",\n\"metadata\": {\n\"title\": \"Mendable | AI for CX and Sales\",\n\"description\": \"AI for CX and Sales\",\n\"language\": null,\n\"sourceURL\": \"https://www.mendable.ai/\"\n}\n}\n}\n\n```  \n[Suggest edits](https://github.com/hellofirecrawl/docs/edit/main/v0/features/scrape.mdx) [Raise issue](https://github.com/hellofirecrawl/docs/issues/new?title=Issue%20on%20docs&body=Path:%20/v0/features/scrape)  \n[Integrations](https://docs.firecrawl.dev/integrations) [Crawl](https://docs.firecrawl.dev/v0/features/crawl)  \nOn this page  \n- [Scraping with Firecrawl](https://docs.firecrawl.dev/v0/features/scrape#scraping-with-firecrawl)\n- [Scrape a URL](https://docs.firecrawl.dev/v0/features/scrape#scrape-a-url)\n- [/scrape endpoint](https://docs.firecrawl.dev/v0/features/scrape#scrape-endpoint)\n- [Installation](https://docs.firecrawl.dev/v0/features/scrape#installation)\n- [Usage](https://docs.firecrawl.dev/v0/features/scrape#usage)\n- [Response](https://docs.firecrawl.dev/v0/features/scrape#response)",
      "metadata": {
        "title": "Scrape | Firecrawl",
        "url": "https://docs.firecrawl.dev/v0/features/scrape"
      }
    },
    {
      "id": "3e141eb2-fe25-4682-9fa0-6f4cab432276",
      "source": "firecrawl/docs/contributing-guide.md",
      "content": "---\ntitle: Running locally | Firecrawl\nurl: https://docs.firecrawl.dev/contributing/guide\n---  \n[Firecrawl Docs home page![light logo](https://mintlify.s3.us-west-1.amazonaws.com/firecrawl/logo/light.svg)![dark logo](https://mintlify.s3.us-west-1.amazonaws.com/firecrawl/logo/dark.svg)](https://firecrawl.dev/)  \nv1  \nSearch or ask...  \nCtrl K  \nSearch...  \nNavigation  \nContributing  \nRunning locally  \n[Documentation](https://docs.firecrawl.dev/introduction) [SDKs](https://docs.firecrawl.dev/sdks/overview) [Learn](https://www.firecrawl.dev/blog/category/tutorials) [API Reference](https://docs.firecrawl.dev/api-reference/introduction)  \nWelcome to [Firecrawl](https://firecrawl.dev/) ðŸ”¥! Here are some instructions on how to get the project locally so you can run it on your own and contribute.  \nIf youâ€™re contributing, note that the process is similar to other open-source repos, i.e., fork Firecrawl, make changes, run tests, PR.  \nIf you have any questions or would like help getting on board, join our Discord community [here](https://discord.gg/gSmWdAkdwd) for more information or submit an issue on Github [here](https://github.com/mendableai/firecrawl/issues/new/choose)!",
      "metadata": {
        "title": "Running locally | Firecrawl",
        "url": "https://docs.firecrawl.dev/contributing/guide"
      }
    },
    {
      "id": "1f329c77-0baf-4fb5-9299-d937648a1224",
      "source": "firecrawl/docs/contributing-guide.md",
      "content": "[](https://docs.firecrawl.dev/contributing/guide#running-the-project-locally) Running the project locally\n\nFirst, start by installing dependencies:  \n1. node.js [instructions](https://nodejs.org/en/learn/getting-started/how-to-install-nodejs)\n2. pnpm [instructions](https://pnpm.io/installation)\n3. redis [instructions](https://redis.io/docs/latest/operate/oss_and_stack/install/install-redis/)  \nSet environment variables in a `.env` file in the `/apps/api/` directory. You can copy over the template in `.env.example`.  \nTo start, we wonâ€™t set up authentication, or any optional sub services (pdf parsing, JS blocking support, AI features)  \nCopy  \n```\n# ./apps/api/.env\n\n# ===== Required ENVS ======\nNUM_WORKERS_PER_QUEUE=8\nPORT=3002\nHOST=0.0.0.0\n\n#for self-hosting using docker, use redis://redis:6379. For running locally, use redis://localhost:6379\nREDIS_URL=redis://localhost:6379\n\n#for self-hosting using docker, use redis://redis:6379. For running locally, use redis://localhost:6379\nREDIS_RATE_LIMIT_URL=redis://localhost:6379\nPLAYWRIGHT_MICROSERVICE_URL=http://playwright-service:3000/html\n\n## To turn on DB authentication, you need to set up supabase.\nUSE_DB_AUTHENTICATION=false\n\n# ===== Optional ENVS ======\n\n# Supabase Setup (used to support DB authentication, advanced logging, etc.)\nSUPABASE_ANON_TOKEN=\nSUPABASE_URL=\nSUPABASE_SERVICE_TOKEN=\n\n# Other Optionals\n# use if you've set up authentication and want to test with a real API key\nTEST_API_KEY=\n# set if you'd like to test the scraping rate limit\nRATE_LIMIT_TEST_API_KEY_SCRAPE=\n# set if you'd like to test the crawling rate limit\nRATE_LIMIT_TEST_API_KEY_CRAWL=\n# set if you'd like to use scraping Be to handle JS blocking\nSCRAPING_BEE_API_KEY=\n# add for LLM dependednt features (image alt generation, etc.)\nOPENAI_API_KEY=\nBULL_AUTH_KEY=@\n# use if you're configuring basic logging with logtail\nLOGTAIL_KEY=\n# set if you have a llamaparse key you'd like to use to parse pdfs\nLLAMAPARSE_API_KEY=\n# set if you'd like to send slack server health status messages\nSLACK_WEBHOOK_URL=\n# set if you'd like to send posthog events like job logs\nPOSTHOG_API_KEY=\n# set if you'd like to send posthog events like job logs\nPOSTHOG_HOST=\n\n# set if you'd like to use the fire engine closed beta\nFIRE_ENGINE_BETA_URL=\n\n# Proxy Settings for Playwright (Alternative you can can use a proxy service like oxylabs, which rotates IPs for you on every request)\nPROXY_SERVER=\nPROXY_USERNAME=\nPROXY_PASSWORD=\n# set if you'd like to block media requests to save proxy bandwidth\nBLOCK_MEDIA=\n\n# Set this to the URL of your webhook when using the self-hosted version of FireCrawl\nSELF_HOSTED_WEBHOOK_URL=\n\n# Resend API Key for transactional emails\nRESEND_API_KEY=\n\n# LOGGING_LEVEL determines the verbosity of logs that the system will output.\n# Available levels are:\n# NONE - No logs will be output.\n# ERROR - For logging error messages that indicate a failure in a specific operation.\n# WARN - For logging potentially harmful situations that are not necessarily errors.\n# INFO - For logging informational messages that highlight the progress of the application.\n# DEBUG - For logging detailed information on the flow through the system, primarily used for debugging.\n# TRACE - For logging more detailed information than the DEBUG level.\n# Set LOGGING_LEVEL to one of the above options to control logging output.\nLOGGING_LEVEL=INFO\n\n```",
      "metadata": {
        "title": "Running locally | Firecrawl",
        "url": "https://docs.firecrawl.dev/contributing/guide"
      }
    },
    {
      "id": "5a1ea5f4-6d2b-41ab-9683-dd9bebbb3eb8",
      "source": "firecrawl/docs/contributing-guide.md",
      "content": "[](https://docs.firecrawl.dev/contributing/guide#running-the-project-locally) Running the project locally > [](https://docs.firecrawl.dev/contributing/guide#installing-dependencies) Installing dependencies\n\nFirst, install the dependencies using pnpm.  \nCopy  \n```bash\n# cd apps/api # to make sure you're in the right folder\npnpm install # make sure you have pnpm version 9+!\n\n```",
      "metadata": {
        "title": "Running locally | Firecrawl",
        "url": "https://docs.firecrawl.dev/contributing/guide"
      }
    },
    {
      "id": "2b6ecdcc-1488-45bf-841e-f8f39aa74db5",
      "source": "firecrawl/docs/contributing-guide.md",
      "content": "[](https://docs.firecrawl.dev/contributing/guide#running-the-project-locally) Running the project locally > [](https://docs.firecrawl.dev/contributing/guide#running-the-project) Running the project\n\nYouâ€™re going to need to open 3 terminals for running the services. Here is [a video guide accurate as of Oct 2024](https://youtu.be/LHqg5QNI4UY) (optional: 4 terminals for running the services and testing).",
      "metadata": {
        "title": "Running locally | Firecrawl",
        "url": "https://docs.firecrawl.dev/contributing/guide"
      }
    },
    {
      "id": "461e4f75-6973-42bf-aa53-a183a0757ae4",
      "source": "firecrawl/docs/contributing-guide.md",
      "content": "[](https://docs.firecrawl.dev/contributing/guide#running-the-project-locally) Running the project locally > [](https://docs.firecrawl.dev/contributing/guide#terminal-1-setting-up-redis) Terminal 1 - setting up redis\n\nRun the command anywhere within your project  \nCopy  \n```bash\nredis-server\n\n```",
      "metadata": {
        "title": "Running locally | Firecrawl",
        "url": "https://docs.firecrawl.dev/contributing/guide"
      }
    },
    {
      "id": "7e8e11df-8a4d-4b28-bcef-0dfc05452791",
      "source": "firecrawl/docs/contributing-guide.md",
      "content": "[](https://docs.firecrawl.dev/contributing/guide#running-the-project-locally) Running the project locally > [](https://docs.firecrawl.dev/contributing/guide#terminal-2-setting-up-workers) Terminal 2 - setting up workers\n\nNow, navigate to the apps/api/ directory and run:  \nCopy  \n```bash\npnpm run workers\n# if you are going to use the [llm-extract feature](https://github.com/mendableai/firecrawl/pull/586/), you should also export OPENAI_API_KEY=sk-______\n\n```  \nThis will start the workers who are responsible for processing crawl jobs.",
      "metadata": {
        "title": "Running locally | Firecrawl",
        "url": "https://docs.firecrawl.dev/contributing/guide"
      }
    },
    {
      "id": "d4c38594-1723-477e-a2da-e8abcd874c43",
      "source": "firecrawl/docs/contributing-guide.md",
      "content": "[](https://docs.firecrawl.dev/contributing/guide#running-the-project-locally) Running the project locally > [](https://docs.firecrawl.dev/contributing/guide#terminal-3-setting-up-the-main-server) Terminal 3 - setting up the main server\n\nTo do this, navigate to the apps/api/ directory. If you havenâ€™t installed pnpm already, you can do so here: [https://pnpm.io/installation](https://pnpm.io/installation)  \nNext, run your server with:  \nCopy  \n```bash\npnpm run start\n\n```",
      "metadata": {
        "title": "Running locally | Firecrawl",
        "url": "https://docs.firecrawl.dev/contributing/guide"
      }
    },
    {
      "id": "12cceccc-5341-48dc-8d28-14d2d2eef733",
      "source": "firecrawl/docs/contributing-guide.md",
      "content": "[](https://docs.firecrawl.dev/contributing/guide#running-the-project-locally) Running the project locally > [](https://docs.firecrawl.dev/contributing/guide#optional-terminal-4-sending-our-first-request) _(Optional)_ Terminal 4 - sending our first request\n\nAlright, now letâ€™s send our first request.  \nCopy  \n```curl\ncurl -X GET http://localhost:3002/test\n\n```  \nThis should return the response Hello, world!  \nIf youâ€™d like to test the crawl endpoint, you can run this  \nCopy  \n```curl\ncurl -X POST http://localhost:3002/v0/crawl \\\n-H 'Content-Type: application/json' \\\n-d '{\n\"url\": \"https://docs.firecrawl.dev\"\n}'\n\n```",
      "metadata": {
        "title": "Running locally | Firecrawl",
        "url": "https://docs.firecrawl.dev/contributing/guide"
      }
    },
    {
      "id": "a629fea2-b3a5-4f88-b8ab-5025f243c07f",
      "source": "firecrawl/docs/contributing-guide.md",
      "content": "[](https://docs.firecrawl.dev/contributing/guide#tests) Tests:\n\nThe best way to do this is run the test with `npm run test:local-no-auth` if youâ€™d like to run the tests without authentication.  \nIf youâ€™d like to run the tests with authentication, run `npm run test:prod`  \n[Suggest edits](https://github.com/hellofirecrawl/docs/edit/main/contributing/guide.mdx) [Raise issue](https://github.com/hellofirecrawl/docs/issues/new?title=Issue%20on%20docs&body=Path:%20/contributing/guide)  \n[Open Source vs Cloud](https://docs.firecrawl.dev/contributing/open-source-or-cloud) [Self-hosting](https://docs.firecrawl.dev/contributing/self-host)  \nOn this page  \n- [Running the project locally](https://docs.firecrawl.dev/contributing/guide#running-the-project-locally)\n- [Installing dependencies](https://docs.firecrawl.dev/contributing/guide#installing-dependencies)\n- [Running the project](https://docs.firecrawl.dev/contributing/guide#running-the-project)\n- [Terminal 1 - setting up redis](https://docs.firecrawl.dev/contributing/guide#terminal-1-setting-up-redis)\n- [Terminal 2 - setting up workers](https://docs.firecrawl.dev/contributing/guide#terminal-2-setting-up-workers)\n- [Terminal 3 - setting up the main server](https://docs.firecrawl.dev/contributing/guide#terminal-3-setting-up-the-main-server)\n- [(Optional) Terminal 4 - sending our first request](https://docs.firecrawl.dev/contributing/guide#optional-terminal-4-sending-our-first-request)\n- [Tests:](https://docs.firecrawl.dev/contributing/guide#tests)",
      "metadata": {
        "title": "Running locally | Firecrawl",
        "url": "https://docs.firecrawl.dev/contributing/guide"
      }
    },
    {
      "id": "6114c1d2-086b-4ac6-bec1-0a38ce2173e4",
      "source": "firecrawl/docs/api-reference-endpoint-extract-get.md",
      "content": "---\ntitle: Firecrawl Docs\nurl: https://docs.firecrawl.dev/api-reference/endpoint/extract-get\n---  \n[Firecrawl Docs home page![light logo](https://mintlify.s3.us-west-1.amazonaws.com/firecrawl/logo/light.svg)![dark logo](https://mintlify.s3.us-west-1.amazonaws.com/firecrawl/logo/dark.svg)](https://firecrawl.dev/)  \nv1  \nSearch or ask...  \nCtrl K  \nSearch...  \nNavigation  \nExtract Endpoints  \nGet Extract Status  \n[Documentation](https://docs.firecrawl.dev/introduction) [SDKs](https://docs.firecrawl.dev/sdks/overview) [Learn](https://www.firecrawl.dev/blog/category/tutorials) [API Reference](https://docs.firecrawl.dev/api-reference/introduction)  \nGET  \n/  \nextract  \n/  \n{id}  \nTry it  \ncURL  \nPython  \nJavaScript  \nPHP  \nGo  \nJava  \nCopy  \n```\ncurl --request GET \\\n--url https://api.firecrawl.dev/v1/extract/{id} \\\n--header 'Authorization: Bearer <token>'\n```  \n200  \nCopy  \n```\n{\n\"success\": true,\n\"data\": {},\n\"status\": \"completed\",\n\"expiresAt\": \"2023-11-07T05:31:56Z\"\n}\n```",
      "metadata": {
        "title": "Firecrawl Docs",
        "url": "https://docs.firecrawl.dev/api-reference/endpoint/extract-get"
      }
    },
    {
      "id": "959229f7-3ed6-40d3-9086-4818d131cfcd",
      "source": "firecrawl/docs/api-reference-endpoint-extract-get.md",
      "content": "Authorizations\n\n[](https://docs.firecrawl.dev/api-reference/endpoint/extract-get#authorization-authorization)  \nAuthorization  \nstring  \nheader  \nrequired  \nBearer authentication header of the form `Bearer <token>`, where `<token>` is your auth token.",
      "metadata": {
        "title": "Firecrawl Docs",
        "url": "https://docs.firecrawl.dev/api-reference/endpoint/extract-get"
      }
    },
    {
      "id": "3932e4cb-4a15-4b99-9f6d-22285e7c025f",
      "source": "firecrawl/docs/api-reference-endpoint-extract-get.md",
      "content": "Path Parameters\n\n[](https://docs.firecrawl.dev/api-reference/endpoint/extract-get#parameter-id)  \nid  \nstring  \nrequired  \nThe ID of the extract job",
      "metadata": {
        "title": "Firecrawl Docs",
        "url": "https://docs.firecrawl.dev/api-reference/endpoint/extract-get"
      }
    },
    {
      "id": "7a4ee53c-d92b-49d1-b442-8a9c18ea71a3",
      "source": "firecrawl/docs/api-reference-endpoint-extract-get.md",
      "content": "Response\n\n200 - application/json  \n[](https://docs.firecrawl.dev/api-reference/endpoint/extract-get#response-success)  \nsuccess  \nboolean  \n[](https://docs.firecrawl.dev/api-reference/endpoint/extract-get#response-data)  \ndata  \nobject  \n[](https://docs.firecrawl.dev/api-reference/endpoint/extract-get#response-status)  \nstatus  \nenum<string>  \nThe current status of the extract job  \nAvailable options:  \n`completed`,  \n`pending`,  \n`failed`,  \n`cancelled`  \n[](https://docs.firecrawl.dev/api-reference/endpoint/extract-get#response-expires-at)  \nexpiresAt  \nstring  \n[Suggest edits](https://github.com/hellofirecrawl/docs/edit/main/api-reference/endpoint/extract-get.mdx) [Raise issue](https://github.com/hellofirecrawl/docs/issues/new?title=Issue%20on%20docs&body=Path:%20/api-reference/endpoint/extract-get)  \n[Extract](https://docs.firecrawl.dev/api-reference/endpoint/extract) [Search](https://docs.firecrawl.dev/api-reference/endpoint/search)  \ncURL  \nPython  \nJavaScript  \nPHP  \nGo  \nJava  \nCopy  \n```\ncurl --request GET \\\n--url https://api.firecrawl.dev/v1/extract/{id} \\\n--header 'Authorization: Bearer <token>'\n```  \n200  \nCopy  \n```\n{\n\"success\": true,\n\"data\": {},\n\"status\": \"completed\",\n\"expiresAt\": \"2023-11-07T05:31:56Z\"\n}\n```",
      "metadata": {
        "title": "Firecrawl Docs",
        "url": "https://docs.firecrawl.dev/api-reference/endpoint/extract-get"
      }
    },
    {
      "id": "30d7ac6c-dcdd-41e9-b1d0-66ccaa3679cf",
      "source": "firecrawl/docs/v0-sdks-rust.md",
      "content": "---\ntitle: Rust SDK | Firecrawl\nurl: https://docs.firecrawl.dev/v0/sdks/rust\n---  \n[Firecrawl Docs home page![light logo](https://mintlify.s3.us-west-1.amazonaws.com/firecrawl/logo/light.svg)![dark logo](https://mintlify.s3.us-west-1.amazonaws.com/firecrawl/logo/dark.svg)](https://firecrawl.dev/)  \nv0  \nSearch or ask...  \nCtrl K  \nSearch...  \nNavigation  \nSDKs  \nRust  \n[Documentation](https://docs.firecrawl.dev/v0/introduction) [SDKs](https://docs.firecrawl.dev/v0/sdks/python) [Learn](https://www.firecrawl.dev/blog/category/tutorials) [API Reference](https://docs.firecrawl.dev/v0/api-reference/introduction)  \n> Note: this is using [v0 version of the Firecrawl API](https://docs.firecrawl.dev/v0/introduction) which is being deprecated. We recommend switching to [v1](https://docs.firecrawl.dev/sdks/rust).",
      "metadata": {
        "title": "Rust SDK | Firecrawl",
        "url": "https://docs.firecrawl.dev/v0/sdks/rust"
      }
    },
    {
      "id": "916b72a8-4131-4b13-9219-475401aaddd9",
      "source": "firecrawl/docs/v0-sdks-rust.md",
      "content": "[](https://docs.firecrawl.dev/v0/sdks/rust#installation) Installation\n\nTo install the Firecrawl Rust SDK, add the following to your `Cargo.toml`:  \nCopy  \n```toml\n[dependencies]\nfirecrawl = \"^0.1\"\ntokio = { version = \"^1\", features = [\"full\"] }\nserde = { version = \"^1.0\", features = [\"derive\"] }\nserde_json = \"^1.0\"\nuuid = { version = \"^1.10\", features = [\"v4\"] }\n\n[build-dependencies]\ntokio = { version = \"1\", features = [\"full\"] }\n\n```",
      "metadata": {
        "title": "Rust SDK | Firecrawl",
        "url": "https://docs.firecrawl.dev/v0/sdks/rust"
      }
    },
    {
      "id": "b5ebed2a-7389-42db-a805-f78f26c5c445",
      "source": "firecrawl/docs/v0-sdks-rust.md",
      "content": "[](https://docs.firecrawl.dev/v0/sdks/rust#usage) Usage\n\n1. Get an API key from [firecrawl.dev](https://firecrawl.dev/)\n2. Set the API key as an environment variable named `FIRECRAWL_API_KEY` or pass it as a parameter to the `FirecrawlApp` struct.  \nHereâ€™s an example of how to use the SDK in Rust:  \nCopy  \n```rust\nuse firecrawl::FirecrawlApp;\n\n#[tokio::main]\nasync fn main() {\nlet api_key = \"YOUR_API_KEY\";\nlet api_url = \"https://api.firecrawl.dev\";\nlet app = FirecrawlApp::new(api_key, api_url).expect(\"Failed to initialize FirecrawlApp\");\n\n// Scrape a single URL\nlet scrape_result = app.scrape_url(\"https://docs.firecrawl.dev\", None).await;\nmatch scrape_result {\nOk(data) => println!(\"Scraped Data: {}\", data),\nErr(e) => eprintln!(\"Error occurred while scraping: {}\", e),\n}\n// Crawl a website\nlet crawl_params = json!({\n\"pageOptions\": {\n\"onlyMainContent\": true\n}\n});\n\nlet crawl_result = app.crawl_url(\"https://docs.firecrawl.dev\", Some(crawl_params)).await;\n\nmatch crawl_result {\nOk(data) => println!(\"Crawl Result: {}\", data),\nErr(e) => eprintln!(\"Error occurred while crawling: {}\", e),\n}\n}\n\n```",
      "metadata": {
        "title": "Rust SDK | Firecrawl",
        "url": "https://docs.firecrawl.dev/v0/sdks/rust"
      }
    },
    {
      "id": "19464c40-8559-439d-9f9a-31c27e50f168",
      "source": "firecrawl/docs/v0-sdks-rust.md",
      "content": "[](https://docs.firecrawl.dev/v0/sdks/rust#usage) Usage > [](https://docs.firecrawl.dev/v0/sdks/rust#scraping-a-url) Scraping a URL\n\nTo scrape a single URL with error handling, use the `scrape_url` method. It takes the URL as a parameter and returns the scraped data as a `serde_json::Value`.  \nCopy  \n```rust\nlet scrape_result = app.scrape_url(\"https://docs.firecrawl.dev\", None).await;\n\nmatch scrape_result {\nOk(data) => println!(\"Scraped Data: {}\", data),\nErr(e) => eprintln!(\"Failed to scrape URL: {}\", e),\n}\n\n```",
      "metadata": {
        "title": "Rust SDK | Firecrawl",
        "url": "https://docs.firecrawl.dev/v0/sdks/rust"
      }
    },
    {
      "id": "2423aee0-57be-4ede-ae26-c12cd45cd360",
      "source": "firecrawl/docs/v0-sdks-rust.md",
      "content": "[](https://docs.firecrawl.dev/v0/sdks/rust#usage) Usage > [](https://docs.firecrawl.dev/v0/sdks/rust#crawling-a-website) Crawling a Website\n\nTo crawl a website, use the `crawl_url` method. It takes the starting URL and optional parameters as arguments. The `params` argument allows you to specify additional options for the crawl job, such as the maximum number of pages to crawl, allowed domains, and the output format.  \nCopy  \n```rust\nlet crawl_params = json!({\n\"crawlerOptions\": {\n\"excludes\": [\"blog/\"],\n\"includes\": [], // leave empty for all pages\n\"limit\": 1000\n},\n\"pageOptions\": {\n\"onlyMainContent\": true\n}\n});\nlet crawl_result = app.crawl_url(\"https://docs.firecrawl.dev\", Some(crawl_params)).await;\n\nmatch crawl_result {\nOk(data) => println!(\"Crawl Result: {}\", data),\nErr(e) => eprintln!(\"Failed to crawl URL: {}\", e),\n}\n\n```",
      "metadata": {
        "title": "Rust SDK | Firecrawl",
        "url": "https://docs.firecrawl.dev/v0/sdks/rust"
      }
    },
    {
      "id": "4a408417-6a77-4010-8b3f-500b81e3b9af",
      "source": "firecrawl/docs/v0-sdks-rust.md",
      "content": "[](https://docs.firecrawl.dev/v0/sdks/rust#usage) Usage > [](https://docs.firecrawl.dev/v0/sdks/rust#checking-crawl-status) Checking Crawl Status\n\nTo check the status of a crawl job, use the `check_crawl_status` method. It takes the job ID as a parameter and returns the current status of the crawl job.  \nCopy  \n```rust\nlet job_id = \"your_job_id_here\";\nlet status = app.check_crawl_status(job_id).await;\n\nmatch status {\nOk(data) => println!(\"Crawl Status: {}\", data),\nErr(e) => eprintln!(\"Failed to check crawl status: {}\", e),\n}\n\n```",
      "metadata": {
        "title": "Rust SDK | Firecrawl",
        "url": "https://docs.firecrawl.dev/v0/sdks/rust"
      }
    },
    {
      "id": "dd9ba2f8-add0-42bf-9d00-53adca6bee2d",
      "source": "firecrawl/docs/v0-sdks-rust.md",
      "content": "[](https://docs.firecrawl.dev/v0/sdks/rust#usage) Usage > [](https://docs.firecrawl.dev/v0/sdks/rust#canceling-a-crawl-job) Canceling a Crawl Job\n\nTo cancel a crawl job, use the `cancel_crawl_job` method. It takes the job ID as a parameter and returns the cancellation status of the crawl job.  \nCopy  \n```rust\nlet job_id = \"your_job_id_here\";\nlet canceled = app.cancel_crawl_job(job_id).await;\n\nmatch canceled {\nOk(status) => println!(\"Cancellation Status: {}\", status),\nErr(e) => eprintln!(\"Failed to cancel crawl job: {}\", e),\n}\n\n```",
      "metadata": {
        "title": "Rust SDK | Firecrawl",
        "url": "https://docs.firecrawl.dev/v0/sdks/rust"
      }
    },
    {
      "id": "3ef5f7d4-3cc2-4762-98ae-3e99f751d79e",
      "source": "firecrawl/docs/v0-sdks-rust.md",
      "content": "[](https://docs.firecrawl.dev/v0/sdks/rust#usage) Usage > [](https://docs.firecrawl.dev/v0/sdks/rust#extracting-structured-data-from-a-url) Extracting structured data from a URL\n\nWith LLM extraction, you can easily extract structured data from any URL. Here is how you to use it:  \nCopy  \n```rust\nlet json_schema = json!({\n\"type\": \"object\",\n\"properties\": {\n\"top\": {\n\"type\": \"array\",\n\"items\": {\n\"type\": \"object\",\n\"properties\": {\n\"title\": {\"type\": \"string\"},\n\"points\": {\"type\": \"number\"},\n\"by\": {\"type\": \"string\"},\n\"commentsURL\": {\"type\": \"string\"}\n},\n\"required\": [\"title\", \"points\", \"by\", \"commentsURL\"]\n},\n\"minItems\": 5,\n\"maxItems\": 5,\n\"description\": \"Top 5 stories on Hacker News\"\n}\n},\n\"required\": [\"top\"]\n});\n\nlet llm_extraction_params = json!({\n\"extractorOptions\": {\n\"extractionSchema\": json_schema\n}\n});\n\nlet scrape_result = app.scrape_url(\"https://news.ycombinator.com\", Some(llm_extraction_params)).await;\n\nmatch scrape_result {\nOk(data) => println!(\"LLM Extraction Result: {}\", data),\nErr(e) => eprintln!(\"Failed to perform LLM extraction: {}\", e),\n}\n\n```",
      "metadata": {
        "title": "Rust SDK | Firecrawl",
        "url": "https://docs.firecrawl.dev/v0/sdks/rust"
      }
    },
    {
      "id": "640c2b61-ab3e-4db9-aa3e-9749f6f10920",
      "source": "firecrawl/docs/v0-sdks-rust.md",
      "content": "[](https://docs.firecrawl.dev/v0/sdks/rust#usage) Usage > [](https://docs.firecrawl.dev/v0/sdks/rust#search-for-a-query) Search for a query\n\nTo search the web, get the most relevant results, scrape each page and return the markdown, use the `search` method. The method takes the query as a parameter and returns the search results.  \nCopy  \n```rust\nlet query = \"What is firecrawl?\";\nlet search_result = app.search(query).await;\n\nmatch search_result {\nOk(data) => println!(\"Search Result: {}\", data),\nErr(e) => eprintln!(\"Failed to search: {}\", e),\n}\n\n```",
      "metadata": {
        "title": "Rust SDK | Firecrawl",
        "url": "https://docs.firecrawl.dev/v0/sdks/rust"
      }
    },
    {
      "id": "ce17de46-379c-43c9-869b-f2939cb385d5",
      "source": "firecrawl/docs/v0-sdks-rust.md",
      "content": "[](https://docs.firecrawl.dev/v0/sdks/rust#error-handling) Error Handling\n\nThe SDK handles errors returned by the Firecrawl API and raises appropriate exceptions. If an error occurs during a request, an exception will be raised with a descriptive error message.  \n[Suggest edits](https://github.com/hellofirecrawl/docs/edit/main/v0/sdks/rust.mdx) [Raise issue](https://github.com/hellofirecrawl/docs/issues/new?title=Issue%20on%20docs&body=Path:%20/v0/sdks/rust)  \n[Go](https://docs.firecrawl.dev/v0/sdks/go)  \nOn this page  \n- [Installation](https://docs.firecrawl.dev/v0/sdks/rust#installation)\n- [Usage](https://docs.firecrawl.dev/v0/sdks/rust#usage)\n- [Scraping a URL](https://docs.firecrawl.dev/v0/sdks/rust#scraping-a-url)\n- [Crawling a Website](https://docs.firecrawl.dev/v0/sdks/rust#crawling-a-website)\n- [Checking Crawl Status](https://docs.firecrawl.dev/v0/sdks/rust#checking-crawl-status)\n- [Canceling a Crawl Job](https://docs.firecrawl.dev/v0/sdks/rust#canceling-a-crawl-job)\n- [Extracting structured data from a URL](https://docs.firecrawl.dev/v0/sdks/rust#extracting-structured-data-from-a-url)\n- [Search for a query](https://docs.firecrawl.dev/v0/sdks/rust#search-for-a-query)\n- [Error Handling](https://docs.firecrawl.dev/v0/sdks/rust#error-handling)",
      "metadata": {
        "title": "Rust SDK | Firecrawl",
        "url": "https://docs.firecrawl.dev/v0/sdks/rust"
      }
    },
    {
      "id": "5d57c7bc-e15f-479f-b280-05df03a48559",
      "source": "firecrawl/docs/v0-features-crawl.md",
      "content": "---\ntitle: Crawl | Firecrawl\nurl: https://docs.firecrawl.dev/v0/features/crawl\n---  \n[Firecrawl Docs home page![light logo](https://mintlify.s3.us-west-1.amazonaws.com/firecrawl/logo/light.svg)![dark logo](https://mintlify.s3.us-west-1.amazonaws.com/firecrawl/logo/dark.svg)](https://firecrawl.dev/)  \nv0  \nSearch or ask...  \nCtrl K  \nSearch...  \nNavigation  \nFeatures  \nCrawl  \n[Documentation](https://docs.firecrawl.dev/v0/introduction) [SDKs](https://docs.firecrawl.dev/v0/sdks/python) [Learn](https://www.firecrawl.dev/blog/category/tutorials) [API Reference](https://docs.firecrawl.dev/v0/api-reference/introduction)  \nFirecrawl thoroughly crawls websites, ensuring comprehensive data extraction while bypassing any web blocker mechanisms. Hereâ€™s how it works:  \n1. **URL Analysis:**\nBegins with a specified URL, identifying links by looking at the sitemap and then crawling the website. If no sitemap is found, it will crawl the website following the links.  \n2. **Recursive Traversal:**\nRecursively follows each link to uncover all subpages.  \n3. **Content Scraping:**\nGathers content from every visited page while handling any complexities like JavaScript rendering or rate limits.  \n4. **Result Compilation:**\nConverts collected data into clean markdown or structured output, perfect for LLM processing or any other task.  \nThis method guarantees an exhaustive crawl and data collection from any starting URL.",
      "metadata": {
        "title": "Crawl | Firecrawl",
        "url": "https://docs.firecrawl.dev/v0/features/crawl"
      }
    },
    {
      "id": "e8defd03-35e3-4624-a9ae-45068381efcb",
      "source": "firecrawl/docs/v0-features-crawl.md",
      "content": "[](https://docs.firecrawl.dev/v0/features/crawl#crawling) Crawling > [](https://docs.firecrawl.dev/v0/features/crawl#crawl-endpoint) /crawl endpoint\n\nUsed to crawl a URL and all accessible subpages. This submits a crawl job and returns a job ID to check the status of the crawl.",
      "metadata": {
        "title": "Crawl | Firecrawl",
        "url": "https://docs.firecrawl.dev/v0/features/crawl"
      }
    },
    {
      "id": "e4fa220e-9fec-4622-8dc0-6c49d97e7eae",
      "source": "firecrawl/docs/v0-features-crawl.md",
      "content": "[](https://docs.firecrawl.dev/v0/features/crawl#crawling) Crawling > [](https://docs.firecrawl.dev/v0/features/crawl#installation) Installation\n\nPython  \nJavaScript  \nGo  \nRust  \nCopy  \n```bash\npip install firecrawl-py\n\n```",
      "metadata": {
        "title": "Crawl | Firecrawl",
        "url": "https://docs.firecrawl.dev/v0/features/crawl"
      }
    },
    {
      "id": "197e7793-eb4f-4cb0-b3c5-a14ac87862ad",
      "source": "firecrawl/docs/v0-features-crawl.md",
      "content": "[](https://docs.firecrawl.dev/v0/features/crawl#crawling) Crawling > [](https://docs.firecrawl.dev/v0/features/crawl#usage) Usage\n\nPython  \nJavaScript  \nGo  \nRust  \ncURL  \nCopy  \n```python\nfrom firecrawl import FirecrawlApp\n\napp = FirecrawlApp(api_key=\"YOUR_API_KEY\")\n\ncrawl_result = app.crawl_url('mendable.ai', {'crawlerOptions': {'excludes': ['blog/*']}})\n\n# Get the markdown\nfor result in crawl_result:\nprint(result['markdown'])\n\n```",
      "metadata": {
        "title": "Crawl | Firecrawl",
        "url": "https://docs.firecrawl.dev/v0/features/crawl"
      }
    },
    {
      "id": "a78cd86d-76de-4817-9fb1-9468c10825a3",
      "source": "firecrawl/docs/v0-features-crawl.md",
      "content": "[](https://docs.firecrawl.dev/v0/features/crawl#crawling) Crawling > [](https://docs.firecrawl.dev/v0/features/crawl#job-id-response) Job ID Response\n\nIf you are not using the sdk or prefer to use webhook or a different polling method, you can set the `wait_until_done` to `false`.\nThis will return a jobId.  \nFor cURL, /crawl will always return a jobId where you can use to check the status of the crawl.  \nCopy  \n```json\n{ \"jobId\": \"1234-5678-9101\" }\n\n```",
      "metadata": {
        "title": "Crawl | Firecrawl",
        "url": "https://docs.firecrawl.dev/v0/features/crawl"
      }
    },
    {
      "id": "ef5cd865-99dc-481b-b7eb-3014d3a404d8",
      "source": "firecrawl/docs/v0-features-crawl.md",
      "content": "[](https://docs.firecrawl.dev/v0/features/crawl#crawling) Crawling > [](https://docs.firecrawl.dev/v0/features/crawl#check-crawl-job) Check Crawl Job\n\nUsed to check the status of a crawl job and get its result.  \nPython  \nJavaScript  \nGo  \nRust  \ncURL  \nCopy  \n```python\nstatus = app.check_crawl_status(job_id)\n\n```",
      "metadata": {
        "title": "Crawl | Firecrawl",
        "url": "https://docs.firecrawl.dev/v0/features/crawl"
      }
    },
    {
      "id": "7faabf37-9ec8-4f1e-a1cf-1ea7b5d0ca32",
      "source": "firecrawl/docs/v0-features-crawl.md",
      "content": "[](https://docs.firecrawl.dev/v0/features/crawl#crawling) Crawling > [](https://docs.firecrawl.dev/v0/features/crawl#check-crawl-job) Check Crawl Job > [](https://docs.firecrawl.dev/v0/features/crawl#response) Response\n\nCopy  \n```json\n{\n\"status\": \"completed\",\n\"current\": 22,\n\"total\": 22,\n\"data\": [\\\n{\\\n\"content\": \"Raw Content \",\\\n\"markdown\": \"# Markdown Content\",\\\n\"provider\": \"web-scraper\",\\\n\"metadata\": {\\\n\"title\": \"Mendable | AI for CX and Sales\",\\\n\"description\": \"AI for CX and Sales\",\\\n\"language\": null,\\\n\"sourceURL\": \"https://www.mendable.ai/\"\\\n}\\\n}\\\n]\n}\n\n```  \n[Suggest edits](https://github.com/hellofirecrawl/docs/edit/main/v0/features/crawl.mdx) [Raise issue](https://github.com/hellofirecrawl/docs/issues/new?title=Issue%20on%20docs&body=Path:%20/v0/features/crawl)  \n[Scrape](https://docs.firecrawl.dev/v0/features/scrape) [LLM Extract](https://docs.firecrawl.dev/v0/features/extract)  \nOn this page  \n- [Crawling](https://docs.firecrawl.dev/v0/features/crawl#crawling)\n- [/crawl endpoint](https://docs.firecrawl.dev/v0/features/crawl#crawl-endpoint)\n- [Installation](https://docs.firecrawl.dev/v0/features/crawl#installation)\n- [Usage](https://docs.firecrawl.dev/v0/features/crawl#usage)\n- [Job ID Response](https://docs.firecrawl.dev/v0/features/crawl#job-id-response)\n- [Check Crawl Job](https://docs.firecrawl.dev/v0/features/crawl#check-crawl-job)\n- [Response](https://docs.firecrawl.dev/v0/features/crawl#response)",
      "metadata": {
        "title": "Crawl | Firecrawl",
        "url": "https://docs.firecrawl.dev/v0/features/crawl"
      }
    },
    {
      "id": "102d0286-2172-4a85-9a1f-74dfca7ae0f8",
      "source": "firecrawl/docs/api-reference-endpoint-credit-usage.md",
      "content": "---\ntitle: Firecrawl Docs\nurl: https://docs.firecrawl.dev/api-reference/endpoint/credit-usage\n---  \n[Firecrawl Docs home page![light logo](https://mintlify.s3.us-west-1.amazonaws.com/firecrawl/logo/light.svg)![dark logo](https://mintlify.s3.us-west-1.amazonaws.com/firecrawl/logo/dark.svg)](https://firecrawl.dev/)  \nv1  \nSearch or ask...  \nCtrl K  \nSearch...  \nNavigation  \nAccount Endpoints  \nCredit Usage  \n[Documentation](https://docs.firecrawl.dev/introduction) [SDKs](https://docs.firecrawl.dev/sdks/overview) [Learn](https://www.firecrawl.dev/blog/category/tutorials) [API Reference](https://docs.firecrawl.dev/api-reference/introduction)  \nGET  \n/  \nteam  \n/  \ncredit-usage  \nTry it  \ncURL  \nPython  \nJavaScript  \nPHP  \nGo  \nJava  \nCopy  \n```\ncurl --request GET \\\n--url https://api.firecrawl.dev/v1/team/credit-usage \\\n--header 'Authorization: Bearer <token>'\n```  \n200  \n404  \n500  \nCopy  \n```\n{\n\"success\": true,\n\"data\": {\n\"remaining_credits\": 1000\n}\n}\n```",
      "metadata": {
        "title": "Firecrawl Docs",
        "url": "https://docs.firecrawl.dev/api-reference/endpoint/credit-usage"
      }
    },
    {
      "id": "9b217173-fbdf-4b24-a5e3-39bf1d3ce314",
      "source": "firecrawl/docs/api-reference-endpoint-credit-usage.md",
      "content": "Authorizations\n\n[](https://docs.firecrawl.dev/api-reference/endpoint/credit-usage#authorization-authorization)  \nAuthorization  \nstring  \nheader  \nrequired  \nBearer authentication header of the form `Bearer <token>`, where `<token>` is your auth token.",
      "metadata": {
        "title": "Firecrawl Docs",
        "url": "https://docs.firecrawl.dev/api-reference/endpoint/credit-usage"
      }
    },
    {
      "id": "f6e10a5d-1b1c-4625-93c5-62d96e0df395",
      "source": "firecrawl/docs/api-reference-endpoint-credit-usage.md",
      "content": "Response\n\n200 - application/json  \n[](https://docs.firecrawl.dev/api-reference/endpoint/credit-usage#response-success)  \nsuccess  \nboolean  \n[](https://docs.firecrawl.dev/api-reference/endpoint/credit-usage#response-data)  \ndata  \nobject  \nShowchild attributes  \n[](https://docs.firecrawl.dev/api-reference/endpoint/credit-usage#response-data-remaining-credits)  \ndata.remaining_credits  \nnumber  \nNumber of credits remaining for the team  \n[Suggest edits](https://github.com/hellofirecrawl/docs/edit/main/api-reference/endpoint/credit-usage.mdx) [Raise issue](https://github.com/hellofirecrawl/docs/issues/new?title=Issue%20on%20docs&body=Path:%20/api-reference/endpoint/credit-usage)  \n[Search](https://docs.firecrawl.dev/api-reference/endpoint/search)  \ncURL  \nPython  \nJavaScript  \nPHP  \nGo  \nJava  \nCopy  \n```\ncurl --request GET \\\n--url https://api.firecrawl.dev/v1/team/credit-usage \\\n--header 'Authorization: Bearer <token>'\n```  \n200  \n404  \n500  \nCopy  \n```\n{\n\"success\": true,\n\"data\": {\n\"remaining_credits\": 1000\n}\n}\n```",
      "metadata": {
        "title": "Firecrawl Docs",
        "url": "https://docs.firecrawl.dev/api-reference/endpoint/credit-usage"
      }
    },
    {
      "id": "9acb5432-75c4-45f1-9f22-38c464f5817d",
      "source": "firecrawl/docs/introduction.md",
      "content": "---\ntitle: Quickstart | Firecrawl\nurl: https://docs.firecrawl.dev/introduction\n---  \n[Firecrawl Docs home page![light logo](https://mintlify.s3.us-west-1.amazonaws.com/firecrawl/logo/light.svg)![dark logo](https://mintlify.s3.us-west-1.amazonaws.com/firecrawl/logo/dark.svg)](https://firecrawl.dev/)  \nv1  \nSearch or ask...  \nCtrl K  \nSearch...  \nNavigation  \nGet Started  \nQuickstart  \n[Documentation](https://docs.firecrawl.dev/introduction) [SDKs](https://docs.firecrawl.dev/sdks/overview) [Learn](https://www.firecrawl.dev/blog/category/tutorials) [API Reference](https://docs.firecrawl.dev/api-reference/introduction)  \n![Hero Light](https://mintlify.s3.us-west-1.amazonaws.com/firecrawl/images/hero.png)",
      "metadata": {
        "title": "Quickstart | Firecrawl",
        "url": "https://docs.firecrawl.dev/introduction"
      }
    },
    {
      "id": "22060259-7478-4df9-88f8-1eb6018b4f6a",
      "source": "firecrawl/docs/introduction.md",
      "content": "[](https://docs.firecrawl.dev/introduction#welcome-to-firecrawl) Welcome to Firecrawl\n\n[Firecrawl](https://firecrawl.dev/?ref=github) is an API service that takes a URL, crawls it, and converts it into clean markdown. We crawl all accessible subpages and give you clean markdown for each. No sitemap required.",
      "metadata": {
        "title": "Quickstart | Firecrawl",
        "url": "https://docs.firecrawl.dev/introduction"
      }
    },
    {
      "id": "23a25908-cbc9-432e-a70d-dfc49a886db9",
      "source": "firecrawl/docs/introduction.md",
      "content": "[](https://docs.firecrawl.dev/introduction#how-to-use-it) How to use it?\n\nWe provide an easy to use API with our hosted version. You can find the playground and documentation [here](https://firecrawl.dev/playground). You can also self host the backend if youâ€™d like.  \nCheck out the following resources to get started:  \n- [x] **API**: [Documentation](https://docs.firecrawl.dev/api-reference/introduction)\n- [x] **SDKs**: [Python](https://docs.firecrawl.dev/sdks/python), [Node](https://docs.firecrawl.dev/sdks/node), [Go](https://docs.firecrawl.dev/sdks/go), [Rust](https://docs.firecrawl.dev/sdks/rust)\n- [x] **LLM Frameworks**: [Langchain (python)](https://python.langchain.com/docs/integrations/document_loaders/firecrawl/), [Langchain (js)](https://js.langchain.com/docs/integrations/document_loaders/web_loaders/firecrawl), [Llama Index](https://docs.llamaindex.ai/en/latest/examples/data_connectors/WebPageDemo/#using-firecrawl-reader), [Crew.ai](https://docs.crewai.com/), [Composio](https://composio.dev/tools/firecrawl/all), [PraisonAI](https://docs.praison.ai/firecrawl/), [Superinterface](https://superinterface.ai/docs/assistants/functions/firecrawl), [Vectorize](https://docs.vectorize.io/integrations/source-connectors/firecrawl)\n- [x] **Low-code Frameworks**: [Dify](https://dify.ai/blog/dify-ai-blog-integrated-with-firecrawl), [Langflow](https://docs.langflow.org/), [Flowise AI](https://docs.flowiseai.com/integrations/langchain/document-loaders/firecrawl), [Cargo](https://docs.getcargo.io/integration/firecrawl), [Pipedream](https://pipedream.com/apps/firecrawl/)\n- [x] **Others**: [Zapier](https://zapier.com/apps/firecrawl/integrations), [Pabbly Connect](https://www.pabbly.com/connect/integrations/firecrawl/)\n- [ ] Want an SDK or Integration? Let us know by opening an issue.  \n**Self-host:** To self-host refer to guide [here](https://docs.firecrawl.dev/contributing/self-host).",
      "metadata": {
        "title": "Quickstart | Firecrawl",
        "url": "https://docs.firecrawl.dev/introduction"
      }
    },
    {
      "id": "2a167c88-1eb5-47c2-bce9-7df9f4c1a4ce",
      "source": "firecrawl/docs/introduction.md",
      "content": "[](https://docs.firecrawl.dev/introduction#how-to-use-it) How to use it? > [](https://docs.firecrawl.dev/introduction#api-key) API Key\n\nTo use the API, you need to sign up on [Firecrawl](https://firecrawl.dev/) and get an API key.",
      "metadata": {
        "title": "Quickstart | Firecrawl",
        "url": "https://docs.firecrawl.dev/introduction"
      }
    },
    {
      "id": "5a23a198-4e5b-4a5e-b782-12cc1b12c4b0",
      "source": "firecrawl/docs/introduction.md",
      "content": "[](https://docs.firecrawl.dev/introduction#how-to-use-it) How to use it? > [](https://docs.firecrawl.dev/introduction#features) Features\n\n- [**Scrape**](https://docs.firecrawl.dev/introduction#scraping): scrapes a URL and get its content in LLM-ready format (markdown, structured data via [LLM Extract](https://docs.firecrawl.dev/introduction#extraction), screenshot, html)\n- [**Crawl**](https://docs.firecrawl.dev/introduction#crawling): scrapes all the URLs of a web page and return content in LLM-ready format\n- [**Map**](https://docs.firecrawl.dev/features/map): input a website and get all the website urls - extremely fast",
      "metadata": {
        "title": "Quickstart | Firecrawl",
        "url": "https://docs.firecrawl.dev/introduction"
      }
    },
    {
      "id": "73d34af5-6f6a-442c-94f2-c25555ead82e",
      "source": "firecrawl/docs/introduction.md",
      "content": "[](https://docs.firecrawl.dev/introduction#how-to-use-it) How to use it? > [](https://docs.firecrawl.dev/introduction#powerful-capabilities) Powerful Capabilities\n\n- **LLM-ready formats**: markdown, structured data, screenshot, HTML, links, metadata\n- **The hard stuff**: proxies, anti-bot mechanisms, dynamic content (js-rendered), output parsing, orchestration\n- **Customizability**: exclude tags, crawl behind auth walls with custom headers, max crawl depth, etcâ€¦\n- **Media parsing**: pdfs, docx, images.\n- **Reliability first**: designed to get the data you need - no matter how hard it is.\n- **Actions**: click, scroll, input, wait and more before extracting data  \nYou can find all of Firecrawlâ€™s capabilities and how to use them in our [documentation](https://docs.firecrawl.dev/)",
      "metadata": {
        "title": "Quickstart | Firecrawl",
        "url": "https://docs.firecrawl.dev/introduction"
      }
    },
    {
      "id": "8e2bdb45-82ef-4252-98b3-f75b86477a73",
      "source": "firecrawl/docs/introduction.md",
      "content": "[](https://docs.firecrawl.dev/introduction#crawling) Crawling\n\nUsed to crawl a URL and all accessible subpages. This submits a crawl job and returns a job ID to check the status of the crawl.",
      "metadata": {
        "title": "Quickstart | Firecrawl",
        "url": "https://docs.firecrawl.dev/introduction"
      }
    },
    {
      "id": "c6aabbce-595f-4e3e-9a69-2eb20e2d2fa5",
      "source": "firecrawl/docs/introduction.md",
      "content": "[](https://docs.firecrawl.dev/introduction#crawling) Crawling > [](https://docs.firecrawl.dev/introduction#installation) Installation\n\nPython  \nNode  \nGo  \nRust  \nCopy  \n```bash\npip install firecrawl-py\n\n```",
      "metadata": {
        "title": "Quickstart | Firecrawl",
        "url": "https://docs.firecrawl.dev/introduction"
      }
    },
    {
      "id": "411d6054-9c58-4acb-b72d-b4fc3265beab",
      "source": "firecrawl/docs/introduction.md",
      "content": "[](https://docs.firecrawl.dev/introduction#crawling) Crawling > [](https://docs.firecrawl.dev/introduction#usage) Usage\n\nPython  \nNode  \nGo  \nRust  \ncURL  \nCopy  \n```python\nfrom firecrawl import FirecrawlApp\n\napp = FirecrawlApp(api_key=\"fc-YOUR_API_KEY\")\n\n# Crawl a website:\ncrawl_status = app.crawl_url(\n'https://firecrawl.dev',\nparams={\n'limit': 100,\n'scrapeOptions': {'formats': ['markdown', 'html']}\n},\npoll_interval=30\n)\nprint(crawl_status)\n\n```  \nIf youâ€™re using cURL or `async crawl` functions on SDKs, this will return an `ID` where you can use to check the status of the crawl.  \nCopy  \n```json\n{\n\"success\": true,\n\"id\": \"123-456-789\",\n\"url\": \"https://api.firecrawl.dev/v1/crawl/123-456-789\"\n}\n\n```",
      "metadata": {
        "title": "Quickstart | Firecrawl",
        "url": "https://docs.firecrawl.dev/introduction"
      }
    },
    {
      "id": "0e8b127a-17b6-40d9-b384-48054257cfd2",
      "source": "firecrawl/docs/introduction.md",
      "content": "[](https://docs.firecrawl.dev/introduction#crawling) Crawling > [](https://docs.firecrawl.dev/introduction#check-crawl-job) Check Crawl Job\n\nUsed to check the status of a crawl job and get its result.  \nPython  \nNode  \nGo  \nRust  \ncURL  \nCopy  \n```python\ncrawl_status = app.check_crawl_status(\"<crawl_id>\")\nprint(crawl_status)\n\n```",
      "metadata": {
        "title": "Quickstart | Firecrawl",
        "url": "https://docs.firecrawl.dev/introduction"
      }
    },
    {
      "id": "880bdf9e-dd60-4d9a-8d6e-eb9a7e6fd176",
      "source": "firecrawl/docs/introduction.md",
      "content": "[](https://docs.firecrawl.dev/introduction#crawling) Crawling > [](https://docs.firecrawl.dev/introduction#check-crawl-job) Check Crawl Job > [](https://docs.firecrawl.dev/introduction#response) Response\n\nThe response will be different depending on the status of the crawl. For not completed or large responses exceeding 10MB, a `next` URL parameter is provided. You must request this URL to retrieve the next 10MB of data. If the `next` parameter is absent, it indicates the end of the crawl data.  \nScraping  \nCompleted  \nCopy  \n```json\n{\n\"status\": \"scraping\",\n\"total\": 36,\n\"completed\": 10,\n\"creditsUsed\": 10,\n\"expiresAt\": \"2024-00-00T00:00:00.000Z\",\n\"next\": \"https://api.firecrawl.dev/v1/crawl/123-456-789?skip=10\",\n\"data\": [\\\n{\\\n\"markdown\": \"[Firecrawl Docs home page![light logo](https://mintlify.s3-us-west-1.amazonaws.com/firecrawl/logo/light.svg)!...\",\\\n\"html\": \"<!DOCTYPE html><html lang=\"en\" class=\"js-focus-visible lg:[--scroll-mt:9.5rem]\" data-js-focus-visible=\"\">...\",\\\n\"metadata\": {\\\n\"title\": \"Build a 'Chat with website' using Groq Llama 3 | Firecrawl\",\\\n\"language\": \"en\",\\\n\"sourceURL\": \"https://docs.firecrawl.dev/learn/rag-llama3\",\\\n\"description\": \"Learn how to use Firecrawl, Groq Llama 3, and Langchain to build a 'Chat with your website' bot.\",\\\n\"ogLocaleAlternate\": [],\\\n\"statusCode\": 200\\\n}\\\n},\\\n...\\\n]\\\n}\\\n\\\n```\\\n\\",
      "metadata": {
        "title": "Quickstart | Firecrawl",
        "url": "https://docs.firecrawl.dev/introduction"
      }
    },
    {
      "id": "7a00ea4d-7999-41ce-97de-2fcc1d669e18",
      "source": "firecrawl/docs/introduction.md",
      "content": "[](https://docs.firecrawl.dev/introduction#scraping) Scraping\\\n\n\\\nTo scrape a single URL, use the `scrape_url` method. It takes the URL as a parameter and returns the scraped data as a dictionary.\\\n\\\nPython\\\n\\\nNode\\\n\\\nGo\\\n\\\nRust\\\n\\\ncURL\\\n\\\nCopy\\\n\\\n```python\\\nfrom firecrawl import FirecrawlApp\\\n\\\napp = FirecrawlApp(api_key=\"fc-YOUR_API_KEY\")\\\n\\\n# Scrape a website:\\\nscrape_result = app.scrape_url('firecrawl.dev', params={'formats': ['markdown', 'html']})\\\nprint(scrape_result)\\\n\\\n```\\\n\\",
      "metadata": {
        "title": "Quickstart | Firecrawl",
        "url": "https://docs.firecrawl.dev/introduction"
      }
    },
    {
      "id": "41ce21ac-afd5-4707-8de2-5484dce1a9fc",
      "source": "firecrawl/docs/introduction.md",
      "content": "[](https://docs.firecrawl.dev/introduction#scraping) Scraping\\ > [](https://docs.firecrawl.dev/introduction#response-2) Response\\\n\n\\\nSDKs will return the data object directly. cURL will return the payload exactly as shown below.\\\n\\\nCopy\\\n\\\n```json\\\n{\\\n\"success\": true,\\\n\"data\" : {\\\n\"markdown\": \"Launch Week I is here! [See our Day 2 Release ðŸš€](https://www.firecrawl.dev/blog/launch-week-i-day-2-doubled-rate-limits)[ðŸ’¥ Get 2 months free...\",\\\n\"html\": \"<!DOCTYPE html><html lang=\"en\" class=\"light\" style=\"color-scheme: light;\"><body class=\"__variable_36bd41 __variable_d7dc5d font-inter ...\",\\\n\"metadata\": {\\\n\"title\": \"Home - Firecrawl\",\\\n\"description\": \"Firecrawl crawls and converts any website into clean markdown.\",\\\n\"language\": \"en\",\\\n\"keywords\": \"Firecrawl,Markdown,Data,Mendable,Langchain\",\\\n\"robots\": \"follow, index\",\\\n\"ogTitle\": \"Firecrawl\",\\\n\"ogDescription\": \"Turn any website into LLM-ready data.\",\\\n\"ogUrl\": \"https://www.firecrawl.dev/\",\\\n\"ogImage\": \"https://www.firecrawl.dev/og.png?123\",\\\n\"ogLocaleAlternate\": [],\\\n\"ogSiteName\": \"Firecrawl\",\\\n\"sourceURL\": \"https://firecrawl.dev\",\\\n\"statusCode\": 200\\\n}\\\n}\\\n}\\\n\\\n```\\\n\\",
      "metadata": {
        "title": "Quickstart | Firecrawl",
        "url": "https://docs.firecrawl.dev/introduction"
      }
    },
    {
      "id": "6140f7e2-fbcc-4f4a-90e2-f6e44d4f884a",
      "source": "firecrawl/docs/introduction.md",
      "content": "[](https://docs.firecrawl.dev/introduction#extraction) Extraction\\\n\n\\\nWith LLM extraction, you can easily extract structured data from any URL. We support pydantic schemas to make it easier for you too. Here is how you to use it:\\\n\\\nv1 is only supported on node, python and cURL at this time.\\\n\\\nPython\\\n\\\nNode\\\n\\\ncURL\\\n\\\nCopy\\\n\\\n```python\\\nfrom firecrawl import FirecrawlApp\\\nfrom pydantic import BaseModel, Field\\\n\\\n# Initialize the FirecrawlApp with your API key\\\napp = FirecrawlApp(api_key='your_api_key')\\\n\\\nclass ExtractSchema(BaseModel):\\\ncompany_mission: str\\\nsupports_sso: bool\\\nis_open_source: bool\\\nis_in_yc: bool\\\n\\\ndata = app.scrape_url('https://docs.firecrawl.dev/', {\\\n'formats': ['json'],\\\n'jsonOptions': {\\\n'schema': ExtractSchema.model_json_schema(),\\\n}\\\n})\\\nprint(data[\"json\"])\\\n\\\n```\\\n\\\nOutput:\\\n\\\nJSON\\\n\\\nCopy\\\n\\\n```json\\\n{\\\n\"success\": true,\\\n\"data\": {\\\n\"json\": {\\\n\"company_mission\": \"Train a secure AI on your technical resources that answers customer and employee questions so your team doesn't have to\",\\\n\"supports_sso\": true,\\\n\"is_open_source\": false,\\\n\"is_in_yc\": true\\\n},\\\n\"metadata\": {\\\n\"title\": \"Mendable\",\\\n\"description\": \"Mendable allows you to easily build AI chat applications. Ingest, customize, then deploy with one line of code anywhere you want. Brought to you by SideGuide\",\\\n\"robots\": \"follow, index\",\\\n\"ogTitle\": \"Mendable\",\\\n\"ogDescription\": \"Mendable allows you to easily build AI chat applications. Ingest, customize, then deploy with one line of code anywhere you want. Brought to you by SideGuide\",\\\n\"ogUrl\": \"https://docs.firecrawl.dev/\",\\\n\"ogImage\": \"https://docs.firecrawl.dev/mendable_new_og1.png\",\\\n\"ogLocaleAlternate\": [],\\\n\"ogSiteName\": \"Mendable\",\\\n\"sourceURL\": \"https://docs.firecrawl.dev/\"\\\n},\\\n}\\\n}\\\n\\\n```\\\n\\",
      "metadata": {
        "title": "Quickstart | Firecrawl",
        "url": "https://docs.firecrawl.dev/introduction"
      }
    },
    {
      "id": "afb50f96-f83e-4a9e-89f3-6416dacdc207",
      "source": "firecrawl/docs/introduction.md",
      "content": "[](https://docs.firecrawl.dev/introduction#extraction) Extraction\\ > [](https://docs.firecrawl.dev/introduction#extracting-without-schema-new) Extracting without schema (New)\\\n\n\\\nYou can now extract without a schema by just passing a `prompt` to the endpoint. The llm chooses the structure of the data.\\\n\\\ncURL\\\n\\\nCopy\\\n\\\n```bash\\\ncurl -X POST https://api.firecrawl.dev/v1/scrape \\\n-H 'Content-Type: application/json' \\\n-H 'Authorization: Bearer YOUR_API_KEY' \\\n-d '{\\\n\"url\": \"https://docs.firecrawl.dev/\",\\\n\"formats\": [\"json\"],\\\n\"jsonOptions\": {\\\n\"prompt\": \"Extract the company mission from the page.\"\\\n}\\\n}'\\\n\\\n```\\\n\\\nOutput:\\\n\\\nJSON\\\n\\\nCopy\\\n\\\n```json\\\n{\\\n\"success\": true,\\\n\"data\": {\\\n\"json\": {\\\n\"company_mission\": \"Train a secure AI on your technical resources that answers customer and employee questions so your team doesn't have to\",\\\n},\\\n\"metadata\": {\\\n\"title\": \"Mendable\",\\\n\"description\": \"Mendable allows you to easily build AI chat applications. Ingest, customize, then deploy with one line of code anywhere you want. Brought to you by SideGuide\",\\\n\"robots\": \"follow, index\",\\\n\"ogTitle\": \"Mendable\",\\\n\"ogDescription\": \"Mendable allows you to easily build AI chat applications. Ingest, customize, then deploy with one line of code anywhere you want. Brought to you by SideGuide\",\\\n\"ogUrl\": \"https://docs.firecrawl.dev/\",\\\n\"ogImage\": \"https://docs.firecrawl.dev/mendable_new_og1.png\",\\\n\"ogLocaleAlternate\": [],\\\n\"ogSiteName\": \"Mendable\",\\\n\"sourceURL\": \"https://docs.firecrawl.dev/\"\\\n},\\\n}\\\n}\\\n\\\n```\\\n\\",
      "metadata": {
        "title": "Quickstart | Firecrawl",
        "url": "https://docs.firecrawl.dev/introduction"
      }
    },
    {
      "id": "81d79ea8-d6ee-4fea-9a79-bf4b98590a9b",
      "source": "firecrawl/docs/introduction.md",
      "content": "[](https://docs.firecrawl.dev/introduction#extraction) Extraction\\ > [](https://docs.firecrawl.dev/introduction#extraction-v0) Extraction (v0)\\\n\n\\\nPython\\\n\\\nJavaScript\\\n\\\nGo\\\n\\\nRust\\\n\\\ncURL\\\n\\\nCopy\\\n\\\n```python\\\n\\\napp = FirecrawlApp(version=\"v0\")\\\n\\\nclass ArticleSchema(BaseModel):\\\ntitle: str\\\npoints: int\\\nby: str\\\ncommentsURL: str\\\n\\\nclass TopArticlesSchema(BaseModel):\\\ntop: List[ArticleSchema] = Field(..., max_items=5, description=\"Top 5 stories\")\\\n\\\ndata = app.scrape_url('https://news.ycombinator.com', {\\\n'extractorOptions': {\\\n'extractionSchema': TopArticlesSchema.model_json_schema(),\\\n'mode': 'llm-extraction'\\\n},\\\n'pageOptions':{\\\n'onlyMainContent': True\\\n}\\\n})\\\nprint(data[\"llm_extraction\"])\\\n\\\n```\\\n\\",
      "metadata": {
        "title": "Quickstart | Firecrawl",
        "url": "https://docs.firecrawl.dev/introduction"
      }
    },
    {
      "id": "95c56819-601f-4bc5-a03e-6f45758967fd",
      "source": "firecrawl/docs/introduction.md",
      "content": "[](https://docs.firecrawl.dev/introduction#interacting-with-the-page-with-actions) Interacting with the page with Actions\\\n\n\\\nFirecrawl allows you to perform various actions on a web page before scraping its content. This is particularly useful for interacting with dynamic content, navigating through pages, or accessing content that requires user interaction.\\\n\\\nHere is an example of how to use actions to navigate to google.com, search for Firecrawl, click on the first result, and take a screenshot.\\\n\\\nIt is important to almost always use the `wait` action before/after executing other actions to give enough time for the page to load.\\\n\\",
      "metadata": {
        "title": "Quickstart | Firecrawl",
        "url": "https://docs.firecrawl.dev/introduction"
      }
    },
    {
      "id": "ce565f63-6302-468a-af73-a3034c4ecc33",
      "source": "firecrawl/docs/introduction.md",
      "content": "[](https://docs.firecrawl.dev/introduction#interacting-with-the-page-with-actions) Interacting with the page with Actions\\ > [](https://docs.firecrawl.dev/introduction#example) Example\\\n\n\\\nPython\\\n\\\nNode\\\n\\\ncURL\\\n\\\nCopy\\\n\\\n```python\\\nfrom firecrawl import FirecrawlApp\\\n\\\napp = FirecrawlApp(api_key=\"fc-YOUR_API_KEY\")\\\n\\\n# Scrape a website:\\\nscrape_result = app.scrape_url('firecrawl.dev',\\\nparams={\\\n'formats': ['markdown', 'html'],\\\n'actions': [\\\n{\"type\": \"wait\", \"milliseconds\": 2000},\\\n{\"type\": \"click\", \"selector\": \"textarea[title=\"Search\"]\"},\\\n{\"type\": \"wait\", \"milliseconds\": 2000},\\\n{\"type\": \"write\", \"text\": \"firecrawl\"},\\\n{\"type\": \"wait\", \"milliseconds\": 2000},\\\n{\"type\": \"press\", \"key\": \"ENTER\"},\\\n{\"type\": \"wait\", \"milliseconds\": 3000},\\\n{\"type\": \"click\", \"selector\": \"h3\"},\\\n{\"type\": \"wait\", \"milliseconds\": 3000},\\\n{\"type\": \"scrape\"},\\\n{\"type\": \"screenshot\"}\\\n]\\\n}\\\n)\\\nprint(scrape_result)\\\n\\\n```\\\n\\",
      "metadata": {
        "title": "Quickstart | Firecrawl",
        "url": "https://docs.firecrawl.dev/introduction"
      }
    },
    {
      "id": "12ee7367-1cf7-4959-a1a7-e3db10744421",
      "source": "firecrawl/docs/introduction.md",
      "content": "[](https://docs.firecrawl.dev/introduction#interacting-with-the-page-with-actions) Interacting with the page with Actions\\ > [](https://docs.firecrawl.dev/introduction#output) Output\\\n\n\\\nJSON\\\n\\\nCopy\\\n\\\n```json\\\n{\\\n\"success\": true,\\\n\"data\": {\\\n\"markdown\": \"Our first Launch Week is over! [See the recap ðŸš€](blog/firecrawl-launch-week-1-recap)...\",\\\n\"actions\": {\\\n\"screenshots\": [\\\n\"https://alttmdsdujxrfnakrkyi.supabase.co/storage/v1/object/public/media/screenshot-75ef2d87-31e0-4349-a478-fb432a29e241.png\"\\\n],\\\n\"scrapes\": [\\\n{\\\n\"url\": \"https://www.firecrawl.dev/\",\\\n\"html\": \"<html><body><h1>Firecrawl</h1></body></html>\"\\\n}\\\n]\\\n},\\\n\"metadata\": {\\\n\"title\": \"Home - Firecrawl\",\\\n\"description\": \"Firecrawl crawls and converts any website into clean markdown.\",\\\n\"language\": \"en\",\\\n\"keywords\": \"Firecrawl,Markdown,Data,Mendable,Langchain\",\\\n\"robots\": \"follow, index\",\\\n\"ogTitle\": \"Firecrawl\",\\\n\"ogDescription\": \"Turn any website into LLM-ready data.\",\\\n\"ogUrl\": \"https://www.firecrawl.dev/\",\\\n\"ogImage\": \"https://www.firecrawl.dev/og.png?123\",\\\n\"ogLocaleAlternate\": [],\\\n\"ogSiteName\": \"Firecrawl\",\\\n\"sourceURL\": \"http://google.com\",\\\n\"statusCode\": 200\\\n}\\\n}\\\n}\\\n\\\n```\\\n\\",
      "metadata": {
        "title": "Quickstart | Firecrawl",
        "url": "https://docs.firecrawl.dev/introduction"
      }
    },
    {
      "id": "fdca339a-edcb-480a-8d0f-18f5a04d3407",
      "source": "firecrawl/docs/introduction.md",
      "content": "[](https://docs.firecrawl.dev/introduction#open-source-vs-cloud) Open Source vs Cloud\\\n\n\\\nFirecrawl is open source available under the [AGPL-3.0 license](https://github.com/mendableai/firecrawl/blob/main/LICENSE).\\\n\\\nTo deliver the best possible product, we offer a hosted version of Firecrawl alongside our open-source offering. The cloud solution allows us to continuously innovate and maintain a high-quality, sustainable service for all users.\\\n\\\nFirecrawl Cloud is available at [firecrawl.dev](https://firecrawl.dev/) and offers a range of features that are not available in the open source version:\\\n\\\n![Firecrawl Cloud vs Open Source](https://mintlify.s3.us-west-1.amazonaws.com/firecrawl/images/open-source-cloud.png)\\\n\\",
      "metadata": {
        "title": "Quickstart | Firecrawl",
        "url": "https://docs.firecrawl.dev/introduction"
      }
    },
    {
      "id": "6102389a-8eea-425a-969c-952783dbb9c3",
      "source": "firecrawl/docs/introduction.md",
      "content": "[](https://docs.firecrawl.dev/introduction#contributing) Contributing\\\n\n\\\nWe love contributions! Please read our [contributing guide](https://github.com/mendableai/firecrawl/blob/main/CONTRIBUTING.md) before submitting a pull request.\\\n\\\n[Suggest edits](https://github.com/hellofirecrawl/docs/edit/main/introduction.mdx) [Raise issue](https://github.com/hellofirecrawl/docs/issues/new?title=Issue%20on%20docs&body=Path:%20/introduction)\\\n\\\n[Launch Week II (New)](https://docs.firecrawl.dev/launch-week)\\\n\\\nOn this page\\\n\\\n- [Welcome to Firecrawl](https://docs.firecrawl.dev/introduction#welcome-to-firecrawl)\\\n- [How to use it?](https://docs.firecrawl.dev/introduction#how-to-use-it)\\\n- [API Key](https://docs.firecrawl.dev/introduction#api-key)\\\n- [Features](https://docs.firecrawl.dev/introduction#features)\\\n- [Powerful Capabilities](https://docs.firecrawl.dev/introduction#powerful-capabilities)\\\n- [Crawling](https://docs.firecrawl.dev/introduction#crawling)\\\n- [Installation](https://docs.firecrawl.dev/introduction#installation)\\\n- [Usage](https://docs.firecrawl.dev/introduction#usage)\\\n- [Check Crawl Job](https://docs.firecrawl.dev/introduction#check-crawl-job)\\\n- [Response](https://docs.firecrawl.dev/introduction#response)\\\n- [Scraping](https://docs.firecrawl.dev/introduction#scraping)\\\n- [Response](https://docs.firecrawl.dev/introduction#response-2)\\\n- [Extraction](https://docs.firecrawl.dev/introduction#extraction)\\\n- [Extracting without schema (New)](https://docs.firecrawl.dev/introduction#extracting-without-schema-new)\\\n- [Extraction (v0)](https://docs.firecrawl.dev/introduction#extraction-v0)\\\n- [Interacting with the page with Actions](https://docs.firecrawl.dev/introduction#interacting-with-the-page-with-actions)\\\n- [Example](https://docs.firecrawl.dev/introduction#example)\\\n- [Output](https://docs.firecrawl.dev/introduction#output)\\\n- [Open Source vs Cloud](https://docs.firecrawl.dev/introduction#open-source-vs-cloud)\\\n- [Contributing](https://docs.firecrawl.dev/introduction#contributing)\\\n\\\n![Hero Light](https://docs.firecrawl.dev/introduction)\\\n\\\n![Firecrawl Cloud vs Open Source](https://docs.firecrawl.dev/introduction)",
      "metadata": {
        "title": "Quickstart | Firecrawl",
        "url": "https://docs.firecrawl.dev/introduction"
      }
    },
    {
      "id": "0b17854f-e4f4-4b5a-b213-97b5073d0192",
      "source": "firecrawl/docs/integrations-crewai.md",
      "content": "---\ntitle: CrewAI | Firecrawl\nurl: https://docs.firecrawl.dev/integrations/crewai\n---  \n[Firecrawl Docs home page![light logo](https://mintlify.s3.us-west-1.amazonaws.com/firecrawl/logo/light.svg)![dark logo](https://mintlify.s3.us-west-1.amazonaws.com/firecrawl/logo/dark.svg)](https://firecrawl.dev/)  \nv1  \nSearch or ask...  \nCtrl K  \nSearch...  \nNavigation  \nIntegrations  \nCrewAI  \n[Documentation](https://docs.firecrawl.dev/introduction) [SDKs](https://docs.firecrawl.dev/sdks/overview) [Learn](https://www.firecrawl.dev/blog/category/tutorials) [API Reference](https://docs.firecrawl.dev/api-reference/introduction)",
      "metadata": {
        "title": "CrewAI | Firecrawl",
        "url": "https://docs.firecrawl.dev/integrations/crewai"
      }
    },
    {
      "id": "35935066-8ad9-478a-b730-4caf93160db9",
      "source": "firecrawl/docs/integrations-crewai.md",
      "content": "[](https://docs.firecrawl.dev/integrations/crewai#using-firecrawl-with-crewai) Using Firecrawl with CrewAI\n\nFirecrawl is integrated with [CrewAI, the framework for orchestrating AI agents](https://www.crewai.com/). This page introduces all of the Firecrawl tools added to the framework.",
      "metadata": {
        "title": "CrewAI | Firecrawl",
        "url": "https://docs.firecrawl.dev/integrations/crewai"
      }
    },
    {
      "id": "c7e18d74-641d-4224-98f1-6758fe2b8e1b",
      "source": "firecrawl/docs/integrations-crewai.md",
      "content": "[](https://docs.firecrawl.dev/integrations/crewai#using-firecrawl-with-crewai) Using Firecrawl with CrewAI > [](https://docs.firecrawl.dev/integrations/crewai#installing-firecrawl-tools-inside-of-crewai) Installing Firecrawl Tools inside of CrewAI\n\n- Get an API key from your [firecrawl.dev dashboard](https://firecrawl.dev/) and set it in environment variables ( `FIRECRAWL_API_KEY`).\n- Install the [Firecrawl SDK](https://github.com/mendableai/firecrawl) along with `crewai[tools]` package:  \nCopy  \n```\npip install firecrawl-py 'crewai[tools]'\n\n```",
      "metadata": {
        "title": "CrewAI | Firecrawl",
        "url": "https://docs.firecrawl.dev/integrations/crewai"
      }
    },
    {
      "id": "e64d53d8-503a-433f-9b3c-004457e9500a",
      "source": "firecrawl/docs/integrations-crewai.md",
      "content": "[](https://docs.firecrawl.dev/integrations/crewai#tools) Tools > [](https://docs.firecrawl.dev/integrations/crewai#firecrawlcrawlwebsitetool) FirecrawlCrawlWebsiteTool > [](https://docs.firecrawl.dev/integrations/crewai#example) Example\n\nUtilize the FirecrawlScrapeFromWebsiteTool as follows to allow your agent to load websites:  \nCopy  \n```python\nfrom crewai_tools import FirecrawlCrawlWebsiteTool\n\ntool = FirecrawlCrawlWebsiteTool(url='firecrawl.dev')\n\n```",
      "metadata": {
        "title": "CrewAI | Firecrawl",
        "url": "https://docs.firecrawl.dev/integrations/crewai"
      }
    },
    {
      "id": "ac11f48d-885f-497e-b557-e92e27f83cb1",
      "source": "firecrawl/docs/integrations-crewai.md",
      "content": "[](https://docs.firecrawl.dev/integrations/crewai#tools) Tools > [](https://docs.firecrawl.dev/integrations/crewai#firecrawlcrawlwebsitetool) FirecrawlCrawlWebsiteTool > [](https://docs.firecrawl.dev/integrations/crewai#arguments) Arguments\n\n- `api_key`: Optional. Specifies Firecrawl API key. Defaults is the `FIRECRAWL_API_KEY` environment variable.\n- `url`: The base URL to start crawling from.\n- `page_options`: Optional.  \n- `onlyMainContent`: Optional. Only return the main content of the page excluding headers, navs, footers, etc.\n- `includeHtml`: Optional. Include the raw HTML content of the page. Will output a html key in the response.\n- `crawler_options`: Optional. Options for controlling the crawling behavior.  \n- `includes`: Optional. URL patterns to include in the crawl.\n- `exclude`: Optional. URL patterns to exclude from the crawl.\n- `generateImgAltText`: Optional. Generate alt text for images using LLMs (requires a paid plan).\n- `returnOnlyUrls`: Optional. If true, returns only the URLs as a list in the crawl status. Note: the response will be a list of URLs inside the data, not a list of documents.\n- `maxDepth`: Optional. Maximum depth to crawl. Depth 1 is the base URL, depth 2 includes the base URL and its direct children, and so on.\n- `mode`: Optional. The crawling mode to use. Fast mode crawls 4x faster on websites without a sitemap but may not be as accurate and shouldnâ€™t be used on heavily JavaScript-rendered websites.\n- `limit`: Optional. Maximum number of pages to crawl.\n- `timeout`: Optional. Timeout in milliseconds for the crawling operation.",
      "metadata": {
        "title": "CrewAI | Firecrawl",
        "url": "https://docs.firecrawl.dev/integrations/crewai"
      }
    },
    {
      "id": "16289d4e-77e9-4145-92eb-32bd562ef193",
      "source": "firecrawl/docs/integrations-crewai.md",
      "content": "[](https://docs.firecrawl.dev/integrations/crewai#tools) Tools > [](https://docs.firecrawl.dev/integrations/crewai#firecrawlscrapewebsitetool) FirecrawlScrapeWebsiteTool > [](https://docs.firecrawl.dev/integrations/crewai#example-2) Example\n\nUtilize the FirecrawlScrapeWebsiteTool as follows to allow your agent to load websites:  \nCopy  \n```python\nfrom crewai_tools import FirecrawlScrapeWebsiteTool\n\ntool = FirecrawlScrapeWebsiteTool(url='firecrawl.dev')\n\n```",
      "metadata": {
        "title": "CrewAI | Firecrawl",
        "url": "https://docs.firecrawl.dev/integrations/crewai"
      }
    },
    {
      "id": "cd88c1a5-e9db-49d2-a25b-303b46086adf",
      "source": "firecrawl/docs/integrations-crewai.md",
      "content": "[](https://docs.firecrawl.dev/integrations/crewai#tools) Tools > [](https://docs.firecrawl.dev/integrations/crewai#firecrawlscrapewebsitetool) FirecrawlScrapeWebsiteTool > [](https://docs.firecrawl.dev/integrations/crewai#arguments-2) Arguments\n\n- `api_key`: Optional. Specifies Firecrawl API key. Defaults is the `FIRECRAWL_API_KEY` environment variable.\n- `url`: The URL to scrape.\n- `page_options`: Optional.  \n- `onlyMainContent`: Optional. Only return the main content of the page excluding headers, navs, footers, etc.\n- `includeHtml`: Optional. Include the raw HTML content of the page. Will output a html key in the response.\n- `extractor_options`: Optional. Options for LLM-based extraction of structured information from the page content  \n- `mode`: The extraction mode to use, currently supports â€˜llm-extractionâ€™\n- `extractionPrompt`: Optional. A prompt describing what information to extract from the page\n- `extractionSchema`: Optional. The schema for the data to be extracted\n- `timeout`: Optional. Timeout in milliseconds for the request",
      "metadata": {
        "title": "CrewAI | Firecrawl",
        "url": "https://docs.firecrawl.dev/integrations/crewai"
      }
    },
    {
      "id": "9f1f8bf3-b46a-4a58-9915-0670aafc9684",
      "source": "firecrawl/docs/integrations-crewai.md",
      "content": "[](https://docs.firecrawl.dev/integrations/crewai#tools) Tools > [](https://docs.firecrawl.dev/integrations/crewai#firecrawlsearchtool) FirecrawlSearchTool > [](https://docs.firecrawl.dev/integrations/crewai#example-3) Example\n\nUtilize the FirecrawlSearchTool as follows to allow your agent to load websites:  \nCopy  \n```python\nfrom crewai_tools import FirecrawlSearchTool\n\ntool = FirecrawlSearchTool(query='what is firecrawl?')\n\n```",
      "metadata": {
        "title": "CrewAI | Firecrawl",
        "url": "https://docs.firecrawl.dev/integrations/crewai"
      }
    },
    {
      "id": "c831ef53-c68d-414e-93c7-68715d979014",
      "source": "firecrawl/docs/integrations-crewai.md",
      "content": "[](https://docs.firecrawl.dev/integrations/crewai#tools) Tools > [](https://docs.firecrawl.dev/integrations/crewai#firecrawlsearchtool) FirecrawlSearchTool > [](https://docs.firecrawl.dev/integrations/crewai#arguments-3) Arguments\n\n- `api_key`: Optional. Specifies Firecrawl API key. Defaults is the `FIRECRAWL_API_KEY` environment variable.\n- `query`: The search query string to be used for searching.\n- `page_options`: Optional. Options for result formatting.  \n- `onlyMainContent`: Optional. Only return the main content of the page excluding headers, navs, footers, etc.\n- `includeHtml`: Optional. Include the raw HTML content of the page. Will output a html key in the response.\n- `fetchPageContent`: Optional. Fetch the full content of the page.\n- `search_options`: Optional. Options for controlling the crawling behavior.  \n- `limit`: Optional. Maximum number of pages to crawl.  \n[Suggest edits](https://github.com/hellofirecrawl/docs/edit/main/integrations/crewai.mdx) [Raise issue](https://github.com/hellofirecrawl/docs/issues/new?title=Issue%20on%20docs&body=Path:%20/integrations/crewai)  \n[Llamaindex](https://docs.firecrawl.dev/integrations/llamaindex) [Dify](https://docs.firecrawl.dev/integrations/dify)  \nOn this page  \n- [Using Firecrawl with CrewAI](https://docs.firecrawl.dev/integrations/crewai#using-firecrawl-with-crewai)\n- [Installing Firecrawl Tools inside of CrewAI](https://docs.firecrawl.dev/integrations/crewai#installing-firecrawl-tools-inside-of-crewai)\n- [Tools](https://docs.firecrawl.dev/integrations/crewai#tools)\n- [FirecrawlCrawlWebsiteTool](https://docs.firecrawl.dev/integrations/crewai#firecrawlcrawlwebsitetool)\n- [Example](https://docs.firecrawl.dev/integrations/crewai#example)\n- [Arguments](https://docs.firecrawl.dev/integrations/crewai#arguments)\n- [FirecrawlScrapeWebsiteTool](https://docs.firecrawl.dev/integrations/crewai#firecrawlscrapewebsitetool)\n- [Example](https://docs.firecrawl.dev/integrations/crewai#example-2)\n- [Arguments](https://docs.firecrawl.dev/integrations/crewai#arguments-2)\n- [FirecrawlSearchTool](https://docs.firecrawl.dev/integrations/crewai#firecrawlsearchtool)\n- [Example](https://docs.firecrawl.dev/integrations/crewai#example-3)\n- [Arguments](https://docs.firecrawl.dev/integrations/crewai#arguments-3)",
      "metadata": {
        "title": "CrewAI | Firecrawl",
        "url": "https://docs.firecrawl.dev/integrations/crewai"
      }
    },
    {
      "id": "f21eb340-d4e4-49c6-b5f0-68bc53f5c097",
      "source": "firecrawl/docs/integrations-camelai.md",
      "content": "---\ntitle: Camel AI | Firecrawl\nurl: https://docs.firecrawl.dev/integrations/camelai\n---  \n[Firecrawl Docs home page![light logo](https://mintlify.s3.us-west-1.amazonaws.com/firecrawl/logo/light.svg)![dark logo](https://mintlify.s3.us-west-1.amazonaws.com/firecrawl/logo/dark.svg)](https://firecrawl.dev/)  \nv1  \nSearch or ask...  \nCtrl K  \nSearch...  \nNavigation  \nIntegrations  \nCamel AI  \n[Documentation](https://docs.firecrawl.dev/introduction) [SDKs](https://docs.firecrawl.dev/sdks/overview) [Learn](https://www.firecrawl.dev/blog/category/tutorials) [API Reference](https://docs.firecrawl.dev/api-reference/introduction)",
      "metadata": {
        "title": "Camel AI | Firecrawl",
        "url": "https://docs.firecrawl.dev/integrations/camelai"
      }
    },
    {
      "id": "6716a321-4989-4d71-9541-a6baaae8e8a9",
      "source": "firecrawl/docs/integrations-camelai.md",
      "content": "[](https://docs.firecrawl.dev/integrations/camelai#installation) Installation\n\nCopy  \n```bash\npip install camel-ai\n\n```",
      "metadata": {
        "title": "Camel AI | Firecrawl",
        "url": "https://docs.firecrawl.dev/integrations/camelai"
      }
    },
    {
      "id": "f5c6c988-d8ef-4d2e-aec0-522a14a9a2be",
      "source": "firecrawl/docs/integrations-camelai.md",
      "content": "[](https://docs.firecrawl.dev/integrations/camelai#usage) Usage\n\nWith Camel AI and Firecrawl you can quickly build multi-agent systems than use data from the web.",
      "metadata": {
        "title": "Camel AI | Firecrawl",
        "url": "https://docs.firecrawl.dev/integrations/camelai"
      }
    },
    {
      "id": "2164aae1-5b84-401b-a518-905563ef7e40",
      "source": "firecrawl/docs/integrations-camelai.md",
      "content": "[](https://docs.firecrawl.dev/integrations/camelai#usage) Usage > [](https://docs.firecrawl.dev/integrations/camelai#using-firecrawl-to-gather-an-entire-website) Using Firecrawl to Gather an Entire Website\n\nCopy  \n```python\nmock_app = MockFirecrawlApp.return_value\nfirecrawl = Firecrawl(\napi_key='FC_API_KEY', api_url='https://api.test.com'\n)\nurl = 'https://example.com'\nresponse = [{'markdown': 'Markdown content'}]\nmock_app.crawl_url.return_value = respons\nresult = firecrawl.markdown_crawl(url)\n\n```",
      "metadata": {
        "title": "Camel AI | Firecrawl",
        "url": "https://docs.firecrawl.dev/integrations/camelai"
      }
    },
    {
      "id": "1accc80d-1ba6-4f65-80fd-fd80f00f4522",
      "source": "firecrawl/docs/integrations-camelai.md",
      "content": "[](https://docs.firecrawl.dev/integrations/camelai#usage) Usage > [](https://docs.firecrawl.dev/integrations/camelai#using-firecrawl-to-gather-a-single-page) Using Firecrawl to Gather a Single Page\n\nCopy  \n```python\nmock_app = MockFirecrawlApp.return_value\nfirecrawl = Firecrawl(\napi_key='test_api_key', api_url='https://api.test.com'\n)\nurl = 'https://example.com'\nresponse = 'Scraped content'\nmock_app.scrape_url.return_value = response\n\nresult = firecrawl.scrape(url)\n\n```  \n[Suggest edits](https://github.com/hellofirecrawl/docs/edit/main/integrations/camelai.mdx) [Raise issue](https://github.com/hellofirecrawl/docs/issues/new?title=Issue%20on%20docs&body=Path:%20/integrations/camelai)  \n[Langflow](https://docs.firecrawl.dev/integrations/langflow) [RAGaaS](https://docs.firecrawl.dev/integrations/ragaas)  \nOn this page  \n- [Installation](https://docs.firecrawl.dev/integrations/camelai#installation)\n- [Usage](https://docs.firecrawl.dev/integrations/camelai#usage)\n- [Using Firecrawl to Gather an Entire Website](https://docs.firecrawl.dev/integrations/camelai#using-firecrawl-to-gather-an-entire-website)\n- [Using Firecrawl to Gather a Single Page](https://docs.firecrawl.dev/integrations/camelai#using-firecrawl-to-gather-a-single-page)",
      "metadata": {
        "title": "Camel AI | Firecrawl",
        "url": "https://docs.firecrawl.dev/integrations/camelai"
      }
    },
    {
      "id": "263be60b-a270-4847-aa6b-4d7b13970053",
      "source": "firecrawl/docs/api-reference-endpoint-crawl-delete.md",
      "content": "---\ntitle: Firecrawl Docs\nurl: https://docs.firecrawl.dev/api-reference/endpoint/crawl-delete\n---  \n[Firecrawl Docs home page![light logo](https://mintlify.s3.us-west-1.amazonaws.com/firecrawl/logo/light.svg)![dark logo](https://mintlify.s3.us-west-1.amazonaws.com/firecrawl/logo/dark.svg)](https://firecrawl.dev/)  \nv1  \nSearch or ask...  \nCtrl K  \nSearch...  \nNavigation  \nCrawl Endpoints  \nCancel Crawl  \n[Documentation](https://docs.firecrawl.dev/introduction) [SDKs](https://docs.firecrawl.dev/sdks/overview) [Learn](https://www.firecrawl.dev/blog/category/tutorials) [API Reference](https://docs.firecrawl.dev/api-reference/introduction)  \nDELETE  \n/  \ncrawl  \n/  \n{id}  \nTry it  \ncURL  \nPython  \nJavaScript  \nPHP  \nGo  \nJava  \nCopy  \n```\ncurl --request DELETE \\\n--url https://api.firecrawl.dev/v1/crawl/{id} \\\n--header 'Authorization: Bearer <token>'\n```  \n200  \n404  \n500  \nCopy  \n```\n{\n\"status\": \"cancelled\"\n}\n```",
      "metadata": {
        "title": "Firecrawl Docs",
        "url": "https://docs.firecrawl.dev/api-reference/endpoint/crawl-delete"
      }
    },
    {
      "id": "5d911532-fa27-4c91-a8c5-b66c5fde4a8f",
      "source": "firecrawl/docs/api-reference-endpoint-crawl-delete.md",
      "content": "Authorizations\n\n[](https://docs.firecrawl.dev/api-reference/endpoint/crawl-delete#authorization-authorization)  \nAuthorization  \nstring  \nheader  \nrequired  \nBearer authentication header of the form `Bearer <token>`, where `<token>` is your auth token.",
      "metadata": {
        "title": "Firecrawl Docs",
        "url": "https://docs.firecrawl.dev/api-reference/endpoint/crawl-delete"
      }
    },
    {
      "id": "8c601fda-1108-4d77-8ad3-1a4edfae9b57",
      "source": "firecrawl/docs/api-reference-endpoint-crawl-delete.md",
      "content": "Path Parameters\n\n[](https://docs.firecrawl.dev/api-reference/endpoint/crawl-delete#parameter-id)  \nid  \nstring  \nrequired  \nThe ID of the crawl job",
      "metadata": {
        "title": "Firecrawl Docs",
        "url": "https://docs.firecrawl.dev/api-reference/endpoint/crawl-delete"
      }
    },
    {
      "id": "4ae24026-c161-47e3-b5a1-9cc9526c1640",
      "source": "firecrawl/docs/api-reference-endpoint-crawl-delete.md",
      "content": "Response\n\n200 - application/json  \n[](https://docs.firecrawl.dev/api-reference/endpoint/crawl-delete#response-status)  \nstatus  \nenum<string>  \nAvailable options:  \n`cancelled`  \n[Suggest edits](https://github.com/hellofirecrawl/docs/edit/main/api-reference/endpoint/crawl-delete.mdx) [Raise issue](https://github.com/hellofirecrawl/docs/issues/new?title=Issue%20on%20docs&body=Path:%20/api-reference/endpoint/crawl-delete)  \n[Get Crawl Status](https://docs.firecrawl.dev/api-reference/endpoint/crawl-get) [Get Crawl Errors](https://docs.firecrawl.dev/api-reference/endpoint/crawl-get-errors)  \ncURL  \nPython  \nJavaScript  \nPHP  \nGo  \nJava  \nCopy  \n```\ncurl --request DELETE \\\n--url https://api.firecrawl.dev/v1/crawl/{id} \\\n--header 'Authorization: Bearer <token>'\n```  \n200  \n404  \n500  \nCopy  \n```\n{\n\"status\": \"cancelled\"\n}\n```",
      "metadata": {
        "title": "Firecrawl Docs",
        "url": "https://docs.firecrawl.dev/api-reference/endpoint/crawl-delete"
      }
    },
    {
      "id": "4c7315b1-7642-4e14-bea8-3ccab55e5b94",
      "source": "firecrawl/docs/api-reference-endpoint-batch-scrape.md",
      "content": "---\ntitle: Firecrawl Docs\nurl: https://docs.firecrawl.dev/api-reference/endpoint/batch-scrape\n---  \n[Firecrawl Docs home page![light logo](https://mintlify.s3.us-west-1.amazonaws.com/firecrawl/logo/light.svg)![dark logo](https://mintlify.s3.us-west-1.amazonaws.com/firecrawl/logo/dark.svg)](https://firecrawl.dev/)  \nv1  \nSearch or ask...  \nCtrl K  \nSearch...  \nNavigation  \nScrape Endpoints  \nBatch Scrape  \n[Documentation](https://docs.firecrawl.dev/introduction) [SDKs](https://docs.firecrawl.dev/sdks/overview) [Learn](https://www.firecrawl.dev/blog/category/tutorials) [API Reference](https://docs.firecrawl.dev/api-reference/introduction)  \nPOST  \n/  \nbatch  \n/  \nscrape  \nTry it  \ncURL  \nPython  \nJavaScript  \nPHP  \nGo  \nJava  \nCopy  \n```\ncurl --request POST \\\n--url https://api.firecrawl.dev/v1/batch/scrape \\\n--header 'Authorization: Bearer <token>' \\\n--header 'Content-Type: application/json' \\\n--data '{\n\"urls\": [\\\n\"<string>\"\\\n],\n\"webhook\": \"<string>\",\n\"formats\": [\\\n\"markdown\"\\\n],\n\"onlyMainContent\": true,\n\"includeTags\": [\\\n\"<string>\"\\\n],\n\"excludeTags\": [\\\n\"<string>\"\\\n],\n\"headers\": {},\n\"waitFor\": 0,\n\"mobile\": false,\n\"skipTlsVerification\": false,\n\"timeout\": 30000,\n\"jsonOptions\": {\n\"schema\": {},\n\"systemPrompt\": \"<string>\",\n\"prompt\": \"<string>\"\n},\n\"actions\": [\\\n{\\\n\"type\": \"wait\",\\\n\"milliseconds\": 2,\\\n\"selector\": \"#my-element\"\\\n}\\\n],\n\"location\": {\n\"country\": \"US\",\n\"languages\": [\\\n\"en-US\"\\\n]\n},\n\"removeBase64Images\": true,\n\"blockAds\": true,\n\"ignoreInvalidURLs\": false\n}'\n```  \n200  \n402  \n429  \n500  \nCopy  \n```\n{\n\"success\": true,\n\"id\": \"<string>\",\n\"url\": \"<string>\",\n\"invalidURLs\": [\\\n\"<string>\"\\\n]\n}\n```",
      "metadata": {
        "title": "Firecrawl Docs",
        "url": "https://docs.firecrawl.dev/api-reference/endpoint/batch-scrape"
      }
    },
    {
      "id": "7fc5a699-2014-4fb2-9b3b-0ec17db1ffec",
      "source": "firecrawl/docs/api-reference-endpoint-batch-scrape.md",
      "content": "Authorizations\n\n[](https://docs.firecrawl.dev/api-reference/endpoint/batch-scrape#authorization-authorization)  \nAuthorization  \nstring  \nheader  \nrequired  \nBearer authentication header of the form `Bearer <token>`, where `<token>` is your auth token.",
      "metadata": {
        "title": "Firecrawl Docs",
        "url": "https://docs.firecrawl.dev/api-reference/endpoint/batch-scrape"
      }
    },
    {
      "id": "f1da6203-c22c-4e78-aae3-0940d3730eab",
      "source": "firecrawl/docs/api-reference-endpoint-batch-scrape.md",
      "content": "Body\n\napplication/json  \n[](https://docs.firecrawl.dev/api-reference/endpoint/batch-scrape#body-urls)  \nurls  \nstring[]  \n[](https://docs.firecrawl.dev/api-reference/endpoint/batch-scrape#body-webhook)  \nwebhook  \nstringobject  \nThe URL to send the webhook to. This will trigger for batch scrape started (batch_scrape.started), every page scraped (batch_scrape.page) and when the batch scrape is completed (batch_scrape.completed or batch_scrape.failed). The response will be the same as the `/scrape` endpoint.  \n[](https://docs.firecrawl.dev/api-reference/endpoint/batch-scrape#body-formats)  \nformats  \nenum<string>[]  \nFormats to include in the output.  \nAvailable options:  \n`markdown`,  \n`html`,  \n`rawHtml`,  \n`links`,  \n`screenshot`,  \n`extract`,  \n`screenshot@fullPage`,  \n`json`  \n[](https://docs.firecrawl.dev/api-reference/endpoint/batch-scrape#body-only-main-content)  \nonlyMainContent  \nboolean  \ndefault:  \ntrue  \nOnly return the main content of the page excluding headers, navs, footers, etc.  \n[](https://docs.firecrawl.dev/api-reference/endpoint/batch-scrape#body-include-tags)  \nincludeTags  \nstring[]  \nTags to include in the output.  \n[](https://docs.firecrawl.dev/api-reference/endpoint/batch-scrape#body-exclude-tags)  \nexcludeTags  \nstring[]  \nTags to exclude from the output.  \n[](https://docs.firecrawl.dev/api-reference/endpoint/batch-scrape#body-headers)  \nheaders  \nobject  \nHeaders to send with the request. Can be used to send cookies, user-agent, etc.  \n[](https://docs.firecrawl.dev/api-reference/endpoint/batch-scrape#body-wait-for)  \nwaitFor  \ninteger  \ndefault:  \n0  \nSpecify a delay in milliseconds before fetching the content, allowing the page sufficient time to load.  \n[](https://docs.firecrawl.dev/api-reference/endpoint/batch-scrape#body-mobile)  \nmobile  \nboolean  \ndefault:  \nfalse  \nSet to true if you want to emulate scraping from a mobile device. Useful for testing responsive pages and taking mobile screenshots.  \n[](https://docs.firecrawl.dev/api-reference/endpoint/batch-scrape#body-skip-tls-verification)  \nskipTlsVerification  \nboolean  \ndefault:  \nfalse  \nSkip TLS certificate verification when making requests  \n[](https://docs.firecrawl.dev/api-reference/endpoint/batch-scrape#body-timeout)  \ntimeout  \ninteger  \ndefault:  \n30000  \nTimeout in milliseconds for the request  \n[](https://docs.firecrawl.dev/api-reference/endpoint/batch-scrape#body-json-options)  \njsonOptions  \nobject  \nExtract object  \nShowchild attributes  \n[](https://docs.firecrawl.dev/api-reference/endpoint/batch-scrape#body-json-options-schema)  \njsonOptions.schema  \nobject  \nThe schema to use for the extraction (Optional)  \n[](https://docs.firecrawl.dev/api-reference/endpoint/batch-scrape#body-json-options-system-prompt)  \njsonOptions.systemPrompt  \nstring  \nThe system prompt to use for the extraction (Optional)  \n[](https://docs.firecrawl.dev/api-reference/endpoint/batch-scrape#body-json-options-prompt)  \njsonOptions.prompt  \nstring  \nThe prompt to use for the extraction without a schema (Optional)  \n[](https://docs.firecrawl.dev/api-reference/endpoint/batch-scrape#body-actions)  \nactions  \nobject[]  \nActions to perform on the page before grabbing the content  \n- Wait\n- Screenshot\n- Click\n- Write text\n- Press a key\n- Scroll\n- Scrape\n- Execute JavaScript  \nShowchild attributes  \n[](https://docs.firecrawl.dev/api-reference/endpoint/batch-scrape#body-actions-type)  \nactions.type  \nenum<string>  \nrequired  \nWait for a specified amount of milliseconds  \nAvailable options:  \n`wait`  \n[](https://docs.firecrawl.dev/api-reference/endpoint/batch-scrape#body-actions-milliseconds)  \nactions.milliseconds  \ninteger  \nNumber of milliseconds to wait  \nRequired range: `x > 1`  \n[](https://docs.firecrawl.dev/api-reference/endpoint/batch-scrape#body-actions-selector)  \nactions.selector  \nstring  \nQuery selector to find the element by  \n[](https://docs.firecrawl.dev/api-reference/endpoint/batch-scrape#body-location)  \nlocation  \nobject  \nLocation settings for the request. When specified, this will use an appropriate proxy if available and emulate the corresponding language and timezone settings. Defaults to 'US' if not specified.  \nShowchild attributes  \n[](https://docs.firecrawl.dev/api-reference/endpoint/batch-scrape#body-location-country)  \nlocation.country  \nstring  \ndefault:  \nUS  \nISO 3166-1 alpha-2 country code (e.g., 'US', 'AU', 'DE', 'JP')  \n[](https://docs.firecrawl.dev/api-reference/endpoint/batch-scrape#body-location-languages)  \nlocation.languages  \nstring[]  \nPreferred languages and locales for the request in order of priority. Defaults to the language of the specified location. See https://developer.mozilla.org/en-US/docs/Web/HTTP/Headers/Accept-Language  \n[](https://docs.firecrawl.dev/api-reference/endpoint/batch-scrape#body-remove-base64-images)  \nremoveBase64Images  \nboolean  \nRemoves all base 64 images from the output, which may be overwhelmingly long. The image's alt text remains in the output, but the URL is replaced with a placeholder.  \n[](https://docs.firecrawl.dev/api-reference/endpoint/batch-scrape#body-block-ads)  \nblockAds  \nboolean  \ndefault:  \ntrue  \nEnables ad-blocking and cookie popup blocking.  \n[](https://docs.firecrawl.dev/api-reference/endpoint/batch-scrape#body-ignore-invalid-urls)  \nignoreInvalidURLs  \nboolean  \ndefault:  \nfalse  \nIf invalid URLs are specified in the urls array, they will be ignored. Instead of them failing the entire request, a batch scrape using the remaining valid URLs will be created, and the invalid URLs will be returned in the invalidURLs field of the response.",
      "metadata": {
        "title": "Firecrawl Docs",
        "url": "https://docs.firecrawl.dev/api-reference/endpoint/batch-scrape"
      }
    },
    {
      "id": "621fa611-6be2-44a3-a8f0-9cbf0b3b6cd3",
      "source": "firecrawl/docs/api-reference-endpoint-batch-scrape.md",
      "content": "Response\n\n200 - application/json  \n[](https://docs.firecrawl.dev/api-reference/endpoint/batch-scrape#response-success)  \nsuccess  \nboolean  \n[](https://docs.firecrawl.dev/api-reference/endpoint/batch-scrape#response-id)  \nid  \nstring  \n[](https://docs.firecrawl.dev/api-reference/endpoint/batch-scrape#response-url)  \nurl  \nstring  \n[](https://docs.firecrawl.dev/api-reference/endpoint/batch-scrape#response-invalid-urls)  \ninvalidURLs  \nstring[] | null  \nIf ignoreInvalidURLs is true, this is an array containing the invalid URLs that were specified in the request. If there were no invalid URLs, this will be an empty array. If ignoreInvalidURLs is false, this field will be undefined.  \n[Suggest edits](https://github.com/hellofirecrawl/docs/edit/main/api-reference/endpoint/batch-scrape.mdx) [Raise issue](https://github.com/hellofirecrawl/docs/issues/new?title=Issue%20on%20docs&body=Path:%20/api-reference/endpoint/batch-scrape)  \n[Scrape](https://docs.firecrawl.dev/api-reference/endpoint/scrape) [Get Batch Scrape Status](https://docs.firecrawl.dev/api-reference/endpoint/batch-scrape-get)  \ncURL  \nPython  \nJavaScript  \nPHP  \nGo  \nJava  \nCopy  \n```\ncurl --request POST \\\n--url https://api.firecrawl.dev/v1/batch/scrape \\\n--header 'Authorization: Bearer <token>' \\\n--header 'Content-Type: application/json' \\\n--data '{\n\"urls\": [\\\n\"<string>\"\\\n],\n\"webhook\": \"<string>\",\n\"formats\": [\\\n\"markdown\"\\\n],\n\"onlyMainContent\": true,\n\"includeTags\": [\\\n\"<string>\"\\\n],\n\"excludeTags\": [\\\n\"<string>\"\\\n],\n\"headers\": {},\n\"waitFor\": 0,\n\"mobile\": false,\n\"skipTlsVerification\": false,\n\"timeout\": 30000,\n\"jsonOptions\": {\n\"schema\": {},\n\"systemPrompt\": \"<string>\",\n\"prompt\": \"<string>\"\n},\n\"actions\": [\\\n{\\\n\"type\": \"wait\",\\\n\"milliseconds\": 2,\\\n\"selector\": \"#my-element\"\\\n}\\\n],\n\"location\": {\n\"country\": \"US\",\n\"languages\": [\\\n\"en-US\"\\\n]\n},\n\"removeBase64Images\": true,\n\"blockAds\": true,\n\"ignoreInvalidURLs\": false\n}'\n```  \n200  \n402  \n429  \n500  \nCopy  \n```\n{\n\"success\": true,\n\"id\": \"<string>\",\n\"url\": \"<string>\",\n\"invalidURLs\": [\\\n\"<string>\"\\\n]\n}\n```",
      "metadata": {
        "title": "Firecrawl Docs",
        "url": "https://docs.firecrawl.dev/api-reference/endpoint/batch-scrape"
      }
    },
    {
      "id": "d9ef0838-ba7e-4a90-a396-f7b284d215a8",
      "source": "firecrawl/docs/launch-week.md",
      "content": "---\ntitle: Launch Week II | Firecrawl\nurl: https://docs.firecrawl.dev/launch-week\n---  \n[Firecrawl Docs home page![light logo](https://mintlify.s3.us-west-1.amazonaws.com/firecrawl/logo/light.svg)![dark logo](https://mintlify.s3.us-west-1.amazonaws.com/firecrawl/logo/dark.svg)](https://firecrawl.dev/)  \nv1  \nSearch or ask...  \nCtrl K  \nSearch...  \nNavigation  \nGet Started  \nLaunch Week II (New)  \n[Documentation](https://docs.firecrawl.dev/introduction) [SDKs](https://docs.firecrawl.dev/sdks/overview) [Learn](https://www.firecrawl.dev/blog/category/tutorials) [API Reference](https://docs.firecrawl.dev/api-reference/introduction)",
      "metadata": {
        "title": "Launch Week II | Firecrawl",
        "url": "https://docs.firecrawl.dev/launch-week"
      }
    },
    {
      "id": "411ce7cf-07f6-4afb-a651-3cae7c93d810",
      "source": "firecrawl/docs/launch-week.md",
      "content": "[](https://docs.firecrawl.dev/launch-week#day-7-faster-markdown-parsing) Day 7 - Faster Markdown Parsing\n\nWeâ€™ve rebuilt our Markdown parser from the ground up with a focus on speed and performance. This enhancement ensures that your web scraping tasks are more efficient and deliver higher-quality results.",
      "metadata": {
        "title": "Launch Week II | Firecrawl",
        "url": "https://docs.firecrawl.dev/launch-week"
      }
    },
    {
      "id": "686513a5-2b30-4d45-beb3-4526f7d3692e",
      "source": "firecrawl/docs/launch-week.md",
      "content": "[](https://docs.firecrawl.dev/launch-week#day-7-faster-markdown-parsing) Day 7 - Faster Markdown Parsing > [](https://docs.firecrawl.dev/launch-week#whats-new) Whatâ€™s New?\n\n- **Speed Improvements**: Experience parsing speeds up to 4 times faster than before, allowing for quicker data processing and reduced waiting times.\n- **Enhanced Reliability**: Our new parser handles a wider range of HTML content more gracefully, reducing errors and improving consistency.\n- **Cleaner Markdown Output**: Get cleaner and more readable Markdown, making your data easier to work with and integrate into your workflows.",
      "metadata": {
        "title": "Launch Week II | Firecrawl",
        "url": "https://docs.firecrawl.dev/launch-week"
      }
    },
    {
      "id": "7ba7b713-a1c6-46a1-8ea8-0d6c5b49ebb0",
      "source": "firecrawl/docs/launch-week.md",
      "content": "[](https://docs.firecrawl.dev/launch-week#day-6-mobile-scraping-mobile-screenshots) Day 6 - Mobile Scraping (+ Mobile Screenshots)\n\nFirecrawl now introduces **mobile device emulation** for both scraping and screenshots, empowering you to interact with sites as if from a mobile device. This feature is essential for testing mobile-specific content, understanding responsive design, and gaining insights from mobile-specific elements.",
      "metadata": {
        "title": "Launch Week II | Firecrawl",
        "url": "https://docs.firecrawl.dev/launch-week"
      }
    },
    {
      "id": "497cde3a-3be4-4309-99e0-e788be9cfffd",
      "source": "firecrawl/docs/launch-week.md",
      "content": "[](https://docs.firecrawl.dev/launch-week#day-6-mobile-scraping-mobile-screenshots) Day 6 - Mobile Scraping (+ Mobile Screenshots) > [](https://docs.firecrawl.dev/launch-week#why-mobile-scraping) Why Mobile Scraping?\n\nMobile-first experiences are increasingly common, and this feature enables you to:  \n- Take high-fidelity mobile screenshots for a more accurate representation of how a site appears on mobile.\n- Test and verify mobile layouts and UI elements, ensuring the accuracy of your scraping results for responsive websites.\n- Scrape mobile-only content, gaining access to information or layouts that vary from desktop versions.",
      "metadata": {
        "title": "Launch Week II | Firecrawl",
        "url": "https://docs.firecrawl.dev/launch-week"
      }
    },
    {
      "id": "2de13055-ca82-4bb8-86da-6233171a1791",
      "source": "firecrawl/docs/launch-week.md",
      "content": "[](https://docs.firecrawl.dev/launch-week#day-6-mobile-scraping-mobile-screenshots) Day 6 - Mobile Scraping (+ Mobile Screenshots) > [](https://docs.firecrawl.dev/launch-week#usage) Usage\n\nTo activate mobile scraping, simply add `\"mobile\": true` in your request, which will enable Firecrawlâ€™s mobile emulation mode.  \nPython  \nNode  \ncURL  \nCopy  \n```python\nfrom firecrawl import FirecrawlApp\n\napp = FirecrawlApp(api_key=\"fc-YOUR_API_KEY\")\n\n# Scrape a website:\nscrape_result = app.scrape_url('google.com',\nparams={\n'formats': ['markdown', 'html'],\n'mobile': true\n}\n)\nprint(scrape_result)\n\n```  \nFor further details, including additional configuration options, visit the [API Reference](https://docs.firecrawl.dev/api-reference/endpoint/scrape).",
      "metadata": {
        "title": "Launch Week II | Firecrawl",
        "url": "https://docs.firecrawl.dev/launch-week"
      }
    },
    {
      "id": "2811f3c8-829a-4d6e-93ff-890705872213",
      "source": "firecrawl/docs/launch-week.md",
      "content": "[](https://docs.firecrawl.dev/launch-week#day-5-actions-2-new-actions) Day 5 - Actions (2 new actions)\n\nFirecrawl allows you to perform various actions on a web page before scraping its content. This is particularly useful for interacting with dynamic content, navigating through pages, or accessing content that requires user interaction.  \nWeâ€™re excited to introduce two powerful new actions:  \n1. **Scrape**: Capture the current page content at any point during your interaction sequence, returning both URL and HTML.\n2. **Wait for Selector**: Wait for a specific element to appear on the page before proceeding, ensuring more reliable automation.  \nCopy  \n```json\nactions = [\\\n{\"type\": \"scrape\"},\\\n{\"type\": \"wait\", \"selector\": \"#my-element\"},\\\n]\n\n```  \nHere is an example of how to use actions to navigate to google.com, search for Firecrawl, click on the first result, scrape the current page content, and take a screenshot.  \nFor more precise control, you can now use `{type: \"wait\", selector: \"#my-element\"}` to wait for a specific element to appear on the page.",
      "metadata": {
        "title": "Launch Week II | Firecrawl",
        "url": "https://docs.firecrawl.dev/launch-week"
      }
    },
    {
      "id": "5c53afa8-d5cb-459e-8cc0-3745fda2c53b",
      "source": "firecrawl/docs/launch-week.md",
      "content": "[](https://docs.firecrawl.dev/launch-week#day-5-actions-2-new-actions) Day 5 - Actions (2 new actions) > [](https://docs.firecrawl.dev/launch-week#example) Example\n\nPython  \nNode  \ncURL  \nCopy  \n```python\nfrom firecrawl import FirecrawlApp\n\napp = FirecrawlApp(api_key=\"fc-YOUR_API_KEY\")\n\n# Scrape a website:\nscrape_result = app.scrape_url('firecrawl.dev',\nparams={\n'formats': ['markdown', 'html'],\n'actions': [\\\n{\"type\": \"wait\", \"milliseconds\": 2000},\\\n{\"type\": \"click\", \"selector\": \"textarea[title=\"Search\"]\"},\\\n{\"type\": \"wait\", \"milliseconds\": 2000},\\\n{\"type\": \"write\", \"text\": \"firecrawl\"},\\\n{\"type\": \"wait\", \"milliseconds\": 2000},\\\n{\"type\": \"press\", \"key\": \"ENTER\"},\\\n{\"type\": \"wait\", \"milliseconds\": 3000},\\\n{\"type\": \"click\", \"selector\": \"h3\"},\\\n{\"type\": \"wait\", \"milliseconds\": 3000},\\\n{\"type\": \"scrape\"},\\\n{\"type\": \"screenshot\"}\\\n]\n}\n)\nprint(scrape_result)\n\n```",
      "metadata": {
        "title": "Launch Week II | Firecrawl",
        "url": "https://docs.firecrawl.dev/launch-week"
      }
    },
    {
      "id": "370fa90c-889b-486e-91a2-1c5058e98447",
      "source": "firecrawl/docs/launch-week.md",
      "content": "[](https://docs.firecrawl.dev/launch-week#day-5-actions-2-new-actions) Day 5 - Actions (2 new actions) > [](https://docs.firecrawl.dev/launch-week#output) Output\n\nJSON  \nCopy  \n```json\n{\n\"success\": true,\n\"data\": {\n\"markdown\": \"Our first Launch Week is over! [See the recap ðŸš€](blog/firecrawl-launch-week-1-recap)...\",\n\"actions\": {\n\"screenshots\": [\\\n\"https://alttmdsdujxrfnakrkyi.supabase.co/storage/v1/object/public/media/screenshot-75ef2d87-31e0-4349-a478-fb432a29e241.png\"\\\n],\n\"scrapes\": [\\\n{\\\n\"url\": \"https://www.firecrawl.dev/\",\\\n\"html\": \"<html><body><h1>Firecrawl</h1></body></html>\"\\\n}\\\n]\n},\n\"metadata\": {\n\"title\": \"Home - Firecrawl\",\n\"description\": \"Firecrawl crawls and converts any website into clean markdown.\",\n\"language\": \"en\",\n\"keywords\": \"Firecrawl,Markdown,Data,Mendable,Langchain\",\n\"robots\": \"follow, index\",\n\"ogTitle\": \"Firecrawl\",\n\"ogDescription\": \"Turn any website into LLM-ready data.\",\n\"ogUrl\": \"https://www.firecrawl.dev/\",\n\"ogImage\": \"https://www.firecrawl.dev/og.png?123\",\n\"ogLocaleAlternate\": [],\n\"ogSiteName\": \"Firecrawl\",\n\"sourceURL\": \"http://google.com\",\n\"statusCode\": 200\n}\n}\n}\n\n```  \nFor more details about the actions parameters, refer to the [API Reference](https://docs.firecrawl.dev/api-reference/endpoint/scrape).",
      "metadata": {
        "title": "Launch Week II | Firecrawl",
        "url": "https://docs.firecrawl.dev/launch-week"
      }
    },
    {
      "id": "2e83c6bb-a557-45f9-9f02-7358654aa643",
      "source": "firecrawl/docs/launch-week.md",
      "content": "[](https://docs.firecrawl.dev/launch-week#day-4-advanced-iframe-scraping) Day 4 - Advanced iframe scraping\n\nWeâ€™re excited to announce comprehensive iframe scraping support in Firecrawl. Our scraper can now seamlessly handle nested iframes, dynamically loaded content, and cross-origin frames - solving one of web scrapingâ€™s most challenging technical hurdles.",
      "metadata": {
        "title": "Launch Week II | Firecrawl",
        "url": "https://docs.firecrawl.dev/launch-week"
      }
    },
    {
      "id": "687ac9ca-7e1a-4aed-98bd-f34535ba2329",
      "source": "firecrawl/docs/launch-week.md",
      "content": "[](https://docs.firecrawl.dev/launch-week#day-4-advanced-iframe-scraping) Day 4 - Advanced iframe scraping > [](https://docs.firecrawl.dev/launch-week#technical-innovation) Technical Innovation\n\nFirecrawl now implements:  \n- Recursive iframe traversal and content extraction\n- Cross-origin iframe handling with proper security context management\n- Smart automatic wait for iframe content to load\n- Support for dynamically injected iframes\n- Proper handling of sandboxed iframes",
      "metadata": {
        "title": "Launch Week II | Firecrawl",
        "url": "https://docs.firecrawl.dev/launch-week"
      }
    },
    {
      "id": "ddd91d88-e4db-4b7b-a58e-75cfe72d037a",
      "source": "firecrawl/docs/launch-week.md",
      "content": "[](https://docs.firecrawl.dev/launch-week#day-4-advanced-iframe-scraping) Day 4 - Advanced iframe scraping > [](https://docs.firecrawl.dev/launch-week#why-it-matters) Why it matters\n\nMany modern websites use iframes for:  \n- Embedded content and widgets\n- Payment forms and secure inputs\n- Third-party integrations\n- Advertisement frames\n- Social media embeds  \nPreviously, these elements were often black boxes in scraping results. Now, you get complete access to iframe content just like any other part of the page.",
      "metadata": {
        "title": "Launch Week II | Firecrawl",
        "url": "https://docs.firecrawl.dev/launch-week"
      }
    },
    {
      "id": "97456f43-19f3-4c87-861c-609342afbbc6",
      "source": "firecrawl/docs/launch-week.md",
      "content": "[](https://docs.firecrawl.dev/launch-week#day-4-advanced-iframe-scraping) Day 4 - Advanced iframe scraping > [](https://docs.firecrawl.dev/launch-week#usage-2) Usage\n\nNo additional configuration needed! The iframe scraping happens automatically when you use any of our scraping or crawling endpoints. Whether youâ€™re using `/scrape` for single pages or `/crawl` for entire websites, iframe content will be seamlessly integrated into your results.",
      "metadata": {
        "title": "Launch Week II | Firecrawl",
        "url": "https://docs.firecrawl.dev/launch-week"
      }
    },
    {
      "id": "638e9117-8fc8-41b5-948c-636b460d884e",
      "source": "firecrawl/docs/launch-week.md",
      "content": "[](https://docs.firecrawl.dev/launch-week#day-3-credit-packs) Day 3 - Credit Packs\n\nCredit Packs allow you to you can easily top up your plan if your running low.\nAdditionally, we now offer Auto Recharge, which automatically recharges your account when youâ€™re approaching your limit.\nTo enable visit the pricing page at [https://www.firecrawl.dev/pricing](https://www.firecrawl.dev/pricing)",
      "metadata": {
        "title": "Launch Week II | Firecrawl",
        "url": "https://docs.firecrawl.dev/launch-week"
      }
    },
    {
      "id": "5a342dcd-7c4a-4c4c-8049-1c71fc0a24c0",
      "source": "firecrawl/docs/launch-week.md",
      "content": "[](https://docs.firecrawl.dev/launch-week#day-3-credit-packs) Day 3 - Credit Packs > [](https://docs.firecrawl.dev/launch-week#credit-packs) Credit Packs\n\nFlexible monthly credit boosts for your projects.  \n- **$9/mo for 1000 credits**\n- Add to any existing plan\n- Choose the amount you need",
      "metadata": {
        "title": "Launch Week II | Firecrawl",
        "url": "https://docs.firecrawl.dev/launch-week"
      }
    },
    {
      "id": "9fcf7e09-c7e9-44a6-bc74-bdbec64280ad",
      "source": "firecrawl/docs/launch-week.md",
      "content": "[](https://docs.firecrawl.dev/launch-week#day-3-credit-packs) Day 3 - Credit Packs > [](https://docs.firecrawl.dev/launch-week#auto-recharge-credits) Auto Recharge Credits\n\nAutomatically top up your account when credits run low.  \n- **$11 per 1000 credits**\n- Enable auto recharge with any subscription plan",
      "metadata": {
        "title": "Launch Week II | Firecrawl",
        "url": "https://docs.firecrawl.dev/launch-week"
      }
    },
    {
      "id": "c9d82413-09dc-4f7d-8a1e-c5926ee47916",
      "source": "firecrawl/docs/launch-week.md",
      "content": "[](https://docs.firecrawl.dev/launch-week#day-2-geolocation) Day 2 - Geolocation\n\nIntroducing location and language settings for scraping requests. Specify country and preferred languages to get relevant content based on your target location and language preferences.",
      "metadata": {
        "title": "Launch Week II | Firecrawl",
        "url": "https://docs.firecrawl.dev/launch-week"
      }
    },
    {
      "id": "af7e59fd-478d-466d-91fb-61238edc6432",
      "source": "firecrawl/docs/launch-week.md",
      "content": "[](https://docs.firecrawl.dev/launch-week#day-2-geolocation) Day 2 - Geolocation > [](https://docs.firecrawl.dev/launch-week#how-it-works) How it works\n\nWhen you specify the location settings, Firecrawl will use an appropriate proxy if available and emulate the corresponding language and timezone settings. By default, the location is set to â€˜USâ€™ if not specified.",
      "metadata": {
        "title": "Launch Week II | Firecrawl",
        "url": "https://docs.firecrawl.dev/launch-week"
      }
    },
    {
      "id": "5cd6e3dd-22db-4362-b037-f746b31f16f4",
      "source": "firecrawl/docs/launch-week.md",
      "content": "[](https://docs.firecrawl.dev/launch-week#day-2-geolocation) Day 2 - Geolocation > [](https://docs.firecrawl.dev/launch-week#usage-3) Usage\n\nTo use the location and language settings, include the `location` object in your request body with the following properties:  \n- `country`: ISO 3166-1 alpha-2 country code (e.g., â€˜USâ€™, â€˜AUâ€™, â€˜DEâ€™, â€˜JPâ€™). Defaults to â€˜USâ€™.\n- `languages`: An array of preferred languages and locales for the request in order of priority. Defaults to the language of the specified location.  \nPython  \nNode  \ncURL  \nCopy  \n```python\nfrom firecrawl import FirecrawlApp\n\napp = FirecrawlApp(api_key=\"fc-YOUR_API_KEY\")\n\n# Scrape a website:\nscrape_result = app.scrape_url('airbnb.com',\nparams={\n'formats': ['markdown', 'html'],\n'location': {\n'country': 'BR',\n'languages': ['pt-BR']\n}\n}\n)\nprint(scrape_result)\n\n```",
      "metadata": {
        "title": "Launch Week II | Firecrawl",
        "url": "https://docs.firecrawl.dev/launch-week"
      }
    },
    {
      "id": "793b2a84-e220-4026-9e71-e11bbc968a6e",
      "source": "firecrawl/docs/launch-week.md",
      "content": "[](https://docs.firecrawl.dev/launch-week#day-1-batch-scrape) Day 1 - Batch Scrape\n\nYou can now scrape multiple URLs at the same time with our new batch endpoint. Ideal for when you donâ€™t need the scraping results immediately.",
      "metadata": {
        "title": "Launch Week II | Firecrawl",
        "url": "https://docs.firecrawl.dev/launch-week"
      }
    },
    {
      "id": "cf3ea0d4-0f55-43df-a064-34e57c2340f1",
      "source": "firecrawl/docs/launch-week.md",
      "content": "[](https://docs.firecrawl.dev/launch-week#day-1-batch-scrape) Day 1 - Batch Scrape > [](https://docs.firecrawl.dev/launch-week#how-it-works-2) How it works\n\nIt is very similar to how the `/crawl` endpoint works. It submits a batch scrape job and returns a job ID to check the status of the batch scrape.  \nThe sdk provides 2 methods, synchronous and asynchronous. The synchronous method will return the results of the batch scrape job, while the asynchronous method will return a job ID that you can use to check the status of the batch scrape.",
      "metadata": {
        "title": "Launch Week II | Firecrawl",
        "url": "https://docs.firecrawl.dev/launch-week"
      }
    },
    {
      "id": "1ab07c68-9f6d-4ac4-a1f1-d53fbb2a4cf9",
      "source": "firecrawl/docs/launch-week.md",
      "content": "[](https://docs.firecrawl.dev/launch-week#day-1-batch-scrape) Day 1 - Batch Scrape > [](https://docs.firecrawl.dev/launch-week#usage-4) Usage\n\nPython  \nNode  \ncURL  \nCopy  \n```python\nfrom firecrawl import FirecrawlApp\n\napp = FirecrawlApp(api_key=\"fc-YOUR_API_KEY\")\n\n# Scrape multiple websites:\nbatch_scrape_result = app.batch_scrape_urls(['firecrawl.dev', 'mendable.ai'], {'formats': ['markdown', 'html']})\nprint(batch_scrape_result)\n\n# Or, you can use the asynchronous method:\nbatch_scrape_job = app.async_batch_scrape_urls(['firecrawl.dev', 'mendable.ai'], {'formats': ['markdown', 'html']})\nprint(batch_scrape_job)\n\n# (async) You can then use the job ID to check the status of the batch scrape:\nbatch_scrape_status = app.check_batch_scrape_status(batch_scrape_job['id'])\nprint(batch_scrape_status)\n\n```",
      "metadata": {
        "title": "Launch Week II | Firecrawl",
        "url": "https://docs.firecrawl.dev/launch-week"
      }
    },
    {
      "id": "22302032-f3db-4c5c-8621-9e0ddacf9cac",
      "source": "firecrawl/docs/launch-week.md",
      "content": "[](https://docs.firecrawl.dev/launch-week#day-1-batch-scrape) Day 1 - Batch Scrape > [](https://docs.firecrawl.dev/launch-week#response) Response\n\nIf youâ€™re using the sync methods from the SDKs, it will return the results of the batch scrape job. Otherwise, it will return a job ID that you can use to check the status of the batch scrape.",
      "metadata": {
        "title": "Launch Week II | Firecrawl",
        "url": "https://docs.firecrawl.dev/launch-week"
      }
    },
    {
      "id": "37ffad08-f051-46ef-b21c-8cbbe592cce0",
      "source": "firecrawl/docs/launch-week.md",
      "content": "[](https://docs.firecrawl.dev/launch-week#day-1-batch-scrape) Day 1 - Batch Scrape > [](https://docs.firecrawl.dev/launch-week#response) Response > [](https://docs.firecrawl.dev/launch-week#synchronous) Synchronous\n\nCompleted  \nCopy  \n```json\n{\n\"status\": \"completed\",\n\"total\": 36,\n\"completed\": 36,\n\"creditsUsed\": 36,\n\"expiresAt\": \"2024-00-00T00:00:00.000Z\",\n\"next\": \"https://api.firecrawl.dev/v1/batch/scrape/123-456-789?skip=26\",\n\"data\": [\\\n{\\\n\"markdown\": \"[Firecrawl Docs home page![light logo](https://mintlify.s3-us-west-1.amazonaws.com/firecrawl/logo/light.svg)!...\",\\\n\"html\": \"<!DOCTYPE html><html lang=\"en\" class=\"js-focus-visible lg:[--scroll-mt:9.5rem]\" data-js-focus-visible=\"\">...\",\\\n\"metadata\": {\\\n\"title\": \"Build a 'Chat with website' using Groq Llama 3 | Firecrawl\",\\\n\"language\": \"en\",\\\n\"sourceURL\": \"https://docs.firecrawl.dev/learn/rag-llama3\",\\\n\"description\": \"Learn how to use Firecrawl, Groq Llama 3, and Langchain to build a 'Chat with your website' bot.\",\\\n\"ogLocaleAlternate\": [],\\\n\"statusCode\": 200\\\n}\\\n},\\\n...\\\n]\\\n}\\\n\\\n```\\\n\\",
      "metadata": {
        "title": "Launch Week II | Firecrawl",
        "url": "https://docs.firecrawl.dev/launch-week"
      }
    },
    {
      "id": "dc0ca721-4c21-482a-b167-77afa51482f1",
      "source": "firecrawl/docs/launch-week.md",
      "content": "[](https://docs.firecrawl.dev/launch-week#day-1-batch-scrape) Day 1 - Batch Scrape > [](https://docs.firecrawl.dev/launch-week#response) Response > [](https://docs.firecrawl.dev/launch-week#asynchronous) Asynchronous\\\n\n\\\nYou can then use the job ID to check the status of the batch scrape by calling the `/batch/scrape/{id}` endpoint. This endpoint is meant to be used while the job is still running or right after it has completed **as batch scrape jobs expire after 24 hours**.\\\n\\\nCopy\\\n\\\n```json\\\n{\\\n\"success\": true,\\\n\"id\": \"123-456-789\",\\\n\"url\": \"https://api.firecrawl.dev/v1/batch/scrape/123-456-789\"\\\n}\\\n\\\n```\\\n\\\n[Suggest edits](https://github.com/hellofirecrawl/docs/edit/main/launch-week.mdx) [Raise issue](https://github.com/hellofirecrawl/docs/issues/new?title=Issue%20on%20docs&body=Path:%20/launch-week)\\\n\\\n[Quickstart](https://docs.firecrawl.dev/introduction) [Welcome to V1](https://docs.firecrawl.dev/v1-welcome)\\\n\\\nOn this page\\\n\\\n- [Day 7 - Faster Markdown Parsing](https://docs.firecrawl.dev/launch-week#day-7-faster-markdown-parsing)\\\n- [Whatâ€™s New?](https://docs.firecrawl.dev/launch-week#whats-new)\\\n- [Day 6 - Mobile Scraping (+ Mobile Screenshots)](https://docs.firecrawl.dev/launch-week#day-6-mobile-scraping-mobile-screenshots)\\\n- [Why Mobile Scraping?](https://docs.firecrawl.dev/launch-week#why-mobile-scraping)\\\n- [Usage](https://docs.firecrawl.dev/launch-week#usage)\\\n- [Day 5 - Actions (2 new actions)](https://docs.firecrawl.dev/launch-week#day-5-actions-2-new-actions)\\\n- [Example](https://docs.firecrawl.dev/launch-week#example)\\\n- [Output](https://docs.firecrawl.dev/launch-week#output)\\\n- [Day 4 - Advanced iframe scraping](https://docs.firecrawl.dev/launch-week#day-4-advanced-iframe-scraping)\\\n- [Technical Innovation](https://docs.firecrawl.dev/launch-week#technical-innovation)\\\n- [Why it matters](https://docs.firecrawl.dev/launch-week#why-it-matters)\\\n- [Usage](https://docs.firecrawl.dev/launch-week#usage-2)\\\n- [Day 3 - Credit Packs](https://docs.firecrawl.dev/launch-week#day-3-credit-packs)\\\n- [Credit Packs](https://docs.firecrawl.dev/launch-week#credit-packs)\\\n- [Auto Recharge Credits](https://docs.firecrawl.dev/launch-week#auto-recharge-credits)\\\n- [Day 2 - Geolocation](https://docs.firecrawl.dev/launch-week#day-2-geolocation)\\\n- [How it works](https://docs.firecrawl.dev/launch-week#how-it-works)\\\n- [Usage](https://docs.firecrawl.dev/launch-week#usage-3)\\\n- [Day 1 - Batch Scrape](https://docs.firecrawl.dev/launch-week#day-1-batch-scrape)\\\n- [How it works](https://docs.firecrawl.dev/launch-week#how-it-works-2)\\\n- [Usage](https://docs.firecrawl.dev/launch-week#usage-4)\\\n- [Response](https://docs.firecrawl.dev/launch-week#response)\\\n- [Synchronous](https://docs.firecrawl.dev/launch-week#synchronous)\\\n- [Asynchronous](https://docs.firecrawl.dev/launch-week#asynchronous)",
      "metadata": {
        "title": "Launch Week II | Firecrawl",
        "url": "https://docs.firecrawl.dev/launch-week"
      }
    },
    {
      "id": "0a0f47f9-b3a5-4935-9084-f81635c2047a",
      "source": "firecrawl/docs/api-reference-endpoint-crawl-get-errors.md",
      "content": "---\ntitle: Firecrawl Docs\nurl: https://docs.firecrawl.dev/api-reference/endpoint/crawl-get-errors\n---  \n[Firecrawl Docs home page![light logo](https://mintlify.s3.us-west-1.amazonaws.com/firecrawl/logo/light.svg)![dark logo](https://mintlify.s3.us-west-1.amazonaws.com/firecrawl/logo/dark.svg)](https://firecrawl.dev/)  \nv1  \nSearch or ask...  \nCtrl K  \nSearch...  \nNavigation  \nCrawl Endpoints  \nGet Crawl Errors  \n[Documentation](https://docs.firecrawl.dev/introduction) [SDKs](https://docs.firecrawl.dev/sdks/overview) [Learn](https://www.firecrawl.dev/blog/category/tutorials) [API Reference](https://docs.firecrawl.dev/api-reference/introduction)  \nGET  \n/  \ncrawl  \n/  \n{id}  \n/  \nerrors  \nTry it  \ncURL  \nPython  \nJavaScript  \nPHP  \nGo  \nJava  \nCopy  \n```\ncurl --request GET \\\n--url https://api.firecrawl.dev/v1/crawl/{id}/errors \\\n--header 'Authorization: Bearer <token>'\n```  \n200  \n402  \n429  \n500  \nCopy  \n```\n{\n\"errors\": [\\\n{\\\n\"id\": \"<string>\",\\\n\"timestamp\": \"<string>\",\\\n\"url\": \"<string>\",\\\n\"error\": \"<string>\"\\\n}\\\n],\n\"robotsBlocked\": [\\\n\"<string>\"\\\n]\n}\n```",
      "metadata": {
        "title": "Firecrawl Docs",
        "url": "https://docs.firecrawl.dev/api-reference/endpoint/crawl-get-errors"
      }
    },
    {
      "id": "2403a048-410e-4591-87ef-07a75fa45955",
      "source": "firecrawl/docs/api-reference-endpoint-crawl-get-errors.md",
      "content": "Authorizations\n\n[](https://docs.firecrawl.dev/api-reference/endpoint/crawl-get-errors#authorization-authorization)  \nAuthorization  \nstring  \nheader  \nrequired  \nBearer authentication header of the form `Bearer <token>`, where `<token>` is your auth token.",
      "metadata": {
        "title": "Firecrawl Docs",
        "url": "https://docs.firecrawl.dev/api-reference/endpoint/crawl-get-errors"
      }
    },
    {
      "id": "32226c0f-eed8-473e-8c9f-fcaaecf0c83a",
      "source": "firecrawl/docs/api-reference-endpoint-crawl-get-errors.md",
      "content": "Path Parameters\n\n[](https://docs.firecrawl.dev/api-reference/endpoint/crawl-get-errors#parameter-id)  \nid  \nstring  \nrequired  \nThe ID of the crawl job",
      "metadata": {
        "title": "Firecrawl Docs",
        "url": "https://docs.firecrawl.dev/api-reference/endpoint/crawl-get-errors"
      }
    },
    {
      "id": "23afa61d-9e73-4fad-96ea-3ec97ccfe017",
      "source": "firecrawl/docs/api-reference-endpoint-crawl-get-errors.md",
      "content": "Response\n\n200 - application/json  \n[](https://docs.firecrawl.dev/api-reference/endpoint/crawl-get-errors#response-errors)  \nerrors  \nobject[]  \nErrored scrape jobs and error details  \nShowchild attributes  \n[](https://docs.firecrawl.dev/api-reference/endpoint/crawl-get-errors#response-errors-id)  \nerrors.id  \nstring  \n[](https://docs.firecrawl.dev/api-reference/endpoint/crawl-get-errors#response-errors-timestamp)  \nerrors.timestamp  \nstring | null  \nISO timestamp of failure  \n[](https://docs.firecrawl.dev/api-reference/endpoint/crawl-get-errors#response-errors-url)  \nerrors.url  \nstring  \nScraped URL  \n[](https://docs.firecrawl.dev/api-reference/endpoint/crawl-get-errors#response-errors-error)  \nerrors.error  \nstring  \nError message  \n[](https://docs.firecrawl.dev/api-reference/endpoint/crawl-get-errors#response-robots-blocked)  \nrobotsBlocked  \nstring[]  \nList of URLs that were attempted in scraping but were blocked by robots.txt  \n[Suggest edits](https://github.com/hellofirecrawl/docs/edit/main/api-reference/endpoint/crawl-get-errors.mdx) [Raise issue](https://github.com/hellofirecrawl/docs/issues/new?title=Issue%20on%20docs&body=Path:%20/api-reference/endpoint/crawl-get-errors)  \n[Cancel Crawl](https://docs.firecrawl.dev/api-reference/endpoint/crawl-delete) [Map](https://docs.firecrawl.dev/api-reference/endpoint/map)  \ncURL  \nPython  \nJavaScript  \nPHP  \nGo  \nJava  \nCopy  \n```\ncurl --request GET \\\n--url https://api.firecrawl.dev/v1/crawl/{id}/errors \\\n--header 'Authorization: Bearer <token>'\n```  \n200  \n402  \n429  \n500  \nCopy  \n```\n{\n\"errors\": [\\\n{\\\n\"id\": \"<string>\",\\\n\"timestamp\": \"<string>\",\\\n\"url\": \"<string>\",\\\n\"error\": \"<string>\"\\\n}\\\n],\n\"robotsBlocked\": [\\\n\"<string>\"\\\n]\n}\n```",
      "metadata": {
        "title": "Firecrawl Docs",
        "url": "https://docs.firecrawl.dev/api-reference/endpoint/crawl-get-errors"
      }
    },
    {
      "id": "85bdb732-0b61-4ae9-a6b0-2d71f9b13ee6",
      "source": "firecrawl/docs/v0-introduction.md",
      "content": "---\ntitle: Quickstart | Firecrawl\nurl: https://docs.firecrawl.dev/v0/introduction\n---  \n[Firecrawl Docs home page![light logo](https://mintlify.s3.us-west-1.amazonaws.com/firecrawl/logo/light.svg)![dark logo](https://mintlify.s3.us-west-1.amazonaws.com/firecrawl/logo/dark.svg)](https://firecrawl.dev/)  \nv0  \nSearch or ask...  \nCtrl K  \nSearch...  \nNavigation  \nGet Started  \nQuickstart  \n[Documentation](https://docs.firecrawl.dev/v0/introduction) [SDKs](https://docs.firecrawl.dev/v0/sdks/python) [Learn](https://www.firecrawl.dev/blog/category/tutorials) [API Reference](https://docs.firecrawl.dev/v0/api-reference/introduction)  \n![Hero Light](https://mintlify.s3.us-west-1.amazonaws.com/firecrawl/images/hero.png)",
      "metadata": {
        "title": "Quickstart | Firecrawl",
        "url": "https://docs.firecrawl.dev/v0/introduction"
      }
    },
    {
      "id": "7ce7c289-f890-4563-9ccc-867ed3c9b05f",
      "source": "firecrawl/docs/v0-introduction.md",
      "content": "[](https://docs.firecrawl.dev/v0/introduction#welcome-to-firecrawl) Welcome to Firecrawl\n\n[Firecrawl](https://firecrawl.dev/?ref=github) is an API service that takes a URL, crawls it, and converts it into clean markdown. We crawl all accessible subpages and give you clean markdown for each. No sitemap required.",
      "metadata": {
        "title": "Quickstart | Firecrawl",
        "url": "https://docs.firecrawl.dev/v0/introduction"
      }
    },
    {
      "id": "5614fd00-cc76-4775-9778-71794dc7a3e6",
      "source": "firecrawl/docs/v0-introduction.md",
      "content": "[](https://docs.firecrawl.dev/v0/introduction#how-to-use-it) How to use it?\n\nWe provide an easy to use API with our hosted version. You can find the playground and documentation [here](https://firecrawl.dev/playground). You can also self host the backend if youâ€™d like.  \nCheck out the following resources to get started:  \n- [x] **API**: [Documentation](https://docs.firecrawl.dev/api-reference/introduction)\n- [x] **SDKs**: [Python](https://docs.firecrawl.dev/sdks/python), [Node](https://docs.firecrawl.dev/sdks/node), [Go](https://docs.firecrawl.dev/sdks/go), [Rust](https://docs.firecrawl.dev/sdks/rust)\n- [x] **LLM Frameworks**: [Langchain (python)](https://python.langchain.com/docs/integrations/document_loaders/firecrawl/), [Langchain (js)](https://js.langchain.com/docs/integrations/document_loaders/web_loaders/firecrawl), [Llama Index](https://docs.llamaindex.ai/en/latest/examples/data_connectors/WebPageDemo/#using-firecrawl-reader), [Crew.ai](https://docs.crewai.com/), [Composio](https://composio.dev/tools/firecrawl/all), [PraisonAI](https://docs.praison.ai/firecrawl/), [Superinterface](https://superinterface.ai/docs/assistants/functions/firecrawl), [Vectorize](https://docs.vectorize.io/integrations/source-connectors/firecrawl)\n- [x] **Low-code Frameworks**: [Dify](https://dify.ai/blog/dify-ai-blog-integrated-with-firecrawl), [Langflow](https://docs.langflow.org/), [Flowise AI](https://docs.flowiseai.com/integrations/langchain/document-loaders/firecrawl), [Cargo](https://docs.getcargo.io/integration/firecrawl), [Pipedream](https://pipedream.com/apps/firecrawl/)\n- [x] **Others**: [Zapier](https://zapier.com/apps/firecrawl/integrations), [Pabbly Connect](https://www.pabbly.com/connect/integrations/firecrawl/)\n- [ ] Want an SDK or Integration? Let us know by opening an issue.  \n**Self-host:** To self-host refer to guide [here](https://docs.firecrawl.dev/contributing/self-host).",
      "metadata": {
        "title": "Quickstart | Firecrawl",
        "url": "https://docs.firecrawl.dev/v0/introduction"
      }
    },
    {
      "id": "7ea8cf46-b6c4-4316-937c-a8ff89b9948b",
      "source": "firecrawl/docs/v0-introduction.md",
      "content": "[](https://docs.firecrawl.dev/v0/introduction#how-to-use-it) How to use it? > [](https://docs.firecrawl.dev/v0/introduction#api-key) API Key\n\nTo use the API, you need to sign up on [Firecrawl](https://firecrawl.dev/) and get an API key.",
      "metadata": {
        "title": "Quickstart | Firecrawl",
        "url": "https://docs.firecrawl.dev/v0/introduction"
      }
    },
    {
      "id": "d52fa030-4652-4e09-8d11-c83f096efa05",
      "source": "firecrawl/docs/v0-introduction.md",
      "content": "[](https://docs.firecrawl.dev/v0/introduction#crawling) Crawling\n\nUsed to crawl a URL and all accessible subpages. This submits a crawl job and returns a job ID to check the status of the crawl.",
      "metadata": {
        "title": "Quickstart | Firecrawl",
        "url": "https://docs.firecrawl.dev/v0/introduction"
      }
    },
    {
      "id": "777dbd3c-1eae-4acd-8070-2fc9e77da57b",
      "source": "firecrawl/docs/v0-introduction.md",
      "content": "[](https://docs.firecrawl.dev/v0/introduction#crawling) Crawling > [](https://docs.firecrawl.dev/v0/introduction#installation) Installation\n\nPython  \nJavaScript  \nGo  \nRust  \nCopy  \n```bash\npip install firecrawl-py\n\n```",
      "metadata": {
        "title": "Quickstart | Firecrawl",
        "url": "https://docs.firecrawl.dev/v0/introduction"
      }
    },
    {
      "id": "586ea680-cbac-499a-8ed7-b3c59d43c868",
      "source": "firecrawl/docs/v0-introduction.md",
      "content": "[](https://docs.firecrawl.dev/v0/introduction#crawling) Crawling > [](https://docs.firecrawl.dev/v0/introduction#usage) Usage\n\nPython  \nJavaScript  \nGo  \nRust  \ncURL  \nCopy  \n```python\nfrom firecrawl import FirecrawlApp\n\napp = FirecrawlApp(api_key=\"YOUR_API_KEY\")\n\ncrawl_result = app.crawl_url('docs.firecrawl.dev', {'crawlerOptions': {'excludes': ['blog/*']}})\n\n# Get the markdown\nfor result in crawl_result:\nprint(result['markdown'])\n\n```  \nIf you are not using the sdk or prefer to use webhook or a different polling method, you can set the `wait_until_done` to `false`.\nThis will return a jobId.  \nFor cURL, /crawl will always return a jobId where you can use to check the status of the crawl.  \nCopy  \n```json\n{ \"jobId\": \"1234-5678-9101\" }\n\n```",
      "metadata": {
        "title": "Quickstart | Firecrawl",
        "url": "https://docs.firecrawl.dev/v0/introduction"
      }
    },
    {
      "id": "fb7f00f3-ffb6-4426-95be-b5a33f52c09a",
      "source": "firecrawl/docs/v0-introduction.md",
      "content": "[](https://docs.firecrawl.dev/v0/introduction#crawling) Crawling > [](https://docs.firecrawl.dev/v0/introduction#check-crawl-job) Check Crawl Job\n\nUsed to check the status of a crawl job and get its result.  \nPython  \nJavaScript  \nGo  \nRust  \ncURL  \nCopy  \n```python\nstatus = app.check_crawl_status(job_id)\n\n```",
      "metadata": {
        "title": "Quickstart | Firecrawl",
        "url": "https://docs.firecrawl.dev/v0/introduction"
      }
    },
    {
      "id": "e859b366-1ca7-44a0-ac49-e387b2dd4bce",
      "source": "firecrawl/docs/v0-introduction.md",
      "content": "[](https://docs.firecrawl.dev/v0/introduction#crawling) Crawling > [](https://docs.firecrawl.dev/v0/introduction#check-crawl-job) Check Crawl Job > [](https://docs.firecrawl.dev/v0/introduction#response) Response\n\nCopy  \n```json\n{\n\"status\": \"completed\",\n\"current\": 22,\n\"total\": 22,\n\"data\": [\\\n{\\\n\"content\": \"Raw Content \",\\\n\"markdown\": \"# Markdown Content\",\\\n\"provider\": \"web-scraper\",\\\n\"metadata\": {\\\n\"title\": \"Firecrawl | Scrape the web reliably for your LLMs\",\\\n\"description\": \"AI for CX and Sales\",\\\n\"language\": null,\\\n\"sourceURL\": \"https://docs.firecrawl.dev/\"\\\n}\\\n}\\\n]\n}\n\n```",
      "metadata": {
        "title": "Quickstart | Firecrawl",
        "url": "https://docs.firecrawl.dev/v0/introduction"
      }
    },
    {
      "id": "c491be22-4448-44f0-b5e0-4268dbcaf7a3",
      "source": "firecrawl/docs/v0-introduction.md",
      "content": "[](https://docs.firecrawl.dev/v0/introduction#scraping) Scraping\n\nTo scrape a single URL, use the `scrape_url` method. It takes the URL as a parameter and returns the scraped data as a dictionary.  \nPython  \nJavaScript  \nGo  \nRust  \ncURL  \nCopy  \n```python\nfrom firecrawl import FirecrawlApp\n\napp = FirecrawlApp(api_key=\"YOUR_API_KEY\")\n\ncontent = app.scrape_url(\"https://docs.firecrawl.dev\")\n\n```",
      "metadata": {
        "title": "Quickstart | Firecrawl",
        "url": "https://docs.firecrawl.dev/v0/introduction"
      }
    },
    {
      "id": "3e3bb1eb-77c6-49ec-aed0-080494ed99f5",
      "source": "firecrawl/docs/v0-introduction.md",
      "content": "[](https://docs.firecrawl.dev/v0/introduction#scraping) Scraping > [](https://docs.firecrawl.dev/v0/introduction#response-2) Response\n\nCopy  \n```json\n{\n\"success\": true,\n\"data\": {\n\"markdown\": \"<string>\",\n\"content\": \"<string>\",\n\"html\": \"<string>\",\n\"rawHtml\": \"<string>\",\n\"metadata\": {\n\"title\": \"<string>\",\n\"description\": \"<string>\",\n\"language\": \"<string>\",\n\"sourceURL\": \"<string>\",\n\"<any other metadata> \": \"<string>\",\n\"pageStatusCode\": 123,\n\"pageError\": \"<string>\"\n},\n\"llm_extraction\": {},\n\"warning\": \"<string>\"\n}\n}\n\n```",
      "metadata": {
        "title": "Quickstart | Firecrawl",
        "url": "https://docs.firecrawl.dev/v0/introduction"
      }
    },
    {
      "id": "022262c0-3d26-4893-844b-1536e5cb0a89",
      "source": "firecrawl/docs/v0-introduction.md",
      "content": "[](https://docs.firecrawl.dev/v0/introduction#extraction) Extraction\n\nWith LLM extraction, you can easily extract structured data from any URL. We support pydantic schemas to make it easier for you too. Here is how you to use it:  \nPython  \nJavaScript  \nGo  \nRust  \ncURL  \nCopy  \n```python\nclass ArticleSchema(BaseModel):\ntitle: str\npoints: int\nby: str\ncommentsURL: str\n\nclass TopArticlesSchema(BaseModel):\ntop: List[ArticleSchema] = Field(..., max_items=5, description=\"Top 5 stories\")\n\ndata = app.scrape_url('https://news.ycombinator.com', {\n'extractorOptions': {\n'extractionSchema': TopArticlesSchema.model_json_schema(),\n'mode': 'llm-extraction'\n},\n'pageOptions':{\n'onlyMainContent': True\n}\n})\nprint(data[\"llm_extraction\"])\n\n```",
      "metadata": {
        "title": "Quickstart | Firecrawl",
        "url": "https://docs.firecrawl.dev/v0/introduction"
      }
    },
    {
      "id": "d1383ce6-4c8f-4ea1-a110-ddf48403b790",
      "source": "firecrawl/docs/v0-introduction.md",
      "content": "[](https://docs.firecrawl.dev/v0/introduction#contributing) Contributing\n\nWe love contributions! Please read our [contributing guide](https://github.com/mendableai/firecrawl/blob/main/CONTRIBUTING.md) before submitting a pull request.  \n[Suggest edits](https://github.com/hellofirecrawl/docs/edit/main/v0/introduction.mdx) [Raise issue](https://github.com/hellofirecrawl/docs/issues/new?title=Issue%20on%20docs&body=Path:%20/v0/introduction)  \n[Advanced Scraping Guide](https://docs.firecrawl.dev/v0/advanced-scraping-guide)  \nOn this page  \n- [Welcome to Firecrawl](https://docs.firecrawl.dev/v0/introduction#welcome-to-firecrawl)\n- [How to use it?](https://docs.firecrawl.dev/v0/introduction#how-to-use-it)\n- [API Key](https://docs.firecrawl.dev/v0/introduction#api-key)\n- [Crawling](https://docs.firecrawl.dev/v0/introduction#crawling)\n- [Installation](https://docs.firecrawl.dev/v0/introduction#installation)\n- [Usage](https://docs.firecrawl.dev/v0/introduction#usage)\n- [Check Crawl Job](https://docs.firecrawl.dev/v0/introduction#check-crawl-job)\n- [Response](https://docs.firecrawl.dev/v0/introduction#response)\n- [Scraping](https://docs.firecrawl.dev/v0/introduction#scraping)\n- [Response](https://docs.firecrawl.dev/v0/introduction#response-2)\n- [Extraction](https://docs.firecrawl.dev/v0/introduction#extraction)\n- [Contributing](https://docs.firecrawl.dev/v0/introduction#contributing)  \n![Hero Light](https://docs.firecrawl.dev/v0/introduction)",
      "metadata": {
        "title": "Quickstart | Firecrawl",
        "url": "https://docs.firecrawl.dev/v0/introduction"
      }
    },
    {
      "id": "f9caf014-8120-4d24-b918-c691497f38eb",
      "source": "firecrawl/docs/integrations-langchain.md",
      "content": "---\ntitle: Langchain | Firecrawl\nurl: https://docs.firecrawl.dev/integrations/langchain\n---  \n[Firecrawl Docs home page![light logo](https://mintlify.s3.us-west-1.amazonaws.com/firecrawl/logo/light.svg)![dark logo](https://mintlify.s3.us-west-1.amazonaws.com/firecrawl/logo/dark.svg)](https://firecrawl.dev/)  \nv1  \nSearch or ask...  \nCtrl K  \nSearch...  \nNavigation  \nIntegrations  \nLangchain  \n[Documentation](https://docs.firecrawl.dev/introduction) [SDKs](https://docs.firecrawl.dev/sdks/overview) [Learn](https://www.firecrawl.dev/blog/category/tutorials) [API Reference](https://docs.firecrawl.dev/api-reference/introduction)  \n> Note: this integration is still using [v0 version of the Firecrawl API](https://docs.firecrawl.dev/v0/introduction). You can install the 0.0.20 version for the Python SDK or the 0.0.36 for the Node SDK.",
      "metadata": {
        "title": "Langchain | Firecrawl",
        "url": "https://docs.firecrawl.dev/integrations/langchain"
      }
    },
    {
      "id": "e478e55c-fc52-4a4a-8032-05d50134b103",
      "source": "firecrawl/docs/integrations-langchain.md",
      "content": "[](https://docs.firecrawl.dev/integrations/langchain#installation) Installation\n\nCopy  \n```bash\npip install firecrawl-py==0.0.20\n\n```",
      "metadata": {
        "title": "Langchain | Firecrawl",
        "url": "https://docs.firecrawl.dev/integrations/langchain"
      }
    },
    {
      "id": "14d6e498-8b5e-434c-86ce-0a3ac4d66a7d",
      "source": "firecrawl/docs/integrations-langchain.md",
      "content": "[](https://docs.firecrawl.dev/integrations/langchain#usage) Usage\n\nYou will need to get your own API key. See [https://firecrawl.dev](https://firecrawl.dev/)  \nCopy  \n```python\nfrom langchain_community.document_loaders import FireCrawlLoader\n\nloader = FireCrawlLoader(\napi_key=\"YOUR_API_KEY\", url=\"https://firecrawl.dev\", mode=\"crawl\"\n)\n\ndocs = loader.load()\n\n```",
      "metadata": {
        "title": "Langchain | Firecrawl",
        "url": "https://docs.firecrawl.dev/integrations/langchain"
      }
    },
    {
      "id": "db3a6f30-519f-4c55-8a89-b56b7ab0b46e",
      "source": "firecrawl/docs/integrations-langchain.md",
      "content": "[](https://docs.firecrawl.dev/integrations/langchain#usage) Usage > [](https://docs.firecrawl.dev/integrations/langchain#modes) Modes\n\nScrape: Scrape single url and return the markdown.\nCrawl: Crawl the url and all accessible sub pages and return the markdown for each one.  \nCopy  \n```python\nloader = FireCrawlLoader(\napi_key=\"YOUR_API_KEY\",\nurl=\"https://firecrawl.dev\",\nmode=\"scrape\",\n)\n\ndata = loader.load()\n\n```",
      "metadata": {
        "title": "Langchain | Firecrawl",
        "url": "https://docs.firecrawl.dev/integrations/langchain"
      }
    },
    {
      "id": "31cff89a-48d4-47ae-9eb1-d3c37ac725cc",
      "source": "firecrawl/docs/integrations-langchain.md",
      "content": "[](https://docs.firecrawl.dev/integrations/langchain#usage) Usage > [](https://docs.firecrawl.dev/integrations/langchain#crawler-options) Crawler Options\n\nYou can also pass params to the loader. This is a dictionary of options to pass to the crawler. See the FireCrawl API documentation for more information.",
      "metadata": {
        "title": "Langchain | Firecrawl",
        "url": "https://docs.firecrawl.dev/integrations/langchain"
      }
    },
    {
      "id": "4039e1da-747e-45f9-883f-aec8c53b0951",
      "source": "firecrawl/docs/integrations-langchain.md",
      "content": "[](https://docs.firecrawl.dev/integrations/langchain#langchain-js) Langchain JS\n\nTo use it in Langchain JS, you can install it via npm:  \nCopy  \n```bash\nnpm install @mendableai/firecrawl-js\n\n```  \nThen, you can use it like this:  \nCopy  \n```typescript\nimport { FireCrawlLoader } from \"langchain/document_loaders/web/firecrawl\";\n\nconst loader = new FireCrawlLoader({\nurl: \"https://firecrawl.dev\", // The URL to scrape\napiKey: process.env.FIRECRAWL_API_KEY, // Optional, defaults to `FIRECRAWL_API_KEY` in your env.\nmode: \"scrape\", // The mode to run the crawler in. Can be \"scrape\" for single urls or \"crawl\" for all accessible subpages\nparams: {\n// optional parameters based on Firecrawl API docs\n// For API documentation, visit https://docs.firecrawl.dev\n},\n});\n\nconst docs = await loader.load();\n\n```  \n[Suggest edits](https://github.com/hellofirecrawl/docs/edit/main/integrations/langchain.mdx) [Raise issue](https://github.com/hellofirecrawl/docs/issues/new?title=Issue%20on%20docs&body=Path:%20/integrations/langchain)  \n[Extract (New)](https://docs.firecrawl.dev/features/extract) [Llamaindex](https://docs.firecrawl.dev/integrations/llamaindex)  \nOn this page  \n- [Installation](https://docs.firecrawl.dev/integrations/langchain#installation)\n- [Usage](https://docs.firecrawl.dev/integrations/langchain#usage)\n- [Modes](https://docs.firecrawl.dev/integrations/langchain#modes)\n- [Crawler Options](https://docs.firecrawl.dev/integrations/langchain#crawler-options)\n- [Langchain JS](https://docs.firecrawl.dev/integrations/langchain#langchain-js)",
      "metadata": {
        "title": "Langchain | Firecrawl",
        "url": "https://docs.firecrawl.dev/integrations/langchain"
      }
    },
    {
      "id": "63cb12d9-9f9e-4830-a97d-9c1a5c904942",
      "source": "firecrawl/docs/v0-sdks-go.md",
      "content": "---\ntitle: Go SDK | Firecrawl\nurl: https://docs.firecrawl.dev/v0/sdks/go\n---  \n[Firecrawl Docs home page![light logo](https://mintlify.s3.us-west-1.amazonaws.com/firecrawl/logo/light.svg)![dark logo](https://mintlify.s3.us-west-1.amazonaws.com/firecrawl/logo/dark.svg)](https://firecrawl.dev/)  \nv0  \nSearch or ask...  \nCtrl K  \nSearch...  \nNavigation  \nSDKs  \nGo  \n[Documentation](https://docs.firecrawl.dev/v0/introduction) [SDKs](https://docs.firecrawl.dev/v0/sdks/python) [Learn](https://www.firecrawl.dev/blog/category/tutorials) [API Reference](https://docs.firecrawl.dev/v0/api-reference/introduction)  \n> Note: this is using [v0 version of the Firecrawl API](https://docs.firecrawl.dev/v0/introduction) which is being deprecated. We recommend switching to [v1](https://docs.firecrawl.dev/sdks/go).",
      "metadata": {
        "title": "Go SDK | Firecrawl",
        "url": "https://docs.firecrawl.dev/v0/sdks/go"
      }
    },
    {
      "id": "ecffa092-c9f8-4257-9376-c64017ede7bf",
      "source": "firecrawl/docs/v0-sdks-go.md",
      "content": "[](https://docs.firecrawl.dev/v0/sdks/go#installation) Installation\n\nTo install the Firecrawl Go SDK, you can use go get:  \nCopy  \n```bash\ngo get github.com/mendableai/firecrawl-go\n\n```",
      "metadata": {
        "title": "Go SDK | Firecrawl",
        "url": "https://docs.firecrawl.dev/v0/sdks/go"
      }
    },
    {
      "id": "ccee48c8-7e20-4c4b-916d-3bfcc3906587",
      "source": "firecrawl/docs/v0-sdks-go.md",
      "content": "[](https://docs.firecrawl.dev/v0/sdks/go#usage) Usage\n\n1. Get an API key from [firecrawl.dev](https://firecrawl.dev/)\n2. Set the API key as an environment variable named `FIRECRAWL_API_KEY` or pass it as a parameter to the `FirecrawlApp` struct.  \nHereâ€™s an example of how to use the SDK with error handling:  \nCopy  \n```go\nimport (\n\"fmt\"\n\"log\"\n\n\"github.com/mendableai/firecrawl-go\"\n)\n\nfunc main() {\n// Initialize the FirecrawlApp with your API key\napp, err := firecrawl.NewFirecrawlApp(\"YOUR_API_KEY\")\nif err != nil {\nlog.Fatalf(\"Failed to initialize FirecrawlApp: %v\", err)\n}\n\n// Scrape a single URL\nscrapedData, err := app.ScrapeURL(\"docs.firecrawl.dev\", nil)\nif err != nil {\nlog.Fatalf(\"Error occurred while scraping: %v\", err)\n}\nfmt.Println(scrapedData)\n\n// Crawl a website\nparams := map[string]any{\n\"pageOptions\": map[string]any{\n\"onlyMainContent\": true,\n},\n}\n\ncrawlResult, err := app.CrawlURL(\"docs.firecrawl.dev\", params)\nif err != nil {\nlog.Fatalf(\"Error occurred while crawling: %v\", err)\n}\nfmt.Println(crawlResult)\n}\n\n```",
      "metadata": {
        "title": "Go SDK | Firecrawl",
        "url": "https://docs.firecrawl.dev/v0/sdks/go"
      }
    },
    {
      "id": "4f9a5e38-f77a-4dd3-9361-ca207cf6dbd9",
      "source": "firecrawl/docs/v0-sdks-go.md",
      "content": "[](https://docs.firecrawl.dev/v0/sdks/go#usage) Usage > [](https://docs.firecrawl.dev/v0/sdks/go#scraping-a-url) Scraping a URL\n\nTo scrape a single URL with error handling, use the `ScrapeURL` method. It takes the URL as a parameter and returns the scraped data as a dictionary.  \nCopy  \n```go\nscrapedData, err := app.ScrapeURL(\"docs.firecrawl.dev\", nil)\nif err != nil {\nlog.Fatalf(\"Failed to scrape URL: %v\", err)\n}\nfmt.Println(scrapedData)\n\n```",
      "metadata": {
        "title": "Go SDK | Firecrawl",
        "url": "https://docs.firecrawl.dev/v0/sdks/go"
      }
    },
    {
      "id": "47cb3bd5-19ce-4172-a37d-ac5a831836dd",
      "source": "firecrawl/docs/v0-sdks-go.md",
      "content": "[](https://docs.firecrawl.dev/v0/sdks/go#usage) Usage > [](https://docs.firecrawl.dev/v0/sdks/go#crawling-a-website) Crawling a Website\n\nTo crawl a website, use the `CrawlUrl` method. It takes the starting URL and optional parameters as arguments. The `params` argument allows you to specify additional options for the crawl job, such as the maximum number of pages to crawl, allowed domains, and the output format.  \nCopy  \n```go\ncrawlParams := map[string]any{\n\"crawlerOptions\": map[string]any{\n\"excludes\": []string{\"blog/*\"},\n\"includes\": []string{}, // leave empty for all pages\n\"limit\": 1000,\n},\n\"pageOptions\": map[string]any{\n\"onlyMainContent\": true,\n},\n}\ncrawlResult, err := app.CrawlURL(\"docs.firecrawl.dev\", crawlParams, true, 2, idempotencyKey)\nif err != nil {\nlog.Fatalf(\"Failed to crawl URL: %v\", err)\n}\nfmt.Println(crawlResult)\n\n```",
      "metadata": {
        "title": "Go SDK | Firecrawl",
        "url": "https://docs.firecrawl.dev/v0/sdks/go"
      }
    },
    {
      "id": "13436a29-0c19-4e23-8716-6e3db35156fa",
      "source": "firecrawl/docs/v0-sdks-go.md",
      "content": "[](https://docs.firecrawl.dev/v0/sdks/go#usage) Usage > [](https://docs.firecrawl.dev/v0/sdks/go#checking-crawl-status) Checking Crawl Status\n\nTo check the status of a crawl job, use the `CheckCrawlStatus` method. It takes the job ID as a parameter and returns the current status of the crawl job.  \nCopy  \n```go\nstatus, err := app.CheckCrawlStatus(jobId)\nif err != nil {\nlog.Fatalf(\"Failed to check crawl status: %v\", err)\n}\nfmt.Println(status)\n\n```",
      "metadata": {
        "title": "Go SDK | Firecrawl",
        "url": "https://docs.firecrawl.dev/v0/sdks/go"
      }
    },
    {
      "id": "c3dae0d2-d559-4201-ae20-cde5e2c07f94",
      "source": "firecrawl/docs/v0-sdks-go.md",
      "content": "[](https://docs.firecrawl.dev/v0/sdks/go#usage) Usage > [](https://docs.firecrawl.dev/v0/sdks/go#canceling-a-crawl-job) Canceling a Crawl Job\n\nTo cancel a crawl job, use the `CancelCrawlJob` method. It takes the job ID as a parameter and returns the cancellation status of the crawl job.  \nCopy  \n```go\ncanceled, err := app.CancelCrawlJob(jobId)\nif err != nil {\nlog.Fatalf(\"Failed to cancel crawl job: %v\", err)\n}\nfmt.Println(canceled)\n\n```",
      "metadata": {
        "title": "Go SDK | Firecrawl",
        "url": "https://docs.firecrawl.dev/v0/sdks/go"
      }
    },
    {
      "id": "f17bd661-1ca6-4898-bc27-9bb36ec1955d",
      "source": "firecrawl/docs/v0-sdks-go.md",
      "content": "[](https://docs.firecrawl.dev/v0/sdks/go#usage) Usage > [](https://docs.firecrawl.dev/v0/sdks/go#extracting-structured-data-from-a-url) Extracting structured data from a URL\n\nWith LLM extraction, you can easily extract structured data from any URL. Here is how you to use it:  \nCopy  \n```go\njsonSchema := map[string]any{\n\"type\": \"object\",\n\"properties\": map[string]any{\n\"top\": map[string]any{\n\"type\": \"array\",\n\"items\": map[string]any{\n\"type\": \"object\",\n\"properties\": map[string]any{\n\"title\": map[string]string{\"type\": \"string\"},\n\"points\": map[string]string{\"type\": \"number\"},\n\"by\": map[string]string{\"type\": \"string\"},\n\"commentsURL\": map[string]string{\"type\": \"string\"},\n},\n\"required\": []string{\"title\", \"points\", \"by\", \"commentsURL\"},\n},\n\"minItems\": 5,\n\"maxItems\": 5,\n\"description\": \"Top 5 stories on Hacker News\",\n},\n},\n\"required\": []string{\"top\"},\n}\n\nllmExtractionParams := map[string]any{\n\"extractorOptions\": firecrawl.ExtractorOptions{\nExtractionSchema: jsonSchema,\n},\n}\n\nscrapeResult, err := app.ScrapeURL(\"https://news.ycombinator.com\", llmExtractionParams)\nif err != nil {\nlog.Fatalf(\"Failed to perform LLM extraction: %v\", err)\n}\nfmt.Println(scrapeResult)\n\n```",
      "metadata": {
        "title": "Go SDK | Firecrawl",
        "url": "https://docs.firecrawl.dev/v0/sdks/go"
      }
    },
    {
      "id": "05b0c1fd-15a3-4609-bd25-46f9b610a930",
      "source": "firecrawl/docs/v0-sdks-go.md",
      "content": "[](https://docs.firecrawl.dev/v0/sdks/go#usage) Usage > [](https://docs.firecrawl.dev/v0/sdks/go#search-for-a-query) Search for a query\n\nTo search the web, get the most relevant results, scrap each page and return the markdown, use the `Search` method. The method takes the query as a parameter and returns the search results.  \nCopy  \n```go\nquery := \"What is firecrawl?\"\nsearchResult, err := app.Search(query)\nif err != nil {\nlog.Fatalf(\"Failed to search: %v\", err)\n}\nfmt.Println(searchResult)\n\n```",
      "metadata": {
        "title": "Go SDK | Firecrawl",
        "url": "https://docs.firecrawl.dev/v0/sdks/go"
      }
    },
    {
      "id": "23df18cc-2b7c-4e32-88a5-a5871e50136c",
      "source": "firecrawl/docs/v0-sdks-go.md",
      "content": "[](https://docs.firecrawl.dev/v0/sdks/go#error-handling) Error Handling\n\nThe SDK handles errors returned by the Firecrawl API and raises appropriate exceptions. If an error occurs during a request, an exception will be raised with a descriptive error message.  \n[Suggest edits](https://github.com/hellofirecrawl/docs/edit/main/v0/sdks/go.mdx) [Raise issue](https://github.com/hellofirecrawl/docs/issues/new?title=Issue%20on%20docs&body=Path:%20/v0/sdks/go)  \n[Node](https://docs.firecrawl.dev/v0/sdks/node) [Rust](https://docs.firecrawl.dev/v0/sdks/rust)  \nOn this page  \n- [Installation](https://docs.firecrawl.dev/v0/sdks/go#installation)\n- [Usage](https://docs.firecrawl.dev/v0/sdks/go#usage)\n- [Scraping a URL](https://docs.firecrawl.dev/v0/sdks/go#scraping-a-url)\n- [Crawling a Website](https://docs.firecrawl.dev/v0/sdks/go#crawling-a-website)\n- [Checking Crawl Status](https://docs.firecrawl.dev/v0/sdks/go#checking-crawl-status)\n- [Canceling a Crawl Job](https://docs.firecrawl.dev/v0/sdks/go#canceling-a-crawl-job)\n- [Extracting structured data from a URL](https://docs.firecrawl.dev/v0/sdks/go#extracting-structured-data-from-a-url)\n- [Search for a query](https://docs.firecrawl.dev/v0/sdks/go#search-for-a-query)\n- [Error Handling](https://docs.firecrawl.dev/v0/sdks/go#error-handling)",
      "metadata": {
        "title": "Go SDK | Firecrawl",
        "url": "https://docs.firecrawl.dev/v0/sdks/go"
      }
    },
    {
      "id": "7e0e9eab-2274-4912-8e3b-87c693c07bdd",
      "source": "firecrawl/docs/api-reference-endpoint-extract.md",
      "content": "---\ntitle: Firecrawl Docs\nurl: https://docs.firecrawl.dev/api-reference/endpoint/extract\n---  \n[Firecrawl Docs home page![light logo](https://mintlify.s3.us-west-1.amazonaws.com/firecrawl/logo/light.svg)![dark logo](https://mintlify.s3.us-west-1.amazonaws.com/firecrawl/logo/dark.svg)](https://firecrawl.dev/)  \nv1  \nSearch or ask...  \nCtrl K  \nSearch...  \nNavigation  \nExtract Endpoints  \nExtract  \n[Documentation](https://docs.firecrawl.dev/introduction) [SDKs](https://docs.firecrawl.dev/sdks/overview) [Learn](https://www.firecrawl.dev/blog/category/tutorials) [API Reference](https://docs.firecrawl.dev/api-reference/introduction)  \nPOST  \n/  \nextract  \nTry it  \ncURL  \nPython  \nJavaScript  \nPHP  \nGo  \nJava  \nCopy  \n```\ncurl --request POST \\\n--url https://api.firecrawl.dev/v1/extract \\\n--header 'Authorization: Bearer <token>' \\\n--header 'Content-Type: application/json' \\\n--data '{\n\"urls\": [\\\n\"<string>\"\\\n],\n\"prompt\": \"<string>\",\n\"schema\": {\n\"property1\": \"<string>\",\n\"property2\": 123\n},\n\"enableWebSearch\": false,\n\"ignoreSitemap\": false,\n\"includeSubdomains\": true,\n\"showSources\": false\n}'\n```  \n200  \n400  \n500  \nCopy  \n```\n{\n\"success\": true,\n\"id\": \"<string>\"\n}\n```",
      "metadata": {
        "title": "Firecrawl Docs",
        "url": "https://docs.firecrawl.dev/api-reference/endpoint/extract"
      }
    },
    {
      "id": "3dbc35d5-a897-4696-98dc-76aff737da4e",
      "source": "firecrawl/docs/api-reference-endpoint-extract.md",
      "content": "Authorizations\n\n[](https://docs.firecrawl.dev/api-reference/endpoint/extract#authorization-authorization)  \nAuthorization  \nstring  \nheader  \nrequired  \nBearer authentication header of the form `Bearer <token>`, where `<token>` is your auth token.",
      "metadata": {
        "title": "Firecrawl Docs",
        "url": "https://docs.firecrawl.dev/api-reference/endpoint/extract"
      }
    },
    {
      "id": "96f015a0-1a97-49d8-b983-9636655eca18",
      "source": "firecrawl/docs/api-reference-endpoint-extract.md",
      "content": "Body\n\napplication/json  \n[](https://docs.firecrawl.dev/api-reference/endpoint/extract#body-urls)  \nurls  \nstring[]  \nrequired  \n[](https://docs.firecrawl.dev/api-reference/endpoint/extract#body-prompt)  \nprompt  \nstring  \nPrompt to guide the extraction process  \n[](https://docs.firecrawl.dev/api-reference/endpoint/extract#body-schema)  \nschema  \nobject  \nSchema to define the structure of the extracted data  \nShowchild attributes  \n[](https://docs.firecrawl.dev/api-reference/endpoint/extract#body-schema-property1)  \nschema.property1  \nstring  \nrequired  \nDescription of property1  \n[](https://docs.firecrawl.dev/api-reference/endpoint/extract#body-schema-property2)  \nschema.property2  \ninteger  \nrequired  \nDescription of property2  \n[](https://docs.firecrawl.dev/api-reference/endpoint/extract#body-enable-web-search)  \nenableWebSearch  \nboolean  \ndefault:  \nfalse  \nWhen true, the extraction will use web search to find additional data  \n[](https://docs.firecrawl.dev/api-reference/endpoint/extract#body-ignore-sitemap)  \nignoreSitemap  \nboolean  \ndefault:  \nfalse  \nWhen true, sitemap.xml files will be ignored during website scanning  \n[](https://docs.firecrawl.dev/api-reference/endpoint/extract#body-include-subdomains)  \nincludeSubdomains  \nboolean  \ndefault:  \ntrue  \nWhen true, subdomains of the provided URLs will also be scanned  \n[](https://docs.firecrawl.dev/api-reference/endpoint/extract#body-show-sources)  \nshowSources  \nboolean  \ndefault:  \nfalse  \nWhen true, the sources used to extract the data will be included in the response as `sources` key",
      "metadata": {
        "title": "Firecrawl Docs",
        "url": "https://docs.firecrawl.dev/api-reference/endpoint/extract"
      }
    },
    {
      "id": "68571af0-2fe3-4b27-b806-e799c1b0c183",
      "source": "firecrawl/docs/api-reference-endpoint-extract.md",
      "content": "Response\n\n200 - application/json  \n[](https://docs.firecrawl.dev/api-reference/endpoint/extract#response-success)  \nsuccess  \nboolean  \n[](https://docs.firecrawl.dev/api-reference/endpoint/extract#response-id)  \nid  \nstring  \n[Suggest edits](https://github.com/hellofirecrawl/docs/edit/main/api-reference/endpoint/extract.mdx) [Raise issue](https://github.com/hellofirecrawl/docs/issues/new?title=Issue%20on%20docs&body=Path:%20/api-reference/endpoint/extract)  \n[Map](https://docs.firecrawl.dev/api-reference/endpoint/map) [Get Extract Status](https://docs.firecrawl.dev/api-reference/endpoint/extract-get)  \ncURL  \nPython  \nJavaScript  \nPHP  \nGo  \nJava  \nCopy  \n```\ncurl --request POST \\\n--url https://api.firecrawl.dev/v1/extract \\\n--header 'Authorization: Bearer <token>' \\\n--header 'Content-Type: application/json' \\\n--data '{\n\"urls\": [\\\n\"<string>\"\\\n],\n\"prompt\": \"<string>\",\n\"schema\": {\n\"property1\": \"<string>\",\n\"property2\": 123\n},\n\"enableWebSearch\": false,\n\"ignoreSitemap\": false,\n\"includeSubdomains\": true,\n\"showSources\": false\n}'\n```  \n200  \n400  \n500  \nCopy  \n```\n{\n\"success\": true,\n\"id\": \"<string>\"\n}\n```",
      "metadata": {
        "title": "Firecrawl Docs",
        "url": "https://docs.firecrawl.dev/api-reference/endpoint/extract"
      }
    },
    {
      "id": "bddd182e-422f-4717-9b87-0413c33ae64e",
      "source": "firecrawl/docs/api-reference-endpoint-crawl-get.md",
      "content": "---\ntitle: Firecrawl Docs\nurl: https://docs.firecrawl.dev/api-reference/endpoint/crawl-get\n---  \n[Firecrawl Docs home page![light logo](https://mintlify.s3.us-west-1.amazonaws.com/firecrawl/logo/light.svg)![dark logo](https://mintlify.s3.us-west-1.amazonaws.com/firecrawl/logo/dark.svg)](https://firecrawl.dev/)  \nv1  \nSearch or ask...  \nCtrl K  \nSearch...  \nNavigation  \nCrawl Endpoints  \nGet Crawl Status  \n[Documentation](https://docs.firecrawl.dev/introduction) [SDKs](https://docs.firecrawl.dev/sdks/overview) [Learn](https://www.firecrawl.dev/blog/category/tutorials) [API Reference](https://docs.firecrawl.dev/api-reference/introduction)  \nGET  \n/  \ncrawl  \n/  \n{id}  \nTry it  \ncURL  \nPython  \nJavaScript  \nPHP  \nGo  \nJava  \nCopy  \n```\ncurl --request GET \\\n--url https://api.firecrawl.dev/v1/crawl/{id} \\\n--header 'Authorization: Bearer <token>'\n```  \n200  \n402  \n429  \n500  \nCopy  \n```\n{\n\"status\": \"<string>\",\n\"total\": 123,\n\"completed\": 123,\n\"creditsUsed\": 123,\n\"expiresAt\": \"2023-11-07T05:31:56Z\",\n\"next\": \"<string>\",\n\"data\": [\\\n{\\\n\"markdown\": \"<string>\",\\\n\"html\": \"<string>\",\\\n\"rawHtml\": \"<string>\",\\\n\"links\": [\\\n\"<string>\"\\\n],\\\n\"screenshot\": \"<string>\",\\\n\"metadata\": {\\\n\"title\": \"<string>\",\\\n\"description\": \"<string>\",\\\n\"language\": \"<string>\",\\\n\"sourceURL\": \"<string>\",\\\n\"<any other metadata> \": \"<string>\",\\\n\"statusCode\": 123,\\\n\"error\": \"<string>\"\\\n}\\\n}\\\n]\n}\n```",
      "metadata": {
        "title": "Firecrawl Docs",
        "url": "https://docs.firecrawl.dev/api-reference/endpoint/crawl-get"
      }
    },
    {
      "id": "6a6a40b5-cf45-4dd4-af3f-aeb09529d897",
      "source": "firecrawl/docs/api-reference-endpoint-crawl-get.md",
      "content": "Authorizations\n\n[](https://docs.firecrawl.dev/api-reference/endpoint/crawl-get#authorization-authorization)  \nAuthorization  \nstring  \nheader  \nrequired  \nBearer authentication header of the form `Bearer <token>`, where `<token>` is your auth token.",
      "metadata": {
        "title": "Firecrawl Docs",
        "url": "https://docs.firecrawl.dev/api-reference/endpoint/crawl-get"
      }
    },
    {
      "id": "b0201799-cf17-493f-932c-257bef425948",
      "source": "firecrawl/docs/api-reference-endpoint-crawl-get.md",
      "content": "Path Parameters\n\n[](https://docs.firecrawl.dev/api-reference/endpoint/crawl-get#parameter-id)  \nid  \nstring  \nrequired  \nThe ID of the crawl job",
      "metadata": {
        "title": "Firecrawl Docs",
        "url": "https://docs.firecrawl.dev/api-reference/endpoint/crawl-get"
      }
    },
    {
      "id": "b89d55f2-c8df-434e-b251-e63157b73e8f",
      "source": "firecrawl/docs/api-reference-endpoint-crawl-get.md",
      "content": "Response\n\n200 - application/json  \n[](https://docs.firecrawl.dev/api-reference/endpoint/crawl-get#response-status)  \nstatus  \nstring  \nThe current status of the crawl. Can be `scraping`, `completed`, or `failed`.  \n[](https://docs.firecrawl.dev/api-reference/endpoint/crawl-get#response-total)  \ntotal  \ninteger  \nThe total number of pages that were attempted to be crawled.  \n[](https://docs.firecrawl.dev/api-reference/endpoint/crawl-get#response-completed)  \ncompleted  \ninteger  \nThe number of pages that have been successfully crawled.  \n[](https://docs.firecrawl.dev/api-reference/endpoint/crawl-get#response-credits-used)  \ncreditsUsed  \ninteger  \nThe number of credits used for the crawl.  \n[](https://docs.firecrawl.dev/api-reference/endpoint/crawl-get#response-expires-at)  \nexpiresAt  \nstring  \nThe date and time when the crawl will expire.  \n[](https://docs.firecrawl.dev/api-reference/endpoint/crawl-get#response-next)  \nnext  \nstring | null  \nThe URL to retrieve the next 10MB of data. Returned if the crawl is not completed or if the response is larger than 10MB.  \n[](https://docs.firecrawl.dev/api-reference/endpoint/crawl-get#response-data)  \ndata  \nobject[]  \nThe data of the crawl.  \nShowchild attributes  \n[](https://docs.firecrawl.dev/api-reference/endpoint/crawl-get#response-data-markdown)  \ndata.markdown  \nstring  \n[](https://docs.firecrawl.dev/api-reference/endpoint/crawl-get#response-data-html)  \ndata.html  \nstring | null  \nHTML version of the content on page if `includeHtml` is true  \n[](https://docs.firecrawl.dev/api-reference/endpoint/crawl-get#response-data-raw-html)  \ndata.rawHtml  \nstring | null  \nRaw HTML content of the page if `includeRawHtml` is true  \n[](https://docs.firecrawl.dev/api-reference/endpoint/crawl-get#response-data-links)  \ndata.links  \nstring[]  \nList of links on the page if `includeLinks` is true  \n[](https://docs.firecrawl.dev/api-reference/endpoint/crawl-get#response-data-screenshot)  \ndata.screenshot  \nstring | null  \nScreenshot of the page if `includeScreenshot` is true  \n[](https://docs.firecrawl.dev/api-reference/endpoint/crawl-get#response-data-metadata)  \ndata.metadata  \nobject  \nShowchild attributes  \n[](https://docs.firecrawl.dev/api-reference/endpoint/crawl-get#response-data-metadata-title)  \ndata.metadata.title  \nstring  \n[](https://docs.firecrawl.dev/api-reference/endpoint/crawl-get#response-data-metadata-description)  \ndata.metadata.description  \nstring  \n[](https://docs.firecrawl.dev/api-reference/endpoint/crawl-get#response-data-metadata-language)  \ndata.metadata.language  \nstring | null  \n[](https://docs.firecrawl.dev/api-reference/endpoint/crawl-get#response-data-metadata-source-url)  \ndata.metadata.sourceURL  \nstring  \n[](https://docs.firecrawl.dev/api-reference/endpoint/crawl-get#response-data-metadata-any-other-metadata)  \ndata.metadata.<any other metadata>  \nstring  \n[](https://docs.firecrawl.dev/api-reference/endpoint/crawl-get#response-data-metadata-status-code)  \ndata.metadata.statusCode  \ninteger  \nThe status code of the page  \n[](https://docs.firecrawl.dev/api-reference/endpoint/crawl-get#response-data-metadata-error)  \ndata.metadata.error  \nstring | null  \nThe error message of the page  \n[Suggest edits](https://github.com/hellofirecrawl/docs/edit/main/api-reference/endpoint/crawl-get.mdx) [Raise issue](https://github.com/hellofirecrawl/docs/issues/new?title=Issue%20on%20docs&body=Path:%20/api-reference/endpoint/crawl-get)  \n[Crawl](https://docs.firecrawl.dev/api-reference/endpoint/crawl-post) [Cancel Crawl](https://docs.firecrawl.dev/api-reference/endpoint/crawl-delete)  \ncURL  \nPython  \nJavaScript  \nPHP  \nGo  \nJava  \nCopy  \n```\ncurl --request GET \\\n--url https://api.firecrawl.dev/v1/crawl/{id} \\\n--header 'Authorization: Bearer <token>'\n```  \n200  \n402  \n429  \n500  \nCopy  \n```\n{\n\"status\": \"<string>\",\n\"total\": 123,\n\"completed\": 123,\n\"creditsUsed\": 123,\n\"expiresAt\": \"2023-11-07T05:31:56Z\",\n\"next\": \"<string>\",\n\"data\": [\\\n{\\\n\"markdown\": \"<string>\",\\\n\"html\": \"<string>\",\\\n\"rawHtml\": \"<string>\",\\\n\"links\": [\\\n\"<string>\"\\\n],\\\n\"screenshot\": \"<string>\",\\\n\"metadata\": {\\\n\"title\": \"<string>\",\\\n\"description\": \"<string>\",\\\n\"language\": \"<string>\",\\\n\"sourceURL\": \"<string>\",\\\n\"<any other metadata> \": \"<string>\",\\\n\"statusCode\": 123,\\\n\"error\": \"<string>\"\\\n}\\\n}\\\n]\n}\n```",
      "metadata": {
        "title": "Firecrawl Docs",
        "url": "https://docs.firecrawl.dev/api-reference/endpoint/crawl-get"
      }
    },
    {
      "id": "33948b62-1728-40ad-a54f-78c5a962432a",
      "source": "firecrawl/docs/api-reference-endpoint-scrape.md",
      "content": "---\ntitle: Firecrawl Docs\nurl: https://docs.firecrawl.dev/api-reference/endpoint/scrape\n---  \n[Firecrawl Docs home page![light logo](https://mintlify.s3.us-west-1.amazonaws.com/firecrawl/logo/light.svg)![dark logo](https://mintlify.s3.us-west-1.amazonaws.com/firecrawl/logo/dark.svg)](https://firecrawl.dev/)  \nv1  \nSearch or ask...  \nCtrl K  \nSearch...  \nNavigation  \nScrape Endpoints  \nScrape  \n[Documentation](https://docs.firecrawl.dev/introduction) [SDKs](https://docs.firecrawl.dev/sdks/overview) [Learn](https://www.firecrawl.dev/blog/category/tutorials) [API Reference](https://docs.firecrawl.dev/api-reference/introduction)  \nPOST  \n/  \nscrape  \nTry it  \ncURL  \nPython  \nJavaScript  \nPHP  \nGo  \nJava  \nCopy  \n```\ncurl --request POST \\\n--url https://api.firecrawl.dev/v1/scrape \\\n--header 'Authorization: Bearer <token>' \\\n--header 'Content-Type: application/json' \\\n--data '{\n\"url\": \"<string>\",\n\"formats\": [\\\n\"markdown\"\\\n],\n\"onlyMainContent\": true,\n\"includeTags\": [\\\n\"<string>\"\\\n],\n\"excludeTags\": [\\\n\"<string>\"\\\n],\n\"headers\": {},\n\"waitFor\": 0,\n\"mobile\": false,\n\"skipTlsVerification\": false,\n\"timeout\": 30000,\n\"jsonOptions\": {\n\"schema\": {},\n\"systemPrompt\": \"<string>\",\n\"prompt\": \"<string>\"\n},\n\"actions\": [\\\n{\\\n\"type\": \"wait\",\\\n\"milliseconds\": 2,\\\n\"selector\": \"#my-element\"\\\n}\\\n],\n\"location\": {\n\"country\": \"US\",\n\"languages\": [\\\n\"en-US\"\\\n]\n},\n\"removeBase64Images\": true,\n\"blockAds\": true\n}'\n```  \n200  \n402  \n429  \n500  \nCopy  \n```\n{\n\"success\": true,\n\"data\": {\n\"markdown\": \"<string>\",\n\"html\": \"<string>\",\n\"rawHtml\": \"<string>\",\n\"screenshot\": \"<string>\",\n\"links\": [\\\n\"<string>\"\\\n],\n\"actions\": {\n\"screenshots\": [\\\n\"<string>\"\\\n]\n},\n\"metadata\": {\n\"title\": \"<string>\",\n\"description\": \"<string>\",\n\"language\": \"<string>\",\n\"sourceURL\": \"<string>\",\n\"<any other metadata> \": \"<string>\",\n\"statusCode\": 123,\n\"error\": \"<string>\"\n},\n\"llm_extraction\": {},\n\"warning\": \"<string>\"\n}\n}\n```",
      "metadata": {
        "title": "Firecrawl Docs",
        "url": "https://docs.firecrawl.dev/api-reference/endpoint/scrape"
      }
    },
    {
      "id": "b5f6e239-4201-4e00-a990-3cee43aa72c5",
      "source": "firecrawl/docs/api-reference-endpoint-scrape.md",
      "content": "Authorizations\n\n[](https://docs.firecrawl.dev/api-reference/endpoint/scrape#authorization-authorization)  \nAuthorization  \nstring  \nheader  \nrequired  \nBearer authentication header of the form `Bearer <token>`, where `<token>` is your auth token.",
      "metadata": {
        "title": "Firecrawl Docs",
        "url": "https://docs.firecrawl.dev/api-reference/endpoint/scrape"
      }
    },
    {
      "id": "3825cc9c-b171-42a7-bac0-2ed613a76a7e",
      "source": "firecrawl/docs/api-reference-endpoint-scrape.md",
      "content": "Body\n\napplication/json  \n[](https://docs.firecrawl.dev/api-reference/endpoint/scrape#body-url)  \nurl  \nstring  \nrequired  \nThe URL to scrape  \n[](https://docs.firecrawl.dev/api-reference/endpoint/scrape#body-formats)  \nformats  \nenum<string>[]  \nFormats to include in the output.  \nAvailable options:  \n`markdown`,  \n`html`,  \n`rawHtml`,  \n`links`,  \n`screenshot`,  \n`screenshot@fullPage`,  \n`json`  \n[](https://docs.firecrawl.dev/api-reference/endpoint/scrape#body-only-main-content)  \nonlyMainContent  \nboolean  \ndefault:  \ntrue  \nOnly return the main content of the page excluding headers, navs, footers, etc.  \n[](https://docs.firecrawl.dev/api-reference/endpoint/scrape#body-include-tags)  \nincludeTags  \nstring[]  \nTags to include in the output.  \n[](https://docs.firecrawl.dev/api-reference/endpoint/scrape#body-exclude-tags)  \nexcludeTags  \nstring[]  \nTags to exclude from the output.  \n[](https://docs.firecrawl.dev/api-reference/endpoint/scrape#body-headers)  \nheaders  \nobject  \nHeaders to send with the request. Can be used to send cookies, user-agent, etc.  \n[](https://docs.firecrawl.dev/api-reference/endpoint/scrape#body-wait-for)  \nwaitFor  \ninteger  \ndefault:  \n0  \nSpecify a delay in milliseconds before fetching the content, allowing the page sufficient time to load.  \n[](https://docs.firecrawl.dev/api-reference/endpoint/scrape#body-mobile)  \nmobile  \nboolean  \ndefault:  \nfalse  \nSet to true if you want to emulate scraping from a mobile device. Useful for testing responsive pages and taking mobile screenshots.  \n[](https://docs.firecrawl.dev/api-reference/endpoint/scrape#body-skip-tls-verification)  \nskipTlsVerification  \nboolean  \ndefault:  \nfalse  \nSkip TLS certificate verification when making requests  \n[](https://docs.firecrawl.dev/api-reference/endpoint/scrape#body-timeout)  \ntimeout  \ninteger  \ndefault:  \n30000  \nTimeout in milliseconds for the request  \n[](https://docs.firecrawl.dev/api-reference/endpoint/scrape#body-json-options)  \njsonOptions  \nobject  \nExtract object  \nShowchild attributes  \n[](https://docs.firecrawl.dev/api-reference/endpoint/scrape#body-json-options-schema)  \njsonOptions.schema  \nobject  \nThe schema to use for the extraction (Optional)  \n[](https://docs.firecrawl.dev/api-reference/endpoint/scrape#body-json-options-system-prompt)  \njsonOptions.systemPrompt  \nstring  \nThe system prompt to use for the extraction (Optional)  \n[](https://docs.firecrawl.dev/api-reference/endpoint/scrape#body-json-options-prompt)  \njsonOptions.prompt  \nstring  \nThe prompt to use for the extraction without a schema (Optional)  \n[](https://docs.firecrawl.dev/api-reference/endpoint/scrape#body-actions)  \nactions  \nobject[]  \nActions to perform on the page before grabbing the content  \n- Wait\n- Screenshot\n- Click\n- Write text\n- Press a key\n- Scroll\n- Scrape\n- Execute JavaScript  \nShowchild attributes  \n[](https://docs.firecrawl.dev/api-reference/endpoint/scrape#body-actions-type)  \nactions.type  \nenum<string>  \nrequired  \nWait for a specified amount of milliseconds  \nAvailable options:  \n`wait`  \n[](https://docs.firecrawl.dev/api-reference/endpoint/scrape#body-actions-milliseconds)  \nactions.milliseconds  \ninteger  \nNumber of milliseconds to wait  \nRequired range: `x > 1`  \n[](https://docs.firecrawl.dev/api-reference/endpoint/scrape#body-actions-selector)  \nactions.selector  \nstring  \nQuery selector to find the element by  \n[](https://docs.firecrawl.dev/api-reference/endpoint/scrape#body-location)  \nlocation  \nobject  \nLocation settings for the request. When specified, this will use an appropriate proxy if available and emulate the corresponding language and timezone settings. Defaults to 'US' if not specified.  \nShowchild attributes  \n[](https://docs.firecrawl.dev/api-reference/endpoint/scrape#body-location-country)  \nlocation.country  \nstring  \ndefault:  \nUS  \nISO 3166-1 alpha-2 country code (e.g., 'US', 'AU', 'DE', 'JP')  \n[](https://docs.firecrawl.dev/api-reference/endpoint/scrape#body-location-languages)  \nlocation.languages  \nstring[]  \nPreferred languages and locales for the request in order of priority. Defaults to the language of the specified location. See https://developer.mozilla.org/en-US/docs/Web/HTTP/Headers/Accept-Language  \n[](https://docs.firecrawl.dev/api-reference/endpoint/scrape#body-remove-base64-images)  \nremoveBase64Images  \nboolean  \nRemoves all base 64 images from the output, which may be overwhelmingly long. The image's alt text remains in the output, but the URL is replaced with a placeholder.  \n[](https://docs.firecrawl.dev/api-reference/endpoint/scrape#body-block-ads)  \nblockAds  \nboolean  \ndefault:  \ntrue  \nEnables ad-blocking and cookie popup blocking.",
      "metadata": {
        "title": "Firecrawl Docs",
        "url": "https://docs.firecrawl.dev/api-reference/endpoint/scrape"
      }
    },
    {
      "id": "303744cf-a2e4-400b-9409-797057c35921",
      "source": "firecrawl/docs/api-reference-endpoint-scrape.md",
      "content": "Response\n\n200 - application/json  \n[](https://docs.firecrawl.dev/api-reference/endpoint/scrape#response-success)  \nsuccess  \nboolean  \n[](https://docs.firecrawl.dev/api-reference/endpoint/scrape#response-data)  \ndata  \nobject  \nShowchild attributes  \n[](https://docs.firecrawl.dev/api-reference/endpoint/scrape#response-data-markdown)  \ndata.markdown  \nstring  \n[](https://docs.firecrawl.dev/api-reference/endpoint/scrape#response-data-html)  \ndata.html  \nstring | null  \nHTML version of the content on page if `html` is in `formats`  \n[](https://docs.firecrawl.dev/api-reference/endpoint/scrape#response-data-raw-html)  \ndata.rawHtml  \nstring | null  \nRaw HTML content of the page if `rawHtml` is in `formats`  \n[](https://docs.firecrawl.dev/api-reference/endpoint/scrape#response-data-screenshot)  \ndata.screenshot  \nstring | null  \nScreenshot of the page if `screenshot` is in `formats`  \n[](https://docs.firecrawl.dev/api-reference/endpoint/scrape#response-data-links)  \ndata.links  \nstring[]  \nList of links on the page if `links` is in `formats`  \n[](https://docs.firecrawl.dev/api-reference/endpoint/scrape#response-data-actions)  \ndata.actions  \nobject | null  \nResults of the actions specified in the `actions` parameter. Only present if the `actions` parameter was provided in the request  \nShowchild attributes  \n[](https://docs.firecrawl.dev/api-reference/endpoint/scrape#response-data-actions-screenshots)  \ndata.actions.screenshots  \nstring[]  \nScreenshot URLs, in the same order as the screenshot actions provided.  \n[](https://docs.firecrawl.dev/api-reference/endpoint/scrape#response-data-metadata)  \ndata.metadata  \nobject  \nShowchild attributes  \n[](https://docs.firecrawl.dev/api-reference/endpoint/scrape#response-data-metadata-title)  \ndata.metadata.title  \nstring  \n[](https://docs.firecrawl.dev/api-reference/endpoint/scrape#response-data-metadata-description)  \ndata.metadata.description  \nstring  \n[](https://docs.firecrawl.dev/api-reference/endpoint/scrape#response-data-metadata-language)  \ndata.metadata.language  \nstring | null  \n[](https://docs.firecrawl.dev/api-reference/endpoint/scrape#response-data-metadata-source-url)  \ndata.metadata.sourceURL  \nstring  \n[](https://docs.firecrawl.dev/api-reference/endpoint/scrape#response-data-metadata-any-other-metadata)  \ndata.metadata.<any other metadata>  \nstring  \n[](https://docs.firecrawl.dev/api-reference/endpoint/scrape#response-data-metadata-status-code)  \ndata.metadata.statusCode  \ninteger  \nThe status code of the page  \n[](https://docs.firecrawl.dev/api-reference/endpoint/scrape#response-data-metadata-error)  \ndata.metadata.error  \nstring | null  \nThe error message of the page  \n[](https://docs.firecrawl.dev/api-reference/endpoint/scrape#response-data-llm-extraction)  \ndata.llm_extraction  \nobject | null  \nDisplayed when using LLM Extraction. Extracted data from the page following the schema defined.  \n[](https://docs.firecrawl.dev/api-reference/endpoint/scrape#response-data-warning)  \ndata.warning  \nstring | null  \nCan be displayed when using LLM Extraction. Warning message will let you know any issues with the extraction.  \n[Suggest edits](https://github.com/hellofirecrawl/docs/edit/main/api-reference/endpoint/scrape.mdx) [Raise issue](https://github.com/hellofirecrawl/docs/issues/new?title=Issue%20on%20docs&body=Path:%20/api-reference/endpoint/scrape)  \n[Introduction](https://docs.firecrawl.dev/api-reference/introduction) [Batch Scrape](https://docs.firecrawl.dev/api-reference/endpoint/batch-scrape)  \ncURL  \nPython  \nJavaScript  \nPHP  \nGo  \nJava  \nCopy  \n```\ncurl --request POST \\\n--url https://api.firecrawl.dev/v1/scrape \\\n--header 'Authorization: Bearer <token>' \\\n--header 'Content-Type: application/json' \\\n--data '{\n\"url\": \"<string>\",\n\"formats\": [\\\n\"markdown\"\\\n],\n\"onlyMainContent\": true,\n\"includeTags\": [\\\n\"<string>\"\\\n],\n\"excludeTags\": [\\\n\"<string>\"\\\n],\n\"headers\": {},\n\"waitFor\": 0,\n\"mobile\": false,\n\"skipTlsVerification\": false,\n\"timeout\": 30000,\n\"jsonOptions\": {\n\"schema\": {},\n\"systemPrompt\": \"<string>\",\n\"prompt\": \"<string>\"\n},\n\"actions\": [\\\n{\\\n\"type\": \"wait\",\\\n\"milliseconds\": 2,\\\n\"selector\": \"#my-element\"\\\n}\\\n],\n\"location\": {\n\"country\": \"US\",\n\"languages\": [\\\n\"en-US\"\\\n]\n},\n\"removeBase64Images\": true,\n\"blockAds\": true\n}'\n```  \n200  \n402  \n429  \n500  \nCopy  \n```\n{\n\"success\": true,\n\"data\": {\n\"markdown\": \"<string>\",\n\"html\": \"<string>\",\n\"rawHtml\": \"<string>\",\n\"screenshot\": \"<string>\",\n\"links\": [\\\n\"<string>\"\\\n],\n\"actions\": {\n\"screenshots\": [\\\n\"<string>\"\\\n]\n},\n\"metadata\": {\n\"title\": \"<string>\",\n\"description\": \"<string>\",\n\"language\": \"<string>\",\n\"sourceURL\": \"<string>\",\n\"<any other metadata> \": \"<string>\",\n\"statusCode\": 123,\n\"error\": \"<string>\"\n},\n\"llm_extraction\": {},\n\"warning\": \"<string>\"\n}\n}\n```",
      "metadata": {
        "title": "Firecrawl Docs",
        "url": "https://docs.firecrawl.dev/api-reference/endpoint/scrape"
      }
    },
    {
      "id": "64ade31d-180d-4224-a46b-83c57b742063",
      "source": "firecrawl/docs/sdks-overview.md",
      "content": "---\ntitle: SDKs | Firecrawl\nurl: https://docs.firecrawl.dev/sdks/overview\n---  \n[Firecrawl Docs home page![light logo](https://mintlify.s3.us-west-1.amazonaws.com/firecrawl/logo/light.svg)![dark logo](https://mintlify.s3.us-west-1.amazonaws.com/firecrawl/logo/dark.svg)](https://firecrawl.dev/)  \nv1  \nSearch or ask...  \nCtrl K  \nSearch...  \nNavigation  \nOverall  \nOverview  \n[Documentation](https://docs.firecrawl.dev/introduction) [SDKs](https://docs.firecrawl.dev/sdks/overview) [Learn](https://www.firecrawl.dev/blog/category/tutorials) [API Reference](https://docs.firecrawl.dev/api-reference/introduction)",
      "metadata": {
        "title": "SDKs | Firecrawl",
        "url": "https://docs.firecrawl.dev/sdks/overview"
      }
    },
    {
      "id": "a2fa5aba-ed8a-4174-b139-9fabd5e9bce2",
      "source": "firecrawl/docs/sdks-overview.md",
      "content": "[](https://docs.firecrawl.dev/sdks/overview#official-sdks) Official SDKs\n\n[**Python SDK** \\\n\\\nExplore the Python SDK for Firecrawl.](https://docs.firecrawl.dev/sdks/python) [**Node SDK** \\\n\\\nExplore the Node SDK for Firecrawl.](https://docs.firecrawl.dev/sdks/node)",
      "metadata": {
        "title": "SDKs | Firecrawl",
        "url": "https://docs.firecrawl.dev/sdks/overview"
      }
    },
    {
      "id": "de67d795-c0f8-41bd-8e44-14ebadcd98c4",
      "source": "firecrawl/docs/sdks-overview.md",
      "content": "[](https://docs.firecrawl.dev/sdks/overview#community-sdks) Community SDKs\n\n[**Go SDK** \\\n\\\nExplore the Go SDK for Firecrawl.](https://docs.firecrawl.dev/sdks/go) [**Rust SDK** \\\n\\\nExplore the Rust SDK for Firecrawl.](https://docs.firecrawl.dev/sdks/rust)  \n[Suggest edits](https://github.com/hellofirecrawl/docs/edit/main/sdks/overview.mdx) [Raise issue](https://github.com/hellofirecrawl/docs/issues/new?title=Issue%20on%20docs&body=Path:%20/sdks/overview)  \n[Python](https://docs.firecrawl.dev/sdks/python)  \nOn this page  \n- [Official SDKs](https://docs.firecrawl.dev/sdks/overview#official-sdks)\n- [Community SDKs](https://docs.firecrawl.dev/sdks/overview#community-sdks)",
      "metadata": {
        "title": "SDKs | Firecrawl",
        "url": "https://docs.firecrawl.dev/sdks/overview"
      }
    },
    {
      "id": "c031c663-d91d-4f9c-87ce-4a24e0c837f3",
      "source": "firecrawl/docs/v0-api-reference-endpoint-search.md",
      "content": "---\ntitle: Firecrawl Docs\nurl: https://docs.firecrawl.dev/v0/api-reference/endpoint/search\n---  \n[Firecrawl Docs home page![light logo](https://mintlify.s3.us-west-1.amazonaws.com/firecrawl/logo/light.svg)![dark logo](https://mintlify.s3.us-west-1.amazonaws.com/firecrawl/logo/dark.svg)](https://firecrawl.dev/)  \nv0  \nSearch or ask...  \nCtrl K  \nSearch...  \nNavigation  \nEndpoints  \nSearch (Beta)  \n[Documentation](https://docs.firecrawl.dev/v0/introduction) [SDKs](https://docs.firecrawl.dev/v0/sdks/python) [Learn](https://www.firecrawl.dev/blog/category/tutorials) [API Reference](https://docs.firecrawl.dev/v0/api-reference/introduction)  \nPOST  \n/  \nsearch  \nTry it  \ncURL  \nPython  \nJavaScript  \nPHP  \nGo  \nJava  \nCopy  \n```\ncurl --request POST \\\n--url https://api.firecrawl.dev/v0/search \\\n--header 'Authorization: Bearer <token>' \\\n--header 'Content-Type: application/json' \\\n--data '{\n\"query\": \"<string>\",\n\"pageOptions\": {\n\"onlyMainContent\": false,\n\"fetchPageContent\": true,\n\"includeHtml\": false,\n\"includeRawHtml\": false\n},\n\"searchOptions\": {\n\"limit\": 123\n}\n}'\n```  \n200  \n402  \n429  \n500  \nCopy  \n```\n{\n\"success\": true,\n\"data\": [\\\n{\\\n\"url\": \"<string>\",\\\n\"markdown\": \"<string>\",\\\n\"content\": \"<string>\",\\\n\"metadata\": {\\\n\"title\": \"<string>\",\\\n\"description\": \"<string>\",\\\n\"language\": \"<string>\",\\\n\"sourceURL\": \"<string>\"\\\n}\\\n}\\\n]\n}\n```  \nThe search endpoint combines a search API with the power of Firecrawl to provide a powerful search experience for whatever query.  \nIt automatically searches the web for the query and returns the most relevant results from the top pages in markdown format. The advantage of this endpoint is that it actually scrap each website on the top result so you always get the full content.  \nThis endpoint is currently in beta and is subject to change.",
      "metadata": {
        "title": "Firecrawl Docs",
        "url": "https://docs.firecrawl.dev/v0/api-reference/endpoint/search"
      }
    },
    {
      "id": "cc6086e5-1185-409a-b824-3964566df8d8",
      "source": "firecrawl/docs/v0-api-reference-endpoint-search.md",
      "content": "Authorizations\n\n[](https://docs.firecrawl.dev/v0/api-reference/endpoint/search#authorization-authorization)  \nAuthorization  \nstring  \nheader  \nrequired  \nBearer authentication header of the form `Bearer <token>`, where `<token>` is your auth token.",
      "metadata": {
        "title": "Firecrawl Docs",
        "url": "https://docs.firecrawl.dev/v0/api-reference/endpoint/search"
      }
    },
    {
      "id": "da3d62f2-1e84-4c92-8783-290712b49907",
      "source": "firecrawl/docs/v0-api-reference-endpoint-search.md",
      "content": "Body\n\napplication/json  \n[](https://docs.firecrawl.dev/v0/api-reference/endpoint/search#body-query)  \nquery  \nstring  \nrequired  \nThe query to search for  \n[](https://docs.firecrawl.dev/v0/api-reference/endpoint/search#body-page-options)  \npageOptions  \nobject  \nShowchild attributes  \n[](https://docs.firecrawl.dev/v0/api-reference/endpoint/search#body-page-options-only-main-content)  \npageOptions.onlyMainContent  \nboolean  \ndefault:  \nfalse  \nOnly return the main content of the page excluding headers, navs, footers, etc.  \n[](https://docs.firecrawl.dev/v0/api-reference/endpoint/search#body-page-options-fetch-page-content)  \npageOptions.fetchPageContent  \nboolean  \ndefault:  \ntrue  \nFetch the content of each page. If false, defaults to a basic fast serp API.  \n[](https://docs.firecrawl.dev/v0/api-reference/endpoint/search#body-page-options-include-html)  \npageOptions.includeHtml  \nboolean  \ndefault:  \nfalse  \nInclude the HTML version of the content on page. Will output a html key in the response.  \n[](https://docs.firecrawl.dev/v0/api-reference/endpoint/search#body-page-options-include-raw-html)  \npageOptions.includeRawHtml  \nboolean  \ndefault:  \nfalse  \nInclude the raw HTML content of the page. Will output a rawHtml key in the response.  \n[](https://docs.firecrawl.dev/v0/api-reference/endpoint/search#body-search-options)  \nsearchOptions  \nobject  \nShowchild attributes  \n[](https://docs.firecrawl.dev/v0/api-reference/endpoint/search#body-search-options-limit)  \nsearchOptions.limit  \ninteger  \nMaximum number of results. Max is 20 during beta.",
      "metadata": {
        "title": "Firecrawl Docs",
        "url": "https://docs.firecrawl.dev/v0/api-reference/endpoint/search"
      }
    },
    {
      "id": "676c426f-4e9a-43e6-a923-f4bbe8b7a7b7",
      "source": "firecrawl/docs/v0-api-reference-endpoint-search.md",
      "content": "Response\n\n200 - application/json  \n[](https://docs.firecrawl.dev/v0/api-reference/endpoint/search#response-success)  \nsuccess  \nboolean  \n[](https://docs.firecrawl.dev/v0/api-reference/endpoint/search#response-data)  \ndata  \nobject[]  \nShowchild attributes  \n[](https://docs.firecrawl.dev/v0/api-reference/endpoint/search#response-data-url)  \ndata.url  \nstring  \n[](https://docs.firecrawl.dev/v0/api-reference/endpoint/search#response-data-markdown)  \ndata.markdown  \nstring  \n[](https://docs.firecrawl.dev/v0/api-reference/endpoint/search#response-data-content)  \ndata.content  \nstring  \n[](https://docs.firecrawl.dev/v0/api-reference/endpoint/search#response-data-metadata)  \ndata.metadata  \nobject  \nShowchild attributes  \n[](https://docs.firecrawl.dev/v0/api-reference/endpoint/search#response-data-metadata-title)  \ndata.metadata.title  \nstring  \n[](https://docs.firecrawl.dev/v0/api-reference/endpoint/search#response-data-metadata-description)  \ndata.metadata.description  \nstring  \n[](https://docs.firecrawl.dev/v0/api-reference/endpoint/search#response-data-metadata-language)  \ndata.metadata.language  \nstring | null  \n[](https://docs.firecrawl.dev/v0/api-reference/endpoint/search#response-data-metadata-source-url)  \ndata.metadata.sourceURL  \nstring  \n[Suggest edits](https://github.com/hellofirecrawl/docs/edit/main/v0/api-reference/endpoint/search.mdx) [Raise issue](https://github.com/hellofirecrawl/docs/issues/new?title=Issue%20on%20docs&body=Path:%20/v0/api-reference/endpoint/search)  \n[Scrape](https://docs.firecrawl.dev/v0/api-reference/endpoint/scrape) [Crawl](https://docs.firecrawl.dev/v0/api-reference/endpoint/crawl)  \ncURL  \nPython  \nJavaScript  \nPHP  \nGo  \nJava  \nCopy  \n```\ncurl --request POST \\\n--url https://api.firecrawl.dev/v0/search \\\n--header 'Authorization: Bearer <token>' \\\n--header 'Content-Type: application/json' \\\n--data '{\n\"query\": \"<string>\",\n\"pageOptions\": {\n\"onlyMainContent\": false,\n\"fetchPageContent\": true,\n\"includeHtml\": false,\n\"includeRawHtml\": false\n},\n\"searchOptions\": {\n\"limit\": 123\n}\n}'\n```  \n200  \n402  \n429  \n500  \nCopy  \n```\n{\n\"success\": true,\n\"data\": [\\\n{\\\n\"url\": \"<string>\",\\\n\"markdown\": \"<string>\",\\\n\"content\": \"<string>\",\\\n\"metadata\": {\\\n\"title\": \"<string>\",\\\n\"description\": \"<string>\",\\\n\"language\": \"<string>\",\\\n\"sourceURL\": \"<string>\"\\\n}\\\n}\\\n]\n}\n```",
      "metadata": {
        "title": "Firecrawl Docs",
        "url": "https://docs.firecrawl.dev/v0/api-reference/endpoint/search"
      }
    },
    {
      "id": "1573f09d-d16d-4599-bbae-f9b47f712298",
      "source": "firecrawl/docs/advanced-scraping-guide.md",
      "content": "---\ntitle: Advanced Scraping Guide | Firecrawl\nurl: https://docs.firecrawl.dev/advanced-scraping-guide\n---  \n[Firecrawl Docs home page![light logo](https://mintlify.s3.us-west-1.amazonaws.com/firecrawl/logo/light.svg)![dark logo](https://mintlify.s3.us-west-1.amazonaws.com/firecrawl/logo/dark.svg)](https://firecrawl.dev/)  \nv1  \nSearch or ask...  \nCtrl K  \nSearch...  \nNavigation  \nGet Started  \nAdvanced Scraping Guide  \n[Documentation](https://docs.firecrawl.dev/introduction) [SDKs](https://docs.firecrawl.dev/sdks/overview) [Learn](https://www.firecrawl.dev/blog/category/tutorials) [API Reference](https://docs.firecrawl.dev/api-reference/introduction)  \nThis guide will walk you through the different endpoints of Firecrawl and how to use them fully with all its parameters.",
      "metadata": {
        "title": "Advanced Scraping Guide | Firecrawl",
        "url": "https://docs.firecrawl.dev/advanced-scraping-guide"
      }
    },
    {
      "id": "e989f3ff-501c-40d0-832b-fefcfc6f5d2a",
      "source": "firecrawl/docs/advanced-scraping-guide.md",
      "content": "[](https://docs.firecrawl.dev/advanced-scraping-guide#basic-scraping-with-firecrawl-scrape) Basic scraping with Firecrawl (/scrape)\n\nTo scrape a single page and get clean markdown content, you can use the `/scrape` endpoint.  \nPython  \nJavaScript  \nGo  \nRust  \ncURL  \nCopy  \n```python\n# pip install firecrawl-py\n\nfrom firecrawl import FirecrawlApp\n\napp = FirecrawlApp(api_key=\"YOUR_API_KEY\")\n\ncontent = app.scrape_url(\"https://docs.firecrawl.dev\")\n\n```",
      "metadata": {
        "title": "Advanced Scraping Guide | Firecrawl",
        "url": "https://docs.firecrawl.dev/advanced-scraping-guide"
      }
    },
    {
      "id": "56d580c8-55ed-4bf0-9b8e-2d3273d57606",
      "source": "firecrawl/docs/advanced-scraping-guide.md",
      "content": "[](https://docs.firecrawl.dev/advanced-scraping-guide#scraping-pdfs) Scraping PDFs\n\n**Firecrawl supports scraping PDFs by default.** You can use the `/scrape` endpoint to scrape a PDF link and get the text content of the PDF. You can disable this by setting `parsePDF` to `false`.",
      "metadata": {
        "title": "Advanced Scraping Guide | Firecrawl",
        "url": "https://docs.firecrawl.dev/advanced-scraping-guide"
      }
    },
    {
      "id": "2411f27d-eee4-4826-9617-a5712a2225af",
      "source": "firecrawl/docs/advanced-scraping-guide.md",
      "content": "[](https://docs.firecrawl.dev/advanced-scraping-guide#scrape-options) Scrape Options\n\nWhen using the `/scrape` endpoint, you can customize the scraping behavior with many parameters. Here are the available options:",
      "metadata": {
        "title": "Advanced Scraping Guide | Firecrawl",
        "url": "https://docs.firecrawl.dev/advanced-scraping-guide"
      }
    },
    {
      "id": "5c857020-ff51-41d8-88e3-67ed4bd4fc92",
      "source": "firecrawl/docs/advanced-scraping-guide.md",
      "content": "[](https://docs.firecrawl.dev/advanced-scraping-guide#scrape-options) Scrape Options > [](https://docs.firecrawl.dev/advanced-scraping-guide#setting-the-content-formats-on-response-with-formats) Setting the content formats on response with `formats`\n\n- **Type**: `array`\n- **Enum**: `[\"markdown\", \"links\", \"html\", \"rawHtml\", \"screenshot\", \"json\"]`\n- **Description**: Specify the formats to include in the response. Options include:  \n- `markdown`: Returns the scraped content in Markdown format.\n- `links`: Includes all hyperlinks found on the page.\n- `html`: Provides the content in HTML format.\n- `rawHtml`: Delivers the raw HTML content, without any processing.\n- `screenshot`: Includes a screenshot of the page as it appears in the browser.\n- `json`: Extracts structured information from the page using the LLM.\n- **Default**: `[\"markdown\"]`",
      "metadata": {
        "title": "Advanced Scraping Guide | Firecrawl",
        "url": "https://docs.firecrawl.dev/advanced-scraping-guide"
      }
    },
    {
      "id": "f062a952-21c7-4a44-a504-9c3316efb778",
      "source": "firecrawl/docs/advanced-scraping-guide.md",
      "content": "[](https://docs.firecrawl.dev/advanced-scraping-guide#scrape-options) Scrape Options > [](https://docs.firecrawl.dev/advanced-scraping-guide#getting-the-full-page-content-as-markdown-with-onlymaincontent) Getting the full page content as markdown with `onlyMainContent`\n\n- **Type**: `boolean`\n- **Description**: By default, the scraper will only return the main content of the page, excluding headers, navigation bars, footers, etc. Set this to `false` to return the full page content.\n- **Default**: `true`",
      "metadata": {
        "title": "Advanced Scraping Guide | Firecrawl",
        "url": "https://docs.firecrawl.dev/advanced-scraping-guide"
      }
    },
    {
      "id": "b341478b-3b19-4a7d-be24-7c5b7285a64d",
      "source": "firecrawl/docs/advanced-scraping-guide.md",
      "content": "[](https://docs.firecrawl.dev/advanced-scraping-guide#scrape-options) Scrape Options > [](https://docs.firecrawl.dev/advanced-scraping-guide#setting-the-tags-to-include-with-includetags) Setting the tags to include with `includeTags`\n\n- **Type**: `array`\n- **Description**: Specify the HTML tags, classes and ids to include in the response.\n- **Default**: undefined",
      "metadata": {
        "title": "Advanced Scraping Guide | Firecrawl",
        "url": "https://docs.firecrawl.dev/advanced-scraping-guide"
      }
    },
    {
      "id": "a811e003-1522-40b4-b209-426b8580f4d6",
      "source": "firecrawl/docs/advanced-scraping-guide.md",
      "content": "[](https://docs.firecrawl.dev/advanced-scraping-guide#scrape-options) Scrape Options > [](https://docs.firecrawl.dev/advanced-scraping-guide#setting-the-tags-to-exclude-with-excludetags) Setting the tags to exclude with `excludeTags`\n\n- **Type**: `array`\n- **Description**: Specify the HTML tags, classes and ids to exclude from the response.\n- **Default**: undefined",
      "metadata": {
        "title": "Advanced Scraping Guide | Firecrawl",
        "url": "https://docs.firecrawl.dev/advanced-scraping-guide"
      }
    },
    {
      "id": "05855c17-8882-4b22-b5c0-b0d1396b5b40",
      "source": "firecrawl/docs/advanced-scraping-guide.md",
      "content": "[](https://docs.firecrawl.dev/advanced-scraping-guide#scrape-options) Scrape Options > [](https://docs.firecrawl.dev/advanced-scraping-guide#waiting-for-the-page-to-load-with-waitfor) Waiting for the page to load with `waitFor`\n\n- **Type**: `integer`\n- **Description**: To be used only as a last resort. Wait for a specified amount of milliseconds for the page to load before fetching content.\n- **Default**: `0`",
      "metadata": {
        "title": "Advanced Scraping Guide | Firecrawl",
        "url": "https://docs.firecrawl.dev/advanced-scraping-guide"
      }
    },
    {
      "id": "382bc244-e9c5-45fd-ae62-c72e86d94ac3",
      "source": "firecrawl/docs/advanced-scraping-guide.md",
      "content": "[](https://docs.firecrawl.dev/advanced-scraping-guide#scrape-options) Scrape Options > [](https://docs.firecrawl.dev/advanced-scraping-guide#setting-the-maximum-timeout) Setting the maximum `timeout`\n\n- **Type**: `integer`\n- **Description**: Set the maximum duration in milliseconds that the scraper will wait for the page to respond before aborting the operation.\n- **Default**: `30000` (30 seconds)",
      "metadata": {
        "title": "Advanced Scraping Guide | Firecrawl",
        "url": "https://docs.firecrawl.dev/advanced-scraping-guide"
      }
    },
    {
      "id": "ecbd13a9-1ec4-42fc-ae1d-b6f77006bd8c",
      "source": "firecrawl/docs/advanced-scraping-guide.md",
      "content": "[](https://docs.firecrawl.dev/advanced-scraping-guide#scrape-options) Scrape Options > [](https://docs.firecrawl.dev/advanced-scraping-guide#example-usage) Example Usage\n\nCopy  \n```bash\ncurl -X POST https://api.firecrawl.dev/v1/scrape \\\n-H '\nContent-Type: application/json' \\\n-H 'Authorization : Bearer YOUR_API_KEY' \\\n-d '{\n\"url\": \"https://docs.firecrawl.dev\",\n\"formats\": [\"markdown\", \"links\", \"html\", \"rawHtml\", \"screenshot\"],\n\"includeTags\": [\"h1\", \"p\", \"a\", \".main-content\"],\n\"excludeTags\": [\"#ad\", \"#footer\"],\n\"onlyMainContent\": false,\n\"waitFor\": 1000,\n\"timeout\": 15000\n}'\n\n```  \nIn this example, the scraper will:  \n- Return the full page content as markdown.\n- Include the markdown, raw HTML, HTML, links and screenshot in the response.\n- The response will include only the HTML tags `<h1>`, `<p>`, `<a>`, and elements with the class `.main-content`, while excluding any elements with the IDs `#ad` and `#footer`.\n- Wait for 1000 milliseconds (1 second) for the page to load before fetching the content.\n- Set the maximum duration of the scrape request to 15000 milliseconds (15 seconds).  \nHere is the API Reference for it: [Scrape Endpoint Documentation](https://docs.firecrawl.dev/api-reference/endpoint/scrape)",
      "metadata": {
        "title": "Advanced Scraping Guide | Firecrawl",
        "url": "https://docs.firecrawl.dev/advanced-scraping-guide"
      }
    },
    {
      "id": "e07029a7-a544-49a9-894b-d9e87992703e",
      "source": "firecrawl/docs/advanced-scraping-guide.md",
      "content": "[](https://docs.firecrawl.dev/advanced-scraping-guide#extractor-options) Extractor Options\n\nWhen using the `/scrape` endpoint, you can specify options for **extracting structured information** from the page content using the `extract` parameter. Here are the available options:",
      "metadata": {
        "title": "Advanced Scraping Guide | Firecrawl",
        "url": "https://docs.firecrawl.dev/advanced-scraping-guide"
      }
    },
    {
      "id": "b15b75de-66fa-442b-911e-1d686a245761",
      "source": "firecrawl/docs/advanced-scraping-guide.md",
      "content": "[](https://docs.firecrawl.dev/advanced-scraping-guide#extractor-options) Extractor Options > [](https://docs.firecrawl.dev/advanced-scraping-guide#schema) schema\n\n- **Type**: `object`\n- **Required**: False if prompt is provided\n- **Description**: The schema for the data to be extracted. This defines the structure of the extracted data.",
      "metadata": {
        "title": "Advanced Scraping Guide | Firecrawl",
        "url": "https://docs.firecrawl.dev/advanced-scraping-guide"
      }
    },
    {
      "id": "6d1f7678-9623-48cc-9dee-9f837bb49519",
      "source": "firecrawl/docs/advanced-scraping-guide.md",
      "content": "[](https://docs.firecrawl.dev/advanced-scraping-guide#extractor-options) Extractor Options > [](https://docs.firecrawl.dev/advanced-scraping-guide#system-prompt) system prompt\n\n- **Type**: `string`\n- **Required**: False\n- **Description**: System prompt for the LLM.",
      "metadata": {
        "title": "Advanced Scraping Guide | Firecrawl",
        "url": "https://docs.firecrawl.dev/advanced-scraping-guide"
      }
    },
    {
      "id": "a71fb27b-43e7-464a-b537-b76ef4294584",
      "source": "firecrawl/docs/advanced-scraping-guide.md",
      "content": "[](https://docs.firecrawl.dev/advanced-scraping-guide#extractor-options) Extractor Options > [](https://docs.firecrawl.dev/advanced-scraping-guide#prompt) prompt\n\n- **Type**: `string`\n- **Required**: False if schema is provided\n- **Description**: A prompt for the LLM to extract the data in the correct structure.\n- **Example**: `\"Extract the features of the product\"`",
      "metadata": {
        "title": "Advanced Scraping Guide | Firecrawl",
        "url": "https://docs.firecrawl.dev/advanced-scraping-guide"
      }
    },
    {
      "id": "eee36d51-1822-425d-9b5b-1c1a5e0ea75b",
      "source": "firecrawl/docs/advanced-scraping-guide.md",
      "content": "[](https://docs.firecrawl.dev/advanced-scraping-guide#extractor-options) Extractor Options > [](https://docs.firecrawl.dev/advanced-scraping-guide#example-usage-2) Example Usage\n\nCopy  \n```bash\ncurl -X POST https://api.firecrawl.dev/v0/scrape \\\n-H 'Content-Type: application/json' \\\n-H 'Authorization: Bearer YOUR_API_KEY' \\\n-d '{\n\"url\": \"https://firecrawl.dev\",\n\"formats\": [\"markdown\", \"json\"],\n\"json\": {\n\"prompt\": \"Extract the features of the product\"\n}\n}'\n\n```  \nCopy  \n```json\n{\n\"success\": true,\n\"data\": {\n\"content\": \"Raw Content\",\n\"metadata\": {\n\"title\": \"Mendable\",\n\"description\": \"Mendable allows you to easily build AI chat applications. Ingest, customize, then deploy with one line of code anywhere you want. Brought to you by SideGuide\",\n\"robots\": \"follow, index\",\n\"ogTitle\": \"Mendable\",\n\"ogDescription\": \"Mendable allows you to easily build AI chat applications. Ingest, customize, then deploy with one line of code anywhere you want. Brought to you by SideGuide\",\n\"ogUrl\": \"https://docs.firecrawl.dev/\",\n\"ogImage\": \"https://docs.firecrawl.dev/mendable_new_og1.png\",\n\"ogLocaleAlternate\": [],\n\"ogSiteName\": \"Mendable\",\n\"sourceURL\": \"https://docs.firecrawl.dev/\",\n\"statusCode\": 200\n},\n\"extract\": {\n\"product\": \"Firecrawl\",\n\"features\": {\n\"general\": {\n\"description\": \"Turn websites into LLM-ready data.\",\n\"openSource\": true,\n\"freeCredits\": 500,\n\"useCases\": [\\\n\"AI applications\",\\\n\"Data science\",\\\n\"Market research\",\\\n\"Content aggregation\"\\\n]\n},\n\"crawlingAndScraping\": {\n\"crawlAllAccessiblePages\": true,\n\"noSitemapRequired\": true,\n\"dynamicContentHandling\": true,\n\"dataCleanliness\": {\n\"process\": \"Advanced algorithms\",\n\"outputFormat\": \"Markdown\"\n}\n},\n...\n}\n}\n}\n}\n\n```",
      "metadata": {
        "title": "Advanced Scraping Guide | Firecrawl",
        "url": "https://docs.firecrawl.dev/advanced-scraping-guide"
      }
    },
    {
      "id": "3582eb7a-a224-4fd0-8d2c-9f97416e1f66",
      "source": "firecrawl/docs/advanced-scraping-guide.md",
      "content": "[](https://docs.firecrawl.dev/advanced-scraping-guide#actions) Actions\n\nWhen using the `/scrape` endpoint, Firecrawl allows you to perform various actions on a web page before scraping its content. This is particularly useful for interacting with dynamic content, navigating through pages, or accessing content that requires user interaction.",
      "metadata": {
        "title": "Advanced Scraping Guide | Firecrawl",
        "url": "https://docs.firecrawl.dev/advanced-scraping-guide"
      }
    },
    {
      "id": "ffa1e257-9e30-4919-82b7-f83d46cc175d",
      "source": "firecrawl/docs/advanced-scraping-guide.md",
      "content": "[](https://docs.firecrawl.dev/advanced-scraping-guide#actions) Actions > [](https://docs.firecrawl.dev/advanced-scraping-guide#available-actions) Available Actions > [](https://docs.firecrawl.dev/advanced-scraping-guide#wait) wait\n\n- **Type**: `object`\n- **Description**: Wait for a specified amount of milliseconds.\n- **Properties**:  \n- `type`: `\"wait\"`\n- `milliseconds`: Number of milliseconds to wait.\n- **Example**:  \nCopy  \n```json\n{\n\"type\": \"wait\",\n\"milliseconds\": 2000\n}\n\n```",
      "metadata": {
        "title": "Advanced Scraping Guide | Firecrawl",
        "url": "https://docs.firecrawl.dev/advanced-scraping-guide"
      }
    },
    {
      "id": "58428072-cc64-42c7-b411-e2a1015e66c9",
      "source": "firecrawl/docs/advanced-scraping-guide.md",
      "content": "[](https://docs.firecrawl.dev/advanced-scraping-guide#actions) Actions > [](https://docs.firecrawl.dev/advanced-scraping-guide#available-actions) Available Actions > [](https://docs.firecrawl.dev/advanced-scraping-guide#screenshot) screenshot\n\n- **Type**: `object`\n- **Description**: Take a screenshot.\n- **Properties**:  \n- `type`: `\"screenshot\"`\n- `fullPage`: Should the screenshot be full-page or viewport sized? (default: `false`)\n- **Example**:  \nCopy  \n```json\n{\n\"type\": \"screenshot\",\n\"fullPage\": true\n}\n\n```",
      "metadata": {
        "title": "Advanced Scraping Guide | Firecrawl",
        "url": "https://docs.firecrawl.dev/advanced-scraping-guide"
      }
    },
    {
      "id": "ec25dbd9-cf1e-4a7d-b274-65210860b2c7",
      "source": "firecrawl/docs/advanced-scraping-guide.md",
      "content": "[](https://docs.firecrawl.dev/advanced-scraping-guide#actions) Actions > [](https://docs.firecrawl.dev/advanced-scraping-guide#available-actions) Available Actions > [](https://docs.firecrawl.dev/advanced-scraping-guide#click) click\n\n- **Type**: `object`\n- **Description**: Click on an element.\n- **Properties**:  \n- `type`: `\"click\"`\n- `selector`: Query selector to find the element by.\n- **Example**:  \nCopy  \n```json\n{\n\"type\": \"click\",\n\"selector\": \"#load-more-button\"\n}\n\n```",
      "metadata": {
        "title": "Advanced Scraping Guide | Firecrawl",
        "url": "https://docs.firecrawl.dev/advanced-scraping-guide"
      }
    },
    {
      "id": "4f5507c5-df7f-4795-8064-ff1a96c88754",
      "source": "firecrawl/docs/advanced-scraping-guide.md",
      "content": "[](https://docs.firecrawl.dev/advanced-scraping-guide#actions) Actions > [](https://docs.firecrawl.dev/advanced-scraping-guide#available-actions) Available Actions > [](https://docs.firecrawl.dev/advanced-scraping-guide#write) write\n\n- **Type**: `object`\n- **Description**: Write text into an input field.\n- **Properties**:  \n- `type`: `\"write\"`\n- `text`: Text to type.\n- `selector`: Query selector for the input field.\n- **Example**:  \nCopy  \n```json\n{\n\"type\": \"write\",\n\"text\": \"Hello, world!\",\n\"selector\": \"#search-input\"\n}\n\n```",
      "metadata": {
        "title": "Advanced Scraping Guide | Firecrawl",
        "url": "https://docs.firecrawl.dev/advanced-scraping-guide"
      }
    },
    {
      "id": "c51ad226-6b6f-41c5-89f0-42a87ae75d27",
      "source": "firecrawl/docs/advanced-scraping-guide.md",
      "content": "[](https://docs.firecrawl.dev/advanced-scraping-guide#actions) Actions > [](https://docs.firecrawl.dev/advanced-scraping-guide#available-actions) Available Actions > [](https://docs.firecrawl.dev/advanced-scraping-guide#press) press\n\n- **Type**: `object`\n- **Description**: Press a key on the page.\n- **Properties**:  \n- `type`: `\"press\"`\n- `key`: Key to press.\n- **Example**:  \nCopy  \n```json\n{\n\"type\": \"press\",\n\"key\": \"Enter\"\n}\n\n```",
      "metadata": {
        "title": "Advanced Scraping Guide | Firecrawl",
        "url": "https://docs.firecrawl.dev/advanced-scraping-guide"
      }
    },
    {
      "id": "b478a431-5ce9-45d7-86d9-8320a7cf9fad",
      "source": "firecrawl/docs/advanced-scraping-guide.md",
      "content": "[](https://docs.firecrawl.dev/advanced-scraping-guide#actions) Actions > [](https://docs.firecrawl.dev/advanced-scraping-guide#available-actions) Available Actions > [](https://docs.firecrawl.dev/advanced-scraping-guide#scroll) scroll\n\n- **Type**: `object`\n- **Description**: Scroll the page.\n- **Properties**:  \n- `type`: `\"scroll\"`\n- `direction`: Direction to scroll ( `\"up\"` or `\"down\"`).\n- `amount`: Amount to scroll in pixels.\n- **Example**:  \nCopy  \n```json\n{\n\"type\": \"scroll\",\n\"direction\": \"down\",\n\"amount\": 500\n}\n\n```  \nFor more details about the actions parameters, refer to the [API Reference](https://docs.firecrawl.dev/api-reference/endpoint/scrape).",
      "metadata": {
        "title": "Advanced Scraping Guide | Firecrawl",
        "url": "https://docs.firecrawl.dev/advanced-scraping-guide"
      }
    },
    {
      "id": "9d1bf74f-0cc2-4da5-b21f-bcdea03b0e2b",
      "source": "firecrawl/docs/advanced-scraping-guide.md",
      "content": "[](https://docs.firecrawl.dev/advanced-scraping-guide#crawling-multiple-pages) Crawling Multiple Pages\n\nTo crawl multiple pages, you can use the `/crawl` endpoint. This endpoint allows you to specify a base URL you want to crawl and all accessible subpages will be crawled.  \nCopy  \n```bash\ncurl -X POST https://api.firecrawl.dev/v1/crawl \\\n-H 'Content-Type: application/json' \\\n-H 'Authorization: Bearer YOUR_API_KEY' \\\n-d '{\n\"url\": \"https://docs.firecrawl.dev\"\n}'\n\n```  \nReturns a id  \nCopy  \n```json\n{ \"id\": \"1234-5678-9101\" }\n\n```",
      "metadata": {
        "title": "Advanced Scraping Guide | Firecrawl",
        "url": "https://docs.firecrawl.dev/advanced-scraping-guide"
      }
    },
    {
      "id": "100db5de-e49c-41e9-98d5-47518a975dd6",
      "source": "firecrawl/docs/advanced-scraping-guide.md",
      "content": "[](https://docs.firecrawl.dev/advanced-scraping-guide#crawling-multiple-pages) Crawling Multiple Pages > [](https://docs.firecrawl.dev/advanced-scraping-guide#check-crawl-job) Check Crawl Job\n\nUsed to check the status of a crawl job and get its result.  \nCopy  \n```bash\ncurl -X GET https://api.firecrawl.dev/v1/crawl/1234-5678-9101 \\\n-H 'Content-Type: application/json' \\\n-H 'Authorization: Bearer YOUR_API_KEY'\n\n```",
      "metadata": {
        "title": "Advanced Scraping Guide | Firecrawl",
        "url": "https://docs.firecrawl.dev/advanced-scraping-guide"
      }
    },
    {
      "id": "cd0715a2-fe33-49c3-b250-a898113632d3",
      "source": "firecrawl/docs/advanced-scraping-guide.md",
      "content": "[](https://docs.firecrawl.dev/advanced-scraping-guide#crawling-multiple-pages) Crawling Multiple Pages > [](https://docs.firecrawl.dev/advanced-scraping-guide#check-crawl-job) Check Crawl Job > [](https://docs.firecrawl.dev/advanced-scraping-guide#pagination-next-url) Pagination/Next URL\n\nIf the content is larger than 10MB or if the crawl job is still running, the response will include a `next` parameter. This parameter is a URL to the next page of results. You can use this parameter to get the next page of results.",
      "metadata": {
        "title": "Advanced Scraping Guide | Firecrawl",
        "url": "https://docs.firecrawl.dev/advanced-scraping-guide"
      }
    },
    {
      "id": "d8c53da8-b479-44e9-882e-b939ac985a25",
      "source": "firecrawl/docs/advanced-scraping-guide.md",
      "content": "[](https://docs.firecrawl.dev/advanced-scraping-guide#crawling-multiple-pages) Crawling Multiple Pages > [](https://docs.firecrawl.dev/advanced-scraping-guide#crawler-options) Crawler Options\n\nWhen using the `/crawl` endpoint, you can customize the crawling behavior with request body parameters. Here are the available options:",
      "metadata": {
        "title": "Advanced Scraping Guide | Firecrawl",
        "url": "https://docs.firecrawl.dev/advanced-scraping-guide"
      }
    },
    {
      "id": "240bd19d-d822-4afd-b04a-6a619a60854b",
      "source": "firecrawl/docs/advanced-scraping-guide.md",
      "content": "[](https://docs.firecrawl.dev/advanced-scraping-guide#crawling-multiple-pages) Crawling Multiple Pages > [](https://docs.firecrawl.dev/advanced-scraping-guide#crawler-options) Crawler Options > [](https://docs.firecrawl.dev/advanced-scraping-guide#includepaths) `includePaths`\n\n- **Type**: `array`\n- **Description**: URL patterns to include in the crawl. Only URLs matching these patterns will be crawled.\n- **Example**: `[\"/blog/*\", \"/products/*\"]`",
      "metadata": {
        "title": "Advanced Scraping Guide | Firecrawl",
        "url": "https://docs.firecrawl.dev/advanced-scraping-guide"
      }
    },
    {
      "id": "deab30ee-e3cc-4d15-b0e6-57ce46e057ab",
      "source": "firecrawl/docs/advanced-scraping-guide.md",
      "content": "[](https://docs.firecrawl.dev/advanced-scraping-guide#crawling-multiple-pages) Crawling Multiple Pages > [](https://docs.firecrawl.dev/advanced-scraping-guide#crawler-options) Crawler Options > [](https://docs.firecrawl.dev/advanced-scraping-guide#excludepaths) `excludePaths`\n\n- **Type**: `array`\n- **Description**: URL patterns to exclude from the crawl. URLs matching these patterns will be skipped.\n- **Example**: `[\"/admin/*\", \"/login/*\"]`",
      "metadata": {
        "title": "Advanced Scraping Guide | Firecrawl",
        "url": "https://docs.firecrawl.dev/advanced-scraping-guide"
      }
    },
    {
      "id": "cfb707d4-d069-421f-99f3-754c2e5e8281",
      "source": "firecrawl/docs/advanced-scraping-guide.md",
      "content": "[](https://docs.firecrawl.dev/advanced-scraping-guide#crawling-multiple-pages) Crawling Multiple Pages > [](https://docs.firecrawl.dev/advanced-scraping-guide#crawler-options) Crawler Options > [](https://docs.firecrawl.dev/advanced-scraping-guide#maxdepth) `maxDepth`\n\n- **Type**: `integer`\n- **Description**: Maximum depth to crawl relative to the entered URL. A maxDepth of 0 scrapes only the entered URL. A maxDepth of 1 scrapes the entered URL and all pages one level deep. A maxDepth of 2 scrapes the entered URL and all pages up to two levels deep. Higher values follow the same pattern.\n- **Example**: `2`",
      "metadata": {
        "title": "Advanced Scraping Guide | Firecrawl",
        "url": "https://docs.firecrawl.dev/advanced-scraping-guide"
      }
    },
    {
      "id": "134faf18-70eb-48df-877e-b5c99629a26f",
      "source": "firecrawl/docs/advanced-scraping-guide.md",
      "content": "[](https://docs.firecrawl.dev/advanced-scraping-guide#crawling-multiple-pages) Crawling Multiple Pages > [](https://docs.firecrawl.dev/advanced-scraping-guide#crawler-options) Crawler Options > [](https://docs.firecrawl.dev/advanced-scraping-guide#limit) `limit`\n\n- **Type**: `integer`\n- **Description**: Maximum number of pages to crawl.\n- **Default**: `10000`",
      "metadata": {
        "title": "Advanced Scraping Guide | Firecrawl",
        "url": "https://docs.firecrawl.dev/advanced-scraping-guide"
      }
    },
    {
      "id": "4ceab18d-c137-456d-bb5a-ea3cfa5d4814",
      "source": "firecrawl/docs/advanced-scraping-guide.md",
      "content": "[](https://docs.firecrawl.dev/advanced-scraping-guide#crawling-multiple-pages) Crawling Multiple Pages > [](https://docs.firecrawl.dev/advanced-scraping-guide#crawler-options) Crawler Options > [](https://docs.firecrawl.dev/advanced-scraping-guide#allowbackwardlinks) `allowBackwardLinks`\n\n- **Type**: `boolean`\n- **Description**: This option permits the crawler to navigate to URLs that are higher in the directory structure than the base URL. For instance, if the base URL is `example.com/blog/topic`, enabling this option allows crawling to pages like `example.com/blog` or `example.com`, which are backward in the path hierarchy relative to the base URL.\n- **Default**: `false`",
      "metadata": {
        "title": "Advanced Scraping Guide | Firecrawl",
        "url": "https://docs.firecrawl.dev/advanced-scraping-guide"
      }
    },
    {
      "id": "72eec0f4-c4a8-46b8-b678-2a7741467147",
      "source": "firecrawl/docs/advanced-scraping-guide.md",
      "content": "[](https://docs.firecrawl.dev/advanced-scraping-guide#crawling-multiple-pages) Crawling Multiple Pages > [](https://docs.firecrawl.dev/advanced-scraping-guide#allowexternallinks) `allowExternalLinks`\n\n- **Type**: `boolean`\n- **Description**: This option allows the crawler to follow links that point to external domains. Be careful with this option, as it can cause the crawl to stop only based only on the `limit` and `maxDepth` values.\n- **Default**: `false`",
      "metadata": {
        "title": "Advanced Scraping Guide | Firecrawl",
        "url": "https://docs.firecrawl.dev/advanced-scraping-guide"
      }
    },
    {
      "id": "677d5848-71ca-4168-afe4-bf1f40d002b3",
      "source": "firecrawl/docs/advanced-scraping-guide.md",
      "content": "[](https://docs.firecrawl.dev/advanced-scraping-guide#crawling-multiple-pages) Crawling Multiple Pages > [](https://docs.firecrawl.dev/advanced-scraping-guide#allowexternallinks) `allowExternalLinks` > [](https://docs.firecrawl.dev/advanced-scraping-guide#scrapeoptions) scrapeOptions\n\nAs part of the crawler options, you can also specify the `scrapeOptions` parameter. This parameter allows you to customize the scraping behavior for each page.  \n- **Type**: `object`\n- **Description**: Options for the scraper.\n- **Example**: `{\"formats\": [\"markdown\", \"links\", \"html\", \"rawHtml\", \"screenshot\"], \"includeTags\": [\"h1\", \"p\", \"a\", \".main-content\"], \"excludeTags\": [\"#ad\", \"#footer\"], \"onlyMainContent\": false, \"waitFor\": 1000, \"timeout\": 15000}`\n- **Default**: `{ \"formats\": [\"markdown\"] }`\n- **See**: [Scrape Options](https://docs.firecrawl.dev/advanced-scraping-guide#setting-the-content-formats-on-response-with-formats)",
      "metadata": {
        "title": "Advanced Scraping Guide | Firecrawl",
        "url": "https://docs.firecrawl.dev/advanced-scraping-guide"
      }
    },
    {
      "id": "697a2768-9f1c-4a88-81b2-f192cba55507",
      "source": "firecrawl/docs/advanced-scraping-guide.md",
      "content": "[](https://docs.firecrawl.dev/advanced-scraping-guide#crawling-multiple-pages) Crawling Multiple Pages > [](https://docs.firecrawl.dev/advanced-scraping-guide#example-usage-3) Example Usage\n\nCopy  \n```bash\ncurl -X POST https://api.firecrawl.dev/v1/crawl \\\n-H 'Content-Type: application/json' \\\n-H 'Authorization : Bearer YOUR_API_KEY' \\\n-d '{\n\"url\": \"https://docs.firecrawl.dev\",\n\"includePaths\": [\"/blog/*\", \"/products/*\"],\n\"excludePaths\": [\"/admin/*\", \"/login/*\"],\n\"maxDepth\": 2,\n\"limit\": 1000\n}'\n\n```  \nIn this example, the crawler will:  \n- Only crawl URLs that match the patterns `/blog/*` and `/products/*`.\n- Skip URLs that match the patterns `/admin/*` and `/login/*`.\n- Return the full document data for each page.\n- Crawl up to a maximum depth of 2.\n- Crawl a maximum of 1000 pages.",
      "metadata": {
        "title": "Advanced Scraping Guide | Firecrawl",
        "url": "https://docs.firecrawl.dev/advanced-scraping-guide"
      }
    },
    {
      "id": "f8c42ad0-9f7f-40d7-bab4-45c03dd2e439",
      "source": "firecrawl/docs/advanced-scraping-guide.md",
      "content": "[](https://docs.firecrawl.dev/advanced-scraping-guide#mapping-website-links-with-map) Mapping Website Links with `/map`\n\nThe `/map` endpoint is adept at identifying URLs that are contextually related to a given website. This feature is crucial for understanding a siteâ€™s contextual link environment, which can greatly aid in strategic site analysis and navigation planning.",
      "metadata": {
        "title": "Advanced Scraping Guide | Firecrawl",
        "url": "https://docs.firecrawl.dev/advanced-scraping-guide"
      }
    },
    {
      "id": "8174aa65-26d1-432a-81bc-ced474e47acf",
      "source": "firecrawl/docs/advanced-scraping-guide.md",
      "content": "[](https://docs.firecrawl.dev/advanced-scraping-guide#mapping-website-links-with-map) Mapping Website Links with `/map` > [](https://docs.firecrawl.dev/advanced-scraping-guide#usage) Usage\n\nTo use the `/map` endpoint, you need to send a GET request with the URL of the page you want to map. Here is an example using `curl`:  \nCopy  \n```bash\ncurl -X POST https://api.firecrawl.dev/v1/map \\\n-H 'Content-Type: application/json' \\\n-H 'Authorization: Bearer YOUR_API_KEY' \\\n-d '{\n\"url\": \"https://docs.firecrawl.dev\"\n}'\n\n```  \nThis will return a JSON object containing links contextually related to the url.",
      "metadata": {
        "title": "Advanced Scraping Guide | Firecrawl",
        "url": "https://docs.firecrawl.dev/advanced-scraping-guide"
      }
    },
    {
      "id": "ef7492d2-4342-475d-a36a-8483f3edf8bf",
      "source": "firecrawl/docs/advanced-scraping-guide.md",
      "content": "[](https://docs.firecrawl.dev/advanced-scraping-guide#mapping-website-links-with-map) Mapping Website Links with `/map` > [](https://docs.firecrawl.dev/advanced-scraping-guide#example-response) Example Response\n\nCopy  \n```json\n{\n\"success\":true,\n\"links\":[\\\n\"https://docs.firecrawl.dev\",\\\n\"https://docs.firecrawl.dev/api-reference/endpoint/crawl-delete\",\\\n\"https://docs.firecrawl.dev/api-reference/endpoint/crawl-get\",\\\n\"https://docs.firecrawl.dev/api-reference/endpoint/crawl-post\",\\\n\"https://docs.firecrawl.dev/api-reference/endpoint/map\",\\\n\"https://docs.firecrawl.dev/api-reference/endpoint/scrape\",\\\n\"https://docs.firecrawl.dev/api-reference/introduction\",\\\n\"https://docs.firecrawl.dev/articles/search-announcement\",\\\n...\\\n]\n}\n\n```",
      "metadata": {
        "title": "Advanced Scraping Guide | Firecrawl",
        "url": "https://docs.firecrawl.dev/advanced-scraping-guide"
      }
    },
    {
      "id": "55a79cda-809d-40de-8d04-5cb784b82417",
      "source": "firecrawl/docs/advanced-scraping-guide.md",
      "content": "[](https://docs.firecrawl.dev/advanced-scraping-guide#mapping-website-links-with-map) Mapping Website Links with `/map` > [](https://docs.firecrawl.dev/advanced-scraping-guide#map-options) Map Options > [](https://docs.firecrawl.dev/advanced-scraping-guide#search) `search`\n\n- **Type**: `string`\n- **Description**: Search for links containing specific text.\n- **Example**: `\"blog\"`",
      "metadata": {
        "title": "Advanced Scraping Guide | Firecrawl",
        "url": "https://docs.firecrawl.dev/advanced-scraping-guide"
      }
    },
    {
      "id": "fb82db63-0c35-4717-a0a1-d6b5215438ea",
      "source": "firecrawl/docs/advanced-scraping-guide.md",
      "content": "[](https://docs.firecrawl.dev/advanced-scraping-guide#mapping-website-links-with-map) Mapping Website Links with `/map` > [](https://docs.firecrawl.dev/advanced-scraping-guide#map-options) Map Options > [](https://docs.firecrawl.dev/advanced-scraping-guide#limit-2) `limit`\n\n- **Type**: `integer`\n- **Description**: Maximum number of links to return.\n- **Default**: `100`",
      "metadata": {
        "title": "Advanced Scraping Guide | Firecrawl",
        "url": "https://docs.firecrawl.dev/advanced-scraping-guide"
      }
    },
    {
      "id": "a8e7e7bc-4c3a-4683-b7ad-a9859a690024",
      "source": "firecrawl/docs/advanced-scraping-guide.md",
      "content": "[](https://docs.firecrawl.dev/advanced-scraping-guide#mapping-website-links-with-map) Mapping Website Links with `/map` > [](https://docs.firecrawl.dev/advanced-scraping-guide#map-options) Map Options > [](https://docs.firecrawl.dev/advanced-scraping-guide#ignoresitemap) `ignoreSitemap`\n\n- **Type**: `boolean`\n- **Description**: Ignore the website sitemap when crawling\n- **Default**: `true`",
      "metadata": {
        "title": "Advanced Scraping Guide | Firecrawl",
        "url": "https://docs.firecrawl.dev/advanced-scraping-guide"
      }
    },
    {
      "id": "d5bfcff7-d2e3-4586-b864-b698611f5bae",
      "source": "firecrawl/docs/advanced-scraping-guide.md",
      "content": "[](https://docs.firecrawl.dev/advanced-scraping-guide#mapping-website-links-with-map) Mapping Website Links with `/map` > [](https://docs.firecrawl.dev/advanced-scraping-guide#map-options) Map Options > [](https://docs.firecrawl.dev/advanced-scraping-guide#includesubdomains) `includeSubdomains`\n\n- **Type**: `boolean`\n- **Description**: Include subdomains of the website\n- **Default**: `false`  \nHere is the API Reference for it: [Map Endpoint Documentation](https://docs.firecrawl.dev/api-reference/endpoint/map)  \n[Suggest edits](https://github.com/hellofirecrawl/docs/edit/main/advanced-scraping-guide.mdx) [Raise issue](https://github.com/hellofirecrawl/docs/issues/new?title=Issue%20on%20docs&body=Path:%20/advanced-scraping-guide)  \n[Integrations](https://docs.firecrawl.dev/integrations) [Scrape](https://docs.firecrawl.dev/features/scrape)  \nOn this page  \n- [Basic scraping with Firecrawl (/scrape)](https://docs.firecrawl.dev/advanced-scraping-guide#basic-scraping-with-firecrawl-scrape)\n- [Scraping PDFs](https://docs.firecrawl.dev/advanced-scraping-guide#scraping-pdfs)\n- [Scrape Options](https://docs.firecrawl.dev/advanced-scraping-guide#scrape-options)\n- [Setting the content formats on response with formats](https://docs.firecrawl.dev/advanced-scraping-guide#setting-the-content-formats-on-response-with-formats)\n- [Getting the full page content as markdown with onlyMainContent](https://docs.firecrawl.dev/advanced-scraping-guide#getting-the-full-page-content-as-markdown-with-onlymaincontent)\n- [Setting the tags to include with includeTags](https://docs.firecrawl.dev/advanced-scraping-guide#setting-the-tags-to-include-with-includetags)\n- [Setting the tags to exclude with excludeTags](https://docs.firecrawl.dev/advanced-scraping-guide#setting-the-tags-to-exclude-with-excludetags)\n- [Waiting for the page to load with waitFor](https://docs.firecrawl.dev/advanced-scraping-guide#waiting-for-the-page-to-load-with-waitfor)\n- [Setting the maximum timeout](https://docs.firecrawl.dev/advanced-scraping-guide#setting-the-maximum-timeout)\n- [Example Usage](https://docs.firecrawl.dev/advanced-scraping-guide#example-usage)\n- [Extractor Options](https://docs.firecrawl.dev/advanced-scraping-guide#extractor-options)\n- [Using the LLM Extraction](https://docs.firecrawl.dev/advanced-scraping-guide#using-the-llm-extraction)\n- [schema](https://docs.firecrawl.dev/advanced-scraping-guide#schema)\n- [system prompt](https://docs.firecrawl.dev/advanced-scraping-guide#system-prompt)\n- [prompt](https://docs.firecrawl.dev/advanced-scraping-guide#prompt)\n- [Example Usage](https://docs.firecrawl.dev/advanced-scraping-guide#example-usage-2)\n- [Actions](https://docs.firecrawl.dev/advanced-scraping-guide#actions)\n- [Available Actions](https://docs.firecrawl.dev/advanced-scraping-guide#available-actions)\n- [wait](https://docs.firecrawl.dev/advanced-scraping-guide#wait)\n- [screenshot](https://docs.firecrawl.dev/advanced-scraping-guide#screenshot)\n- [click](https://docs.firecrawl.dev/advanced-scraping-guide#click)\n- [write](https://docs.firecrawl.dev/advanced-scraping-guide#write)\n- [press](https://docs.firecrawl.dev/advanced-scraping-guide#press)\n- [scroll](https://docs.firecrawl.dev/advanced-scraping-guide#scroll)\n- [Crawling Multiple Pages](https://docs.firecrawl.dev/advanced-scraping-guide#crawling-multiple-pages)\n- [Check Crawl Job](https://docs.firecrawl.dev/advanced-scraping-guide#check-crawl-job)\n- [Pagination/Next URL](https://docs.firecrawl.dev/advanced-scraping-guide#pagination-next-url)\n- [Crawler Options](https://docs.firecrawl.dev/advanced-scraping-guide#crawler-options)\n- [includePaths](https://docs.firecrawl.dev/advanced-scraping-guide#includepaths)\n- [excludePaths](https://docs.firecrawl.dev/advanced-scraping-guide#excludepaths)\n- [maxDepth](https://docs.firecrawl.dev/advanced-scraping-guide#maxdepth)\n- [limit](https://docs.firecrawl.dev/advanced-scraping-guide#limit)\n- [allowBackwardLinks](https://docs.firecrawl.dev/advanced-scraping-guide#allowbackwardlinks)\n- [allowExternalLinks](https://docs.firecrawl.dev/advanced-scraping-guide#allowexternallinks)\n- [scrapeOptions](https://docs.firecrawl.dev/advanced-scraping-guide#scrapeoptions)\n- [Example Usage](https://docs.firecrawl.dev/advanced-scraping-guide#example-usage-3)\n- [Mapping Website Links with /map](https://docs.firecrawl.dev/advanced-scraping-guide#mapping-website-links-with-map)\n- [Usage](https://docs.firecrawl.dev/advanced-scraping-guide#usage)\n- [Example Response](https://docs.firecrawl.dev/advanced-scraping-guide#example-response)\n- [Map Options](https://docs.firecrawl.dev/advanced-scraping-guide#map-options)\n- [search](https://docs.firecrawl.dev/advanced-scraping-guide#search)\n- [limit](https://docs.firecrawl.dev/advanced-scraping-guide#limit-2)\n- [ignoreSitemap](https://docs.firecrawl.dev/advanced-scraping-guide#ignoresitemap)\n- [includeSubdomains](https://docs.firecrawl.dev/advanced-scraping-guide#includesubdomains)",
      "metadata": {
        "title": "Advanced Scraping Guide | Firecrawl",
        "url": "https://docs.firecrawl.dev/advanced-scraping-guide"
      }
    },
    {
      "id": "dc411289-91ff-4803-91e6-bd5372985752",
      "source": "firecrawl/docs/features-search.md",
      "content": "---\ntitle: Search | Firecrawl\nurl: https://docs.firecrawl.dev/features/search\n---  \n[Firecrawl Docs home page![light logo](https://mintlify.s3.us-west-1.amazonaws.com/firecrawl/logo/light.svg)![dark logo](https://mintlify.s3.us-west-1.amazonaws.com/firecrawl/logo/dark.svg)](https://firecrawl.dev/)  \nv0  \nSearch or ask...  \nCtrl K  \nSearch...  \nNavigation  \nFeatures  \nSearch  \n[Documentation](https://docs.firecrawl.dev/v0/introduction) [SDKs](https://docs.firecrawl.dev/v0/sdks/python) [Learn](https://www.firecrawl.dev/blog/category/tutorials) [API Reference](https://docs.firecrawl.dev/v0/api-reference/introduction)",
      "metadata": {
        "title": "Search | Firecrawl",
        "url": "https://docs.firecrawl.dev/features/search"
      }
    },
    {
      "id": "a1188f3a-445b-4390-ba99-20c3fcfdbf6f",
      "source": "firecrawl/docs/features-search.md",
      "content": "[](https://docs.firecrawl.dev/features/search#searching-the-web-and-scraping-the-results-with-firecrawl) Searching the web and scraping the results with Firecrawl\n\nFirecrawl integrates its SERP (Search Engine Results Page) API with its robust scraping infrastructure to provide a seamless search and scrape functionality through a single endpoint. Hereâ€™s why:  \n1. **Unified Search Query:**\nUsers submit a search query via the SERP endpoint.  \n2. **Automated Result Scraping:**\nFirecrawl automatically processes the search results and utilizes its scraping capabilities to extract data from each result page.  \n3. **Data Delivery:**\nThe scraped data from all result pages is compiled and delivered in a clean markdown - ready to use.  \nThis integration allows users to efficiently perform web searches and obtain comprehensive, scraped data from multiple sources with minimal effort.  \nFor more details, refer to the [Search Endpoint Documentation](https://docs.firecrawl.dev/api-reference/endpoint/search).",
      "metadata": {
        "title": "Search | Firecrawl",
        "url": "https://docs.firecrawl.dev/features/search"
      }
    },
    {
      "id": "4c11fb06-ce65-41f1-ac89-625038165a41",
      "source": "firecrawl/docs/features-search.md",
      "content": "[](https://docs.firecrawl.dev/features/search#search-any-query) Search any query > [](https://docs.firecrawl.dev/features/search#search-endpoint) /search endpoint\n\nUsed to search the web, get the most relevant results, scrape each page and return the markdown.  \nCopy  \n```bash\ncurl -X POST https://api.firecrawl.dev/v0/search \\\n-H 'Content-Type: application/json' \\\n-H 'Authorization: Bearer YOUR_API_KEY' \\\n-d '{\n\"query\": \"firecrawl\",\n\"pageOptions\": {\n\"fetchPageContent\": true // false for a fast serp api\n}\n}'\n\n```  \nCopy  \n```json\n{\n\"success\": true,\n\"data\": [\\\n{\\\n\"url\": \"https://docs.firecrawl.dev\",\\\n\"markdown\": \"# Markdown Content\",\\\n\"provider\": \"web-scraper\",\\\n\"metadata\": {\\\n\"title\": \"Firecrawl | Scrape the web reliably for your LLMs\",\\\n\"description\": \"AI for CX and Sales\",\\\n\"language\": null,\\\n\"sourceURL\": \"https://docs.firecrawl.dev/\"\\\n}\\\n}\\\n]\n}\n\n```",
      "metadata": {
        "title": "Search | Firecrawl",
        "url": "https://docs.firecrawl.dev/features/search"
      }
    },
    {
      "id": "98fb2c44-35dc-43e4-8472-8894554e0060",
      "source": "firecrawl/docs/features-search.md",
      "content": "[](https://docs.firecrawl.dev/features/search#search-any-query) Search any query > [](https://docs.firecrawl.dev/features/search#with-python-sdk) With Python SDK > [](https://docs.firecrawl.dev/features/search#installing-python-sdk) Installing Python SDK\n\nCopy  \n```bash\npip install firecrawl-py\n\n```",
      "metadata": {
        "title": "Search | Firecrawl",
        "url": "https://docs.firecrawl.dev/features/search"
      }
    },
    {
      "id": "f77b16fa-6347-4dfb-8a6d-11f762897876",
      "source": "firecrawl/docs/features-search.md",
      "content": "[](https://docs.firecrawl.dev/features/search#search-any-query) Search any query > [](https://docs.firecrawl.dev/features/search#with-python-sdk) With Python SDK > [](https://docs.firecrawl.dev/features/search#search-a-query) Search a query\n\nCopy  \n```python\nfrom firecrawl import FirecrawlApp\n\napp = FirecrawlApp(api_key=\"YOUR_API_KEY\")\n\nresult = app.search(query=\"What is firecrawl?\")\n\n```  \nThe response will be similar to the one shown in the curl command above.",
      "metadata": {
        "title": "Search | Firecrawl",
        "url": "https://docs.firecrawl.dev/features/search"
      }
    },
    {
      "id": "9c3c1195-cb8d-4f1a-b0d9-f7f044e439d3",
      "source": "firecrawl/docs/features-search.md",
      "content": "[](https://docs.firecrawl.dev/features/search#search-any-query) Search any query > [](https://docs.firecrawl.dev/features/search#with-javascript-sdk) With JavaScript SDK > [](https://docs.firecrawl.dev/features/search#installing-javascript-sdk) Installing JavaScript SDK\n\nCopy  \n```bash\nnpm install @mendable/firecrawl-js\n\n```",
      "metadata": {
        "title": "Search | Firecrawl",
        "url": "https://docs.firecrawl.dev/features/search"
      }
    },
    {
      "id": "53eb20b8-4f47-411f-a47e-95d9b3f95b1b",
      "source": "firecrawl/docs/features-search.md",
      "content": "[](https://docs.firecrawl.dev/features/search#search-any-query) Search any query > [](https://docs.firecrawl.dev/features/search#with-javascript-sdk) With JavaScript SDK > [](https://docs.firecrawl.dev/features/search#search-a-query-2) Search a query\n\nCopy  \n```javascript\nimport FirecrawlApp from '@mendable/firecrawl-js';\n\n// Initialize the FirecrawlApp with your API key\nconst app = new FirecrawlApp({ apiKey: 'YOUR_API_KEY' });\n\n// Perform a search\nconst result = await app.search('What is firecrawl?');\n\n```  \nThe response will be similar to the one shown in the curl command above.",
      "metadata": {
        "title": "Search | Firecrawl",
        "url": "https://docs.firecrawl.dev/features/search"
      }
    },
    {
      "id": "5a92738a-448c-40e3-9d8c-eb12a402f86e",
      "source": "firecrawl/docs/features-search.md",
      "content": "[](https://docs.firecrawl.dev/features/search#search-any-query) Search any query > [](https://docs.firecrawl.dev/features/search#with-go-sdk) With Go SDK > [](https://docs.firecrawl.dev/features/search#installing-go-sdk) Installing Go SDK\n\nCopy  \n```bash\ngo get github.com/mendableai/firecrawl-go\n\n```",
      "metadata": {
        "title": "Search | Firecrawl",
        "url": "https://docs.firecrawl.dev/features/search"
      }
    },
    {
      "id": "c3c746b7-f8b7-47d7-9d13-62acd1179a20",
      "source": "firecrawl/docs/features-search.md",
      "content": "[](https://docs.firecrawl.dev/features/search#search-any-query) Search any query > [](https://docs.firecrawl.dev/features/search#with-go-sdk) With Go SDK > [](https://docs.firecrawl.dev/features/search#search-a-query-3) Search a query\n\nCopy  \n```go\nimport (\n\"fmt\"\n\"log\"\n\n\"github.com/mendableai/firecrawl-go\"\n)\n\nfunc main() {\napp, err := firecrawl.NewFirecrawlApp(\"YOUR_API_KEY\")\nif err != nil {\nlog.Fatalf(\"Failed to initialize FirecrawlApp: %v\", err)\n}\n\nquery := \"What is firecrawl?\"\nsearchResult, err := app.Search(query)\nif err != nil {\nlog.Fatalf(\"Failed to search: %v\", err)\n}\nfmt.Println(searchResult)\n}\n\n```",
      "metadata": {
        "title": "Search | Firecrawl",
        "url": "https://docs.firecrawl.dev/features/search"
      }
    },
    {
      "id": "e87e182b-0ae5-460e-9a00-4e4c06d7d5b3",
      "source": "firecrawl/docs/features-search.md",
      "content": "[](https://docs.firecrawl.dev/features/search#search-any-query) Search any query > [](https://docs.firecrawl.dev/features/search#with-rust-sdk) With Rust SDK > [](https://docs.firecrawl.dev/features/search#installing-rust-sdk) Installing Rust SDK\n\nAdd the following to your `Cargo.toml`:  \nCopy  \n```toml\n[dependencies]\nfirecrawl = \"^0.1\"\ntokio = { version = \"^1\", features = [\"full\"] }\nserde = { version = \"^1.0\", features = [\"derive\"] }\nserde_json = \"^1.0\"\nuuid = { version = \"^1.10\", features = [\"v4\"] }\n\n[build-dependencies]\ntokio = { version = \"1\", features = [\"full\"] }\n\n```",
      "metadata": {
        "title": "Search | Firecrawl",
        "url": "https://docs.firecrawl.dev/features/search"
      }
    },
    {
      "id": "5b1d784b-0f3c-401c-b598-66b3d7401d14",
      "source": "firecrawl/docs/features-search.md",
      "content": "[](https://docs.firecrawl.dev/features/search#search-any-query) Search any query > [](https://docs.firecrawl.dev/features/search#with-rust-sdk) With Rust SDK > [](https://docs.firecrawl.dev/features/search#search-a-query-4) Search a query\n\nCopy  \n```rust\nasync fn main() {\nlet api_key = \"YOUR_API_KEY\";\nlet api_url = \"https://api.firecrawl.dev\";\nlet app = FirecrawlApp::new(api_key, api_url).expect(\"Failed to initialize FirecrawlApp\");\n\nlet query = \"What is firecrawl?\";\nlet search_result = app.search(query).await;\n\nmatch search_result {\nOk(data) => println!(\"Search Result: {}\", data),\nErr(e) => eprintln!(\"Failed to search: {}\", e),\n}\n}\n\n```  \n[Suggest edits](https://github.com/hellofirecrawl/docs/edit/main/features/search.mdx) [Raise issue](https://github.com/hellofirecrawl/docs/issues/new?title=Issue%20on%20docs&body=Path:%20/features/search)  \n[LLM Extract](https://docs.firecrawl.dev/v0/features/extract) [Langchain](https://docs.firecrawl.dev/integrations/langchain)  \nOn this page  \n- [Searching the web and scraping the results with Firecrawl](https://docs.firecrawl.dev/features/search#searching-the-web-and-scraping-the-results-with-firecrawl)\n- [Search any query](https://docs.firecrawl.dev/features/search#search-any-query)\n- [/search endpoint](https://docs.firecrawl.dev/features/search#search-endpoint)\n- [With Python SDK](https://docs.firecrawl.dev/features/search#with-python-sdk)\n- [Installing Python SDK](https://docs.firecrawl.dev/features/search#installing-python-sdk)\n- [Search a query](https://docs.firecrawl.dev/features/search#search-a-query)\n- [With JavaScript SDK](https://docs.firecrawl.dev/features/search#with-javascript-sdk)\n- [Installing JavaScript SDK](https://docs.firecrawl.dev/features/search#installing-javascript-sdk)\n- [Search a query](https://docs.firecrawl.dev/features/search#search-a-query-2)\n- [With Go SDK](https://docs.firecrawl.dev/features/search#with-go-sdk)\n- [Installing Go SDK](https://docs.firecrawl.dev/features/search#installing-go-sdk)\n- [Search a query](https://docs.firecrawl.dev/features/search#search-a-query-3)\n- [With Rust SDK](https://docs.firecrawl.dev/features/search#with-rust-sdk)\n- [Installing Rust SDK](https://docs.firecrawl.dev/features/search#installing-rust-sdk)\n- [Search a query](https://docs.firecrawl.dev/features/search#search-a-query-4)",
      "metadata": {
        "title": "Search | Firecrawl",
        "url": "https://docs.firecrawl.dev/features/search"
      }
    },
    {
      "id": "fb588698-346a-4712-b71a-43f097583103",
      "source": "firecrawl/docs/sdks-python.md",
      "content": "---\ntitle: Python SDK | Firecrawl\nurl: https://docs.firecrawl.dev/sdks/python\n---  \n[Firecrawl Docs home page![light logo](https://mintlify.s3.us-west-1.amazonaws.com/firecrawl/logo/light.svg)![dark logo](https://mintlify.s3.us-west-1.amazonaws.com/firecrawl/logo/dark.svg)](https://firecrawl.dev/)  \nv1  \nSearch or ask...  \nCtrl K  \nSearch...  \nNavigation  \nSDKs  \nPython  \n[Documentation](https://docs.firecrawl.dev/introduction) [SDKs](https://docs.firecrawl.dev/sdks/overview) [Learn](https://www.firecrawl.dev/blog/category/tutorials) [API Reference](https://docs.firecrawl.dev/api-reference/introduction)",
      "metadata": {
        "title": "Python SDK | Firecrawl",
        "url": "https://docs.firecrawl.dev/sdks/python"
      }
    },
    {
      "id": "9b67e6c5-fbe6-44ba-b234-4361fbcd9d69",
      "source": "firecrawl/docs/sdks-python.md",
      "content": "[](https://docs.firecrawl.dev/sdks/python#installation) Installation\n\nTo install the Firecrawl Python SDK, you can use pip:  \nPython  \nCopy  \n```bash\npip install firecrawl-py\n\n```",
      "metadata": {
        "title": "Python SDK | Firecrawl",
        "url": "https://docs.firecrawl.dev/sdks/python"
      }
    },
    {
      "id": "543256eb-a9d5-45a8-81f0-c8f5cec07fc9",
      "source": "firecrawl/docs/sdks-python.md",
      "content": "[](https://docs.firecrawl.dev/sdks/python#usage) Usage\n\n1. Get an API key from [firecrawl.dev](https://firecrawl.dev/)\n2. Set the API key as an environment variable named `FIRECRAWL_API_KEY` or pass it as a parameter to the `FirecrawlApp` class.  \nHereâ€™s an example of how to use the SDK:  \nPython  \nCopy  \n```python\nfrom firecrawl import FirecrawlApp\n\napp = FirecrawlApp(api_key=\"fc-YOUR_API_KEY\")\n\n# Scrape a website:\nscrape_status = app.scrape_url(\n'https://firecrawl.dev',\nparams={'formats': ['markdown', 'html']}\n)\nprint(scrape_status)\n\n# Crawl a website:\ncrawl_status = app.crawl_url(\n'https://firecrawl.dev',\nparams={\n'limit': 100,\n'scrapeOptions': {'formats': ['markdown', 'html']}\n}\n)\nprint(crawl_status)\n\n```",
      "metadata": {
        "title": "Python SDK | Firecrawl",
        "url": "https://docs.firecrawl.dev/sdks/python"
      }
    },
    {
      "id": "65b52a9a-1235-4c40-98bd-6a47ea8ced64",
      "source": "firecrawl/docs/sdks-python.md",
      "content": "[](https://docs.firecrawl.dev/sdks/python#usage) Usage > [](https://docs.firecrawl.dev/sdks/python#scraping-a-url) Scraping a URL\n\nTo scrape a single URL, use the `scrape_url` method. It takes the URL as a parameter and returns the scraped data as a dictionary.  \nPython  \nCopy  \n```python\n# Scrape a website:\nscrape_result = app.scrape_url('firecrawl.dev', params={'formats': ['markdown', 'html']})\nprint(scrape_result)\n\n```",
      "metadata": {
        "title": "Python SDK | Firecrawl",
        "url": "https://docs.firecrawl.dev/sdks/python"
      }
    },
    {
      "id": "18df168f-de43-4fc8-924d-2aefc8e2885d",
      "source": "firecrawl/docs/sdks-python.md",
      "content": "[](https://docs.firecrawl.dev/sdks/python#usage) Usage > [](https://docs.firecrawl.dev/sdks/python#crawling-a-website) Crawling a Website\n\nTo crawl a website, use the `crawl_url` method. It takes the starting URL and optional parameters as arguments. The `params` argument allows you to specify additional options for the crawl job, such as the maximum number of pages to crawl, allowed domains, and the output format.  \nPython  \nCopy  \n```python\ncrawl_status = app.crawl_url(\n'https://firecrawl.dev',\nparams={\n'limit': 100,\n'scrapeOptions': {'formats': ['markdown', 'html']}\n},\npoll_interval=30\n)\nprint(crawl_status)\n\n```",
      "metadata": {
        "title": "Python SDK | Firecrawl",
        "url": "https://docs.firecrawl.dev/sdks/python"
      }
    },
    {
      "id": "681694da-55c6-45dc-9df1-7115cb94d396",
      "source": "firecrawl/docs/sdks-python.md",
      "content": "[](https://docs.firecrawl.dev/sdks/python#usage) Usage > [](https://docs.firecrawl.dev/sdks/python#asynchronous-crawling) Asynchronous Crawling\n\nTo crawl a website asynchronously, use the `crawl_url_async` method. It returns the crawl `ID` which you can use to check the status of the crawl job. It takes the starting URL and optional parameters as arguments. The `params` argument allows you to specify additional options for the crawl job, such as the maximum number of pages to crawl, allowed domains, and the output format.  \nPython  \nCopy  \n```python\ncrawl_status = app.async_crawl_url(\n'https://firecrawl.dev',\nparams={\n'limit': 100,\n'scrapeOptions': {'formats': ['markdown', 'html']}\n}\n)\nprint(crawl_status)\n\n```",
      "metadata": {
        "title": "Python SDK | Firecrawl",
        "url": "https://docs.firecrawl.dev/sdks/python"
      }
    },
    {
      "id": "d3973857-b828-4bec-8aad-50a3d114cbd6",
      "source": "firecrawl/docs/sdks-python.md",
      "content": "[](https://docs.firecrawl.dev/sdks/python#usage) Usage > [](https://docs.firecrawl.dev/sdks/python#checking-crawl-status) Checking Crawl Status\n\nTo check the status of a crawl job, use the `check_crawl_status` method. It takes the job ID as a parameter and returns the current status of the crawl job.  \nPython  \nCopy  \n```python\ncrawl_status = app.check_crawl_status(\"<crawl_id>\")\nprint(crawl_status)\n\n```",
      "metadata": {
        "title": "Python SDK | Firecrawl",
        "url": "https://docs.firecrawl.dev/sdks/python"
      }
    },
    {
      "id": "ea389a70-deb7-4354-8750-b9c84836e946",
      "source": "firecrawl/docs/sdks-python.md",
      "content": "[](https://docs.firecrawl.dev/sdks/python#usage) Usage > [](https://docs.firecrawl.dev/sdks/python#cancelling-a-crawl) Cancelling a Crawl\n\nTo cancel an asynchronous crawl job, use the `cancel_crawl` method. It takes the job ID of the asynchronous crawl as a parameter and returns the cancellation status.  \nPython  \nCopy  \n```python\ncancel_crawl = app.cancel_crawl(id)\nprint(cancel_crawl)\n\n```",
      "metadata": {
        "title": "Python SDK | Firecrawl",
        "url": "https://docs.firecrawl.dev/sdks/python"
      }
    },
    {
      "id": "a2616c6c-c96a-4299-9761-26f231cdb497",
      "source": "firecrawl/docs/sdks-python.md",
      "content": "[](https://docs.firecrawl.dev/sdks/python#usage) Usage > [](https://docs.firecrawl.dev/sdks/python#map-a-website) Map a Website\n\nUse `map_url` to generate a list of URLs from a website. The `params` argument let you customize the mapping process, including options to exclude subdomains or to utilize the sitemap.  \nPython  \nCopy  \n```python\n# Map a website:\nmap_result = app.map_url('https://firecrawl.dev')\nprint(map_result)\n\n```",
      "metadata": {
        "title": "Python SDK | Firecrawl",
        "url": "https://docs.firecrawl.dev/sdks/python"
      }
    },
    {
      "id": "33cadcb7-b9f4-468d-9d6d-5d7d9f82416c",
      "source": "firecrawl/docs/sdks-python.md",
      "content": "[](https://docs.firecrawl.dev/sdks/python#usage) Usage > [](https://docs.firecrawl.dev/sdks/python#crawling-a-website-with-websockets) Crawling a Website with WebSockets\n\nTo crawl a website with WebSockets, use the `crawl_url_and_watch` method. It takes the starting URL and optional parameters as arguments. The `params` argument allows you to specify additional options for the crawl job, such as the maximum number of pages to crawl, allowed domains, and the output format.  \nPython  \nCopy  \n```python\n# inside an async function...\nnest_asyncio.apply()\n\n# Define event handlers\ndef on_document(detail):\nprint(\"DOC\", detail)\n\ndef on_error(detail):\nprint(\"ERR\", detail['error'])\n\ndef on_done(detail):\nprint(\"DONE\", detail['status'])\n\n# Function to start the crawl and watch process\nasync def start_crawl_and_watch():\n# Initiate the crawl job and get the watcher\nwatcher = app.crawl_url_and_watch('firecrawl.dev', { 'excludePaths': ['blog/*'], 'limit': 5 })\n\n# Add event listeners\nwatcher.add_event_listener(\"document\", on_document)\nwatcher.add_event_listener(\"error\", on_error)\nwatcher.add_event_listener(\"done\", on_done)\n\n# Start the watcher\nawait watcher.connect()\n\n# Run the event loop\nawait start_crawl_and_watch()\n\n```",
      "metadata": {
        "title": "Python SDK | Firecrawl",
        "url": "https://docs.firecrawl.dev/sdks/python"
      }
    },
    {
      "id": "5913d0e2-dac1-4ce0-8847-4cb56ccbb252",
      "source": "firecrawl/docs/sdks-python.md",
      "content": "[](https://docs.firecrawl.dev/sdks/python#error-handling) Error Handling\n\nThe SDK handles errors returned by the Firecrawl API and raises appropriate exceptions. If an error occurs during a request, an exception will be raised with a descriptive error message.  \n[Suggest edits](https://github.com/hellofirecrawl/docs/edit/main/sdks/python.mdx) [Raise issue](https://github.com/hellofirecrawl/docs/issues/new?title=Issue%20on%20docs&body=Path:%20/sdks/python)  \n[Overview](https://docs.firecrawl.dev/sdks/overview) [Node](https://docs.firecrawl.dev/sdks/node)  \nOn this page  \n- [Installation](https://docs.firecrawl.dev/sdks/python#installation)\n- [Usage](https://docs.firecrawl.dev/sdks/python#usage)\n- [Scraping a URL](https://docs.firecrawl.dev/sdks/python#scraping-a-url)\n- [Crawling a Website](https://docs.firecrawl.dev/sdks/python#crawling-a-website)\n- [Asynchronous Crawling](https://docs.firecrawl.dev/sdks/python#asynchronous-crawling)\n- [Checking Crawl Status](https://docs.firecrawl.dev/sdks/python#checking-crawl-status)\n- [Cancelling a Crawl](https://docs.firecrawl.dev/sdks/python#cancelling-a-crawl)\n- [Map a Website](https://docs.firecrawl.dev/sdks/python#map-a-website)\n- [Crawling a Website with WebSockets](https://docs.firecrawl.dev/sdks/python#crawling-a-website-with-websockets)\n- [Error Handling](https://docs.firecrawl.dev/sdks/python#error-handling)",
      "metadata": {
        "title": "Python SDK | Firecrawl",
        "url": "https://docs.firecrawl.dev/sdks/python"
      }
    },
    {
      "id": "ceff1689-b2dc-4f00-a194-594bb38a8501",
      "source": "firecrawl/docs/v0-features-extract.md",
      "content": "---\ntitle: Extract | Firecrawl\nurl: https://docs.firecrawl.dev/v0/features/extract\n---  \n[Firecrawl Docs home page![light logo](https://mintlify.s3.us-west-1.amazonaws.com/firecrawl/logo/light.svg)![dark logo](https://mintlify.s3.us-west-1.amazonaws.com/firecrawl/logo/dark.svg)](https://firecrawl.dev/)  \nv0  \nSearch or ask...  \nCtrl K  \nSearch...  \nNavigation  \nFeatures  \nLLM Extract  \n[Documentation](https://docs.firecrawl.dev/v0/introduction) [SDKs](https://docs.firecrawl.dev/v0/sdks/python) [Learn](https://www.firecrawl.dev/blog/category/tutorials) [API Reference](https://docs.firecrawl.dev/v0/api-reference/introduction)",
      "metadata": {
        "title": "Extract | Firecrawl",
        "url": "https://docs.firecrawl.dev/v0/features/extract"
      }
    },
    {
      "id": "44c7ff72-6dc6-4999-9a69-641c6ad5c694",
      "source": "firecrawl/docs/v0-features-extract.md",
      "content": "[](https://docs.firecrawl.dev/v0/features/extract#scrape-and-extract-structured-data-with-firecrawl) Scrape and extract structured data with Firecrawl\n\nFirecrawl leverages Large Language Models (LLMs) to efficiently extract structured data from web pages. Hereâ€™s how:  \n1. **Schema Definition:**\nDefine the URL to scrape and the desired data schema using JSON Schema (following OpenAI tool schema). This schema specifies the data structure you expect to extract from the page.  \n2. **Scrape Endpoint:**\nPass the URL and the schema to the scrape endpoint. Documentation for this endpoint can be found here:\n[Scrape Endpoint Documentation](https://docs.firecrawl.dev/api-reference/endpoint/scrape)  \n3. **Structured Data Retrieval:**\nReceive the scraped data in the structured format defined by your schema. You can then use this data as needed in your application or for further processing.  \nThis method streamlines data extraction, reducing manual handling and enhancing efficiency.",
      "metadata": {
        "title": "Extract | Firecrawl",
        "url": "https://docs.firecrawl.dev/v0/features/extract"
      }
    },
    {
      "id": "ff1bbcb7-fbcc-44d4-be25-9014f4a68bd8",
      "source": "firecrawl/docs/v0-features-extract.md",
      "content": "[](https://docs.firecrawl.dev/v0/features/extract#extract-structured-data) Extract structured data > [](https://docs.firecrawl.dev/v0/features/extract#scrape-with-extract-endpoint) /scrape (with extract) endpoint\n\nUsed to extract structured data from scraped pages.  \nCopy  \n```bash\ncurl -X POST https://api.firecrawl.dev/v0/scrape \\\n-H 'Content-Type: application/json' \\\n-H 'Authorization: Bearer YOUR_API_KEY' \\\n-d '{\n\"url\": \"https://docs.firecrawl.dev/\",\n\"extractorOptions\": {\n\"mode\": \"llm-extraction\",\n\"extractionPrompt\": \"Based on the information on the page, extract the information from the schema. \",\n\"extractionSchema\": {\n\"type\": \"object\",\n\"properties\": {\n\"company_mission\": {\n\"type\": \"string\"\n},\n\"supports_sso\": {\n\"type\": \"boolean\"\n},\n\"is_open_source\": {\n\"type\": \"boolean\"\n},\n\"is_in_yc\": {\n\"type\": \"boolean\"\n}\n},\n\"required\": [\\\n\"company_mission\",\\\n\"supports_sso\",\\\n\"is_open_source\",\\\n\"is_in_yc\"\\\n]\n}\n}\n}'\n\n```  \nCopy  \n```json\n{\n\"success\": true,\n\"data\": {\n\"content\": \"Raw Content\",\n\"metadata\": {\n\"title\": \"Mendable\",\n\"description\": \"Mendable allows you to easily build AI chat applications. Ingest, customize, then deploy with one line of code anywhere you want. Brought to you by SideGuide\",\n\"robots\": \"follow, index\",\n\"ogTitle\": \"Mendable\",\n\"ogDescription\": \"Mendable allows you to easily build AI chat applications. Ingest, customize, then deploy with one line of code anywhere you want. Brought to you by SideGuide\",\n\"ogUrl\": \"https://docs.firecrawl.dev/\",\n\"ogImage\": \"https://docs.firecrawl.dev/mendable_new_og1.png\",\n\"ogLocaleAlternate\": [],\n\"ogSiteName\": \"Mendable\",\n\"sourceURL\": \"https://docs.firecrawl.dev/\"\n},\n\"llm_extraction\": {\n\"company_mission\": \"Train a secure AI on your technical resources that answers customer and employee questions so your team doesn't have to\",\n\"supports_sso\": true,\n\"is_open_source\": false,\n\"is_in_yc\": true\n}\n}\n}\n\n```",
      "metadata": {
        "title": "Extract | Firecrawl",
        "url": "https://docs.firecrawl.dev/v0/features/extract"
      }
    },
    {
      "id": "9759a73d-b745-4068-b341-4e08d40922a9",
      "source": "firecrawl/docs/v0-features-extract.md",
      "content": "[](https://docs.firecrawl.dev/v0/features/extract#extract-structured-data) Extract structured data > [](https://docs.firecrawl.dev/v0/features/extract#using-python-sdk) Using Python SDK\n\nCopy  \n```python\nfrom firecrawl import FirecrawlApp\n\n# Initialize the FirecrawlApp with your API key\napp = FirecrawlApp(api_key='your_api_key', version='v0')\n\nclass ArticleSchema(BaseModel):\ntitle: str\npoints: int\nby: str\ncommentsURL: str\n\nclass TopArticlesSchema(BaseModel):\ntop: List[ArticleSchema] = Field(..., max_items=5, description=\"Top 5 stories\")\n\ndata = app.scrape_url('https://news.ycombinator.com', {\n'extractorOptions': {\n'extractionSchema': TopArticlesSchema.model_json_schema(),\n'mode': 'llm-extraction'\n},\n'pageOptions':{\n'onlyMainContent': True\n}\n})\nprint(data[\"llm_extraction\"])\n\n```",
      "metadata": {
        "title": "Extract | Firecrawl",
        "url": "https://docs.firecrawl.dev/v0/features/extract"
      }
    },
    {
      "id": "24e78bb0-2e19-4565-b84c-4a978bdc5761",
      "source": "firecrawl/docs/v0-features-extract.md",
      "content": "[](https://docs.firecrawl.dev/v0/features/extract#extract-structured-data) Extract structured data > [](https://docs.firecrawl.dev/v0/features/extract#with-javascript-sdk) With JavaScript SDK\n\nCopy  \n```js\nimport FirecrawlApp from \"@mendable/firecrawl-js\";\nimport { z } from \"zod\";\n\nconst app = new FirecrawlApp({\napiKey: \"fc-YOUR_API_KEY\",\nversion: \"v0\"\n});\n\n// Define schema to extract contents into\nconst schema = z.object({\ntop: z\n.array(\nz.object({\ntitle: z.string(),\npoints: z.number(),\nby: z.string(),\ncommentsURL: z.string(),\n})\n)\n.length(5)\n.describe(\"Top 5 stories on Hacker News\"),\n});\n\nconst scrapeResult = await app.scrapeUrl(\"https://news.ycombinator.com\", {\nextractorOptions: { extractionSchema: schema },\n});\n\nconsole.log(scrapeResult.data[\"llm_extraction\"]);\n\n```",
      "metadata": {
        "title": "Extract | Firecrawl",
        "url": "https://docs.firecrawl.dev/v0/features/extract"
      }
    },
    {
      "id": "1853d603-f3ca-42fc-ace6-7cef3601969b",
      "source": "firecrawl/docs/v0-features-extract.md",
      "content": "[](https://docs.firecrawl.dev/v0/features/extract#extract-structured-data) Extract structured data > [](https://docs.firecrawl.dev/v0/features/extract#with-go-sdk) With Go SDK\n\nGo  \nCopy  \n```go\nimport (\n\"fmt\"\n\"log\"\n\n\"github.com/mendableai/firecrawl-go\"\n)\n\nfunc main() {\napp, err := firecrawl.NewFirecrawlApp(\"YOUR_API_KEY\")\nif err != nil {\nlog.Fatalf(\"Failed to initialize FirecrawlApp: %v\", err)\n}\n\njsonSchema := map[string]any{\n\"type\": \"object\",\n\"properties\": map[string]any{\n\"top\": map[string]any{\n\"type\": \"array\",\n\"items\": map[string]any{\n\"type\": \"object\",\n\"properties\": map[string]any{\n\"title\": map[string]string{\"type\": \"string\"},\n\"points\": map[string]string{\"type\": \"number\"},\n\"by\": map[string]string{\"type\": \"string\"},\n\"commentsURL\": map[string]string{\"type\": \"string\"},\n},\n\"required\": []string{\"title\", \"points\", \"by\", \"commentsURL\"},\n},\n\"minItems\": 5,\n\"maxItems\": 5,\n\"description\": \"Top 5 stories on Hacker News\",\n},\n},\n\"required\": []string{\"top\"},\n}\n\nllmExtractionParams := map[string]any{\n\"extractorOptions\": firecrawl.ExtractorOptions{\nExtractionSchema: jsonSchema,\n},\n}\n\nscrapeResult, err := app.ScrapeURL(\"https://news.ycombinator.com\", llmExtractionParams)\nif err != nil {\nlog.Fatalf(\"Failed to perform LLM extraction: %v\", err)\n}\nfmt.Println(scrapeResult)\n}\n\n```",
      "metadata": {
        "title": "Extract | Firecrawl",
        "url": "https://docs.firecrawl.dev/v0/features/extract"
      }
    },
    {
      "id": "dbf080eb-9c3e-48eb-ad2f-4de290e4ae4d",
      "source": "firecrawl/docs/v0-features-extract.md",
      "content": "[](https://docs.firecrawl.dev/v0/features/extract#extract-structured-data) Extract structured data > [](https://docs.firecrawl.dev/v0/features/extract#with-rust-sdk) With Rust SDK\n\nRust  \nCopy  \n```rust\nuse firecrawl::FirecrawlApp;\n\n#[tokio::main]\nasync fn main() {\n// Initialize the FirecrawlApp with the API key\nlet api_key = \"YOUR_API_KEY\";\nlet api_url = \"https://api.firecrawl.dev\";\nlet app = FirecrawlApp::new(api_key, api_url).expect(\"Failed to initialize FirecrawlApp\");\n\n// Define schema to extract contents into\nlet json_schema = json!({\n\"type\": \"object\",\n\"properties\": {\n\"top\": {\n\"type\": \"array\",\n\"items\": {\n\"type\": \"object\",\n\"properties\": {\n\"title\": {\"type\": \"string\"},\n\"points\": {\"type\": \"number\"},\n\"by\": {\"type\": \"string\"},\n\"commentsURL\": {\"type\": \"string\"}\n},\n\"required\": [\"title\", \"points\", \"by\", \"commentsURL\"]\n},\n\"minItems\": 5,\n\"maxItems\": 5,\n\"description\": \"Top 5 stories on Hacker News\"\n}\n},\n\"required\": [\"top\"]\n});\n\nlet llm_extraction_params = json!({\n\"extractorOptions\": {\n\"extractionSchema\": json_schema,\n\"mode\": \"llm-extraction\"\n},\n\"pageOptions\": {\n\"onlyMainContent\": true\n}\n});\n\nlet llm_extraction_result = app\n.scrape_url(\"https://news.ycombinator.com\", Some(llm_extraction_params))\n.await;\nmatch llm_extraction_result {\nOk(data) => println!(\"LLM Extraction Result:n{}\", data[\"llm_extraction\"]),\nErr(e) => eprintln!(\"LLM Extraction failed: {}\", e),\n}\n}\n\n```  \n[Suggest edits](https://github.com/hellofirecrawl/docs/edit/main/v0/features/extract.mdx) [Raise issue](https://github.com/hellofirecrawl/docs/issues/new?title=Issue%20on%20docs&body=Path:%20/v0/features/extract)  \n[Crawl](https://docs.firecrawl.dev/v0/features/crawl) [Search](https://docs.firecrawl.dev/features/search)  \nOn this page  \n- [Scrape and extract structured data with Firecrawl](https://docs.firecrawl.dev/v0/features/extract#scrape-and-extract-structured-data-with-firecrawl)\n- [Extract structured data](https://docs.firecrawl.dev/v0/features/extract#extract-structured-data)\n- [/scrape (with extract) endpoint](https://docs.firecrawl.dev/v0/features/extract#scrape-with-extract-endpoint)\n- [Using Python SDK](https://docs.firecrawl.dev/v0/features/extract#using-python-sdk)\n- [With JavaScript SDK](https://docs.firecrawl.dev/v0/features/extract#with-javascript-sdk)\n- [With Go SDK](https://docs.firecrawl.dev/v0/features/extract#with-go-sdk)\n- [With Rust SDK](https://docs.firecrawl.dev/v0/features/extract#with-rust-sdk)",
      "metadata": {
        "title": "Extract | Firecrawl",
        "url": "https://docs.firecrawl.dev/v0/features/extract"
      }
    },
    {
      "id": "71a01a24-098b-4228-8164-ec7d4895caee",
      "source": "firecrawl/docs/contributing-open-source-or-cloud.md",
      "content": "---\ntitle: Open Source vs Cloud | Firecrawl\nurl: https://docs.firecrawl.dev/contributing/open-source-or-cloud\n---  \n[Firecrawl Docs home page![light logo](https://mintlify.s3.us-west-1.amazonaws.com/firecrawl/logo/light.svg)![dark logo](https://mintlify.s3.us-west-1.amazonaws.com/firecrawl/logo/dark.svg)](https://firecrawl.dev/)  \nv1  \nSearch or ask...  \nCtrl K  \nSearch...  \nNavigation  \nContributing  \nOpen Source vs Cloud  \n[Documentation](https://docs.firecrawl.dev/introduction) [SDKs](https://docs.firecrawl.dev/sdks/overview) [Learn](https://www.firecrawl.dev/blog/category/tutorials) [API Reference](https://docs.firecrawl.dev/api-reference/introduction)  \nFirecrawl is open source available under the [AGPL-3.0 license](https://github.com/mendableai/firecrawl/blob/main/LICENSE).  \nTo deliver the best possible product, we offer a hosted version of Firecrawl alongside our open-source offering. The cloud solution allows us to continuously innovate and maintain a high-quality, sustainable service for all users.  \nFirecrawl Cloud is available at [firecrawl.dev](https://firecrawl.dev/) and offers a range of features that are not available in the open source version:  \n![Firecrawl Cloud vs Open Source](https://mintlify.s3.us-west-1.amazonaws.com/firecrawl/images/open-source-cloud.png)  \n[Suggest edits](https://github.com/hellofirecrawl/docs/edit/main/contributing/open-source-or-cloud.mdx) [Raise issue](https://github.com/hellofirecrawl/docs/issues/new?title=Issue%20on%20docs&body=Path:%20/contributing/open-source-or-cloud)  \n[RAGaaS](https://docs.firecrawl.dev/integrations/ragaas) [Running locally](https://docs.firecrawl.dev/contributing/guide)  \n![Firecrawl Cloud vs Open Source](https://docs.firecrawl.dev/contributing/open-source-or-cloud)",
      "metadata": {
        "title": "Open Source vs Cloud | Firecrawl",
        "url": "https://docs.firecrawl.dev/contributing/open-source-or-cloud"
      }
    },
    {
      "id": "5ec819d1-bbef-471d-a0cf-40f3d6988107",
      "source": "firecrawl/docs/sdks-node.md",
      "content": "---\ntitle: Node SDK | Firecrawl\nurl: https://docs.firecrawl.dev/sdks/node\n---  \n[Firecrawl Docs home page![light logo](https://mintlify.s3.us-west-1.amazonaws.com/firecrawl/logo/light.svg)![dark logo](https://mintlify.s3.us-west-1.amazonaws.com/firecrawl/logo/dark.svg)](https://firecrawl.dev/)  \nv1  \nSearch or ask...  \nCtrl K  \nSearch...  \nNavigation  \nSDKs  \nNode  \n[Documentation](https://docs.firecrawl.dev/introduction) [SDKs](https://docs.firecrawl.dev/sdks/overview) [Learn](https://www.firecrawl.dev/blog/category/tutorials) [API Reference](https://docs.firecrawl.dev/api-reference/introduction)",
      "metadata": {
        "title": "Node SDK | Firecrawl",
        "url": "https://docs.firecrawl.dev/sdks/node"
      }
    },
    {
      "id": "5e16dacd-457c-45ed-8de3-e2b4b3c69d84",
      "source": "firecrawl/docs/sdks-node.md",
      "content": "[](https://docs.firecrawl.dev/sdks/node#installation) Installation\n\nTo install the Firecrawl Node SDK, you can use npm:  \nNode  \nCopy  \n```bash\nnpm install @mendable/firecrawl-js\n\n```",
      "metadata": {
        "title": "Node SDK | Firecrawl",
        "url": "https://docs.firecrawl.dev/sdks/node"
      }
    },
    {
      "id": "b748dc0a-4a08-40c7-a43a-e9b44fd1d50d",
      "source": "firecrawl/docs/sdks-node.md",
      "content": "[](https://docs.firecrawl.dev/sdks/node#usage) Usage\n\n1. Get an API key from [firecrawl.dev](https://firecrawl.dev/)\n2. Set the API key as an environment variable named `FIRECRAWL_API_KEY` or pass it as a parameter to the `FirecrawlApp` class.  \nHereâ€™s an example of how to use the SDK with error handling:  \nNode  \nCopy  \n```js\nimport FirecrawlApp, { CrawlParams, CrawlStatusResponse } from '@mendable/firecrawl-js';\n\nconst app = new FirecrawlApp({apiKey: \"fc-YOUR_API_KEY\"});\n\n// Scrape a website\nconst scrapeResponse = await app.scrapeUrl('https://firecrawl.dev', {\nformats: ['markdown', 'html'],\n});\n\nif (!scrapeResponse.success) {\nthrow new Error(`Failed to scrape: ${scrapeResponse.error}`)\n}\n\nconsole.log(scrapeResponse)\n\n// Crawl a website\nconst crawlResponse = await app.crawlUrl('https://firecrawl.dev', {\nlimit: 100,\nscrapeOptions: {\nformats: ['markdown', 'html'],\n}\n});\n\nif (!crawlResponse.success) {\nthrow new Error(`Failed to crawl: ${crawlResponse.error}`)\n}\n\nconsole.log(crawlResponse)\n\n```",
      "metadata": {
        "title": "Node SDK | Firecrawl",
        "url": "https://docs.firecrawl.dev/sdks/node"
      }
    },
    {
      "id": "bf4c22ce-e60e-4183-942d-3f38f7678632",
      "source": "firecrawl/docs/sdks-node.md",
      "content": "[](https://docs.firecrawl.dev/sdks/node#usage) Usage > [](https://docs.firecrawl.dev/sdks/node#scraping-a-url) Scraping a URL\n\nTo scrape a single URL with error handling, use the `scrapeUrl` method. It takes the URL as a parameter and returns the scraped data as a dictionary.  \nNode  \nCopy  \n```js\n// Scrape a website:\nconst scrapeResult = await app.scrapeUrl('firecrawl.dev', { formats: ['markdown', 'html'] });\n\nif (!scrapeResult.success) {\nthrow new Error(`Failed to scrape: ${scrapeResult.error}`)\n}\n\nconsole.log(scrapeResult)\n\n```",
      "metadata": {
        "title": "Node SDK | Firecrawl",
        "url": "https://docs.firecrawl.dev/sdks/node"
      }
    },
    {
      "id": "70836b26-c951-4ab9-87f3-d706dbdc87e0",
      "source": "firecrawl/docs/sdks-node.md",
      "content": "[](https://docs.firecrawl.dev/sdks/node#usage) Usage > [](https://docs.firecrawl.dev/sdks/node#crawling-a-website) Crawling a Website\n\nTo crawl a website with error handling, use the `crawlUrl` method. It takes the starting URL and optional parameters as arguments. The `params` argument allows you to specify additional options for the crawl job, such as the maximum number of pages to crawl, allowed domains, and the output format.  \nNode  \nCopy  \n```js\nconst crawlResponse = await app.crawlUrl('https://firecrawl.dev', {\nlimit: 100,\nscrapeOptions: {\nformats: ['markdown', 'html'],\n}\n})\n\nif (!crawlResponse.success) {\nthrow new Error(`Failed to crawl: ${crawlResponse.error}`)\n}\n\nconsole.log(crawlResponse)\n\n```",
      "metadata": {
        "title": "Node SDK | Firecrawl",
        "url": "https://docs.firecrawl.dev/sdks/node"
      }
    },
    {
      "id": "6f1d1baa-bac6-4419-a421-260f8a3d4c1c",
      "source": "firecrawl/docs/sdks-node.md",
      "content": "[](https://docs.firecrawl.dev/sdks/node#usage) Usage > [](https://docs.firecrawl.dev/sdks/node#asynchronous-crawling) Asynchronous Crawling\n\nTo crawl a website asynchronously, use the `crawlUrlAsync` method. It returns the crawl `ID` which you can use to check the status of the crawl job. It takes the starting URL and optional parameters as arguments. The `params` argument allows you to specify additional options for the crawl job, such as the maximum number of pages to crawl, allowed domains, and the output format.  \nNode  \nCopy  \n```js\nconst crawlResponse = await app.asyncCrawlUrl('https://firecrawl.dev', {\nlimit: 100,\nscrapeOptions: {\nformats: ['markdown', 'html'],\n}\n});\n\nif (!crawlResponse.success) {\nthrow new Error(`Failed to crawl: ${crawlResponse.error}`)\n}\n\nconsole.log(crawlResponse)\n\n```",
      "metadata": {
        "title": "Node SDK | Firecrawl",
        "url": "https://docs.firecrawl.dev/sdks/node"
      }
    },
    {
      "id": "0892c45e-0bfe-4ff7-a810-3ad8d62f6cd1",
      "source": "firecrawl/docs/sdks-node.md",
      "content": "[](https://docs.firecrawl.dev/sdks/node#usage) Usage > [](https://docs.firecrawl.dev/sdks/node#checking-crawl-status) Checking Crawl Status\n\nTo check the status of a crawl job with error handling, use the `checkCrawlStatus` method. It takes the `ID` as a parameter and returns the current status of the crawl job.  \nNode  \nCopy  \n```js\nconst crawlResponse = await app.checkCrawlStatus(\"<crawl_id>\");\n\nif (!crawlResponse.success) {\nthrow new Error(`Failed to check crawl status: ${crawlResponse.error}`)\n}\n\nconsole.log(crawlResponse)\n\n```",
      "metadata": {
        "title": "Node SDK | Firecrawl",
        "url": "https://docs.firecrawl.dev/sdks/node"
      }
    },
    {
      "id": "35dd5f77-e472-4b2f-8527-4a50b3afd2b0",
      "source": "firecrawl/docs/sdks-node.md",
      "content": "[](https://docs.firecrawl.dev/sdks/node#usage) Usage > [](https://docs.firecrawl.dev/sdks/node#cancelling-a-crawl) Cancelling a Crawl\n\nTo cancel an asynchronous crawl job, use the `cancelCrawl` method. It takes the job ID of the asynchronous crawl as a parameter and returns the cancellation status.  \nNode  \nCopy  \n```js\nconst cancelCrawl = await app.cancelCrawl(id);\nconsole.log(cancelCrawl)\n\n```",
      "metadata": {
        "title": "Node SDK | Firecrawl",
        "url": "https://docs.firecrawl.dev/sdks/node"
      }
    },
    {
      "id": "570ccfbe-9ba1-4018-aa53-2734f057bdad",
      "source": "firecrawl/docs/sdks-node.md",
      "content": "[](https://docs.firecrawl.dev/sdks/node#usage) Usage > [](https://docs.firecrawl.dev/sdks/node#mapping-a-website) Mapping a Website\n\nTo map a website with error handling, use the `mapUrl` method. It takes the starting URL as a parameter and returns the mapped data as a dictionary.  \nNode  \nCopy  \n```js\nconst mapResult = await app.mapUrl('https://firecrawl.dev');\n\nif (!mapResult.success) {\nthrow new Error(`Failed to map: ${mapResult.error}`)\n}\n\nconsole.log(mapResult)\n\n```",
      "metadata": {
        "title": "Node SDK | Firecrawl",
        "url": "https://docs.firecrawl.dev/sdks/node"
      }
    },
    {
      "id": "740dd79d-5973-4e42-af8b-7c56c1f9a3b0",
      "source": "firecrawl/docs/sdks-node.md",
      "content": "[](https://docs.firecrawl.dev/sdks/node#usage) Usage > [](https://docs.firecrawl.dev/sdks/node#crawling-a-website-with-websockets) Crawling a Website with WebSockets\n\nTo crawl a website with WebSockets, use the `crawlUrlAndWatch` method. It takes the starting URL and optional parameters as arguments. The `params` argument allows you to specify additional options for the crawl job, such as the maximum number of pages to crawl, allowed domains, and the output format.  \nNode  \nCopy  \n```js\nconst watch = await app.crawlUrlAndWatch('mendable.ai', { excludePaths: ['blog/*'], limit: 5});\n\nwatch.addEventListener(\"document\", doc => {\nconsole.log(\"DOC\", doc.detail);\n});\n\nwatch.addEventListener(\"error\", err => {\nconsole.error(\"ERR\", err.detail.error);\n});\n\nwatch.addEventListener(\"done\", state => {\nconsole.log(\"DONE\", state.detail.status);\n});\n\n```",
      "metadata": {
        "title": "Node SDK | Firecrawl",
        "url": "https://docs.firecrawl.dev/sdks/node"
      }
    },
    {
      "id": "eeecc2ed-776f-4298-bd17-24e32e88e229",
      "source": "firecrawl/docs/sdks-node.md",
      "content": "[](https://docs.firecrawl.dev/sdks/node#error-handling) Error Handling\n\nThe SDK handles errors returned by the Firecrawl API and raises appropriate exceptions. If an error occurs during a request, an exception will be raised with a descriptive error message. The examples above demonstrate how to handle these errors using `try/catch` blocks.  \n[Suggest edits](https://github.com/hellofirecrawl/docs/edit/main/sdks/node.mdx) [Raise issue](https://github.com/hellofirecrawl/docs/issues/new?title=Issue%20on%20docs&body=Path:%20/sdks/node)  \n[Python](https://docs.firecrawl.dev/sdks/python) [Go](https://docs.firecrawl.dev/sdks/go)  \nOn this page  \n- [Installation](https://docs.firecrawl.dev/sdks/node#installation)\n- [Usage](https://docs.firecrawl.dev/sdks/node#usage)\n- [Scraping a URL](https://docs.firecrawl.dev/sdks/node#scraping-a-url)\n- [Crawling a Website](https://docs.firecrawl.dev/sdks/node#crawling-a-website)\n- [Asynchronous Crawling](https://docs.firecrawl.dev/sdks/node#asynchronous-crawling)\n- [Checking Crawl Status](https://docs.firecrawl.dev/sdks/node#checking-crawl-status)\n- [Cancelling a Crawl](https://docs.firecrawl.dev/sdks/node#cancelling-a-crawl)\n- [Mapping a Website](https://docs.firecrawl.dev/sdks/node#mapping-a-website)\n- [Crawling a Website with WebSockets](https://docs.firecrawl.dev/sdks/node#crawling-a-website-with-websockets)\n- [Error Handling](https://docs.firecrawl.dev/sdks/node#error-handling)",
      "metadata": {
        "title": "Node SDK | Firecrawl",
        "url": "https://docs.firecrawl.dev/sdks/node"
      }
    },
    {
      "id": "64c4a9d1-7f30-4300-aba2-a1644e85ae57",
      "source": "firecrawl/docs/integrations-flowise.md",
      "content": "---\ntitle: Flowise | Firecrawl\nurl: https://docs.firecrawl.dev/integrations/flowise\n---  \n[Firecrawl Docs home page![light logo](https://mintlify.s3.us-west-1.amazonaws.com/firecrawl/logo/light.svg)![dark logo](https://mintlify.s3.us-west-1.amazonaws.com/firecrawl/logo/dark.svg)](https://firecrawl.dev/)  \nv1  \nSearch or ask...  \nCtrl K  \nSearch...  \nNavigation  \nIntegrations  \nFlowise  \n[Documentation](https://docs.firecrawl.dev/introduction) [SDKs](https://docs.firecrawl.dev/sdks/overview) [Learn](https://www.firecrawl.dev/blog/category/tutorials) [API Reference](https://docs.firecrawl.dev/api-reference/introduction)",
      "metadata": {
        "title": "Flowise | Firecrawl",
        "url": "https://docs.firecrawl.dev/integrations/flowise"
      }
    },
    {
      "id": "2b2933bd-c268-452d-ae10-4a2d2845710d",
      "source": "firecrawl/docs/integrations-flowise.md",
      "content": "[](https://docs.firecrawl.dev/integrations/flowise#sync-web-data-in-flowise-workflows) Sync web data in Flowise workflows\n\nFirecrawl can be used inside of [Flowise the Chatflow builder](https://flowiseai.com/). This page introduces how to configure and use a Firecrawl block inside of Flowise.  \n![Firecrawl Flowise block](https://mintlify.s3.us-west-1.amazonaws.com/firecrawl/images/fc_flowise_block.png)",
      "metadata": {
        "title": "Flowise | Firecrawl",
        "url": "https://docs.firecrawl.dev/integrations/flowise"
      }
    },
    {
      "id": "c40ef6ac-0dce-4202-b909-8097e4062965",
      "source": "firecrawl/docs/integrations-flowise.md",
      "content": "[](https://docs.firecrawl.dev/integrations/flowise#sync-web-data-in-flowise-workflows) Sync web data in Flowise workflows > [](https://docs.firecrawl.dev/integrations/flowise#crawling-with-firecrawl-blocks) Crawling with Firecrawl blocks\n\n1. Log in to your Firecrawl account and get your API Key, and then enter it on the block.\n2. (Optional) Connect Text Splitter.\n3. Select the crawl mode to pick up a crawl pages below the target url.\n4. Input target URL to be crawled.\n5. Use the resulting documents in your workflows.",
      "metadata": {
        "title": "Flowise | Firecrawl",
        "url": "https://docs.firecrawl.dev/integrations/flowise"
      }
    },
    {
      "id": "4ae404a7-ccb5-4036-a2b3-2189517eb96e",
      "source": "firecrawl/docs/integrations-flowise.md",
      "content": "[](https://docs.firecrawl.dev/integrations/flowise#sync-web-data-in-flowise-workflows) Sync web data in Flowise workflows > [](https://docs.firecrawl.dev/integrations/flowise#scraping-with-firecrawl-blocks) Scraping with Firecrawl blocks\n\n1. Log in to your Firecrawl account and get your API Key, and then enter it on the block.\n2. (Optional) Connect Text Splitter.\n3. Select the scrape mode to pick up a single page.\n4. Input target URL to be scraped.\n5. Use the resulting documents in your workflows.  \n[Suggest edits](https://github.com/hellofirecrawl/docs/edit/main/integrations/flowise.mdx) [Raise issue](https://github.com/hellofirecrawl/docs/issues/new?title=Issue%20on%20docs&body=Path:%20/integrations/flowise)  \n[Dify](https://docs.firecrawl.dev/integrations/dify) [Langflow](https://docs.firecrawl.dev/integrations/langflow)  \nOn this page  \n- [Sync web data in Flowise workflows](https://docs.firecrawl.dev/integrations/flowise#sync-web-data-in-flowise-workflows)\n- [Crawling with Firecrawl blocks](https://docs.firecrawl.dev/integrations/flowise#crawling-with-firecrawl-blocks)\n- [Scraping with Firecrawl blocks](https://docs.firecrawl.dev/integrations/flowise#scraping-with-firecrawl-blocks)  \n![Firecrawl Flowise block](https://docs.firecrawl.dev/integrations/flowise)",
      "metadata": {
        "title": "Flowise | Firecrawl",
        "url": "https://docs.firecrawl.dev/integrations/flowise"
      }
    },
    {
      "id": "85ac075c-2738-438b-b5d3-0c8b73e77e8a",
      "source": "firecrawl/docs/features-crawl.md",
      "content": "---\ntitle: Crawl | Firecrawl\nurl: https://docs.firecrawl.dev/features/crawl\n---  \n[Firecrawl Docs home page![light logo](https://mintlify.s3.us-west-1.amazonaws.com/firecrawl/logo/light.svg)![dark logo](https://mintlify.s3.us-west-1.amazonaws.com/firecrawl/logo/dark.svg)](https://firecrawl.dev/)  \nv1  \nSearch or ask...  \nCtrl K  \nSearch...  \nNavigation  \nFeatures  \nCrawl  \n[Documentation](https://docs.firecrawl.dev/introduction) [SDKs](https://docs.firecrawl.dev/sdks/overview) [Learn](https://www.firecrawl.dev/blog/category/tutorials) [API Reference](https://docs.firecrawl.dev/api-reference/introduction)  \nFirecrawl thoroughly crawls websites, ensuring comprehensive data extraction while bypassing any web blocker mechanisms. Hereâ€™s how it works:  \n1. **URL Analysis:**\nBegins with a specified URL, identifying links by looking at the sitemap and then crawling the website. If no sitemap is found, it will crawl the website following the links.  \n2. **Recursive Traversal:**\nRecursively follows each link to uncover all subpages.  \n3. **Content Scraping:**\nGathers content from every visited page while handling any complexities like JavaScript rendering or rate limits.  \n4. **Result Compilation:**\nConverts collected data into clean markdown or structured output, perfect for LLM processing or any other task.  \nThis method guarantees an exhaustive crawl and data collection from any starting URL.",
      "metadata": {
        "title": "Crawl | Firecrawl",
        "url": "https://docs.firecrawl.dev/features/crawl"
      }
    },
    {
      "id": "119da524-245e-432c-b61e-d318b0576156",
      "source": "firecrawl/docs/features-crawl.md",
      "content": "[](https://docs.firecrawl.dev/features/crawl#crawling) Crawling > [](https://docs.firecrawl.dev/features/crawl#crawl-endpoint) /crawl endpoint\n\nUsed to crawl a URL and all accessible subpages. This submits a crawl job and returns a job ID to check the status of the crawl.  \nBy default - Crawl will ignore sublinks of a page if they arenâ€™t children of the url you provide. So, the website.com/other-parent/blog-1 wouldnâ€™t be returned if you crawled website.com/blogs/. If you want website.com/other-parent/blog-1, use the `allowBackwardLinks` parameter",
      "metadata": {
        "title": "Crawl | Firecrawl",
        "url": "https://docs.firecrawl.dev/features/crawl"
      }
    },
    {
      "id": "3751ca4d-2c44-4b86-b864-78659e63a95f",
      "source": "firecrawl/docs/features-crawl.md",
      "content": "[](https://docs.firecrawl.dev/features/crawl#crawling) Crawling > [](https://docs.firecrawl.dev/features/crawl#installation) Installation\n\nPython  \nNode  \nGo  \nRust  \nCopy  \n```bash\npip install firecrawl-py\n\n```",
      "metadata": {
        "title": "Crawl | Firecrawl",
        "url": "https://docs.firecrawl.dev/features/crawl"
      }
    },
    {
      "id": "27bd89e8-b06f-4cff-adfc-618f7ca8f5aa",
      "source": "firecrawl/docs/features-crawl.md",
      "content": "[](https://docs.firecrawl.dev/features/crawl#crawling) Crawling > [](https://docs.firecrawl.dev/features/crawl#usage) Usage\n\nPython  \nNode  \nGo  \nRust  \ncURL  \nCopy  \n```python\nfrom firecrawl import FirecrawlApp\n\napp = FirecrawlApp(api_key=\"fc-YOUR_API_KEY\")\n\n# Crawl a website:\ncrawl_status = app.crawl_url(\n'https://firecrawl.dev',\nparams={\n'limit': 100,\n'scrapeOptions': {'formats': ['markdown', 'html']}\n},\npoll_interval=30\n)\nprint(crawl_status)\n\n```",
      "metadata": {
        "title": "Crawl | Firecrawl",
        "url": "https://docs.firecrawl.dev/features/crawl"
      }
    },
    {
      "id": "d3046b92-7897-4abb-9ad2-503c42aa172f",
      "source": "firecrawl/docs/features-crawl.md",
      "content": "[](https://docs.firecrawl.dev/features/crawl#crawling) Crawling > [](https://docs.firecrawl.dev/features/crawl#response) Response\n\nIf youâ€™re using cURL or `async crawl` functions on SDKs, this will return an `ID` where you can use to check the status of the crawl.  \nCopy  \n```json\n{\n\"success\": true,\n\"id\": \"123-456-789\",\n\"url\": \"https://api.firecrawl.dev/v1/crawl/123-456-789\"\n}\n\n```",
      "metadata": {
        "title": "Crawl | Firecrawl",
        "url": "https://docs.firecrawl.dev/features/crawl"
      }
    },
    {
      "id": "a7deeb0b-6985-4d99-9d52-ddbd32e850f1",
      "source": "firecrawl/docs/features-crawl.md",
      "content": "[](https://docs.firecrawl.dev/features/crawl#crawling) Crawling > [](https://docs.firecrawl.dev/features/crawl#check-crawl-job) Check Crawl Job\n\nUsed to check the status of a crawl job and get its result.  \nThis endpoint only works for crawls that are in progress or crawls that have completed recently.  \nPython  \nNode  \nGo  \nRust  \ncURL  \nCopy  \n```python\ncrawl_status = app.check_crawl_status(\"<crawl_id>\")\nprint(crawl_status)\n\n```",
      "metadata": {
        "title": "Crawl | Firecrawl",
        "url": "https://docs.firecrawl.dev/features/crawl"
      }
    },
    {
      "id": "bb749681-b43d-4627-9b32-238b9b1856e5",
      "source": "firecrawl/docs/features-crawl.md",
      "content": "[](https://docs.firecrawl.dev/features/crawl#crawling) Crawling > [](https://docs.firecrawl.dev/features/crawl#check-crawl-job) Check Crawl Job > [](https://docs.firecrawl.dev/features/crawl#response-handling) Response Handling\n\nThe response varies based on the crawlâ€™s status.  \nFor not completed or large responses exceeding 10MB, a `next` URL parameter is provided. You must request this URL to retrieve the next 10MB of data. If the `next` parameter is absent, it indicates the end of the crawl data.  \nThe skip parameter sets the maximum number of results returned for each chunk of results returned.  \nThe skip and next parameter are only relavent when hitting the api directly. If youâ€™re using the SDK, we handle this for you and will return all the results at once.  \nScraping  \nCompleted  \nCopy  \n```json\n{\n\"status\": \"scraping\",\n\"total\": 36,\n\"completed\": 10,\n\"creditsUsed\": 10,\n\"expiresAt\": \"2024-00-00T00:00:00.000Z\",\n\"next\": \"https://api.firecrawl.dev/v1/crawl/123-456-789?skip=10\",\n\"data\": [\\\n{\\\n\"markdown\": \"[Firecrawl Docs home page![light logo](https://mintlify.s3-us-west-1.amazonaws.com/firecrawl/logo/light.svg)!...\",\\\n\"html\": \"<!DOCTYPE html><html lang=\"en\" class=\"js-focus-visible lg:[--scroll-mt:9.5rem]\" data-js-focus-visible=\"\">...\",\\\n\"metadata\": {\\\n\"title\": \"Build a 'Chat with website' using Groq Llama 3 | Firecrawl\",\\\n\"language\": \"en\",\\\n\"sourceURL\": \"https://docs.firecrawl.dev/learn/rag-llama3\",\\\n\"description\": \"Learn how to use Firecrawl, Groq Llama 3, and Langchain to build a 'Chat with your website' bot.\",\\\n\"ogLocaleAlternate\": [],\\\n\"statusCode\": 200\\\n}\\\n},\\\n...\\\n]\\\n}\\\n\\\n```\\\n\\",
      "metadata": {
        "title": "Crawl | Firecrawl",
        "url": "https://docs.firecrawl.dev/features/crawl"
      }
    },
    {
      "id": "234848f4-20ec-4ed4-891f-b25a3308b12a",
      "source": "firecrawl/docs/features-crawl.md",
      "content": "[](https://docs.firecrawl.dev/features/crawl#crawl-websocket) Crawl WebSocket\\\n\n\\\nFirecrawlâ€™s WebSocket-based method, `Crawl URL and Watch`, enables real-time data extraction and monitoring. Start a crawl with a URL and customize it with options like page limits, allowed domains, and output formats, ideal for immediate data processing needs.\\\n\\\nPython\\\n\\\nNode\\\n\\\nCopy\\\n\\\n```python\\\n# inside an async function...\\\nnest_asyncio.apply()\\\n\\\n# Define event handlers\\\ndef on_document(detail):\\\nprint(\"DOC\", detail)\\\n\\\ndef on_error(detail):\\\nprint(\"ERR\", detail['error'])\\\n\\\ndef on_done(detail):\\\nprint(\"DONE\", detail['status'])\\\n\\\n# Function to start the crawl and watch process\\\nasync def start_crawl_and_watch():\\\n# Initiate the crawl job and get the watcher\\\nwatcher = app.crawl_url_and_watch('firecrawl.dev', { 'excludePaths': ['blog/*'], 'limit': 5 })\\\n\\\n# Add event listeners\\\nwatcher.add_event_listener(\"document\", on_document)\\\nwatcher.add_event_listener(\"error\", on_error)\\\nwatcher.add_event_listener(\"done\", on_done)\\\n\\\n# Start the watcher\\\nawait watcher.connect()\\\n\\\n# Run the event loop\\\nawait start_crawl_and_watch()\\\n\\\n```\\\n\\",
      "metadata": {
        "title": "Crawl | Firecrawl",
        "url": "https://docs.firecrawl.dev/features/crawl"
      }
    },
    {
      "id": "603a8e42-785c-4ab1-b8bb-f2a628023355",
      "source": "firecrawl/docs/features-crawl.md",
      "content": "[](https://docs.firecrawl.dev/features/crawl#crawl-webhook) Crawl Webhook\\\n\n\\\nYou can now pass a `webhook` parameter to the `/crawl` endpoint. This will send a POST request to the URL you specify when the crawl is started, updated and completed.\\\n\\\nThe webhook will now trigger for every page crawled and not just the whole result at the end.\\\n\\\ncURL\\\n\\\nCopy\\\n\\\n```bash\\\ncurl -X POST https://api.firecrawl.dev/v1/crawl \\\n-H 'Content-Type: application/json' \\\n-H 'Authorization: Bearer YOUR_API_KEY' \\\n-d '{\\\n\"url\": \"https://docs.firecrawl.dev\",\\\n\"limit\": 100,\\\n\"webhook\": \"https://example.com/webhook\"\\\n}'\\\n\\\n```\\\n\\",
      "metadata": {
        "title": "Crawl | Firecrawl",
        "url": "https://docs.firecrawl.dev/features/crawl"
      }
    },
    {
      "id": "b3c72951-42d3-41f6-a018-1dc40b8bab3e",
      "source": "firecrawl/docs/features-crawl.md",
      "content": "[](https://docs.firecrawl.dev/features/crawl#crawl-webhook) Crawl Webhook\\ > [](https://docs.firecrawl.dev/features/crawl#webhook-events) Webhook Events\\\n\n\\\nThere are now 4 types of events:\\\n\\\n- `crawl.started` - Triggered when the crawl is started.\\\n- `crawl.page` - Triggered for every page crawled.\\\n- `crawl.completed` - Triggered when the crawl is completed to let you know itâ€™s done (Beta)**\\\n- `crawl.failed` - Triggered when the crawl fails.\\\n\\",
      "metadata": {
        "title": "Crawl | Firecrawl",
        "url": "https://docs.firecrawl.dev/features/crawl"
      }
    },
    {
      "id": "8d51a0d1-8bc4-45a7-bc03-0fb6fb6dd834",
      "source": "firecrawl/docs/features-crawl.md",
      "content": "[](https://docs.firecrawl.dev/features/crawl#crawl-webhook) Crawl Webhook\\ > [](https://docs.firecrawl.dev/features/crawl#webhook-response) Webhook Response\\\n\n\\\n- `success` - If the webhook was successful in crawling the page correctly.\\\n- `type` - The type of event that occurred.\\\n- `id` - The ID of the crawl.\\\n- `data` - The data that was scraped (Array). This will only be non empty on `crawl.page` and will contain 1 item if the page was scraped successfully. The response is the same as the `/scrape` endpoint.\\\n- `error` - If the webhook failed, this will contain the error message.\\\n\\\n**Beta consideration\\\n\\\n- There is a very tiny chance that the `crawl.completed` event may be triggered while the final `crawl.page` events are still being processed. Weâ€™re working on a fix for this.\\\n\\\n[Suggest edits](https://github.com/hellofirecrawl/docs/edit/main/features/crawl.mdx) [Raise issue](https://github.com/hellofirecrawl/docs/issues/new?title=Issue%20on%20docs&body=Path:%20/features/crawl)\\\n\\\n[LLM Extract](https://docs.firecrawl.dev/features/llm-extract) [Map](https://docs.firecrawl.dev/features/map)\\\n\\\nOn this page\\\n\\\n- [Crawling](https://docs.firecrawl.dev/features/crawl#crawling)\\\n- [/crawl endpoint](https://docs.firecrawl.dev/features/crawl#crawl-endpoint)\\\n- [Installation](https://docs.firecrawl.dev/features/crawl#installation)\\\n- [Usage](https://docs.firecrawl.dev/features/crawl#usage)\\\n- [Response](https://docs.firecrawl.dev/features/crawl#response)\\\n- [Check Crawl Job](https://docs.firecrawl.dev/features/crawl#check-crawl-job)\\\n- [Response Handling](https://docs.firecrawl.dev/features/crawl#response-handling)\\\n- [Crawl WebSocket](https://docs.firecrawl.dev/features/crawl#crawl-websocket)\\\n- [Crawl Webhook](https://docs.firecrawl.dev/features/crawl#crawl-webhook)\\\n- [Webhook Events](https://docs.firecrawl.dev/features/crawl#webhook-events)\\\n- [Webhook Response](https://docs.firecrawl.dev/features/crawl#webhook-response)",
      "metadata": {
        "title": "Crawl | Firecrawl",
        "url": "https://docs.firecrawl.dev/features/crawl"
      }
    },
    {
      "id": "4f849f41-868e-4187-bd1f-df9194dea312",
      "source": "firecrawl/docs/api-reference-endpoint-search.md",
      "content": "---\ntitle: Firecrawl Docs\nurl: https://docs.firecrawl.dev/api-reference/endpoint/search\n---  \n[Firecrawl Docs home page![light logo](https://mintlify.s3.us-west-1.amazonaws.com/firecrawl/logo/light.svg)![dark logo](https://mintlify.s3.us-west-1.amazonaws.com/firecrawl/logo/dark.svg)](https://firecrawl.dev/)  \nv1  \nSearch or ask...  \nCtrl K  \nSearch...  \nNavigation  \nSearch Endpoints  \nSearch  \n[Documentation](https://docs.firecrawl.dev/introduction) [SDKs](https://docs.firecrawl.dev/sdks/overview) [Learn](https://www.firecrawl.dev/blog/category/tutorials) [API Reference](https://docs.firecrawl.dev/api-reference/introduction)  \nPOST  \n/  \nsearch  \nTry it  \ncURL  \nPython  \nJavaScript  \nPHP  \nGo  \nJava  \nCopy  \n```\ncurl --request POST \\\n--url https://api.firecrawl.dev/v1/search \\\n--header 'Authorization: Bearer <token>' \\\n--header 'Content-Type: application/json' \\\n--data '{\n\"query\": \"<string>\",\n\"limit\": 5,\n\"tbs\": \"<string>\",\n\"lang\": \"en\",\n\"country\": \"us\",\n\"location\": \"<string>\",\n\"timeout\": 60000,\n\"scrapeOptions\": {}\n}'\n```  \n200  \n408  \n500  \nCopy  \n```\n{\n\"success\": true,\n\"data\": [\\\n{\\\n\"title\": \"<string>\",\\\n\"description\": \"<string>\",\\\n\"url\": \"<string>\",\\\n\"markdown\": \"<string>\",\\\n\"html\": \"<string>\",\\\n\"rawHtml\": \"<string>\",\\\n\"links\": [\\\n\"<string>\"\\\n],\\\n\"screenshot\": \"<string>\",\\\n\"metadata\": {\\\n\"title\": \"<string>\",\\\n\"description\": \"<string>\",\\\n\"sourceURL\": \"<string>\",\\\n\"statusCode\": 123,\\\n\"error\": \"<string>\"\\\n}\\\n}\\\n],\n\"warning\": \"<string>\"\n}\n```  \nThe search endpoint combines web search (SERP) with Firecrawlâ€™s scraping capabilities to return full page content for any query.  \nInclude `scrapeOptions` with `formats: [\"markdown\"]` to get complete markdown content for each search result otherwise you will default to getting the SERP results (url, title, description).",
      "metadata": {
        "title": "Firecrawl Docs",
        "url": "https://docs.firecrawl.dev/api-reference/endpoint/search"
      }
    },
    {
      "id": "6647f187-b889-49b2-a424-32f520e4b741",
      "source": "firecrawl/docs/api-reference-endpoint-search.md",
      "content": "Authorizations\n\n[](https://docs.firecrawl.dev/api-reference/endpoint/search#authorization-authorization)  \nAuthorization  \nstring  \nheader  \nrequired  \nBearer authentication header of the form `Bearer <token>`, where `<token>` is your auth token.",
      "metadata": {
        "title": "Firecrawl Docs",
        "url": "https://docs.firecrawl.dev/api-reference/endpoint/search"
      }
    },
    {
      "id": "0e0b60d7-5a84-4f4a-85c6-0c4645656cb4",
      "source": "firecrawl/docs/api-reference-endpoint-search.md",
      "content": "Body\n\napplication/json  \n[](https://docs.firecrawl.dev/api-reference/endpoint/search#body-query)  \nquery  \nstring  \nrequired  \nThe search query  \n[](https://docs.firecrawl.dev/api-reference/endpoint/search#body-limit)  \nlimit  \ninteger  \ndefault:  \n5  \nMaximum number of results to return  \nRequired range: `1 < x < 10`  \n[](https://docs.firecrawl.dev/api-reference/endpoint/search#body-tbs)  \ntbs  \nstring  \nTime-based search parameter  \n[](https://docs.firecrawl.dev/api-reference/endpoint/search#body-lang)  \nlang  \nstring  \ndefault:  \nen  \nLanguage code for search results  \n[](https://docs.firecrawl.dev/api-reference/endpoint/search#body-country)  \ncountry  \nstring  \ndefault:  \nus  \nCountry code for search results  \n[](https://docs.firecrawl.dev/api-reference/endpoint/search#body-location)  \nlocation  \nstring  \nLocation parameter for search results  \n[](https://docs.firecrawl.dev/api-reference/endpoint/search#body-timeout)  \ntimeout  \ninteger  \ndefault:  \n60000  \nTimeout in milliseconds  \n[](https://docs.firecrawl.dev/api-reference/endpoint/search#body-scrape-options)  \nscrapeOptions  \nobject  \nOptions for scraping search results  \nShowchild attributes  \n[](https://docs.firecrawl.dev/api-reference/endpoint/search#body-scrape-options-formats)  \nscrapeOptions.formats  \nenum<string>[]  \nFormats to include in the output  \nAvailable options:  \n`markdown`,  \n`html`,  \n`rawHtml`,  \n`links`,  \n`screenshot`,  \n`screenshot@fullPage`,  \n`extract`",
      "metadata": {
        "title": "Firecrawl Docs",
        "url": "https://docs.firecrawl.dev/api-reference/endpoint/search"
      }
    },
    {
      "id": "e046a928-3f15-4245-b654-4057719dfe8f",
      "source": "firecrawl/docs/api-reference-endpoint-search.md",
      "content": "Response\n\n200 - application/json  \n[](https://docs.firecrawl.dev/api-reference/endpoint/search#response-success)  \nsuccess  \nboolean  \n[](https://docs.firecrawl.dev/api-reference/endpoint/search#response-data)  \ndata  \nobject[]  \nShowchild attributes  \n[](https://docs.firecrawl.dev/api-reference/endpoint/search#response-data-title)  \ndata.title  \nstring  \nTitle from search result  \n[](https://docs.firecrawl.dev/api-reference/endpoint/search#response-data-description)  \ndata.description  \nstring  \nDescription from search result  \n[](https://docs.firecrawl.dev/api-reference/endpoint/search#response-data-url)  \ndata.url  \nstring  \nURL of the search result  \n[](https://docs.firecrawl.dev/api-reference/endpoint/search#response-data-markdown)  \ndata.markdown  \nstring | null  \nMarkdown content if scraping was requested  \n[](https://docs.firecrawl.dev/api-reference/endpoint/search#response-data-html)  \ndata.html  \nstring | null  \nHTML content if requested in formats  \n[](https://docs.firecrawl.dev/api-reference/endpoint/search#response-data-raw-html)  \ndata.rawHtml  \nstring | null  \nRaw HTML content if requested in formats  \n[](https://docs.firecrawl.dev/api-reference/endpoint/search#response-data-links)  \ndata.links  \nstring[]  \nLinks found if requested in formats  \n[](https://docs.firecrawl.dev/api-reference/endpoint/search#response-data-screenshot)  \ndata.screenshot  \nstring | null  \nScreenshot URL if requested in formats  \n[](https://docs.firecrawl.dev/api-reference/endpoint/search#response-data-metadata)  \ndata.metadata  \nobject  \nShowchild attributes  \n[](https://docs.firecrawl.dev/api-reference/endpoint/search#response-data-metadata-title)  \ndata.metadata.title  \nstring  \n[](https://docs.firecrawl.dev/api-reference/endpoint/search#response-data-metadata-description)  \ndata.metadata.description  \nstring  \n[](https://docs.firecrawl.dev/api-reference/endpoint/search#response-data-metadata-source-url)  \ndata.metadata.sourceURL  \nstring  \n[](https://docs.firecrawl.dev/api-reference/endpoint/search#response-data-metadata-status-code)  \ndata.metadata.statusCode  \ninteger  \n[](https://docs.firecrawl.dev/api-reference/endpoint/search#response-data-metadata-error)  \ndata.metadata.error  \nstring | null  \n[](https://docs.firecrawl.dev/api-reference/endpoint/search#response-warning)  \nwarning  \nstring | null  \nWarning message if any issues occurred  \n[Suggest edits](https://github.com/hellofirecrawl/docs/edit/main/api-reference/endpoint/search.mdx) [Raise issue](https://github.com/hellofirecrawl/docs/issues/new?title=Issue%20on%20docs&body=Path:%20/api-reference/endpoint/search)  \n[Get Extract Status](https://docs.firecrawl.dev/api-reference/endpoint/extract-get) [Credit Usage](https://docs.firecrawl.dev/api-reference/endpoint/credit-usage)  \ncURL  \nPython  \nJavaScript  \nPHP  \nGo  \nJava  \nCopy  \n```\ncurl --request POST \\\n--url https://api.firecrawl.dev/v1/search \\\n--header 'Authorization: Bearer <token>' \\\n--header 'Content-Type: application/json' \\\n--data '{\n\"query\": \"<string>\",\n\"limit\": 5,\n\"tbs\": \"<string>\",\n\"lang\": \"en\",\n\"country\": \"us\",\n\"location\": \"<string>\",\n\"timeout\": 60000,\n\"scrapeOptions\": {}\n}'\n```  \n200  \n408  \n500  \nCopy  \n```\n{\n\"success\": true,\n\"data\": [\\\n{\\\n\"title\": \"<string>\",\\\n\"description\": \"<string>\",\\\n\"url\": \"<string>\",\\\n\"markdown\": \"<string>\",\\\n\"html\": \"<string>\",\\\n\"rawHtml\": \"<string>\",\\\n\"links\": [\\\n\"<string>\"\\\n],\\\n\"screenshot\": \"<string>\",\\\n\"metadata\": {\\\n\"title\": \"<string>\",\\\n\"description\": \"<string>\",\\\n\"sourceURL\": \"<string>\",\\\n\"statusCode\": 123,\\\n\"error\": \"<string>\"\\\n}\\\n}\\\n],\n\"warning\": \"<string>\"\n}\n```",
      "metadata": {
        "title": "Firecrawl Docs",
        "url": "https://docs.firecrawl.dev/api-reference/endpoint/search"
      }
    },
    {
      "id": "48177d0e-2d98-489b-899a-2dfa122c68ef",
      "source": "firecrawl/docs/v0-advanced-scraping-guide.md",
      "content": "---\ntitle: Advanced Scraping Guide | Firecrawl\nurl: https://docs.firecrawl.dev/v0/advanced-scraping-guide\n---  \n[Firecrawl Docs home page![light logo](https://mintlify.s3.us-west-1.amazonaws.com/firecrawl/logo/light.svg)![dark logo](https://mintlify.s3.us-west-1.amazonaws.com/firecrawl/logo/dark.svg)](https://firecrawl.dev/)  \nv0  \nSearch or ask...  \nCtrl K  \nSearch...  \nNavigation  \nGet Started  \nAdvanced Scraping Guide  \n[Documentation](https://docs.firecrawl.dev/v0/introduction) [SDKs](https://docs.firecrawl.dev/v0/sdks/python) [Learn](https://www.firecrawl.dev/blog/category/tutorials) [API Reference](https://docs.firecrawl.dev/v0/api-reference/introduction)  \nThis guide will walk you through the different endpoints of Firecrawl and how to use them fully with all its parameters.",
      "metadata": {
        "title": "Advanced Scraping Guide | Firecrawl",
        "url": "https://docs.firecrawl.dev/v0/advanced-scraping-guide"
      }
    },
    {
      "id": "46f32ffe-c7d3-4f9b-88da-a2cdb144f022",
      "source": "firecrawl/docs/v0-advanced-scraping-guide.md",
      "content": "[](https://docs.firecrawl.dev/v0/advanced-scraping-guide#basic-scraping-with-firecrawl-scrape) Basic scraping with Firecrawl (/scrape)\n\nTo scrape a single page and get clean markdown content, you can use the `/scrape` endpoint.  \nPython  \nJavaScript  \nGo  \nRust  \ncURL  \nCopy  \n```python\n# pip install firecrawl-py\n\nfrom firecrawl import FirecrawlApp\n\napp = FirecrawlApp(api_key=\"YOUR_API_KEY\")\n\ncontent = app.scrape_url(\"https://docs.firecrawl.dev\")\n\n```",
      "metadata": {
        "title": "Advanced Scraping Guide | Firecrawl",
        "url": "https://docs.firecrawl.dev/v0/advanced-scraping-guide"
      }
    },
    {
      "id": "3980fd64-2339-4637-9429-c824733fff8b",
      "source": "firecrawl/docs/v0-advanced-scraping-guide.md",
      "content": "[](https://docs.firecrawl.dev/v0/advanced-scraping-guide#scraping-pdfs) Scraping pdfs\n\n**Firecrawl supports scraping pdfs by default.** You can use the `/scrape` endpoint to scrape a pdf link and get the text content of the pdf. You can disable this by setting `pageOptions.parsePDF` to `false`.",
      "metadata": {
        "title": "Advanced Scraping Guide | Firecrawl",
        "url": "https://docs.firecrawl.dev/v0/advanced-scraping-guide"
      }
    },
    {
      "id": "7817e705-87b3-4810-ba9e-b0d15a3e1015",
      "source": "firecrawl/docs/v0-advanced-scraping-guide.md",
      "content": "[](https://docs.firecrawl.dev/v0/advanced-scraping-guide#page-options) Page Options\n\nWhen using the `/scrape` endpoint, you can customize the scraping behavior with the `pageOptions` parameter. Here are the available options:",
      "metadata": {
        "title": "Advanced Scraping Guide | Firecrawl",
        "url": "https://docs.firecrawl.dev/v0/advanced-scraping-guide"
      }
    },
    {
      "id": "8514cea3-87b4-4b2a-b224-097f941a3423",
      "source": "firecrawl/docs/v0-advanced-scraping-guide.md",
      "content": "[](https://docs.firecrawl.dev/v0/advanced-scraping-guide#page-options) Page Options > [](https://docs.firecrawl.dev/v0/advanced-scraping-guide#getting-cleaner-content-with-onlymaincontent) Getting cleaner content with `onlyMainContent`\n\n- **Type**: `boolean`\n- **Description**: Only return the main content of the page, excluding headers, navigation bars, footers, etc.\n- **Default**: `false`",
      "metadata": {
        "title": "Advanced Scraping Guide | Firecrawl",
        "url": "https://docs.firecrawl.dev/v0/advanced-scraping-guide"
      }
    },
    {
      "id": "ab609611-60c2-4807-a9cd-b914a302dc27",
      "source": "firecrawl/docs/v0-advanced-scraping-guide.md",
      "content": "[](https://docs.firecrawl.dev/v0/advanced-scraping-guide#page-options) Page Options > [](https://docs.firecrawl.dev/v0/advanced-scraping-guide#getting-the-html-with-includehtml) Getting the HTML with `includeHtml`\n\n- **Type**: `boolean`\n- **Description**: Include the HTML version content of the page. This will add an `html` key in the response.\n- **Default**: `false`",
      "metadata": {
        "title": "Advanced Scraping Guide | Firecrawl",
        "url": "https://docs.firecrawl.dev/v0/advanced-scraping-guide"
      }
    },
    {
      "id": "91b58da6-90d2-4cdc-bc84-a32052049f5f",
      "source": "firecrawl/docs/v0-advanced-scraping-guide.md",
      "content": "[](https://docs.firecrawl.dev/v0/advanced-scraping-guide#page-options) Page Options > [](https://docs.firecrawl.dev/v0/advanced-scraping-guide#getting-the-raw-html-with-includerawhtml) Getting the raw HTML with `includeRawHtml`\n\n- **Type**: `boolean`\n- **Description**: Include the raw HTML content of the page. This will add an `rawHtml` key in the response.\n- **Default**: `false`",
      "metadata": {
        "title": "Advanced Scraping Guide | Firecrawl",
        "url": "https://docs.firecrawl.dev/v0/advanced-scraping-guide"
      }
    },
    {
      "id": "083e7fbb-487d-458d-abcb-6a8bffecb339",
      "source": "firecrawl/docs/v0-advanced-scraping-guide.md",
      "content": "[](https://docs.firecrawl.dev/v0/advanced-scraping-guide#page-options) Page Options > [](https://docs.firecrawl.dev/v0/advanced-scraping-guide#getting-a-screenshot-of-the-page-with-screenshot) Getting a screenshot of the page with `screenshot`\n\n- **Type**: `boolean`\n- **Decription**: Include a screenshot of the top of the page that you are scraping.\n- **Default**: `false`",
      "metadata": {
        "title": "Advanced Scraping Guide | Firecrawl",
        "url": "https://docs.firecrawl.dev/v0/advanced-scraping-guide"
      }
    },
    {
      "id": "f76f73eb-2bba-44cc-961e-f54c65846f3f",
      "source": "firecrawl/docs/v0-advanced-scraping-guide.md",
      "content": "[](https://docs.firecrawl.dev/v0/advanced-scraping-guide#page-options) Page Options > [](https://docs.firecrawl.dev/v0/advanced-scraping-guide#waiting-for-the-page-to-load-with-waitfor) Waiting for the page to load with `waitFor`\n\n- **Type**: `integer`\n- **Description**: To be used only as a last resort. Wait for a specified amount of milliseconds for the page to load before fetching content.\n- **Default**: `0`",
      "metadata": {
        "title": "Advanced Scraping Guide | Firecrawl",
        "url": "https://docs.firecrawl.dev/v0/advanced-scraping-guide"
      }
    },
    {
      "id": "ea1a7ab7-f974-43d2-835c-4204b9a3ee3d",
      "source": "firecrawl/docs/v0-advanced-scraping-guide.md",
      "content": "[](https://docs.firecrawl.dev/v0/advanced-scraping-guide#page-options) Page Options > [](https://docs.firecrawl.dev/v0/advanced-scraping-guide#example-usage) Example Usage\n\nCopy  \n```bash\ncurl -X POST https://api.firecrawl.dev/v0/scrape \\\n-H '\nContent-Type: application/json' \\\n-H 'Authorization : Bearer YOUR_API_KEY' \\\n-d '{\n\"url\": \"https://docs.firecrawl.dev\",\n\"pageOptions\": {\n\"onlyMainContent\": true,\n\"includeHtml\": true,\n\"includeRawHtml\":true,\n\"screenshot\": true,\n\"waitFor\": 5000\n}\n}'\n\n```  \nIn this example, the scraper will:  \n- Return only the main content of the page.\n- Include the raw HTML content in the response in the `html` key.\n- Wait for 5000 milliseconds (5 seconds) for the page to load before fetching the content.  \nHere is the API Reference for it: [Scrape Endpoint Documentation](https://docs.firecrawl.dev/api-reference/endpoint/scrape)",
      "metadata": {
        "title": "Advanced Scraping Guide | Firecrawl",
        "url": "https://docs.firecrawl.dev/v0/advanced-scraping-guide"
      }
    },
    {
      "id": "8b535976-f624-4a90-b10d-63715121bd28",
      "source": "firecrawl/docs/v0-advanced-scraping-guide.md",
      "content": "[](https://docs.firecrawl.dev/v0/advanced-scraping-guide#extractor-options) Extractor Options\n\nWhen using the `/scrape` endpoint, you can specify options for **extracting structured information** from the page content using the `extractorOptions` parameter. Here are the available options:",
      "metadata": {
        "title": "Advanced Scraping Guide | Firecrawl",
        "url": "https://docs.firecrawl.dev/v0/advanced-scraping-guide"
      }
    },
    {
      "id": "3036ebae-be20-4af2-869d-74b04e96e2a5",
      "source": "firecrawl/docs/v0-advanced-scraping-guide.md",
      "content": "[](https://docs.firecrawl.dev/v0/advanced-scraping-guide#extractor-options) Extractor Options > [](https://docs.firecrawl.dev/v0/advanced-scraping-guide#mode) mode\n\n- **Type**: `string`  \n- **Enum**: `[\"llm-extraction\", \"llm-extraction-from-raw-html\"]`  \n- **Description**: The extraction mode to use.\n- `llm-extraction`: Extracts information from the cleaned and parsed content.\n- `llm-extraction-from-raw-html`: Extracts information directly from the raw HTML.\n- **Type**: `string`  \n- **Description**: A prompt describing what information to extract from the page.",
      "metadata": {
        "title": "Advanced Scraping Guide | Firecrawl",
        "url": "https://docs.firecrawl.dev/v0/advanced-scraping-guide"
      }
    },
    {
      "id": "6b0fdae5-a33a-488b-8565-99c72aeb23cc",
      "source": "firecrawl/docs/v0-advanced-scraping-guide.md",
      "content": "[](https://docs.firecrawl.dev/v0/advanced-scraping-guide#extractor-options) Extractor Options > [](https://docs.firecrawl.dev/v0/advanced-scraping-guide#extractionschema) extractionSchema\n\n- **Type**: `object`\n- **Description**: The schema for the data to be extracted. This defines the structure of the extracted data.",
      "metadata": {
        "title": "Advanced Scraping Guide | Firecrawl",
        "url": "https://docs.firecrawl.dev/v0/advanced-scraping-guide"
      }
    },
    {
      "id": "f224392e-9d2f-4b81-8a74-de7757c829b7",
      "source": "firecrawl/docs/v0-advanced-scraping-guide.md",
      "content": "[](https://docs.firecrawl.dev/v0/advanced-scraping-guide#extractor-options) Extractor Options > [](https://docs.firecrawl.dev/v0/advanced-scraping-guide#example-usage-2) Example Usage\n\nCopy  \n```bash\ncurl -X POST https://api.firecrawl.dev/v0/scrape \\\n-H 'Content-Type: application/json' \\\n-H 'Authorization: Bearer YOUR_API_KEY' \\\n-d '{\n\"url\": \"https://docs.firecrawl.dev/\",\n\"extractorOptions\": {\n\"mode\": \"llm-extraction\",\n\"extractionPrompt\": \"Based on the information on the page, extract the information from the schema. \",\n\"extractionSchema\": {\n\"type\": \"object\",\n\"properties\": {\n\"company_mission\": {\n\"type\": \"string\"\n},\n\"supports_sso\": {\n\"type\": \"boolean\"\n},\n\"is_open_source\": {\n\"type\": \"boolean\"\n},\n\"is_in_yc\": {\n\"type\": \"boolean\"\n}\n},\n\"required\": [\\\n\"company_mission\",\\\n\"supports_sso\",\\\n\"is_open_source\",\\\n\"is_in_yc\"\\\n]\n}\n}\n}'\n\n```  \nCopy  \n```json\n{\n\"success\": true,\n\"data\": {\n\"content\": \"Raw Content\",\n\"metadata\": {\n\"title\": \"Mendable\",\n\"description\": \"Mendable allows you to easily build AI chat applications. Ingest, customize, then deploy with one line of code anywhere you want. Brought to you by SideGuide\",\n\"robots\": \"follow, index\",\n\"ogTitle\": \"Mendable\",\n\"ogDescription\": \"Mendable allows you to easily build AI chat applications. Ingest, customize, then deploy with one line of code anywhere you want. Brought to you by SideGuide\",\n\"ogUrl\": \"https://docs.firecrawl.dev/\",\n\"ogImage\": \"https://docs.firecrawl.dev/mendable_new_og1.png\",\n\"ogLocaleAlternate\": [],\n\"ogSiteName\": \"Mendable\",\n\"sourceURL\": \"https://docs.firecrawl.dev/\"\n},\n\"llm_extraction\": {\n\"company_mission\": \"Train a secure AI on your technical resources that answers customer and employee questions so your team doesn't have to\",\n\"supports_sso\": true,\n\"is_open_source\": false,\n\"is_in_yc\": true\n}\n}\n}\n\n```",
      "metadata": {
        "title": "Advanced Scraping Guide | Firecrawl",
        "url": "https://docs.firecrawl.dev/v0/advanced-scraping-guide"
      }
    },
    {
      "id": "89d44f8c-9eda-4cd1-9268-a8a5563b4b2c",
      "source": "firecrawl/docs/v0-advanced-scraping-guide.md",
      "content": "[](https://docs.firecrawl.dev/v0/advanced-scraping-guide#adjusting-timeout) Adjusting Timeout\n\nYou can adjust the timeout for the scraping process using the `timeout` parameter in milliseconds.",
      "metadata": {
        "title": "Advanced Scraping Guide | Firecrawl",
        "url": "https://docs.firecrawl.dev/v0/advanced-scraping-guide"
      }
    },
    {
      "id": "d7d1f8fa-52fc-4a1a-80ce-36ffa78dac2e",
      "source": "firecrawl/docs/v0-advanced-scraping-guide.md",
      "content": "[](https://docs.firecrawl.dev/v0/advanced-scraping-guide#adjusting-timeout) Adjusting Timeout > [](https://docs.firecrawl.dev/v0/advanced-scraping-guide#example-usage-3) Example Usage\n\nCopy  \n```bash\ncurl -X POST https://api.firecrawl.dev/v0/scrape \\\n-H '\nContent-Type: application/json' \\\n-H 'Authorization: Bearer YOUR_API_KEY' \\\n-d '{\n\"url\": \"https://docs.firecrawl.dev\",\n\"timeout\": 50000\n}'\n\n```",
      "metadata": {
        "title": "Advanced Scraping Guide | Firecrawl",
        "url": "https://docs.firecrawl.dev/v0/advanced-scraping-guide"
      }
    },
    {
      "id": "1d7f59fb-ae92-4cc0-91f5-445558a98682",
      "source": "firecrawl/docs/v0-advanced-scraping-guide.md",
      "content": "[](https://docs.firecrawl.dev/v0/advanced-scraping-guide#crawling-multiple-pages) Crawling Multiple Pages\n\nTo crawl multiple pages, you can use the `/crawl` endpoint. This endpoint allows you to specify a base URL you want to crawl and all accessible subpages will be crawled.  \nCopy  \n```bash\ncurl -X POST https://api.firecrawl.dev/v0/crawl \\\n-H 'Content-Type: application/json' \\\n-H 'Authorization: Bearer YOUR_API_KEY' \\\n-d '{\n\"url\": \"https://docs.firecrawl.dev\"\n}'\n\n```  \nReturns a jobId  \nCopy  \n```json\n{ \"jobId\": \"1234-5678-9101\" }\n\n```",
      "metadata": {
        "title": "Advanced Scraping Guide | Firecrawl",
        "url": "https://docs.firecrawl.dev/v0/advanced-scraping-guide"
      }
    },
    {
      "id": "f049d25c-236c-47e8-b378-1781c3f9fce0",
      "source": "firecrawl/docs/v0-advanced-scraping-guide.md",
      "content": "[](https://docs.firecrawl.dev/v0/advanced-scraping-guide#crawling-multiple-pages) Crawling Multiple Pages > [](https://docs.firecrawl.dev/v0/advanced-scraping-guide#check-crawl-job) Check Crawl Job\n\nUsed to check the status of a crawl job and get its result.  \nCopy  \n```bash\ncurl -X GET https://api.firecrawl.dev/v0/crawl/status/1234-5678-9101 \\\n-H 'Content-Type: application/json' \\\n-H 'Authorization: Bearer YOUR_API_KEY'\n\n```",
      "metadata": {
        "title": "Advanced Scraping Guide | Firecrawl",
        "url": "https://docs.firecrawl.dev/v0/advanced-scraping-guide"
      }
    },
    {
      "id": "015e8d36-1817-4b73-b698-d34456fdb23b",
      "source": "firecrawl/docs/v0-advanced-scraping-guide.md",
      "content": "[](https://docs.firecrawl.dev/v0/advanced-scraping-guide#crawling-multiple-pages) Crawling Multiple Pages > [](https://docs.firecrawl.dev/v0/advanced-scraping-guide#crawler-options) Crawler Options\n\nWhen using the `/crawl` endpoint, you can customize the crawling behavior with the `crawlerOptions` parameter. Here are the available options:",
      "metadata": {
        "title": "Advanced Scraping Guide | Firecrawl",
        "url": "https://docs.firecrawl.dev/v0/advanced-scraping-guide"
      }
    },
    {
      "id": "d23948b7-e0eb-4a50-8dc0-cc305811b647",
      "source": "firecrawl/docs/v0-advanced-scraping-guide.md",
      "content": "[](https://docs.firecrawl.dev/v0/advanced-scraping-guide#crawling-multiple-pages) Crawling Multiple Pages > [](https://docs.firecrawl.dev/v0/advanced-scraping-guide#crawler-options) Crawler Options > [](https://docs.firecrawl.dev/v0/advanced-scraping-guide#includes) `includes`\n\n- **Type**: `array`\n- **Description**: URL patterns to include in the crawl. Only URLs matching these patterns will be crawled.\n- **Example**: `[\"/blog/*\", \"/products/*\"]`",
      "metadata": {
        "title": "Advanced Scraping Guide | Firecrawl",
        "url": "https://docs.firecrawl.dev/v0/advanced-scraping-guide"
      }
    },
    {
      "id": "1fe4ac6d-4f8e-4b76-99a1-374d647d398a",
      "source": "firecrawl/docs/v0-advanced-scraping-guide.md",
      "content": "[](https://docs.firecrawl.dev/v0/advanced-scraping-guide#crawling-multiple-pages) Crawling Multiple Pages > [](https://docs.firecrawl.dev/v0/advanced-scraping-guide#crawler-options) Crawler Options > [](https://docs.firecrawl.dev/v0/advanced-scraping-guide#excludes) `excludes`\n\n- **Type**: `array`\n- **Description**: URL patterns to exclude from the crawl. URLs matching these patterns will be skipped.\n- **Example**: `[\"/admin/*\", \"/login/*\"]`",
      "metadata": {
        "title": "Advanced Scraping Guide | Firecrawl",
        "url": "https://docs.firecrawl.dev/v0/advanced-scraping-guide"
      }
    },
    {
      "id": "c99de714-b64e-48d1-98cb-c76fcfada8b3",
      "source": "firecrawl/docs/v0-advanced-scraping-guide.md",
      "content": "[](https://docs.firecrawl.dev/v0/advanced-scraping-guide#crawling-multiple-pages) Crawling Multiple Pages > [](https://docs.firecrawl.dev/v0/advanced-scraping-guide#crawler-options) Crawler Options > [](https://docs.firecrawl.dev/v0/advanced-scraping-guide#returnonlyurls) `returnOnlyUrls`\n\n- **Type**: `boolean`\n- **Description**: If set to `true`, the response will only include a list of URLs instead of the full document data.\n- **Default**: `false`",
      "metadata": {
        "title": "Advanced Scraping Guide | Firecrawl",
        "url": "https://docs.firecrawl.dev/v0/advanced-scraping-guide"
      }
    },
    {
      "id": "03ed300b-5ffa-4afb-96c4-7f583947bb31",
      "source": "firecrawl/docs/v0-advanced-scraping-guide.md",
      "content": "[](https://docs.firecrawl.dev/v0/advanced-scraping-guide#crawling-multiple-pages) Crawling Multiple Pages > [](https://docs.firecrawl.dev/v0/advanced-scraping-guide#crawler-options) Crawler Options > [](https://docs.firecrawl.dev/v0/advanced-scraping-guide#maxdepth) `maxDepth`\n\n- **Type**: `integer`\n- **Description**: Maximum depth to crawl relative to the entered URL. A maxDepth of 0 scrapes only the entered URL. A maxDepth of 1 scrapes the entered URL and all pages one level deep. A maxDepth of 2 scrapes the entered URL and all pages up to two levels deep. Higher values follow the same pattern.\n- **Example**: `2`",
      "metadata": {
        "title": "Advanced Scraping Guide | Firecrawl",
        "url": "https://docs.firecrawl.dev/v0/advanced-scraping-guide"
      }
    },
    {
      "id": "ab075a1e-e354-41fd-ad4e-0708798481b7",
      "source": "firecrawl/docs/v0-advanced-scraping-guide.md",
      "content": "[](https://docs.firecrawl.dev/v0/advanced-scraping-guide#crawling-multiple-pages) Crawling Multiple Pages > [](https://docs.firecrawl.dev/v0/advanced-scraping-guide#crawler-options) Crawler Options > [](https://docs.firecrawl.dev/v0/advanced-scraping-guide#mode-2) `mode`\n\n- **Type**: `string`\n- **Enum**: `[\"default\", \"fast\"]`\n- **Description**: The crawling mode to use. `fast` mode crawls websites without a sitemap 4x faster but may be less accurate and is not recommended for heavily JavaScript-rendered websites.\n- **Default**: `default`",
      "metadata": {
        "title": "Advanced Scraping Guide | Firecrawl",
        "url": "https://docs.firecrawl.dev/v0/advanced-scraping-guide"
      }
    },
    {
      "id": "8b7e5dfc-fb9f-4be1-8268-27d243330e5a",
      "source": "firecrawl/docs/v0-advanced-scraping-guide.md",
      "content": "[](https://docs.firecrawl.dev/v0/advanced-scraping-guide#crawling-multiple-pages) Crawling Multiple Pages > [](https://docs.firecrawl.dev/v0/advanced-scraping-guide#crawler-options) Crawler Options > [](https://docs.firecrawl.dev/v0/advanced-scraping-guide#limit) `limit`\n\n- **Type**: `integer`\n- **Description**: Maximum number of pages to crawl.\n- **Default**: `10000`",
      "metadata": {
        "title": "Advanced Scraping Guide | Firecrawl",
        "url": "https://docs.firecrawl.dev/v0/advanced-scraping-guide"
      }
    },
    {
      "id": "1db14940-424d-46c0-a66e-5c03aaf98003",
      "source": "firecrawl/docs/v0-advanced-scraping-guide.md",
      "content": "[](https://docs.firecrawl.dev/v0/advanced-scraping-guide#crawling-multiple-pages) Crawling Multiple Pages > [](https://docs.firecrawl.dev/v0/advanced-scraping-guide#example-usage-4) Example Usage\n\nCopy  \n```bash\ncurl -X POST https://api.firecrawl.dev/v0/crawl \\\n-H 'Content-Type: application/json' \\\n-H 'Authorization : Bearer YOUR_API_KEY' \\\n-d '{\n\"url\": \"https://docs.firecrawl.dev\",\n\"crawlerOptions\": {\n\"includes\": [\"/blog/*\", \"/products/*\"],\n\"excludes\": [\"/admin/*\", \"/login/*\"],\n\"returnOnlyUrls\": false,\n\"maxDepth\": 2,\n\"mode\": \"fast\",\n\"limit\": 1000\n}\n}'\n\n```  \nIn this example, the crawler will:  \n- Only crawl URLs that match the patterns `/blog/*` and `/products/*`.\n- Skip URLs that match the patterns `/admin/*` and `/login/*`.\n- Return the full document data for each page.\n- Crawl up to a maximum depth of 2.\n- Use the fast crawling mode.\n- Crawl a maximum of 1000 pages.",
      "metadata": {
        "title": "Advanced Scraping Guide | Firecrawl",
        "url": "https://docs.firecrawl.dev/v0/advanced-scraping-guide"
      }
    },
    {
      "id": "8bff0bf5-d160-4533-9399-0bf9017e47c0",
      "source": "firecrawl/docs/v0-advanced-scraping-guide.md",
      "content": "[](https://docs.firecrawl.dev/v0/advanced-scraping-guide#page-options-crawler-options) Page Options + Crawler Options\n\nYou can combine the `pageOptions` and `crawlerOptions` parameters to customize both the full crawling behavior.",
      "metadata": {
        "title": "Advanced Scraping Guide | Firecrawl",
        "url": "https://docs.firecrawl.dev/v0/advanced-scraping-guide"
      }
    },
    {
      "id": "333f07e8-119e-44da-9121-f4be344c423b",
      "source": "firecrawl/docs/v0-advanced-scraping-guide.md",
      "content": "[](https://docs.firecrawl.dev/v0/advanced-scraping-guide#page-options-crawler-options) Page Options + Crawler Options > [](https://docs.firecrawl.dev/v0/advanced-scraping-guide#example-usage-5) Example Usage\n\nCopy  \n```bash\ncurl -X POST https://api.firecrawl.dev/v0/crawl \\\n-H 'Content-Type: application/json' \\\n-H 'Authorization: Bearer YOUR_API_KEY' \\\n-d '{\n\"url\": \"https://docs.firecrawl.dev\",\n\"pageOptions\": {\n\"onlyMainContent\": true,\n\"includeHtml\": true,\n\"includeRawHtml\": true,\n\"screenshot\": true,\n\"waitFor\": 5000\n},\n\"crawlerOptions\": {\n\"includes\": [\"/blog/*\", \"/products/*\"],\n\"maxDepth\": 2,\n\"mode\": \"fast\",\n}\n}'\n\n```  \nIn this example, the crawler will:  \n- Return only the main content for each page.\n- Include the raw HTML content for each page.\n- Wait for 5000 milliseconds for each page to load before fetching its content.\n- Only crawl URLs that match the patterns `/blog/*` and `/products/*`.\n- Crawl up to a maximum depth of 2.\n- Use the fast crawling mode.",
      "metadata": {
        "title": "Advanced Scraping Guide | Firecrawl",
        "url": "https://docs.firecrawl.dev/v0/advanced-scraping-guide"
      }
    },
    {
      "id": "df7c718a-c911-4a40-8802-9184cb466891",
      "source": "firecrawl/docs/v0-advanced-scraping-guide.md",
      "content": "[](https://docs.firecrawl.dev/v0/advanced-scraping-guide#extractor-options-crawler-options) Extractor Options + Crawler Options\n\nComing soonâ€¦  \n[Suggest edits](https://github.com/hellofirecrawl/docs/edit/main/v0/advanced-scraping-guide.mdx) [Raise issue](https://github.com/hellofirecrawl/docs/issues/new?title=Issue%20on%20docs&body=Path:%20/v0/advanced-scraping-guide)  \n[Quickstart](https://docs.firecrawl.dev/v0/introduction) [Rate Limits](https://docs.firecrawl.dev/rate-limits)  \nOn this page  \n- [Basic scraping with Firecrawl (/scrape)](https://docs.firecrawl.dev/v0/advanced-scraping-guide#basic-scraping-with-firecrawl-scrape)\n- [Scraping pdfs](https://docs.firecrawl.dev/v0/advanced-scraping-guide#scraping-pdfs)\n- [Page Options](https://docs.firecrawl.dev/v0/advanced-scraping-guide#page-options)\n- [Getting cleaner content with onlyMainContent](https://docs.firecrawl.dev/v0/advanced-scraping-guide#getting-cleaner-content-with-onlymaincontent)\n- [Getting the HTML with includeHtml](https://docs.firecrawl.dev/v0/advanced-scraping-guide#getting-the-html-with-includehtml)\n- [Getting the raw HTML with includeRawHtml](https://docs.firecrawl.dev/v0/advanced-scraping-guide#getting-the-raw-html-with-includerawhtml)\n- [Getting a screenshot of the page with screenshot](https://docs.firecrawl.dev/v0/advanced-scraping-guide#getting-a-screenshot-of-the-page-with-screenshot)\n- [Waiting for the page to load with waitFor](https://docs.firecrawl.dev/v0/advanced-scraping-guide#waiting-for-the-page-to-load-with-waitfor)\n- [Example Usage](https://docs.firecrawl.dev/v0/advanced-scraping-guide#example-usage)\n- [Extractor Options](https://docs.firecrawl.dev/v0/advanced-scraping-guide#extractor-options)\n- [mode](https://docs.firecrawl.dev/v0/advanced-scraping-guide#mode)\n- [extractionSchema](https://docs.firecrawl.dev/v0/advanced-scraping-guide#extractionschema)\n- [Example Usage](https://docs.firecrawl.dev/v0/advanced-scraping-guide#example-usage-2)\n- [Adjusting Timeout](https://docs.firecrawl.dev/v0/advanced-scraping-guide#adjusting-timeout)\n- [Example Usage](https://docs.firecrawl.dev/v0/advanced-scraping-guide#example-usage-3)\n- [Crawling Multiple Pages](https://docs.firecrawl.dev/v0/advanced-scraping-guide#crawling-multiple-pages)\n- [Check Crawl Job](https://docs.firecrawl.dev/v0/advanced-scraping-guide#check-crawl-job)\n- [Crawler Options](https://docs.firecrawl.dev/v0/advanced-scraping-guide#crawler-options)\n- [includes](https://docs.firecrawl.dev/v0/advanced-scraping-guide#includes)\n- [excludes](https://docs.firecrawl.dev/v0/advanced-scraping-guide#excludes)\n- [returnOnlyUrls](https://docs.firecrawl.dev/v0/advanced-scraping-guide#returnonlyurls)\n- [maxDepth](https://docs.firecrawl.dev/v0/advanced-scraping-guide#maxdepth)\n- [mode](https://docs.firecrawl.dev/v0/advanced-scraping-guide#mode-2)\n- [limit](https://docs.firecrawl.dev/v0/advanced-scraping-guide#limit)\n- [Example Usage](https://docs.firecrawl.dev/v0/advanced-scraping-guide#example-usage-4)\n- [Page Options + Crawler Options](https://docs.firecrawl.dev/v0/advanced-scraping-guide#page-options-crawler-options)\n- [Example Usage](https://docs.firecrawl.dev/v0/advanced-scraping-guide#example-usage-5)\n- [Extractor Options + Crawler Options](https://docs.firecrawl.dev/v0/advanced-scraping-guide#extractor-options-crawler-options)",
      "metadata": {
        "title": "Advanced Scraping Guide | Firecrawl",
        "url": "https://docs.firecrawl.dev/v0/advanced-scraping-guide"
      }
    },
    {
      "id": "fb47e834-68d8-41db-b2be-eeb884e0924e",
      "source": "firecrawl/docs/features-extract.md",
      "content": "---\ntitle: Extract | Firecrawl\nurl: https://docs.firecrawl.dev/features/extract\n---  \n[Firecrawl Docs home page![light logo](https://mintlify.s3.us-west-1.amazonaws.com/firecrawl/logo/light.svg)![dark logo](https://mintlify.s3.us-west-1.amazonaws.com/firecrawl/logo/dark.svg)](https://firecrawl.dev/)  \nv1  \nSearch or ask...  \nCtrl K  \nSearch...  \nNavigation  \nFeatures  \nExtract  \n[Documentation](https://docs.firecrawl.dev/introduction) [SDKs](https://docs.firecrawl.dev/sdks/overview) [Learn](https://www.firecrawl.dev/blog/category/tutorials) [API Reference](https://docs.firecrawl.dev/api-reference/introduction)",
      "metadata": {
        "title": "Extract | Firecrawl",
        "url": "https://docs.firecrawl.dev/features/extract"
      }
    },
    {
      "id": "9a4c7395-4231-4146-bc46-67466c0f9e7b",
      "source": "firecrawl/docs/features-extract.md",
      "content": "[](https://docs.firecrawl.dev/features/extract#introducing-extract-open-beta) Introducing `/extract` (Open Beta)\n\nThe `/extract` endpoint simplifies collecting structured data from any number of URLs or entire domains. Provide a list of URLs, optionally with wildcards (e.g., `example.com/*`), and a prompt or schema describing the information you want. Firecrawl handles the details of crawling, parsing, and collating large or small datasets.",
      "metadata": {
        "title": "Extract | Firecrawl",
        "url": "https://docs.firecrawl.dev/features/extract"
      }
    },
    {
      "id": "52a1a287-4908-4328-be6d-6ffd81753228",
      "source": "firecrawl/docs/features-extract.md",
      "content": "[](https://docs.firecrawl.dev/features/extract#using-extract) Using `/extract`\n\nYou can extract structured data from one or multiple URLs, including wildcards:  \n- **Single Page**  \nExample: `https://firecrawl.dev/some-page`\n- **Multiple Pages / Full Domain**  \nExample: `https://firecrawl.dev/*`  \nWhen you use `/*`, Firecrawl will automatically crawl and parse all URLs it can discover in that domain, then extract the requested data. This feature is experimental; email [help@firecrawl.dev](mailto:help@firecrawl.dev) if you have issues.",
      "metadata": {
        "title": "Extract | Firecrawl",
        "url": "https://docs.firecrawl.dev/features/extract"
      }
    },
    {
      "id": "759fbd6c-6f5b-4fb7-b431-ea6432444c33",
      "source": "firecrawl/docs/features-extract.md",
      "content": "[](https://docs.firecrawl.dev/features/extract#using-extract) Using `/extract` > [](https://docs.firecrawl.dev/features/extract#example-usage) Example Usage\n\nPython  \nNode  \ncURL  \nCopy  \n```python\nfrom firecrawl import FirecrawlApp\nfrom pydantic import BaseModel, Field\n\n# Initialize the FirecrawlApp with your API key\napp = FirecrawlApp(api_key='your_api_key')\n\nclass ExtractSchema(BaseModel):\ncompany_mission: str\nsupports_sso: bool\nis_open_source: bool\nis_in_yc: bool\n\ndata = app.extract([\\\n'https://docs.firecrawl.dev/*',\\\n'https://firecrawl.dev/',\\\n'https://www.ycombinator.com/companies/'\\\n], {\n'prompt': 'Extract the company mission, whether it supports SSO, whether it is open source, and whether it is in Y Combinator from the page.',\n'schema': ExtractSchema.model_json_schema(),\n})\nprint(data)\n\n```  \n**Key Parameters:**  \n- **urls**: An array of one or more URLs. Supports wildcards ( `/*`) for broader crawling.\n- **prompt** (Optional unless no schema): A natural language prompt describing the data you want or specifying how you want that data structured.\n- **schema** (Optional unless no prompt): A more rigid structure if you already know the JSON layout.\n- **enableWebSearch** (Optional): When `true`, extraction can follow links outside the specified domain.  \nSee [API Reference](https://docs.firecrawl.dev/api-reference/endpoint/extract) for more details.",
      "metadata": {
        "title": "Extract | Firecrawl",
        "url": "https://docs.firecrawl.dev/features/extract"
      }
    },
    {
      "id": "8515e0a9-3c41-4582-af0b-0a7033411332",
      "source": "firecrawl/docs/features-extract.md",
      "content": "[](https://docs.firecrawl.dev/features/extract#using-extract) Using `/extract` > [](https://docs.firecrawl.dev/features/extract#response-sdks) Response (sdks)\n\nJSON  \nCopy  \n```json\n{\n\"success\": true,\n\"data\": {\n\"company_mission\": \"Firecrawl is the easiest way to extract data from the web. Developers use us to reliably convert URLs into LLM-ready markdown or structured data with a single API call.\",\n\"supports_sso\": false,\n\"is_open_source\": true,\n\"is_in_yc\": true\n}\n}\n\n```",
      "metadata": {
        "title": "Extract | Firecrawl",
        "url": "https://docs.firecrawl.dev/features/extract"
      }
    },
    {
      "id": "f5c32c6b-2ae4-4cd6-9da3-2b87cbf6d7cc",
      "source": "firecrawl/docs/features-extract.md",
      "content": "[](https://docs.firecrawl.dev/features/extract#asynchronous-extraction-and-status-checking) Asynchronous Extraction & Status Checking\n\nWhen you submit an extraction jobâ€”either directly via the API or through the SDKâ€™s asynchronous methodsâ€”youâ€™ll receive a Job ID. You can use this ID to:  \n- Check Job Status: Send a request to the /extract/ endpoint to see if the job is still running or has finished.\n- Automatically Poll (Default SDK Behavior): If you use the default extract method (Python/Node), the SDK automatically polls this endpoint for you and returns the final results once the job completes.\n- Manually Poll (Async SDK Methods): If you use the asynchronous methodsâ€”async_extract (Python) or asyncExtract (Node)â€”the SDK immediately returns a Job ID that you can track. Use get_extract_status (Python) or getExtractStatus (Node) to check the jobâ€™s progress on your own schedule.  \nThis endpoint only works for jobs in progress or recently completed (within 24\nhours).  \nBelow are code examples for checking an extraction jobâ€™s status using Python, Node.js, and cURL:  \nPython  \nNode  \ncURL  \nCopy  \n```python\nfrom firecrawl import FirecrawlApp\n\napp = FirecrawlApp(\napi_key=\"fc-YOUR_API_KEY\"\n)\n\n# Start an extraction job first\nextract_job = app.async_extract([\\\n'https://docs.firecrawl.dev/*',\\\n'https://firecrawl.dev/'\\\n], prompt=\"Extract the company mission and features from these pages.\")\n\n# Get the status of the extraction job\njob_status = app.get_extract_status(extract_job.job_id)\n\nprint(job_status)\n# Example output:\n# {\n# \"status\": \"completed\",\n# \"progress\": 100,\n# \"results\": [{\\\n# \"url\": \"https://docs.firecrawl.dev\",\\\n# \"data\": { ... }\\\n# }]\n# }\n\n```",
      "metadata": {
        "title": "Extract | Firecrawl",
        "url": "https://docs.firecrawl.dev/features/extract"
      }
    },
    {
      "id": "c41a1777-8f09-436c-a15b-6f717ff64962",
      "source": "firecrawl/docs/features-extract.md",
      "content": "[](https://docs.firecrawl.dev/features/extract#asynchronous-extraction-and-status-checking) Asynchronous Extraction & Status Checking > [](https://docs.firecrawl.dev/features/extract#possible-states) Possible States\n\n- **completed**: The extraction finished successfully.\n- **pending**: Firecrawl is still processing your request.\n- **failed**: An error occurred; data was not fully extracted.\n- **cancelled**: The job was cancelled by the user.",
      "metadata": {
        "title": "Extract | Firecrawl",
        "url": "https://docs.firecrawl.dev/features/extract"
      }
    },
    {
      "id": "97792b07-2c4b-4bde-ae5b-2ffb39eec964",
      "source": "firecrawl/docs/features-extract.md",
      "content": "[](https://docs.firecrawl.dev/features/extract#asynchronous-extraction-and-status-checking) Asynchronous Extraction & Status Checking > [](https://docs.firecrawl.dev/features/extract#possible-states) Possible States > [](https://docs.firecrawl.dev/features/extract#pending-example) Pending Example\n\nJSON  \nCopy  \n```json\n{\n\"success\": true,\n\"data\": [],\n\"status\": \"processing\",\n\"expiresAt\": \"2025-01-08T20:58:12.000Z\"\n}\n\n```",
      "metadata": {
        "title": "Extract | Firecrawl",
        "url": "https://docs.firecrawl.dev/features/extract"
      }
    },
    {
      "id": "b0a2fca4-b15d-4838-8c46-ec1e177f9a01",
      "source": "firecrawl/docs/features-extract.md",
      "content": "[](https://docs.firecrawl.dev/features/extract#asynchronous-extraction-and-status-checking) Asynchronous Extraction & Status Checking > [](https://docs.firecrawl.dev/features/extract#possible-states) Possible States > [](https://docs.firecrawl.dev/features/extract#completed-example) Completed Example\n\nJSON  \nCopy  \n```json\n{\n\"success\": true,\n\"data\": {\n\"company_mission\": \"Firecrawl is the easiest way to extract data from the web. Developers use us to reliably convert URLs into LLM-ready markdown or structured data with a single API call.\",\n\"supports_sso\": false,\n\"is_open_source\": true,\n\"is_in_yc\": true\n},\n\"status\": \"completed\",\n\"expiresAt\": \"2025-01-08T20:58:12.000Z\"\n}\n\n```",
      "metadata": {
        "title": "Extract | Firecrawl",
        "url": "https://docs.firecrawl.dev/features/extract"
      }
    },
    {
      "id": "0a095fce-d54b-4045-b925-1cb42e11230d",
      "source": "firecrawl/docs/features-extract.md",
      "content": "[](https://docs.firecrawl.dev/features/extract#extracting-without-a-schema) Extracting without a Schema\n\nIf you prefer not to define a strict structure, you can simply provide a `prompt`. The underlying model will choose a structure for you, which can be useful for more exploratory or flexible requests.  \nPython  \nNode  \ncURL  \nCopy  \n```python\nfrom firecrawl import FirecrawlApp\n\n# Initialize the FirecrawlApp with your API key\napp = FirecrawlApp(api_key='your_api_key')\n\ndata = app.extract([\\\n'https://docs.firecrawl.dev/',\\\n'https://firecrawl.dev/'\\\n], {\n'prompt': \"Extract Firecrawl's mission from the page.\"\n})\nprint(data)\n\n```  \nJSON  \nCopy  \n```json\n{\n\"success\": true,\n\"data\": {\n\"company_mission\": \"Turn websites into LLM-ready data. Power your AI apps with clean data crawled from any website.\"\n}\n}\n\n```",
      "metadata": {
        "title": "Extract | Firecrawl",
        "url": "https://docs.firecrawl.dev/features/extract"
      }
    },
    {
      "id": "58fa6ca7-02f1-4d0c-b91e-ce80f89573c4",
      "source": "firecrawl/docs/features-extract.md",
      "content": "[](https://docs.firecrawl.dev/features/extract#improving-results-with-web-search) Improving Results with Web Search\n\nSetting `enableWebSearch = true` in your request will expand the crawl beyond the provided URL set. This can capture supporting or related information from linked pages.  \nHereâ€™s an example that extracts information about dash cams, enriching the results with data from related pages:  \nPython  \nNode  \ncURL  \nCopy  \n```python\nfrom firecrawl import FirecrawlApp\n\n# Initialize the FirecrawlApp with your API key\n\napp = FirecrawlApp(api_key='your_api_key')\n\ndata = app.extract([\\\n'https://nextbase.com/dash-cams/622gw-dash-cam'\\\n], {\n'prompt': \"Extract details about the best dash cams including prices, features, pros/cons and reviews.\",\n'enableWebSearch': True # Enable web search for better context\n})\nprint(data)\n\n```",
      "metadata": {
        "title": "Extract | Firecrawl",
        "url": "https://docs.firecrawl.dev/features/extract"
      }
    },
    {
      "id": "b7e11512-c83d-471a-9260-56c40fedaaaa",
      "source": "firecrawl/docs/features-extract.md",
      "content": "[](https://docs.firecrawl.dev/features/extract#improving-results-with-web-search) Improving Results with Web Search > [](https://docs.firecrawl.dev/features/extract#example-response-with-web-search) Example Response with Web Search\n\nJSON  \nCopy  \n```json\n{\n\"success\": true,\n\"data\": {\n\"dash_cams\": [\\\n{\\\n\"name\": \"Nextbase 622GW\",\\\n\"price\": \"$399.99\",\\\n\"features\": [\\\n\"4K video recording\",\\\n\"Image stabilization\",\\\n\"Alexa built-in\",\\\n\"What3Words integration\"\\\n],\\\n/* Information below enriched with other websites like\\\nhttps://www.techradar.com/best/best-dash-cam found\\\nvia enableWebSearch parameter */\\\n\"pros\": [\\\n\"Excellent video quality\",\\\n\"Great night vision\",\\\n\"Built-in GPS\"\\\n],\\\n\"cons\": [\"Premium price point\", \"App can be finicky\"]\\\n}\\\n],\n}\n\n```  \nThe response includes additional context gathered from related pages, providing more comprehensive and accurate information.",
      "metadata": {
        "title": "Extract | Firecrawl",
        "url": "https://docs.firecrawl.dev/features/extract"
      }
    },
    {
      "id": "68f0cb85-1911-40b2-b455-f7f190fff2be",
      "source": "firecrawl/docs/features-extract.md",
      "content": "[](https://docs.firecrawl.dev/features/extract#known-limitations-beta) Known Limitations (Beta)\n\n1. **Large-Scale Site Coverage**  \nFull coverage of massive sites (e.g., â€œall products on Amazonâ€) in a single request is not yet supported.  \n2. **Complex Logical Queries**  \nRequests like â€œfind every post from 2025â€ may not reliably return all expected data. More advanced query capabilities are in progress.  \n3. **Occasional Inconsistencies**  \nResults might differ across runs, particularly for very large or dynamic sites. Usually it captures core details, but some variation is possible.  \n4. **Beta State**  \nSince `/extract` is still in Beta, features and performance will continue to evolve. We welcome bug reports and feedback to help us improve.",
      "metadata": {
        "title": "Extract | Firecrawl",
        "url": "https://docs.firecrawl.dev/features/extract"
      }
    },
    {
      "id": "114d6c37-4338-4c59-8ae1-d1a64ca65d8c",
      "source": "firecrawl/docs/features-extract.md",
      "content": "[](https://docs.firecrawl.dev/features/extract#billing-and-usage-tracking) Billing and Usage Tracking\n\nYou can check our the pricing for /extract on the [Extract landing page pricing page](https://www.firecrawl.dev/extract#pricing) and monitor usage via the [Extract page on the dashboard](https://www.firecrawl.dev/app/extract).  \nHave feedback or need help? Email [help@firecrawl.dev](mailto:help@firecrawl.dev).  \n[Suggest edits](https://github.com/hellofirecrawl/docs/edit/main/features/extract.mdx) [Raise issue](https://github.com/hellofirecrawl/docs/issues/new?title=Issue%20on%20docs&body=Path:%20/features/extract)  \n[Map](https://docs.firecrawl.dev/features/map) [Langchain](https://docs.firecrawl.dev/integrations/langchain)  \nOn this page  \n- [Introducing /extract (Open Beta)](https://docs.firecrawl.dev/features/extract#introducing-extract-open-beta)\n- [Using /extract](https://docs.firecrawl.dev/features/extract#using-extract)\n- [Example Usage](https://docs.firecrawl.dev/features/extract#example-usage)\n- [Response (sdks)](https://docs.firecrawl.dev/features/extract#response-sdks)\n- [Asynchronous Extraction & Status Checking](https://docs.firecrawl.dev/features/extract#asynchronous-extraction-and-status-checking)\n- [Possible States](https://docs.firecrawl.dev/features/extract#possible-states)\n- [Pending Example](https://docs.firecrawl.dev/features/extract#pending-example)\n- [Completed Example](https://docs.firecrawl.dev/features/extract#completed-example)\n- [Extracting without a Schema](https://docs.firecrawl.dev/features/extract#extracting-without-a-schema)\n- [Improving Results with Web Search](https://docs.firecrawl.dev/features/extract#improving-results-with-web-search)\n- [Example Response with Web Search](https://docs.firecrawl.dev/features/extract#example-response-with-web-search)\n- [Known Limitations (Beta)](https://docs.firecrawl.dev/features/extract#known-limitations-beta)\n- [Billing and Usage Tracking](https://docs.firecrawl.dev/features/extract#billing-and-usage-tracking)",
      "metadata": {
        "title": "Extract | Firecrawl",
        "url": "https://docs.firecrawl.dev/features/extract"
      }
    },
    {
      "id": "9f8d527e-a905-4185-a720-579a2de6d46d",
      "source": "firecrawl/docs/v0-api-reference-endpoint-status.md",
      "content": "---\ntitle: Firecrawl Docs\nurl: https://docs.firecrawl.dev/v0/api-reference/endpoint/status\n---  \n[Firecrawl Docs home page![light logo](https://mintlify.s3.us-west-1.amazonaws.com/firecrawl/logo/light.svg)![dark logo](https://mintlify.s3.us-west-1.amazonaws.com/firecrawl/logo/dark.svg)](https://firecrawl.dev/)  \nv0  \nSearch or ask...  \nCtrl K  \nSearch...  \nNavigation  \nEndpoints  \nGet Crawl Status  \n[Documentation](https://docs.firecrawl.dev/v0/introduction) [SDKs](https://docs.firecrawl.dev/v0/sdks/python) [Learn](https://www.firecrawl.dev/blog/category/tutorials) [API Reference](https://docs.firecrawl.dev/v0/api-reference/introduction)  \nGET  \n/  \ncrawl  \n/  \nstatus  \n/  \n{jobId}  \nTry it  \ncURL  \nPython  \nJavaScript  \nPHP  \nGo  \nJava  \nCopy  \n```\ncurl --request GET \\\n--url https://api.firecrawl.dev/v0/crawl/status/{jobId} \\\n--header 'Authorization: Bearer <token>'\n```  \n200  \n402  \n429  \n500  \nCopy  \n```\n{\n\"status\": \"<string>\",\n\"current\": 123,\n\"total\": 123,\n\"data\": [\\\n{\\\n\"markdown\": \"<string>\",\\\n\"content\": \"<string>\",\\\n\"html\": \"<string>\",\\\n\"rawHtml\": \"<string>\",\\\n\"index\": 123,\\\n\"metadata\": {\\\n\"title\": \"<string>\",\\\n\"description\": \"<string>\",\\\n\"language\": \"<string>\",\\\n\"sourceURL\": \"<string>\",\\\n\"<any other metadata> \": \"<string>\",\\\n\"pageStatusCode\": 123,\\\n\"pageError\": \"<string>\"\\\n}\\\n}\\\n],\n\"partial_data\": [\\\n{\\\n\"markdown\": \"<string>\",\\\n\"content\": \"<string>\",\\\n\"html\": \"<string>\",\\\n\"rawHtml\": \"<string>\",\\\n\"index\": 123,\\\n\"metadata\": {\\\n\"title\": \"<string>\",\\\n\"description\": \"<string>\",\\\n\"language\": \"<string>\",\\\n\"sourceURL\": \"<string>\",\\\n\"<any other metadata> \": \"<string>\",\\\n\"pageStatusCode\": 123,\\\n\"pageError\": \"<string>\"\\\n}\\\n}\\\n]\n}\n```  \nThis endpoint retrieves the status of a crawl job. If the job is not completed, the response includes content within `partial_data`. Once the job is completed, the content is available under `data`.  \n**We recommend keeping track of the crawl jobs yourself as the crawl status results can expire after 24 hours.**",
      "metadata": {
        "title": "Firecrawl Docs",
        "url": "https://docs.firecrawl.dev/v0/api-reference/endpoint/status"
      }
    },
    {
      "id": "34e4fa8d-0de1-4f70-8381-1e592e2715b4",
      "source": "firecrawl/docs/v0-api-reference-endpoint-status.md",
      "content": "Authorizations\n\n[](https://docs.firecrawl.dev/v0/api-reference/endpoint/status#authorization-authorization)  \nAuthorization  \nstring  \nheader  \nrequired  \nBearer authentication header of the form `Bearer <token>`, where `<token>` is your auth token.",
      "metadata": {
        "title": "Firecrawl Docs",
        "url": "https://docs.firecrawl.dev/v0/api-reference/endpoint/status"
      }
    },
    {
      "id": "2c840961-ab1b-4a5f-9e32-fccb27565f28",
      "source": "firecrawl/docs/v0-api-reference-endpoint-status.md",
      "content": "Path Parameters\n\n[](https://docs.firecrawl.dev/v0/api-reference/endpoint/status#parameter-job-id)  \njobId  \nstring  \nrequired  \nID of the crawl job",
      "metadata": {
        "title": "Firecrawl Docs",
        "url": "https://docs.firecrawl.dev/v0/api-reference/endpoint/status"
      }
    },
    {
      "id": "34dfad5e-03e6-4b39-bf19-8bd944a46b8d",
      "source": "firecrawl/docs/v0-api-reference-endpoint-status.md",
      "content": "Response\n\n200 - application/json  \n[](https://docs.firecrawl.dev/v0/api-reference/endpoint/status#response-status)  \nstatus  \nstring  \nStatus of the job (completed, active, failed, paused)  \n[](https://docs.firecrawl.dev/v0/api-reference/endpoint/status#response-current)  \ncurrent  \ninteger  \nCurrent page number  \n[](https://docs.firecrawl.dev/v0/api-reference/endpoint/status#response-total)  \ntotal  \ninteger  \nTotal number of pages  \n[](https://docs.firecrawl.dev/v0/api-reference/endpoint/status#response-data)  \ndata  \nobject[]  \nData returned from the job (null when it is in progress)  \nShowchild attributes  \n[](https://docs.firecrawl.dev/v0/api-reference/endpoint/status#response-data-markdown)  \ndata.markdown  \nstring  \n[](https://docs.firecrawl.dev/v0/api-reference/endpoint/status#response-data-content)  \ndata.content  \nstring  \n[](https://docs.firecrawl.dev/v0/api-reference/endpoint/status#response-data-html)  \ndata.html  \nstring | null  \nHTML version of the content on page if `includeHtml` is true  \n[](https://docs.firecrawl.dev/v0/api-reference/endpoint/status#response-data-raw-html)  \ndata.rawHtml  \nstring | null  \nRaw HTML content of the page if `includeRawHtml` is true  \n[](https://docs.firecrawl.dev/v0/api-reference/endpoint/status#response-data-index)  \ndata.index  \ninteger  \nThe number of the page that was crawled. This is useful for `partial_data` so you know which page the data is from.  \n[](https://docs.firecrawl.dev/v0/api-reference/endpoint/status#response-data-metadata)  \ndata.metadata  \nobject  \nShowchild attributes  \n[](https://docs.firecrawl.dev/v0/api-reference/endpoint/status#response-data-metadata-title)  \ndata.metadata.title  \nstring  \n[](https://docs.firecrawl.dev/v0/api-reference/endpoint/status#response-data-metadata-description)  \ndata.metadata.description  \nstring  \n[](https://docs.firecrawl.dev/v0/api-reference/endpoint/status#response-data-metadata-language)  \ndata.metadata.language  \nstring | null  \n[](https://docs.firecrawl.dev/v0/api-reference/endpoint/status#response-data-metadata-source-url)  \ndata.metadata.sourceURL  \nstring  \n[](https://docs.firecrawl.dev/v0/api-reference/endpoint/status#response-data-metadata-any-other-metadata)  \ndata.metadata.<any other metadata>  \nstring  \n[](https://docs.firecrawl.dev/v0/api-reference/endpoint/status#response-data-metadata-page-status-code)  \ndata.metadata.pageStatusCode  \ninteger  \nThe status code of the page  \n[](https://docs.firecrawl.dev/v0/api-reference/endpoint/status#response-data-metadata-page-error)  \ndata.metadata.pageError  \nstring | null  \nThe error message of the page  \n[](https://docs.firecrawl.dev/v0/api-reference/endpoint/status#response-partial-data)  \npartial_data  \nobject[]  \nPartial documents returned as it is being crawled (streaming). **This feature is currently in alpha - expect breaking changes** When a page is ready, it will append to the partial_data array, so there is no need to wait for the entire website to be crawled. When the crawl is done, partial_data will become empty and the result will be available in `data`. There is a max of 50 items in the array response. The oldest item (top of the array) will be removed when the new item is added to the array.  \nShowchild attributes  \n[](https://docs.firecrawl.dev/v0/api-reference/endpoint/status#response-partial-data-markdown)  \npartial_data.markdown  \nstring  \n[](https://docs.firecrawl.dev/v0/api-reference/endpoint/status#response-partial-data-content)  \npartial_data.content  \nstring  \n[](https://docs.firecrawl.dev/v0/api-reference/endpoint/status#response-partial-data-html)  \npartial_data.html  \nstring | null  \nHTML version of the content on page if `includeHtml` is true  \n[](https://docs.firecrawl.dev/v0/api-reference/endpoint/status#response-partial-data-raw-html)  \npartial_data.rawHtml  \nstring | null  \nRaw HTML content of the page if `includeRawHtml` is true  \n[](https://docs.firecrawl.dev/v0/api-reference/endpoint/status#response-partial-data-index)  \npartial_data.index  \ninteger  \nThe number of the page that was crawled. This is useful for `partial_data` so you know which page the data is from.  \n[](https://docs.firecrawl.dev/v0/api-reference/endpoint/status#response-partial-data-metadata)  \npartial_data.metadata  \nobject  \nShowchild attributes  \n[](https://docs.firecrawl.dev/v0/api-reference/endpoint/status#response-partial-data-metadata-title)  \npartial_data.metadata.title  \nstring  \n[](https://docs.firecrawl.dev/v0/api-reference/endpoint/status#response-partial-data-metadata-description)  \npartial_data.metadata.description  \nstring  \n[](https://docs.firecrawl.dev/v0/api-reference/endpoint/status#response-partial-data-metadata-language)  \npartial_data.metadata.language  \nstring | null  \n[](https://docs.firecrawl.dev/v0/api-reference/endpoint/status#response-partial-data-metadata-source-url)  \npartial_data.metadata.sourceURL  \nstring  \n[](https://docs.firecrawl.dev/v0/api-reference/endpoint/status#response-partial-data-metadata-any-other-metadata)  \npartial_data.metadata.<any other metadata>  \nstring  \n[](https://docs.firecrawl.dev/v0/api-reference/endpoint/status#response-partial-data-metadata-page-status-code)  \npartial_data.metadata.pageStatusCode  \ninteger  \nThe status code of the page  \n[](https://docs.firecrawl.dev/v0/api-reference/endpoint/status#response-partial-data-metadata-page-error)  \npartial_data.metadata.pageError  \nstring | null  \nThe error message of the page  \n[Suggest edits](https://github.com/hellofirecrawl/docs/edit/main/v0/api-reference/endpoint/status.mdx) [Raise issue](https://github.com/hellofirecrawl/docs/issues/new?title=Issue%20on%20docs&body=Path:%20/v0/api-reference/endpoint/status)  \n[Crawl](https://docs.firecrawl.dev/v0/api-reference/endpoint/crawl) [Cancel Crawl Job](https://docs.firecrawl.dev/v0/api-reference/endpoint/crawl-cancel)  \ncURL  \nPython  \nJavaScript  \nPHP  \nGo  \nJava  \nCopy  \n```\ncurl --request GET \\\n--url https://api.firecrawl.dev/v0/crawl/status/{jobId} \\\n--header 'Authorization: Bearer <token>'\n```  \n200  \n402  \n429  \n500  \nCopy  \n```\n{\n\"status\": \"<string>\",\n\"current\": 123,\n\"total\": 123,\n\"data\": [\\\n{\\\n\"markdown\": \"<string>\",\\\n\"content\": \"<string>\",\\\n\"html\": \"<string>\",\\\n\"rawHtml\": \"<string>\",\\\n\"index\": 123,\\\n\"metadata\": {\\\n\"title\": \"<string>\",\\\n\"description\": \"<string>\",\\\n\"language\": \"<string>\",\\\n\"sourceURL\": \"<string>\",\\\n\"<any other metadata> \": \"<string>\",\\\n\"pageStatusCode\": 123,\\\n\"pageError\": \"<string>\"\\\n}\\\n}\\\n],\n\"partial_data\": [\\\n{\\\n\"markdown\": \"<string>\",\\\n\"content\": \"<string>\",\\\n\"html\": \"<string>\",\\\n\"rawHtml\": \"<string>\",\\\n\"index\": 123,\\\n\"metadata\": {\\\n\"title\": \"<string>\",\\\n\"description\": \"<string>\",\\\n\"language\": \"<string>\",\\\n\"sourceURL\": \"<string>\",\\\n\"<any other metadata> \": \"<string>\",\\\n\"pageStatusCode\": 123,\\\n\"pageError\": \"<string>\"\\\n}\\\n}\\\n]\n}\n```",
      "metadata": {
        "title": "Firecrawl Docs",
        "url": "https://docs.firecrawl.dev/v0/api-reference/endpoint/status"
      }
    },
    {
      "id": "f2aab525-b443-4a52-9d40-f1b0eb5c2f2f",
      "source": "firecrawl/docs/integrations-langflow.md",
      "content": "---\ntitle: Langflow | Firecrawl\nurl: https://docs.firecrawl.dev/integrations/langflow\n---  \n[Firecrawl Docs home page![light logo](https://mintlify.s3.us-west-1.amazonaws.com/firecrawl/logo/light.svg)![dark logo](https://mintlify.s3.us-west-1.amazonaws.com/firecrawl/logo/dark.svg)](https://firecrawl.dev/)  \nv1  \nSearch or ask...  \nCtrl K  \nSearch...  \nNavigation  \nIntegrations  \nLangflow  \n[Documentation](https://docs.firecrawl.dev/introduction) [SDKs](https://docs.firecrawl.dev/sdks/overview) [Learn](https://www.firecrawl.dev/blog/category/tutorials) [API Reference](https://docs.firecrawl.dev/api-reference/introduction)",
      "metadata": {
        "title": "Langflow | Firecrawl",
        "url": "https://docs.firecrawl.dev/integrations/langflow"
      }
    },
    {
      "id": "a6bddb4e-a2dc-49f3-8b76-48cf0e98b817",
      "source": "firecrawl/docs/integrations-langflow.md",
      "content": "[](https://docs.firecrawl.dev/integrations/langflow#sync-web-data-in-langflow-workflows) Sync web data in Langflow workflows\n\nFirecrawl can be used inside of [Langflow, the AI workflow builder](https://www.langflow.org/). This page introduces how to configure and use a Firecrawl block inside of Langflow.  \n![Firecrawl Langflow block](https://mintlify.s3.us-west-1.amazonaws.com/firecrawl/images/fc_langflow_block.png)",
      "metadata": {
        "title": "Langflow | Firecrawl",
        "url": "https://docs.firecrawl.dev/integrations/langflow"
      }
    },
    {
      "id": "f30f7fb1-9a56-4cad-ab50-875f6af0d10b",
      "source": "firecrawl/docs/integrations-langflow.md",
      "content": "[](https://docs.firecrawl.dev/integrations/langflow#sync-web-data-in-langflow-workflows) Sync web data in Langflow workflows > [](https://docs.firecrawl.dev/integrations/langflow#scraping-with-firecrawl-blocks) Scraping with Firecrawl blocks\n\n1. Log in to your Firecrawl account and get your API Key, and then enter it on the block or pass it in from another part of the workflow.\n2. (Optional) Connect Text Splitter.\n3. Select the scrape mode to pick up a single page.\n4. Input target URL to be scraped or pass it in from another part of the workflow.\n5. Set up any Page Options and Extractor Options depending on what website and data you are trying to get. You can also pass these in from another part of the workflow.\n6. Use the data in your workflows.  \n[Suggest edits](https://github.com/hellofirecrawl/docs/edit/main/integrations/langflow.mdx) [Raise issue](https://github.com/hellofirecrawl/docs/issues/new?title=Issue%20on%20docs&body=Path:%20/integrations/langflow)  \n[Flowise](https://docs.firecrawl.dev/integrations/flowise) [Camel AI](https://docs.firecrawl.dev/integrations/camelai)  \nOn this page  \n- [Sync web data in Langflow workflows](https://docs.firecrawl.dev/integrations/langflow#sync-web-data-in-langflow-workflows)\n- [Scraping with Firecrawl blocks](https://docs.firecrawl.dev/integrations/langflow#scraping-with-firecrawl-blocks)  \n![Firecrawl Langflow block](https://docs.firecrawl.dev/integrations/langflow)",
      "metadata": {
        "title": "Langflow | Firecrawl",
        "url": "https://docs.firecrawl.dev/integrations/langflow"
      }
    },
    {
      "id": "12c60eb0-4e64-456b-a734-1a7642abb5c0",
      "source": "firecrawl/docs/v0-api-reference-endpoint-scrape.md",
      "content": "---\ntitle: Firecrawl Docs\nurl: https://docs.firecrawl.dev/v0/api-reference/endpoint/scrape\n---  \n[Firecrawl Docs home page![light logo](https://mintlify.s3.us-west-1.amazonaws.com/firecrawl/logo/light.svg)![dark logo](https://mintlify.s3.us-west-1.amazonaws.com/firecrawl/logo/dark.svg)](https://firecrawl.dev/)  \nv0  \nSearch or ask...  \nCtrl K  \nSearch...  \nNavigation  \nEndpoints  \nScrape  \n[Documentation](https://docs.firecrawl.dev/v0/introduction) [SDKs](https://docs.firecrawl.dev/v0/sdks/python) [Learn](https://www.firecrawl.dev/blog/category/tutorials) [API Reference](https://docs.firecrawl.dev/v0/api-reference/introduction)  \nPOST  \n/  \nscrape  \nTry it  \ncURL  \nPython  \nJavaScript  \nPHP  \nGo  \nJava  \nCopy  \n```\ncurl --request POST \\\n--url https://api.firecrawl.dev/v0/scrape \\\n--header 'Authorization: Bearer <token>' \\\n--header 'Content-Type: application/json' \\\n--data '{\n\"url\": \"<string>\",\n\"pageOptions\": {\n\"headers\": {},\n\"includeHtml\": false,\n\"includeRawHtml\": false,\n\"onlyIncludeTags\": [\\\n\"<string>\"\\\n],\n\"onlyMainContent\": false,\n\"removeTags\": [\\\n\"<string>\"\\\n],\n\"replaceAllPathsWithAbsolutePaths\": false,\n\"screenshot\": false,\n\"fullPageScreenshot\": false,\n\"waitFor\": 0\n},\n\"extractorOptions\": {},\n\"timeout\": 30000\n}'\n```  \n200  \n402  \n429  \n500  \nCopy  \n```\n{\n\"success\": true,\n\"data\": {\n\"markdown\": \"<string>\",\n\"content\": \"<string>\",\n\"html\": \"<string>\",\n\"rawHtml\": \"<string>\",\n\"metadata\": {\n\"title\": \"<string>\",\n\"description\": \"<string>\",\n\"language\": \"<string>\",\n\"sourceURL\": \"<string>\",\n\"<any other metadata> \": \"<string>\",\n\"pageStatusCode\": 123,\n\"pageError\": \"<string>\"\n},\n\"llm_extraction\": {},\n\"warning\": \"<string>\"\n}\n}\n```",
      "metadata": {
        "title": "Firecrawl Docs",
        "url": "https://docs.firecrawl.dev/v0/api-reference/endpoint/scrape"
      }
    },
    {
      "id": "5ff518ab-1a9b-49e7-996f-0eefc872c563",
      "source": "firecrawl/docs/v0-api-reference-endpoint-scrape.md",
      "content": "Authorizations\n\n[](https://docs.firecrawl.dev/v0/api-reference/endpoint/scrape#authorization-authorization)  \nAuthorization  \nstring  \nheader  \nrequired  \nBearer authentication header of the form `Bearer <token>`, where `<token>` is your auth token.",
      "metadata": {
        "title": "Firecrawl Docs",
        "url": "https://docs.firecrawl.dev/v0/api-reference/endpoint/scrape"
      }
    },
    {
      "id": "8188b925-5f20-424c-9297-af0647d9299c",
      "source": "firecrawl/docs/v0-api-reference-endpoint-scrape.md",
      "content": "Body\n\napplication/json  \n[](https://docs.firecrawl.dev/v0/api-reference/endpoint/scrape#body-url)  \nurl  \nstring  \nrequired  \nThe URL to scrape  \n[](https://docs.firecrawl.dev/v0/api-reference/endpoint/scrape#body-page-options)  \npageOptions  \nobject  \nShowchild attributes  \n[](https://docs.firecrawl.dev/v0/api-reference/endpoint/scrape#body-page-options-headers)  \npageOptions.headers  \nobject  \nHeaders to send with the request. Can be used to send cookies, user-agent, etc.  \n[](https://docs.firecrawl.dev/v0/api-reference/endpoint/scrape#body-page-options-include-html)  \npageOptions.includeHtml  \nboolean  \ndefault:  \nfalse  \nInclude the HTML version of the content on page. Will output a html key in the response.  \n[](https://docs.firecrawl.dev/v0/api-reference/endpoint/scrape#body-page-options-include-raw-html)  \npageOptions.includeRawHtml  \nboolean  \ndefault:  \nfalse  \nInclude the raw HTML content of the page. Will output a rawHtml key in the response.  \n[](https://docs.firecrawl.dev/v0/api-reference/endpoint/scrape#body-page-options-only-include-tags)  \npageOptions.onlyIncludeTags  \nstring[]  \nOnly include tags, classes and ids from the page in the final output. Use comma separated values. Example: 'script, .ad, #footer'  \n[](https://docs.firecrawl.dev/v0/api-reference/endpoint/scrape#body-page-options-only-main-content)  \npageOptions.onlyMainContent  \nboolean  \ndefault:  \nfalse  \nOnly return the main content of the page excluding headers, navs, footers, etc.  \n[](https://docs.firecrawl.dev/v0/api-reference/endpoint/scrape#body-page-options-remove-tags)  \npageOptions.removeTags  \nstring[]  \nTags, classes and ids to remove from the page. Use comma separated values. Example: 'script, .ad, #footer'  \n[](https://docs.firecrawl.dev/v0/api-reference/endpoint/scrape#body-page-options-replace-all-paths-with-absolute-paths)  \npageOptions.replaceAllPathsWithAbsolutePaths  \nboolean  \ndefault:  \nfalse  \nReplace all relative paths with absolute paths for images and links  \n[](https://docs.firecrawl.dev/v0/api-reference/endpoint/scrape#body-page-options-screenshot)  \npageOptions.screenshot  \nboolean  \ndefault:  \nfalse  \nInclude a screenshot of the top of the page that you are scraping.  \n[](https://docs.firecrawl.dev/v0/api-reference/endpoint/scrape#body-page-options-full-page-screenshot)  \npageOptions.fullPageScreenshot  \nboolean  \ndefault:  \nfalse  \nInclude a full page screenshot of the page that you are scraping.  \n[](https://docs.firecrawl.dev/v0/api-reference/endpoint/scrape#body-page-options-wait-for)  \npageOptions.waitFor  \ninteger  \ndefault:  \n0  \nWait x amount of milliseconds for the page to load to fetch content  \n[](https://docs.firecrawl.dev/v0/api-reference/endpoint/scrape#body-extractor-options)  \nextractorOptions  \nobject  \nOptions for extraction of structured information from the page content. Note: LLM-based extraction is not performed by default and only occurs when explicitly configured. The 'markdown' mode simply returns the scraped markdown and is the default mode for scraping.  \nShowchild attributes  \n[](https://docs.firecrawl.dev/v0/api-reference/endpoint/scrape#body-extractor-options-mode)  \nextractorOptions.mode  \nenum<string>  \nThe extraction mode to use. 'markdown': Returns the scraped markdown content, does not perform LLM extraction. 'llm-extraction': Extracts information from the cleaned and parsed content using LLM. 'llm-extraction-from-raw-html': Extracts information directly from the raw HTML using LLM. 'llm-extraction-from-markdown': Extracts information from the markdown content using LLM.  \nAvailable options:  \n`markdown`,  \n`llm-extraction`,  \n`llm-extraction-from-raw-html`,  \n`llm-extraction-from-markdown`  \n[](https://docs.firecrawl.dev/v0/api-reference/endpoint/scrape#body-extractor-options-extraction-prompt)  \nextractorOptions.extractionPrompt  \nstring  \nA prompt describing what information to extract from the page, applicable for LLM extraction modes.  \n[](https://docs.firecrawl.dev/v0/api-reference/endpoint/scrape#body-extractor-options-extraction-schema)  \nextractorOptions.extractionSchema  \nobject  \nThe schema for the data to be extracted, required only for LLM extraction modes.  \n[](https://docs.firecrawl.dev/v0/api-reference/endpoint/scrape#body-timeout)  \ntimeout  \ninteger  \ndefault:  \n30000  \nTimeout in milliseconds for the request",
      "metadata": {
        "title": "Firecrawl Docs",
        "url": "https://docs.firecrawl.dev/v0/api-reference/endpoint/scrape"
      }
    },
    {
      "id": "406c287c-2298-409b-bc37-388621602b59",
      "source": "firecrawl/docs/v0-api-reference-endpoint-scrape.md",
      "content": "Response\n\n200 - application/json  \n[](https://docs.firecrawl.dev/v0/api-reference/endpoint/scrape#response-success)  \nsuccess  \nboolean  \n[](https://docs.firecrawl.dev/v0/api-reference/endpoint/scrape#response-data)  \ndata  \nobject  \nShowchild attributes  \n[](https://docs.firecrawl.dev/v0/api-reference/endpoint/scrape#response-data-markdown)  \ndata.markdown  \nstring  \n[](https://docs.firecrawl.dev/v0/api-reference/endpoint/scrape#response-data-content)  \ndata.content  \nstring  \n[](https://docs.firecrawl.dev/v0/api-reference/endpoint/scrape#response-data-html)  \ndata.html  \nstring | null  \nHTML version of the content on page if `includeHtml` is true  \n[](https://docs.firecrawl.dev/v0/api-reference/endpoint/scrape#response-data-raw-html)  \ndata.rawHtml  \nstring | null  \nRaw HTML content of the page if `includeRawHtml` is true  \n[](https://docs.firecrawl.dev/v0/api-reference/endpoint/scrape#response-data-metadata)  \ndata.metadata  \nobject  \nShowchild attributes  \n[](https://docs.firecrawl.dev/v0/api-reference/endpoint/scrape#response-data-metadata-title)  \ndata.metadata.title  \nstring  \n[](https://docs.firecrawl.dev/v0/api-reference/endpoint/scrape#response-data-metadata-description)  \ndata.metadata.description  \nstring  \n[](https://docs.firecrawl.dev/v0/api-reference/endpoint/scrape#response-data-metadata-language)  \ndata.metadata.language  \nstring | null  \n[](https://docs.firecrawl.dev/v0/api-reference/endpoint/scrape#response-data-metadata-source-url)  \ndata.metadata.sourceURL  \nstring  \n[](https://docs.firecrawl.dev/v0/api-reference/endpoint/scrape#response-data-metadata-any-other-metadata)  \ndata.metadata.<any other metadata>  \nstring  \n[](https://docs.firecrawl.dev/v0/api-reference/endpoint/scrape#response-data-metadata-page-status-code)  \ndata.metadata.pageStatusCode  \ninteger  \nThe status code of the page  \n[](https://docs.firecrawl.dev/v0/api-reference/endpoint/scrape#response-data-metadata-page-error)  \ndata.metadata.pageError  \nstring | null  \nThe error message of the page  \n[](https://docs.firecrawl.dev/v0/api-reference/endpoint/scrape#response-data-llm-extraction)  \ndata.llm_extraction  \nobject | null  \nDisplayed when using LLM Extraction. Extracted data from the page following the schema defined.  \n[](https://docs.firecrawl.dev/v0/api-reference/endpoint/scrape#response-data-warning)  \ndata.warning  \nstring | null  \nCan be displayed when using LLM Extraction. Warning message will let you know any issues with the extraction.  \n[Suggest edits](https://github.com/hellofirecrawl/docs/edit/main/v0/api-reference/endpoint/scrape.mdx) [Raise issue](https://github.com/hellofirecrawl/docs/issues/new?title=Issue%20on%20docs&body=Path:%20/v0/api-reference/endpoint/scrape)  \n[Introduction](https://docs.firecrawl.dev/v0/api-reference/introduction) [Search (Beta)](https://docs.firecrawl.dev/v0/api-reference/endpoint/search)  \ncURL  \nPython  \nJavaScript  \nPHP  \nGo  \nJava  \nCopy  \n```\ncurl --request POST \\\n--url https://api.firecrawl.dev/v0/scrape \\\n--header 'Authorization: Bearer <token>' \\\n--header 'Content-Type: application/json' \\\n--data '{\n\"url\": \"<string>\",\n\"pageOptions\": {\n\"headers\": {},\n\"includeHtml\": false,\n\"includeRawHtml\": false,\n\"onlyIncludeTags\": [\\\n\"<string>\"\\\n],\n\"onlyMainContent\": false,\n\"removeTags\": [\\\n\"<string>\"\\\n],\n\"replaceAllPathsWithAbsolutePaths\": false,\n\"screenshot\": false,\n\"fullPageScreenshot\": false,\n\"waitFor\": 0\n},\n\"extractorOptions\": {},\n\"timeout\": 30000\n}'\n```  \n200  \n402  \n429  \n500  \nCopy  \n```\n{\n\"success\": true,\n\"data\": {\n\"markdown\": \"<string>\",\n\"content\": \"<string>\",\n\"html\": \"<string>\",\n\"rawHtml\": \"<string>\",\n\"metadata\": {\n\"title\": \"<string>\",\n\"description\": \"<string>\",\n\"language\": \"<string>\",\n\"sourceURL\": \"<string>\",\n\"<any other metadata> \": \"<string>\",\n\"pageStatusCode\": 123,\n\"pageError\": \"<string>\"\n},\n\"llm_extraction\": {},\n\"warning\": \"<string>\"\n}\n}\n```",
      "metadata": {
        "title": "Firecrawl Docs",
        "url": "https://docs.firecrawl.dev/v0/api-reference/endpoint/scrape"
      }
    },
    {
      "id": "d9db094e-4fa3-4e8f-aafd-8e80b54f3707",
      "source": "firecrawl/docs/integrations-llamaindex.md",
      "content": "---\ntitle: Llamaindex | Firecrawl\nurl: https://docs.firecrawl.dev/integrations/llamaindex\n---  \n[Firecrawl Docs home page![light logo](https://mintlify.s3.us-west-1.amazonaws.com/firecrawl/logo/light.svg)![dark logo](https://mintlify.s3.us-west-1.amazonaws.com/firecrawl/logo/dark.svg)](https://firecrawl.dev/)  \nv1  \nSearch or ask...  \nCtrl K  \nSearch...  \nNavigation  \nIntegrations  \nLlamaindex  \n[Documentation](https://docs.firecrawl.dev/introduction) [SDKs](https://docs.firecrawl.dev/sdks/overview) [Learn](https://www.firecrawl.dev/blog/category/tutorials) [API Reference](https://docs.firecrawl.dev/api-reference/introduction)  \n> Note: this integration is still using [v0 version of the Firecrawl API](https://docs.firecrawl.dev/v0/introduction). You can install the 0.0.20 version for the Python SDK or the 0.0.36 for the Node SDK.",
      "metadata": {
        "title": "Llamaindex | Firecrawl",
        "url": "https://docs.firecrawl.dev/integrations/llamaindex"
      }
    },
    {
      "id": "35141a25-bf7c-4eb7-ace6-b253cdc18805",
      "source": "firecrawl/docs/integrations-llamaindex.md",
      "content": "[](https://docs.firecrawl.dev/integrations/llamaindex#installation) Installation\n\nCopy  \n```bash\npip install firecrawl-py==0.0.20 llama_index llama-index llama-index-readers-web\n\n```",
      "metadata": {
        "title": "Llamaindex | Firecrawl",
        "url": "https://docs.firecrawl.dev/integrations/llamaindex"
      }
    },
    {
      "id": "2f567642-7a2b-4091-a362-aa7885ebb422",
      "source": "firecrawl/docs/integrations-llamaindex.md",
      "content": "[](https://docs.firecrawl.dev/integrations/llamaindex#usage) Usage > [](https://docs.firecrawl.dev/integrations/llamaindex#using-firecrawl-to-gather-an-entire-website) Using FireCrawl to Gather an Entire Website\n\nCopy  \n```python\nfrom llama_index.readers.web import FireCrawlWebReader\nfrom llama_index.core import SummaryIndex\nimport os\n\n# Initialize FireCrawlWebReader to crawl a website\nfirecrawl_reader = FireCrawlWebReader(\napi_key=\"<your_api_key>\", # Replace with your actual API key from https://www.firecrawl.dev/\nmode=\"scrape\", # Choose between \"crawl\" and \"scrape\" for single page scraping\nparams={\"additional\": \"parameters\"} # Optional additional parameters\n)\n\n# Set the environment variable for the virtual key\nos.environ[\"OPENAI_API_KEY\"] = \"<OPENAI_API_KEY>\"\n\n# Load documents from a single page URL\ndocuments = firecrawl_reader.load_data(url=\"http://paulgraham.com/\")\nindex = SummaryIndex.from_documents(documents)\n\n# Set Logging to DEBUG for more detailed outputs\nquery_engine = index.as_query_engine()\nresponse = query_engine.query(\"What did the author do growing up?\")\ndisplay(Markdown(f\"<b>{response}</b>\"))\n\n```",
      "metadata": {
        "title": "Llamaindex | Firecrawl",
        "url": "https://docs.firecrawl.dev/integrations/llamaindex"
      }
    },
    {
      "id": "86e3f58c-dcd9-4544-8068-c16a0b919d99",
      "source": "firecrawl/docs/integrations-llamaindex.md",
      "content": "[](https://docs.firecrawl.dev/integrations/llamaindex#usage) Usage > [](https://docs.firecrawl.dev/integrations/llamaindex#using-firecrawl-to-gather-a-single-page) Using FireCrawl to Gather a Single Page\n\nCopy  \n```python\nfrom llama_index.readers.web import FireCrawlWebReader\n\n# Initialize the FireCrawlWebReader with your API key and desired mode\nfirecrawl_reader = FireCrawlWebReader(\napi_key=\"<your_api_key>\", # Replace with your actual API key from https://www.firecrawl.dev/\nmode=\"scrape\", # Choose between \"crawl\" and \"scrape\"\nparams={\"additional\": \"parameters\"} # Optional additional parameters\n)\n\n# Load documents from a specified URL\ndocuments = firecrawl_reader.load_data(url=\"http://paulgraham.com/worked.html\")\nindex = SummaryIndex.from_documents(documents)\n\n# Set Logging to DEBUG for more detailed outputs\nquery_engine = index.as_query_engine()\nresponse = query_engine.query(\"What did the author do growing up?\")\ndisplay(Markdown(f\"<b>{response}</b>\"))\n\n```  \n[Suggest edits](https://github.com/hellofirecrawl/docs/edit/main/integrations/llamaindex.mdx) [Raise issue](https://github.com/hellofirecrawl/docs/issues/new?title=Issue%20on%20docs&body=Path:%20/integrations/llamaindex)  \n[Langchain](https://docs.firecrawl.dev/integrations/langchain) [CrewAI](https://docs.firecrawl.dev/integrations/crewai)  \nOn this page  \n- [Installation](https://docs.firecrawl.dev/integrations/llamaindex#installation)\n- [Usage](https://docs.firecrawl.dev/integrations/llamaindex#usage)\n- [Using FireCrawl to Gather an Entire Website](https://docs.firecrawl.dev/integrations/llamaindex#using-firecrawl-to-gather-an-entire-website)\n- [Using FireCrawl to Gather a Single Page](https://docs.firecrawl.dev/integrations/llamaindex#using-firecrawl-to-gather-a-single-page)",
      "metadata": {
        "title": "Llamaindex | Firecrawl",
        "url": "https://docs.firecrawl.dev/integrations/llamaindex"
      }
    },
    {
      "id": "2606b4ea-5aa7-4dfa-80f7-70b461f76a2b",
      "source": "firecrawl/docs/features-scrape.md",
      "content": "---\ntitle: Scrape | Firecrawl\nurl: https://docs.firecrawl.dev/features/scrape\n---  \n[Firecrawl Docs home page![light logo](https://mintlify.s3.us-west-1.amazonaws.com/firecrawl/logo/light.svg)![dark logo](https://mintlify.s3.us-west-1.amazonaws.com/firecrawl/logo/dark.svg)](https://firecrawl.dev/)  \nv1  \nSearch or ask...  \nCtrl K  \nSearch...  \nNavigation  \nScrape  \nScrape  \n[Documentation](https://docs.firecrawl.dev/introduction) [SDKs](https://docs.firecrawl.dev/sdks/overview) [Learn](https://www.firecrawl.dev/blog/category/tutorials) [API Reference](https://docs.firecrawl.dev/api-reference/introduction)  \nFirecrawl converts web pages into markdown, ideal for LLM applications.  \n- It manages complexities: proxies, caching, rate limits, js-blocked content\n- Handles dynamic content: dynamic websites, js-rendered sites, PDFs, images\n- Outputs clean markdown, structured data, screenshots or html.  \nFor details, see the [Scrape Endpoint API Reference](https://docs.firecrawl.dev/api-reference/endpoint/scrape).",
      "metadata": {
        "title": "Scrape | Firecrawl",
        "url": "https://docs.firecrawl.dev/features/scrape"
      }
    },
    {
      "id": "f000a76e-fb01-4949-a7e0-ff48b0efed65",
      "source": "firecrawl/docs/features-scrape.md",
      "content": "[](https://docs.firecrawl.dev/features/scrape#scraping-a-url-with-firecrawl) Scraping a URL with Firecrawl > [](https://docs.firecrawl.dev/features/scrape#scrape-endpoint) /scrape endpoint\n\nUsed to scrape a URL and get its content.",
      "metadata": {
        "title": "Scrape | Firecrawl",
        "url": "https://docs.firecrawl.dev/features/scrape"
      }
    },
    {
      "id": "a4aaf5f0-f9c5-492b-88cd-84ebac4b7aba",
      "source": "firecrawl/docs/features-scrape.md",
      "content": "[](https://docs.firecrawl.dev/features/scrape#scraping-a-url-with-firecrawl) Scraping a URL with Firecrawl > [](https://docs.firecrawl.dev/features/scrape#installation) Installation\n\nPython  \nNode  \nGo  \nRust  \nCopy  \n```bash\npip install firecrawl-py\n\n```",
      "metadata": {
        "title": "Scrape | Firecrawl",
        "url": "https://docs.firecrawl.dev/features/scrape"
      }
    },
    {
      "id": "2902d7d1-96bc-4705-a1af-c235d5c3f712",
      "source": "firecrawl/docs/features-scrape.md",
      "content": "[](https://docs.firecrawl.dev/features/scrape#scraping-a-url-with-firecrawl) Scraping a URL with Firecrawl > [](https://docs.firecrawl.dev/features/scrape#usage) Usage\n\nPython  \nNode  \nGo  \nRust  \ncURL  \nCopy  \n```python\nfrom firecrawl import FirecrawlApp\n\napp = FirecrawlApp(api_key=\"fc-YOUR_API_KEY\")\n\n# Scrape a website:\nscrape_result = app.scrape_url('firecrawl.dev', params={'formats': ['markdown', 'html']})\nprint(scrape_result)\n\n```  \nFor more details about the parameters, refer to the [API Reference](https://docs.firecrawl.dev/api-reference/endpoint/scrape).",
      "metadata": {
        "title": "Scrape | Firecrawl",
        "url": "https://docs.firecrawl.dev/features/scrape"
      }
    },
    {
      "id": "5cb15569-e285-4335-a33a-9cdb5664aeb6",
      "source": "firecrawl/docs/features-scrape.md",
      "content": "[](https://docs.firecrawl.dev/features/scrape#scraping-a-url-with-firecrawl) Scraping a URL with Firecrawl > [](https://docs.firecrawl.dev/features/scrape#response) Response\n\nSDKs will return the data object directly. cURL will return the payload exactly as shown below.  \nCopy  \n```json\n{\n\"success\": true,\n\"data\" : {\n\"markdown\": \"Launch Week I is here! [See our Day 2 Release ðŸš€](https://www.firecrawl.dev/blog/launch-week-i-day-2-doubled-rate-limits)[ðŸ’¥ Get 2 months free...\",\\\n\"html\": \"<!DOCTYPE html><html lang=\"en\" class=\"light\" style=\"color-scheme: light;\"><body class=\"__variable_36bd41 __variable_d7dc5d font-inter ...\",\\\n\"metadata\": {\\\n\"title\": \"Home - Firecrawl\",\\\n\"description\": \"Firecrawl crawls and converts any website into clean markdown.\",\\\n\"language\": \"en\",\\\n\"keywords\": \"Firecrawl,Markdown,Data,Mendable,Langchain\",\\\n\"robots\": \"follow, index\",\\\n\"ogTitle\": \"Firecrawl\",\\\n\"ogDescription\": \"Turn any website into LLM-ready data.\",\\\n\"ogUrl\": \"https://www.firecrawl.dev/\",\\\n\"ogImage\": \"https://www.firecrawl.dev/og.png?123\",\\\n\"ogLocaleAlternate\": [],\\\n\"ogSiteName\": \"Firecrawl\",\\\n\"sourceURL\": \"https://firecrawl.dev\",\\\n\"statusCode\": 200\\\n}\\\n}\\\n}\\\n\\\n```\\\n\\",
      "metadata": {
        "title": "Scrape | Firecrawl",
        "url": "https://docs.firecrawl.dev/features/scrape"
      }
    },
    {
      "id": "ce38e063-4047-4082-808b-5bcbd25cff11",
      "source": "firecrawl/docs/features-scrape.md",
      "content": "[](https://docs.firecrawl.dev/features/scrape#scrape-formats) Scrape Formats\\\n\n\\\nYou can now choose what formats you want your output in. You can specify multiple output formats. Supported formats are:\\\n\\\n- Markdown (markdown)\\\n- HTML (html)\\\n- Raw HTML (rawHtml) (with no modifications)\\\n- Screenshot (screenshot or screenshot@fullPage)\\\n- Links (links)\\\n- Extract (extract) - structured output\\\n\\\nOutput keys will match the format you choose.\\\n\\",
      "metadata": {
        "title": "Scrape | Firecrawl",
        "url": "https://docs.firecrawl.dev/features/scrape"
      }
    },
    {
      "id": "e2b02128-0083-4f28-a4ea-d3a866f6278b",
      "source": "firecrawl/docs/features-scrape.md",
      "content": "[](https://docs.firecrawl.dev/features/scrape#extract-structured-data) Extract structured data\\\n\n\\",
      "metadata": {
        "title": "Scrape | Firecrawl",
        "url": "https://docs.firecrawl.dev/features/scrape"
      }
    },
    {
      "id": "8ad61260-c0f6-4b88-ba9f-a6473f68e05f",
      "source": "firecrawl/docs/features-scrape.md",
      "content": "[](https://docs.firecrawl.dev/features/scrape#extract-structured-data) Extract structured data\\ > [](https://docs.firecrawl.dev/features/scrape#scrape-with-extract-endpoint) /scrape (with extract) endpoint\\\n\n\\\nUsed to extract structured data from scraped pages.\\\n\\\nPython\\\n\\\nNode\\\n\\\ncURL\\\n\\\nCopy\\\n\\\n```python\\\nfrom firecrawl import FirecrawlApp\\\nfrom pydantic import BaseModel, Field\\\n\\\n# Initialize the FirecrawlApp with your API key\\\napp = FirecrawlApp(api_key='your_api_key')\\\n\\\nclass ExtractSchema(BaseModel):\\\ncompany_mission: str\\\nsupports_sso: bool\\\nis_open_source: bool\\\nis_in_yc: bool\\\n\\\ndata = app.scrape_url('https://docs.firecrawl.dev/', {\\\n'formats': ['json'],\\\n'jsonOptions': {\\\n'schema': ExtractSchema.model_json_schema(),\\\n}\\\n})\\\nprint(data[\"json\"])\\\n\\\n```\\\n\\\nOutput:\\\n\\\nJSON\\\n\\\nCopy\\\n\\\n```json\\\n{\\\n\"success\": true,\\\n\"data\": {\\\n\"json\": {\\\n\"company_mission\": \"Train a secure AI on your technical resources that answers customer and employee questions so your team doesn't have to\",\\\n\"supports_sso\": true,\\\n\"is_open_source\": false,\\\n\"is_in_yc\": true\\\n},\\\n\"metadata\": {\\\n\"title\": \"Mendable\",\\\n\"description\": \"Mendable allows you to easily build AI chat applications. Ingest, customize, then deploy with one line of code anywhere you want. Brought to you by SideGuide\",\\\n\"robots\": \"follow, index\",\\\n\"ogTitle\": \"Mendable\",\\\n\"ogDescription\": \"Mendable allows you to easily build AI chat applications. Ingest, customize, then deploy with one line of code anywhere you want. Brought to you by SideGuide\",\\\n\"ogUrl\": \"https://docs.firecrawl.dev/\",\\\n\"ogImage\": \"https://docs.firecrawl.dev/mendable_new_og1.png\",\\\n\"ogLocaleAlternate\": [],\\\n\"ogSiteName\": \"Mendable\",\\\n\"sourceURL\": \"https://docs.firecrawl.dev/\"\\\n},\\\n}\\\n}\\\n\\\n```\\\n\\",
      "metadata": {
        "title": "Scrape | Firecrawl",
        "url": "https://docs.firecrawl.dev/features/scrape"
      }
    },
    {
      "id": "e7830def-66dc-435a-9655-4b04e8f2e9d6",
      "source": "firecrawl/docs/features-scrape.md",
      "content": "[](https://docs.firecrawl.dev/features/scrape#extract-structured-data) Extract structured data\\ > [](https://docs.firecrawl.dev/features/scrape#extracting-without-schema-new) Extracting without schema (New)\\\n\n\\\nYou can now extract without a schema by just passing a `prompt` to the endpoint. The llm chooses the structure of the data.\\\n\\\ncURL\\\n\\\nCopy\\\n\\\n```bash\\\ncurl -X POST https://api.firecrawl.dev/v1/scrape \\\n-H 'Content-Type: application/json' \\\n-H 'Authorization: Bearer YOUR_API_KEY' \\\n-d '{\\\n\"url\": \"https://docs.firecrawl.dev/\",\\\n\"formats\": [\"json\"],\\\n\"jsonOptions\": {\\\n\"prompt\": \"Extract the company mission from the page.\"\\\n}\\\n}'\\\n\\\n```\\\n\\\nOutput:\\\n\\\nJSON\\\n\\\nCopy\\\n\\\n```json\\\n{\\\n\"success\": true,\\\n\"data\": {\\\n\"json\": {\\\n\"company_mission\": \"Train a secure AI on your technical resources that answers customer and employee questions so your team doesn't have to\",\\\n},\\\n\"metadata\": {\\\n\"title\": \"Mendable\",\\\n\"description\": \"Mendable allows you to easily build AI chat applications. Ingest, customize, then deploy with one line of code anywhere you want. Brought to you by SideGuide\",\\\n\"robots\": \"follow, index\",\\\n\"ogTitle\": \"Mendable\",\\\n\"ogDescription\": \"Mendable allows you to easily build AI chat applications. Ingest, customize, then deploy with one line of code anywhere you want. Brought to you by SideGuide\",\\\n\"ogUrl\": \"https://docs.firecrawl.dev/\",\\\n\"ogImage\": \"https://docs.firecrawl.dev/mendable_new_og1.png\",\\\n\"ogLocaleAlternate\": [],\\\n\"ogSiteName\": \"Mendable\",\\\n\"sourceURL\": \"https://docs.firecrawl.dev/\"\\\n},\\\n}\\\n}\\\n\\\n```\\\n\\",
      "metadata": {
        "title": "Scrape | Firecrawl",
        "url": "https://docs.firecrawl.dev/features/scrape"
      }
    },
    {
      "id": "0f149dda-13de-410a-bbe0-1d4e72b9df48",
      "source": "firecrawl/docs/features-scrape.md",
      "content": "[](https://docs.firecrawl.dev/features/scrape#extract-structured-data) Extract structured data\\ > [](https://docs.firecrawl.dev/features/scrape#extract-object) Extract object\\\n\n\\\nThe `extract` object accepts the following parameters:\\\n\\\n- `schema`: The schema to use for the extraction.\\\n- `systemPrompt`: The system prompt to use for the extraction.\\\n- `prompt`: The prompt to use for the extraction without a schema.\\\n\\",
      "metadata": {
        "title": "Scrape | Firecrawl",
        "url": "https://docs.firecrawl.dev/features/scrape"
      }
    },
    {
      "id": "53f2cbaf-1e6b-4454-a5aa-566df91b2aa1",
      "source": "firecrawl/docs/features-scrape.md",
      "content": "[](https://docs.firecrawl.dev/features/scrape#interacting-with-the-page-with-actions) Interacting with the page with Actions\\\n\n\\\nFirecrawl allows you to perform various actions on a web page before scraping its content. This is particularly useful for interacting with dynamic content, navigating through pages, or accessing content that requires user interaction.\\\n\\\nHere is an example of how to use actions to navigate to google.com, search for Firecrawl, click on the first result, and take a screenshot.\\\n\\\nIt is important to almost always use the `wait` action before/after executing other actions to give enough time for the page to load.\\\n\\",
      "metadata": {
        "title": "Scrape | Firecrawl",
        "url": "https://docs.firecrawl.dev/features/scrape"
      }
    },
    {
      "id": "369c496d-7cd6-49c0-873f-f208d67d12d9",
      "source": "firecrawl/docs/features-scrape.md",
      "content": "[](https://docs.firecrawl.dev/features/scrape#interacting-with-the-page-with-actions) Interacting with the page with Actions\\ > [](https://docs.firecrawl.dev/features/scrape#example) Example\\\n\n\\\nPython\\\n\\\nNode\\\n\\\ncURL\\\n\\\nCopy\\\n\\\n```python\\\nfrom firecrawl import FirecrawlApp\\\n\\\napp = FirecrawlApp(api_key=\"fc-YOUR_API_KEY\")\\\n\\\n# Scrape a website:\\\nscrape_result = app.scrape_url('firecrawl.dev',\\\nparams={\\\n'formats': ['markdown', 'html'],\\\n'actions': [\\\n{\"type\": \"wait\", \"milliseconds\": 2000},\\\n{\"type\": \"click\", \"selector\": \"textarea[title=\"Search\"]\"},\\\n{\"type\": \"wait\", \"milliseconds\": 2000},\\\n{\"type\": \"write\", \"text\": \"firecrawl\"},\\\n{\"type\": \"wait\", \"milliseconds\": 2000},\\\n{\"type\": \"press\", \"key\": \"ENTER\"},\\\n{\"type\": \"wait\", \"milliseconds\": 3000},\\\n{\"type\": \"click\", \"selector\": \"h3\"},\\\n{\"type\": \"wait\", \"milliseconds\": 3000},\\\n{\"type\": \"scrape\"},\\\n{\"type\": \"screenshot\"}\\\n]\\\n}\\\n)\\\nprint(scrape_result)\\\n\\\n```\\\n\\",
      "metadata": {
        "title": "Scrape | Firecrawl",
        "url": "https://docs.firecrawl.dev/features/scrape"
      }
    },
    {
      "id": "bdfe1773-ef84-4ae3-b0f9-f35ac7ba85f0",
      "source": "firecrawl/docs/features-scrape.md",
      "content": "[](https://docs.firecrawl.dev/features/scrape#interacting-with-the-page-with-actions) Interacting with the page with Actions\\ > [](https://docs.firecrawl.dev/features/scrape#output) Output\\\n\n\\\nJSON\\\n\\\nCopy\\\n\\\n```json\\\n{\\\n\"success\": true,\\\n\"data\": {\\\n\"markdown\": \"Our first Launch Week is over! [See the recap ðŸš€](blog/firecrawl-launch-week-1-recap)...\",\\\n\"actions\": {\\\n\"screenshots\": [\\\n\"https://alttmdsdujxrfnakrkyi.supabase.co/storage/v1/object/public/media/screenshot-75ef2d87-31e0-4349-a478-fb432a29e241.png\"\\\n],\\\n\"scrapes\": [\\\n{\\\n\"url\": \"https://www.firecrawl.dev/\",\\\n\"html\": \"<html><body><h1>Firecrawl</h1></body></html>\"\\\n}\\\n]\\\n},\\\n\"metadata\": {\\\n\"title\": \"Home - Firecrawl\",\\\n\"description\": \"Firecrawl crawls and converts any website into clean markdown.\",\\\n\"language\": \"en\",\\\n\"keywords\": \"Firecrawl,Markdown,Data,Mendable,Langchain\",\\\n\"robots\": \"follow, index\",\\\n\"ogTitle\": \"Firecrawl\",\\\n\"ogDescription\": \"Turn any website into LLM-ready data.\",\\\n\"ogUrl\": \"https://www.firecrawl.dev/\",\\\n\"ogImage\": \"https://www.firecrawl.dev/og.png?123\",\\\n\"ogLocaleAlternate\": [],\\\n\"ogSiteName\": \"Firecrawl\",\\\n\"sourceURL\": \"http://google.com\",\\\n\"statusCode\": 200\\\n}\\\n}\\\n}\\\n\\\n```\\\n\\\nFor more details about the actions parameters, refer to the [API Reference](https://docs.firecrawl.dev/api-reference/endpoint/scrape).\\\n\\",
      "metadata": {
        "title": "Scrape | Firecrawl",
        "url": "https://docs.firecrawl.dev/features/scrape"
      }
    },
    {
      "id": "861381cf-307e-438d-82d6-27f75e6574a3",
      "source": "firecrawl/docs/features-scrape.md",
      "content": "[](https://docs.firecrawl.dev/features/scrape#location-and-language) Location and Language\\\n\n\\\nSpecify country and preferred languages to get relevant content based on your target location and language preferences.\\\n\\",
      "metadata": {
        "title": "Scrape | Firecrawl",
        "url": "https://docs.firecrawl.dev/features/scrape"
      }
    },
    {
      "id": "c4708eac-e4d4-4bf6-bab7-d80172d51d22",
      "source": "firecrawl/docs/features-scrape.md",
      "content": "[](https://docs.firecrawl.dev/features/scrape#location-and-language) Location and Language\\ > [](https://docs.firecrawl.dev/features/scrape#how-it-works) How it works\\\n\n\\\nWhen you specify the location settings, Firecrawl will use an appropriate proxy if available and emulate the corresponding language and timezone settings. By default, the location is set to â€˜USâ€™ if not specified.\\\n\\",
      "metadata": {
        "title": "Scrape | Firecrawl",
        "url": "https://docs.firecrawl.dev/features/scrape"
      }
    },
    {
      "id": "e8f37881-cd57-4027-9033-87a2c1c813fb",
      "source": "firecrawl/docs/features-scrape.md",
      "content": "[](https://docs.firecrawl.dev/features/scrape#location-and-language) Location and Language\\ > [](https://docs.firecrawl.dev/features/scrape#usage-2) Usage\\\n\n\\\nTo use the location and language settings, include the `location` object in your request body with the following properties:\\\n\\\n- `country`: ISO 3166-1 alpha-2 country code (e.g., â€˜USâ€™, â€˜AUâ€™, â€˜DEâ€™, â€˜JPâ€™). Defaults to â€˜USâ€™.\\\n- `languages`: An array of preferred languages and locales for the request in order of priority. Defaults to the language of the specified location.\\\n\\\nPython\\\n\\\nNode\\\n\\\ncURL\\\n\\\nCopy\\\n\\\n```python\\\nfrom firecrawl import FirecrawlApp\\\n\\\napp = FirecrawlApp(api_key=\"fc-YOUR_API_KEY\")\\\n\\\n# Scrape a website:\\\nscrape_result = app.scrape_url('airbnb.com',\\\nparams={\\\n'formats': ['markdown', 'html'],\\\n'location': {\\\n'country': 'BR',\\\n'languages': ['pt-BR']\\\n}\\\n}\\\n)\\\nprint(scrape_result)\\\n\\\n```\\\n\\",
      "metadata": {
        "title": "Scrape | Firecrawl",
        "url": "https://docs.firecrawl.dev/features/scrape"
      }
    },
    {
      "id": "0728a826-3c8f-4dcc-8280-3846520150dd",
      "source": "firecrawl/docs/features-scrape.md",
      "content": "[](https://docs.firecrawl.dev/features/scrape#batch-scraping-multiple-urls) Batch scraping multiple URLs\\\n\n\\\nYou can now batch scrape multiple URLs at the same time. It takes the starting URLs and optional parameters as arguments. The params argument allows you to specify additional options for the batch scrape job, such as the output formats.\\\n\\",
      "metadata": {
        "title": "Scrape | Firecrawl",
        "url": "https://docs.firecrawl.dev/features/scrape"
      }
    },
    {
      "id": "2ce5d400-68d6-4b8b-9073-c65867de1bea",
      "source": "firecrawl/docs/features-scrape.md",
      "content": "[](https://docs.firecrawl.dev/features/scrape#batch-scraping-multiple-urls) Batch scraping multiple URLs\\ > [](https://docs.firecrawl.dev/features/scrape#how-it-works-2) How it works\\\n\n\\\nIt is very similar to how the `/crawl` endpoint works. It submits a batch scrape job and returns a job ID to check the status of the batch scrape.\\\n\\\nThe sdk provides 2 methods, synchronous and asynchronous. The synchronous method will return the results of the batch scrape job, while the asynchronous method will return a job ID that you can use to check the status of the batch scrape.\\\n\\",
      "metadata": {
        "title": "Scrape | Firecrawl",
        "url": "https://docs.firecrawl.dev/features/scrape"
      }
    },
    {
      "id": "2f806a0d-75da-43ea-89af-ed7b713094a6",
      "source": "firecrawl/docs/features-scrape.md",
      "content": "[](https://docs.firecrawl.dev/features/scrape#batch-scraping-multiple-urls) Batch scraping multiple URLs\\ > [](https://docs.firecrawl.dev/features/scrape#usage-3) Usage\\\n\n\\\nPython\\\n\\\nNode\\\n\\\ncURL\\\n\\\nCopy\\\n\\\n```python\\\nfrom firecrawl import FirecrawlApp\\\n\\\napp = FirecrawlApp(api_key=\"fc-YOUR_API_KEY\")\\\n\\\n# Scrape multiple websites:\\\nbatch_scrape_result = app.batch_scrape_urls(['firecrawl.dev', 'mendable.ai'], {'formats': ['markdown', 'html']})\\\nprint(batch_scrape_result)\\\n\\\n# Or, you can use the asynchronous method:\\\nbatch_scrape_job = app.async_batch_scrape_urls(['firecrawl.dev', 'mendable.ai'], {'formats': ['markdown', 'html']})\\\nprint(batch_scrape_job)\\\n\\\n# (async) You can then use the job ID to check the status of the batch scrape:\\\nbatch_scrape_status = app.check_batch_scrape_status(batch_scrape_job['id'])\\\nprint(batch_scrape_status)\\\n\\\n```\\\n\\",
      "metadata": {
        "title": "Scrape | Firecrawl",
        "url": "https://docs.firecrawl.dev/features/scrape"
      }
    },
    {
      "id": "915f67de-c967-4678-85c7-5c36e9d5aa68",
      "source": "firecrawl/docs/features-scrape.md",
      "content": "[](https://docs.firecrawl.dev/features/scrape#batch-scraping-multiple-urls) Batch scraping multiple URLs\\ > [](https://docs.firecrawl.dev/features/scrape#response-2) Response\\\n\n\\\nIf youâ€™re using the sync methods from the SDKs, it will return the results of the batch scrape job. Otherwise, it will return a job ID that you can use to check the status of the batch scrape.\\\n\\",
      "metadata": {
        "title": "Scrape | Firecrawl",
        "url": "https://docs.firecrawl.dev/features/scrape"
      }
    },
    {
      "id": "0ef1bbbb-25d5-486c-a8be-62e863e782a6",
      "source": "firecrawl/docs/features-scrape.md",
      "content": "[](https://docs.firecrawl.dev/features/scrape#batch-scraping-multiple-urls) Batch scraping multiple URLs\\ > [](https://docs.firecrawl.dev/features/scrape#response-2) Response\\ > [](https://docs.firecrawl.dev/features/scrape#synchronous) Synchronous\\\n\n\\\nCompleted\\\n\\\nCopy\\\n\\\n```json\\\n{\\\n\"status\": \"completed\",\\\n\"total\": 36,\\\n\"completed\": 36,\\\n\"creditsUsed\": 36,\\\n\"expiresAt\": \"2024-00-00T00:00:00.000Z\",\\\n\"next\": \"https://api.firecrawl.dev/v1/batch/scrape/123-456-789?skip=26\",\\\n\"data\": [\\\n{\\\n\"markdown\": \"[Firecrawl Docs home page![light logo](https://mintlify.s3-us-west-1.amazonaws.com/firecrawl/logo/light.svg)!...\",\\\n\"html\": \"<!DOCTYPE html><html lang=\"en\" class=\"js-focus-visible lg:[--scroll-mt:9.5rem]\" data-js-focus-visible=\"\">...\",\\\n\"metadata\": {\\\n\"title\": \"Build a 'Chat with website' using Groq Llama 3 | Firecrawl\",\\\n\"language\": \"en\",\\\n\"sourceURL\": \"https://docs.firecrawl.dev/learn/rag-llama3\",\\\n\"description\": \"Learn how to use Firecrawl, Groq Llama 3, and Langchain to build a 'Chat with your website' bot.\",\\\n\"ogLocaleAlternate\": [],\\\n\"statusCode\": 200\\\n}\\\n},\\\n...\\\n]\\\n}\\\n\\\n```\\\n\\",
      "metadata": {
        "title": "Scrape | Firecrawl",
        "url": "https://docs.firecrawl.dev/features/scrape"
      }
    },
    {
      "id": "6d2c76a0-f300-4c32-9e27-273e0410ac08",
      "source": "firecrawl/docs/features-scrape.md",
      "content": "[](https://docs.firecrawl.dev/features/scrape#batch-scraping-multiple-urls) Batch scraping multiple URLs\\ > [](https://docs.firecrawl.dev/features/scrape#response-2) Response\\ > [](https://docs.firecrawl.dev/features/scrape#asynchronous) Asynchronous\\\n\n\\\nYou can then use the job ID to check the status of the batch scrape by calling the `/batch/scrape/{id}` endpoint. This endpoint is meant to be used while the job is still running or right after it has completed **as batch scrape jobs expire after 24 hours**.\\\n\\\nCopy\\\n\\\n```json\\\n{\\\n\"success\": true,\\\n\"id\": \"123-456-789\",\\\n\"url\": \"https://api.firecrawl.dev/v1/batch/scrape/123-456-789\"\\\n}\\\n\\\n```\\\n\\\n[Suggest edits](https://github.com/hellofirecrawl/docs/edit/main/features/scrape.mdx) [Raise issue](https://github.com/hellofirecrawl/docs/issues/new?title=Issue%20on%20docs&body=Path:%20/features/scrape)\\\n\\\n[Advanced Scraping Guide](https://docs.firecrawl.dev/advanced-scraping-guide) [Batch Scrape](https://docs.firecrawl.dev/features/batch-scrape)\\\n\\\nOn this page\\\n\\\n- [Scraping a URL with Firecrawl](https://docs.firecrawl.dev/features/scrape#scraping-a-url-with-firecrawl)\\\n- [/scrape endpoint](https://docs.firecrawl.dev/features/scrape#scrape-endpoint)\\\n- [Installation](https://docs.firecrawl.dev/features/scrape#installation)\\\n- [Usage](https://docs.firecrawl.dev/features/scrape#usage)\\\n- [Response](https://docs.firecrawl.dev/features/scrape#response)\\\n- [Scrape Formats](https://docs.firecrawl.dev/features/scrape#scrape-formats)\\\n- [Extract structured data](https://docs.firecrawl.dev/features/scrape#extract-structured-data)\\\n- [/scrape (with extract) endpoint](https://docs.firecrawl.dev/features/scrape#scrape-with-extract-endpoint)\\\n- [Extracting without schema (New)](https://docs.firecrawl.dev/features/scrape#extracting-without-schema-new)\\\n- [Extract object](https://docs.firecrawl.dev/features/scrape#extract-object)\\\n- [Interacting with the page with Actions](https://docs.firecrawl.dev/features/scrape#interacting-with-the-page-with-actions)\\\n- [Example](https://docs.firecrawl.dev/features/scrape#example)\\\n- [Output](https://docs.firecrawl.dev/features/scrape#output)\\\n- [Location and Language](https://docs.firecrawl.dev/features/scrape#location-and-language)\\\n- [How it works](https://docs.firecrawl.dev/features/scrape#how-it-works)\\\n- [Usage](https://docs.firecrawl.dev/features/scrape#usage-2)\\\n- [Batch scraping multiple URLs](https://docs.firecrawl.dev/features/scrape#batch-scraping-multiple-urls)\\\n- [How it works](https://docs.firecrawl.dev/features/scrape#how-it-works-2)\\\n- [Usage](https://docs.firecrawl.dev/features/scrape#usage-3)\\\n- [Response](https://docs.firecrawl.dev/features/scrape#response-2)\\\n- [Synchronous](https://docs.firecrawl.dev/features/scrape#synchronous)\\\n- [Asynchronous](https://docs.firecrawl.dev/features/scrape#asynchronous)",
      "metadata": {
        "title": "Scrape | Firecrawl",
        "url": "https://docs.firecrawl.dev/features/scrape"
      }
    },
    {
      "id": "af4704f4-b342-4773-9113-b210c6263b25",
      "source": "firecrawl/docs/sdks-rust.md",
      "content": "---\ntitle: Rust SDK | Firecrawl\nurl: https://docs.firecrawl.dev/sdks/rust\n---  \n[Firecrawl Docs home page![light logo](https://mintlify.s3.us-west-1.amazonaws.com/firecrawl/logo/light.svg)![dark logo](https://mintlify.s3.us-west-1.amazonaws.com/firecrawl/logo/dark.svg)](https://firecrawl.dev/)  \nv1  \nSearch or ask...  \nCtrl K  \nSearch...  \nNavigation  \nSDKs  \nRust  \n[Documentation](https://docs.firecrawl.dev/introduction) [SDKs](https://docs.firecrawl.dev/sdks/overview) [Learn](https://www.firecrawl.dev/blog/category/tutorials) [API Reference](https://docs.firecrawl.dev/api-reference/introduction)",
      "metadata": {
        "title": "Rust SDK | Firecrawl",
        "url": "https://docs.firecrawl.dev/sdks/rust"
      }
    },
    {
      "id": "a385ac16-b013-44b9-a3fd-093ce30b6a40",
      "source": "firecrawl/docs/sdks-rust.md",
      "content": "[](https://docs.firecrawl.dev/sdks/rust#installation) Installation\n\nTo install the Firecrawl Rust SDK, add the following to your `Cargo.toml`:  \nRust  \nCopy  \n```yaml\n# Add this to your Cargo.toml\n[dependencies]\nfirecrawl = \"^1.0\"\ntokio = { version = \"^1\", features = [\"full\"] }\n\n```",
      "metadata": {
        "title": "Rust SDK | Firecrawl",
        "url": "https://docs.firecrawl.dev/sdks/rust"
      }
    },
    {
      "id": "58a5c299-9276-4a32-9562-3a3bb5d5af15",
      "source": "firecrawl/docs/sdks-rust.md",
      "content": "[](https://docs.firecrawl.dev/sdks/rust#usage) Usage\n\nFirst, you need to obtain an API key from [firecrawl.dev](https://firecrawl.dev/). Then, you need to initialize the `FirecrawlApp`. From there, you can access functions like `FirecrawlApp::scrape_url`, which let you use our API.  \nHereâ€™s an example of how to use the SDK in Rust:  \nRust  \nCopy  \n```rust\nuse firecrawl::{crawl::{CrawlOptions, CrawlScrapeOptions, CrawlScrapeFormats}, FirecrawlApp, scrape::{ScrapeOptions, ScrapeFormats}};\n\n#[tokio::main]\nasync fn main() {\n// Initialize the FirecrawlApp with the API key\nlet app = FirecrawlApp::new(\"fc-YOUR_API_KEY\").expect(\"Failed to initialize FirecrawlApp\");\n\n// Scrape a URL\nlet options = ScrapeOptions {\nformats vec! [ ScrapeFormats::Markdown, ScrapeFormats::HTML ].into(),\n..Default::default()\n};\n\nlet scrape_result = app.scrape_url(\"https://firecrawl.dev\", options).await;\n\nmatch scrape_result {\nOk(data) => println!(\"Scrape Result:n{}\", data.markdown.unwrap()),\nErr(e) => eprintln!(\"Map failed: {}\", e),\n}\n\n// Crawl a website\nlet crawl_options = CrawlOptions {\nscrape_options: CrawlScrapeOptions {\nformats: vec![ CrawlScrapeFormats::Markdown, CrawlScrapeFormats::HTML ].into(),\n..Default::default()\n}.into(),\nlimit: 100.into(),\n..Default::default()\n};\n\nlet crawl_result = app\n.crawl_url(\"https://mendable.ai\", crawl_options)\n.await;\n\nmatch crawl_result {\nOk(data) => println!(\"Crawl Result (used {} credits):n{:#?}\", data.credits_used, data.data),\nErr(e) => eprintln!(\"Crawl failed: {}\", e),\n}\n}\n\n```",
      "metadata": {
        "title": "Rust SDK | Firecrawl",
        "url": "https://docs.firecrawl.dev/sdks/rust"
      }
    },
    {
      "id": "6f1eadbc-cad8-4330-b36d-dbc4599ca6d4",
      "source": "firecrawl/docs/sdks-rust.md",
      "content": "[](https://docs.firecrawl.dev/sdks/rust#usage) Usage > [](https://docs.firecrawl.dev/sdks/rust#scraping-a-url) Scraping a URL\n\nTo scrape a single URL, use the `scrape_url` method. It takes the URL as a parameter and returns the scraped data as a `Document`.  \nRust  \nCopy  \n```rust\nlet options = ScrapeOptions {\nformats vec! [ ScrapeFormats::Markdown, ScrapeFormats::HTML ].into(),\n..Default::default()\n};\n\nlet scrape_result = app.scrape_url(\"https://firecrawl.dev\", options).await;\n\nmatch scrape_result {\nOk(data) => println!(\"Scrape Result:n{}\", data.markdown.unwrap()),\nErr(e) => eprintln!(\"Map failed: {}\", e),\n}\n\n```",
      "metadata": {
        "title": "Rust SDK | Firecrawl",
        "url": "https://docs.firecrawl.dev/sdks/rust"
      }
    },
    {
      "id": "26c41d7d-f83f-4444-ad62-420ab295648f",
      "source": "firecrawl/docs/sdks-rust.md",
      "content": "[](https://docs.firecrawl.dev/sdks/rust#usage) Usage > [](https://docs.firecrawl.dev/sdks/rust#scraping-with-extract) Scraping with Extract\n\nWith Extract, you can easily extract structured data from any URL. You need to specify your schema in the JSON Schema format, using the `serde_json::json!` macro.  \nRust  \nCopy  \n```rust\nlet json_schema = json!({\n\"type\": \"object\",\n\"properties\": {\n\"top\": {\n\"type\": \"array\",\n\"items\": {\n\"type\": \"object\",\n\"properties\": {\n\"title\": {\"type\": \"string\"},\n\"points\": {\"type\": \"number\"},\n\"by\": {\"type\": \"string\"},\n\"commentsURL\": {\"type\": \"string\"}\n},\n\"required\": [\"title\", \"points\", \"by\", \"commentsURL\"]\n},\n\"minItems\": 5,\n\"maxItems\": 5,\n\"description\": \"Top 5 stories on Hacker News\"\n}\n},\n\"required\": [\"top\"]\n});\n\nlet llm_extraction_options = ScrapeOptions {\nformats: vec![ ScrapeFormats::Extract ].into(),\nextract: ExtractOptions {\nschema: json_schema.into(),\n..Default::default()\n}.into(),\n..Default::default()\n};\n\nlet llm_extraction_result = app\n.scrape_url(\"https://news.ycombinator.com\", llm_extraction_options)\n.await;\n\nmatch llm_extraction_result {\nOk(data) => println!(\"LLM Extraction Result:n{:#?}\", data.extract.unwrap()),\nErr(e) => eprintln!(\"LLM Extraction failed: {}\", e),\n}\n\n```",
      "metadata": {
        "title": "Rust SDK | Firecrawl",
        "url": "https://docs.firecrawl.dev/sdks/rust"
      }
    },
    {
      "id": "5baef88a-06bf-400f-98d7-cb45ad5ee0a7",
      "source": "firecrawl/docs/sdks-rust.md",
      "content": "[](https://docs.firecrawl.dev/sdks/rust#usage) Usage > [](https://docs.firecrawl.dev/sdks/rust#crawling-a-website) Crawling a Website\n\nTo crawl a website, use the `crawl_url` method. This will wait for the crawl to complete, which may take a long time based on your starting URL and your options.  \nRust  \nCopy  \n```rust\nlet crawl_options = CrawlOptions {\nscrape_options: CrawlScrapeOptions {\nformats: vec![ CrawlScrapeFormats::Markdown, CrawlScrapeFormats::HTML ].into(),\n..Default::default()\n}.into(),\nlimit: 100.into(),\n..Default::default()\n};\n\nlet crawl_result = app\n.crawl_url(\"https://mendable.ai\", crawl_options)\n.await;\n\nmatch crawl_result {\nOk(data) => println!(\"Crawl Result (used {} credits):n{:#?}\", data.credits_used, data.data),\nErr(e) => eprintln!(\"Crawl failed: {}\", e),\n}\n\n```",
      "metadata": {
        "title": "Rust SDK | Firecrawl",
        "url": "https://docs.firecrawl.dev/sdks/rust"
      }
    },
    {
      "id": "c856473b-2c67-47a4-b906-f6f396ad8c0b",
      "source": "firecrawl/docs/sdks-rust.md",
      "content": "[](https://docs.firecrawl.dev/sdks/rust#usage) Usage > [](https://docs.firecrawl.dev/sdks/rust#crawling-a-website) Crawling a Website > [](https://docs.firecrawl.dev/sdks/rust#crawling-asynchronously) Crawling asynchronously\n\nTo crawl without waiting for the result, use the `crawl_url_async` method. It takes the same parameters, but it returns a `CrawlAsyncRespone` struct, containing the crawlâ€™s ID. You can use that ID with the `check_crawl_status` method to check the status at any time. Do note that completed crawls are deleted after 24 hours.  \nRust  \nCopy  \n```rust\nlet crawl_id = app.crawl_url_async(\"https://mendable.ai\", None).await?.id;\n\n// ... later ...\n\nlet status = app.check_crawl_status(crawl_id).await?;\n\nif status.status == CrawlStatusTypes::Completed {\nprintln!(\"Crawl is done: {:#?}\", status.data);\n} else {\n// ... wait some more ...\n}\n\n```",
      "metadata": {
        "title": "Rust SDK | Firecrawl",
        "url": "https://docs.firecrawl.dev/sdks/rust"
      }
    },
    {
      "id": "705c6a21-d852-437f-b239-59f8fa0ff0c3",
      "source": "firecrawl/docs/sdks-rust.md",
      "content": "[](https://docs.firecrawl.dev/sdks/rust#usage) Usage > [](https://docs.firecrawl.dev/sdks/rust#map-a-url) Map a URL\n\nMap all associated links from a starting URL.  \nRust  \nCopy  \n```rust\nlet map_result = app.map_url(\"https://firecrawl.dev\", None).await;\n\nmatch map_result {\nOk(data) => println!(\"Mapped URLs: {:#?}\", data),\nErr(e) => eprintln!(\"Map failed: {}\", e),\n}\n\n```",
      "metadata": {
        "title": "Rust SDK | Firecrawl",
        "url": "https://docs.firecrawl.dev/sdks/rust"
      }
    },
    {
      "id": "37f41ff3-b074-40a3-a8fe-baf5fd6a03f4",
      "source": "firecrawl/docs/sdks-rust.md",
      "content": "[](https://docs.firecrawl.dev/sdks/rust#error-handling) Error Handling\n\nThe SDK handles errors returned by the Firecrawl API and by our dependencies, and combines them into the `FirecrawlError` enum, implementing `Error`, `Debug` and `Display`. All of our methods return a `Result<T, FirecrawlError>`.  \n[Suggest edits](https://github.com/hellofirecrawl/docs/edit/main/sdks/rust.mdx) [Raise issue](https://github.com/hellofirecrawl/docs/issues/new?title=Issue%20on%20docs&body=Path:%20/sdks/rust)  \n[Go](https://docs.firecrawl.dev/sdks/go)  \nOn this page  \n- [Installation](https://docs.firecrawl.dev/sdks/rust#installation)\n- [Usage](https://docs.firecrawl.dev/sdks/rust#usage)\n- [Scraping a URL](https://docs.firecrawl.dev/sdks/rust#scraping-a-url)\n- [Scraping with Extract](https://docs.firecrawl.dev/sdks/rust#scraping-with-extract)\n- [Crawling a Website](https://docs.firecrawl.dev/sdks/rust#crawling-a-website)\n- [Crawling asynchronously](https://docs.firecrawl.dev/sdks/rust#crawling-asynchronously)\n- [Map a URL](https://docs.firecrawl.dev/sdks/rust#map-a-url)\n- [Error Handling](https://docs.firecrawl.dev/sdks/rust#error-handling)",
      "metadata": {
        "title": "Rust SDK | Firecrawl",
        "url": "https://docs.firecrawl.dev/sdks/rust"
      }
    },
    {
      "id": "0f9a5d83-2f22-4962-ab0d-3b9bd8874aec",
      "source": "firecrawl/docs/v0-api-reference-endpoint-crawl-cancel.md",
      "content": "---\ntitle: Firecrawl Docs\nurl: https://docs.firecrawl.dev/v0/api-reference/endpoint/crawl-cancel\n---  \n[Firecrawl Docs home page![light logo](https://mintlify.s3.us-west-1.amazonaws.com/firecrawl/logo/light.svg)![dark logo](https://mintlify.s3.us-west-1.amazonaws.com/firecrawl/logo/dark.svg)](https://firecrawl.dev/)  \nv0  \nSearch or ask...  \nCtrl K  \nSearch...  \nNavigation  \nEndpoints  \nCancel Crawl Job  \n[Documentation](https://docs.firecrawl.dev/v0/introduction) [SDKs](https://docs.firecrawl.dev/v0/sdks/python) [Learn](https://www.firecrawl.dev/blog/category/tutorials) [API Reference](https://docs.firecrawl.dev/v0/api-reference/introduction)  \nDELETE  \n/  \ncrawl  \n/  \ncancel  \n/  \n{jobId}  \nTry it  \ncURL  \nPython  \nJavaScript  \nPHP  \nGo  \nJava  \nCopy  \n```\ncurl --request DELETE \\\n--url https://api.firecrawl.dev/v0/crawl/cancel/{jobId} \\\n--header 'Authorization: Bearer <token>'\n```  \n200  \n402  \n429  \n500  \nCopy  \n```\n{\n\"status\": \"<string>\"\n}\n```",
      "metadata": {
        "title": "Firecrawl Docs",
        "url": "https://docs.firecrawl.dev/v0/api-reference/endpoint/crawl-cancel"
      }
    },
    {
      "id": "87c15415-1436-4981-9ea6-56def310711c",
      "source": "firecrawl/docs/v0-api-reference-endpoint-crawl-cancel.md",
      "content": "Authorizations\n\n[](https://docs.firecrawl.dev/v0/api-reference/endpoint/crawl-cancel#authorization-authorization)  \nAuthorization  \nstring  \nheader  \nrequired  \nBearer authentication header of the form `Bearer <token>`, where `<token>` is your auth token.",
      "metadata": {
        "title": "Firecrawl Docs",
        "url": "https://docs.firecrawl.dev/v0/api-reference/endpoint/crawl-cancel"
      }
    },
    {
      "id": "fede0e1d-5a4b-4438-9e8e-bb946e7c2f08",
      "source": "firecrawl/docs/v0-api-reference-endpoint-crawl-cancel.md",
      "content": "Path Parameters\n\n[](https://docs.firecrawl.dev/v0/api-reference/endpoint/crawl-cancel#parameter-job-id)  \njobId  \nstring  \nrequired  \nID of the crawl job",
      "metadata": {
        "title": "Firecrawl Docs",
        "url": "https://docs.firecrawl.dev/v0/api-reference/endpoint/crawl-cancel"
      }
    },
    {
      "id": "43aa6666-b37f-4c9b-b019-3a0664c2f59a",
      "source": "firecrawl/docs/v0-api-reference-endpoint-crawl-cancel.md",
      "content": "Response\n\n200 - application/json  \n[](https://docs.firecrawl.dev/v0/api-reference/endpoint/crawl-cancel#response-status)  \nstatus  \nstring  \nReturns cancelled.  \n[Suggest edits](https://github.com/hellofirecrawl/docs/edit/main/v0/api-reference/endpoint/crawl-cancel.mdx) [Raise issue](https://github.com/hellofirecrawl/docs/issues/new?title=Issue%20on%20docs&body=Path:%20/v0/api-reference/endpoint/crawl-cancel)  \n[Get Crawl Status](https://docs.firecrawl.dev/v0/api-reference/endpoint/status)  \ncURL  \nPython  \nJavaScript  \nPHP  \nGo  \nJava  \nCopy  \n```\ncurl --request DELETE \\\n--url https://api.firecrawl.dev/v0/crawl/cancel/{jobId} \\\n--header 'Authorization: Bearer <token>'\n```  \n200  \n402  \n429  \n500  \nCopy  \n```\n{\n\"status\": \"<string>\"\n}\n```",
      "metadata": {
        "title": "Firecrawl Docs",
        "url": "https://docs.firecrawl.dev/v0/api-reference/endpoint/crawl-cancel"
      }
    },
    {
      "id": "559dfe39-33cb-498f-beae-5ed90a3f8808",
      "source": "firecrawl/docs/api-reference-endpoint-batch-scrape-get-errors.md",
      "content": "---\ntitle: Firecrawl Docs\nurl: https://docs.firecrawl.dev/api-reference/endpoint/batch-scrape-get-errors\n---  \n[Firecrawl Docs home page![light logo](https://mintlify.s3.us-west-1.amazonaws.com/firecrawl/logo/light.svg)![dark logo](https://mintlify.s3.us-west-1.amazonaws.com/firecrawl/logo/dark.svg)](https://firecrawl.dev/)  \nv1  \nSearch or ask...  \nCtrl K  \nSearch...  \nNavigation  \nScrape Endpoints  \nGet Batch Scrape Errors  \n[Documentation](https://docs.firecrawl.dev/introduction) [SDKs](https://docs.firecrawl.dev/sdks/overview) [Learn](https://www.firecrawl.dev/blog/category/tutorials) [API Reference](https://docs.firecrawl.dev/api-reference/introduction)  \nGET  \n/  \nbatch  \n/  \nscrape  \n/  \n{id}  \n/  \nerrors  \nTry it  \ncURL  \nPython  \nJavaScript  \nPHP  \nGo  \nJava  \nCopy  \n```\ncurl --request GET \\\n--url https://api.firecrawl.dev/v1/batch/scrape/{id}/errors \\\n--header 'Authorization: Bearer <token>'\n```  \n200  \n402  \n429  \n500  \nCopy  \n```\n{\n\"errors\": [\\\n{\\\n\"id\": \"<string>\",\\\n\"timestamp\": \"<string>\",\\\n\"url\": \"<string>\",\\\n\"error\": \"<string>\"\\\n}\\\n],\n\"robotsBlocked\": [\\\n\"<string>\"\\\n]\n}\n```",
      "metadata": {
        "title": "Firecrawl Docs",
        "url": "https://docs.firecrawl.dev/api-reference/endpoint/batch-scrape-get-errors"
      }
    },
    {
      "id": "dda75ada-8eab-4b50-949e-20a37f487e0a",
      "source": "firecrawl/docs/api-reference-endpoint-batch-scrape-get-errors.md",
      "content": "Authorizations\n\n[](https://docs.firecrawl.dev/api-reference/endpoint/batch-scrape-get-errors#authorization-authorization)  \nAuthorization  \nstring  \nheader  \nrequired  \nBearer authentication header of the form `Bearer <token>`, where `<token>` is your auth token.",
      "metadata": {
        "title": "Firecrawl Docs",
        "url": "https://docs.firecrawl.dev/api-reference/endpoint/batch-scrape-get-errors"
      }
    },
    {
      "id": "82845744-6c38-41bf-a6b2-1d2c9dc6c275",
      "source": "firecrawl/docs/api-reference-endpoint-batch-scrape-get-errors.md",
      "content": "Path Parameters\n\n[](https://docs.firecrawl.dev/api-reference/endpoint/batch-scrape-get-errors#parameter-id)  \nid  \nstring  \nrequired  \nThe ID of the batch scrape job",
      "metadata": {
        "title": "Firecrawl Docs",
        "url": "https://docs.firecrawl.dev/api-reference/endpoint/batch-scrape-get-errors"
      }
    },
    {
      "id": "edb5e64f-f65d-4cb2-80fd-1647a78e44e3",
      "source": "firecrawl/docs/api-reference-endpoint-batch-scrape-get-errors.md",
      "content": "Response\n\n200 - application/json  \n[](https://docs.firecrawl.dev/api-reference/endpoint/batch-scrape-get-errors#response-errors)  \nerrors  \nobject[]  \nErrored scrape jobs and error details  \nShowchild attributes  \n[](https://docs.firecrawl.dev/api-reference/endpoint/batch-scrape-get-errors#response-errors-id)  \nerrors.id  \nstring  \n[](https://docs.firecrawl.dev/api-reference/endpoint/batch-scrape-get-errors#response-errors-timestamp)  \nerrors.timestamp  \nstring | null  \nISO timestamp of failure  \n[](https://docs.firecrawl.dev/api-reference/endpoint/batch-scrape-get-errors#response-errors-url)  \nerrors.url  \nstring  \nScraped URL  \n[](https://docs.firecrawl.dev/api-reference/endpoint/batch-scrape-get-errors#response-errors-error)  \nerrors.error  \nstring  \nError message  \n[](https://docs.firecrawl.dev/api-reference/endpoint/batch-scrape-get-errors#response-robots-blocked)  \nrobotsBlocked  \nstring[]  \nList of URLs that were attempted in scraping but were blocked by robots.txt  \n[Suggest edits](https://github.com/hellofirecrawl/docs/edit/main/api-reference/endpoint/batch-scrape-get-errors.mdx) [Raise issue](https://github.com/hellofirecrawl/docs/issues/new?title=Issue%20on%20docs&body=Path:%20/api-reference/endpoint/batch-scrape-get-errors)  \n[Get Batch Scrape Status](https://docs.firecrawl.dev/api-reference/endpoint/batch-scrape-get) [Crawl](https://docs.firecrawl.dev/api-reference/endpoint/crawl-post)  \ncURL  \nPython  \nJavaScript  \nPHP  \nGo  \nJava  \nCopy  \n```\ncurl --request GET \\\n--url https://api.firecrawl.dev/v1/batch/scrape/{id}/errors \\\n--header 'Authorization: Bearer <token>'\n```  \n200  \n402  \n429  \n500  \nCopy  \n```\n{\n\"errors\": [\\\n{\\\n\"id\": \"<string>\",\\\n\"timestamp\": \"<string>\",\\\n\"url\": \"<string>\",\\\n\"error\": \"<string>\"\\\n}\\\n],\n\"robotsBlocked\": [\\\n\"<string>\"\\\n]\n}\n```",
      "metadata": {
        "title": "Firecrawl Docs",
        "url": "https://docs.firecrawl.dev/api-reference/endpoint/batch-scrape-get-errors"
      }
    },
    {
      "id": "58051df1-0296-4b74-a343-712153e9d810",
      "source": "firecrawl/blog/mastering-firecrawl-extract-endpoint.md",
      "content": "---\ntitle: Mastering the Extract Endpoint in Firecrawl\nurl: https://www.firecrawl.dev/blog/mastering-firecrawl-extract-endpoint\n---  \nIntroducing /extract - Get web data with a prompt [Try now](https://www.firecrawl.dev/extract)  \nJan 23, 2025  \nâ€¢  \n[![Bex Tuychiev image](https://www.firecrawl.dev/bex.jpg)Bex Tuychiev](https://x.com/bextuychiev)  \n# Mastering the Extract Endpoint in Firecrawl  \n![Mastering the Extract Endpoint in Firecrawl image](https://www.firecrawl.dev/images/blog/extract_endpoint/mastering-extract-endpoint.jpg)",
      "metadata": {
        "title": "Mastering the Extract Endpoint in Firecrawl",
        "url": "https://www.firecrawl.dev/blog/mastering-firecrawl-extract-endpoint"
      }
    },
    {
      "id": "b3e23aed-ca61-4f45-899e-35da9fee8df3",
      "source": "firecrawl/blog/mastering-firecrawl-extract-endpoint.md",
      "content": "Introduction\n\nGetting useful data from websites can be tricky. While humans can easily read and understand web content, turning that information into structured data for AI applications often requires complex code and constant maintenance. Traditional web scraping tools break when websites change, and writing separate code for each website quickly becomes overwhelming.  \nFirecrawlâ€™s extract endpoint changes this by using AI to automatically understand and pull data from any website. Whether you need to gather product information from multiple stores, collect training data for AI models, or keep track of competitor prices, the extract endpoint can handle it with just a few lines of code. Similar to our previous guides about Firecrawlâ€™s [scrape](https://www.firecrawl.dev/blog/mastering-firecrawl-scrape-endpoint), [crawl](https://www.firecrawl.dev/blog/mastering-the-crawl-endpoint-in-firecrawl), and [map](https://www.firecrawl.dev/blog/how-to-generate-sitemaps-using-firecrawl-map-endpoint) endpoints, this article will show you how to use this powerful new tool to gather structured data from the web reliably and efficiently.",
      "metadata": {
        "title": "Mastering the Extract Endpoint in Firecrawl",
        "url": "https://www.firecrawl.dev/blog/mastering-firecrawl-extract-endpoint"
      }
    },
    {
      "id": "01d984d3-3265-49cc-9c90-4c16b0adc059",
      "source": "firecrawl/blog/mastering-firecrawl-extract-endpoint.md",
      "content": "Understanding the Extract Endpoint\n\nThe extract endpoint is different from Firecrawlâ€™s other tools because it can understand and process entire websites at once, not just single pages. Think of it like having a smart assistant who can read through a whole website and pick out exactly the information you need, whether thatâ€™s from one page or thousands of pages.  \nHereâ€™s what makes the extract endpoint special:  \n- It can process multiple URLs at once, saving you time and effort\n- You can use simple English to describe what data you want, no extra coding required\n- It works with entire websites by adding `/*` to the URL (like `website.com/*`)\n- If requested information is not found in the given URL, the **web search** feature can automatically search related links on the page to find and extract the missing data  \nThe extract endpoint is perfect for three main tasks:  \n1. **Full website data collection**: Gathering information from every page on a website, like product details from an online store\n2. **Data enrichment**: Adding extra information to your existing data by finding details across multiple websites\n3. **AI training data**: Creating clean, structured datasets to train AI models  \nAt the time of writing the article, you will have 500k free output tokens for the endpoint when you sign up for an account. [The pricing page](https://www.firecrawl.dev/extract#pricing) also contains information on other pricing options.",
      "metadata": {
        "title": "Mastering the Extract Endpoint in Firecrawl",
        "url": "https://www.firecrawl.dev/blog/mastering-firecrawl-extract-endpoint"
      }
    },
    {
      "id": "ed872331-333b-495c-aaaa-97e4e457536b",
      "source": "firecrawl/blog/mastering-firecrawl-extract-endpoint.md",
      "content": "Getting Started with Extract\n\nLetâ€™s set up everything you need to start using the extract endpoint. First, youâ€™ll need to sign up for a free Firecrawl account at [firecrawl.dev](https://firecrawl.dev/) to get your API key. Once you have your account, follow these simple steps to get started:  \n1. Create a new project folder and set up your environment:  \n```bash\n# Create a new folder and move into it\nmkdir extract-project\ncd extract-project\n\n# Create a virtual environment (optional but recommended)\npython -m venv venv\nsource venv/bin/activate # For Unix/macOS\nvenvScriptsactivate # For Windows\n\n# Install the required packages\npip install firecrawl-py python-dotenv pydantic\n\n```  \n2. Create a `.env` file to safely store your API key:  \n```bash\n# Create the file and add your API key\necho \"FIRECRAWL_API_KEY='your-key-here'\" >> .env\n\n# Create a .gitignore file to keep your API key private\necho \".env\" >> .gitignore\n\n```  \n3. Create a simple Python script to test the setup:  \n```python\nfrom firecrawl import FirecrawlApp\nfrom dotenv import load_dotenv\nfrom pydantic import BaseModel\n\n# Load your API key\nload_dotenv()\n\n# Create a Firecrawl app instance\napp = FirecrawlApp()\n\n# Create a Pydantic model\nclass PageData(BaseModel):\ntitle: str\ntrusted_companies: list[str]\n\n# Test the connection\nresult = app.extract(\nurls=[\"https://firecrawl.dev\"],\nparams={\n\"prompt\": \"Extract the contents of the page based on the schema provided.\",\n\"schema\": PageData.model_json_schema(),\n},\n)\n\n```  \n```python\nprint(result['data'])\n\n```  \n```python\n{\n'title': 'Turn websites into _LLM-ready_ data',\n'trusted_companies': [\\\n'Zapier', 'Gamma',\\\n'Nvidia', 'PHMG',\\\n'StackAI', 'Teller.io',\\\n'Carrefour', 'Vendr',\\\n'OpenGov.sg', 'CyberAgent',\\\n'Continue.dev', 'Bain',\\\n'JasperAI', 'Palladium Digital',\\\n'Checkr', 'JetBrains',\\\n'You.com'\\\n]\n}\n\n```  \nFirst, we import the necessary libraries:  \n- `FirecrawlApp`: The main class for interacting with Firecrawlâ€™s API\n- `load_dotenv`: For loading environment variables from the `.env` file\n- `BaseModel` from `pydantic`: For creating data schemas  \nAfter loading the API key from the `.env` file, we create a `FirecrawlApp` instance which will handle our API requests.  \nWe then define a `PageData` class using Pydantic that specifies the structure of data we want to extract:  \n- `title`: A string field for the page title\n- `trusted_companies`: A list of strings for company names  \nFinally, we make an API call using `app.extract()` with:  \n- A URL to scrape ( `firecrawl.dev`)\n- Parameters including:\n- A prompt telling the AI what to extract\n- The schema converted to JSON format  \nThe result will contain the extracted data matching our `PageData` schema structure.  \nThatâ€™s all you need to start using the extract endpoint! In the next section, weâ€™ll look at different ways to extract data and how to get exactly the information you need.",
      "metadata": {
        "title": "Mastering the Extract Endpoint in Firecrawl",
        "url": "https://www.firecrawl.dev/blog/mastering-firecrawl-extract-endpoint"
      }
    },
    {
      "id": "521ad8f5-b911-4701-bd70-d4936e177165",
      "source": "firecrawl/blog/mastering-firecrawl-extract-endpoint.md",
      "content": "Basic Extraction Patterns\n\nLetâ€™s start with some common patterns for extracting data using Firecrawl.",
      "metadata": {
        "title": "Mastering the Extract Endpoint in Firecrawl",
        "url": "https://www.firecrawl.dev/blog/mastering-firecrawl-extract-endpoint"
      }
    },
    {
      "id": "90d52bee-bb92-4e1a-b7c7-0a99949d0232",
      "source": "firecrawl/blog/mastering-firecrawl-extract-endpoint.md",
      "content": "Basic Extraction Patterns > Using Nested Schemas\n\nLetâ€™s look at how to extract more complex data using nested schemas. These are like folders within folders - they help organize related information together.  \n```python\nfrom pydantic import BaseModel\nfrom typing import Optional, List\n\n# Define a schema for author information\nclass Author(BaseModel):\nname: str\nbio: Optional[str]\nsocial_links: List[str]\n\n# Define a schema for blog posts\nclass BlogPost(BaseModel):\ntitle: str\nauthor: Author\npublish_date: str\ncontent_summary: str\n\n# Use the nested schema with Firecrawl\nresult = app.extract(\nurls=[\"https://example-blog.com/post/1\"],\nparams={\n\"prompt\": \"Extract the blog post information including author details\",\n\"schema\": BlogPost.model_json_schema()\n}\n)\n\n```  \nThis code will give you organized data like this inside `result['data']`:  \n```python\n{\n'title': 'How to Make Great Pizza',\n'author': {\n'name': 'Chef Maria',\n'bio': 'Professional chef with 15 years experience',\n'social_links': ['https://twitter.com/chefmaria', 'https://instagram.com/chefmaria']\n},\n'publish_date': '2024-01-15',\n'content_summary': 'A detailed guide to making restaurant-quality pizza at home'\n}\n\n```  \nNow that weâ€™ve seen how to extract structured data from a single page, letâ€™s look at how to handle multiple items.",
      "metadata": {
        "title": "Mastering the Extract Endpoint in Firecrawl",
        "url": "https://www.firecrawl.dev/blog/mastering-firecrawl-extract-endpoint"
      }
    },
    {
      "id": "f6fc745a-c5f0-4691-9e32-963d3f2ae264",
      "source": "firecrawl/blog/mastering-firecrawl-extract-endpoint.md",
      "content": "Basic Extraction Patterns > Capturing multiple items of the same type\n\nFirecrawl follows your Pydantic schema definition to the letter. This can lead to interesting scenarios like below:  \n```python\nclass Car(BaseModel):\nmake: str\nbrand: Author\nmanufacture_date: str\nmileage: int\n\n```  \nIf you use the above schema for scraping a car dealership website, you will only get a single car in the result.  \nTo get multiple cars, you need to wrap your schema in a container class:  \n```python\nclass Inventory(BaseModel):\ncars: List[Car]\n\nresult = app.extract(\nurls=[\"https://cardealership.com/inventory\"],\nparams={\n\"prompt\": \"Extract the full inventory including all cars and metadata\",\n\"schema\": Inventory.model_json_schema()\n}\n)\n\n```  \nThis will give you organized data with multiple cars in a structured array.",
      "metadata": {
        "title": "Mastering the Extract Endpoint in Firecrawl",
        "url": "https://www.firecrawl.dev/blog/mastering-firecrawl-extract-endpoint"
      }
    },
    {
      "id": "c01212b6-8f87-4777-a8aa-743ed05e06de",
      "source": "firecrawl/blog/mastering-firecrawl-extract-endpoint.md",
      "content": "Basic Extraction Patterns > Processing Entire Websites\n\nWhen you want to get data from an entire website, you can use the `/*` pattern. This tells Firecrawl to look at all pages on the website.  \n```python\nclass ProductInfo(BaseModel):\nname: str\nprice: float\ndescription: str\nin_stock: bool\n\n# This will check all pages on the website\nresult = app.extract(\nurls=[\"https://example-store.com/*\"],\nparams={\n\"prompt\": \"Find all product information on the website\",\n\"schema\": ProductInfo.model_json_schema()\n}\n)\n\n```  \nRemember that using `/*` will process multiple pages, so it will use more credits and thus, output more tokens. Currently, large-scale site coverage is not supported for massive websites like Amazon, eBay or Airbnb. Also, complex logical questions like find all comments made between 10am-12pm in 2011 are not yet fully operational.  \nSince `extract` endpoint is still in Beta, features and performance will continue to evolve.",
      "metadata": {
        "title": "Mastering the Extract Endpoint in Firecrawl",
        "url": "https://www.firecrawl.dev/blog/mastering-firecrawl-extract-endpoint"
      }
    },
    {
      "id": "2fa84a77-c8b3-43f7-944d-468ab8da9f5c",
      "source": "firecrawl/blog/mastering-firecrawl-extract-endpoint.md",
      "content": "Basic Extraction Patterns > Making Your Extractions Better\n\nHere are some real examples of how to get better results:  \n```python\n# Bad prompt\nresult = app.extract(\nurls=[\"https://store.com\"],\nparams={\n\"prompt\": \"Get prices\", # Too vague!\n\"schema\": ProductInfo.model_json_schema()\n}\n)\n\n# Good prompt\nresult = app.extract(\nurls=[\"https://store.com\"],\nparams={\n\"prompt\": \"Find all product prices in USD, including any sale prices. If there's a range, get the lowest price.\", # Clear and specific\n\"schema\": ProductInfo.model_json_schema()\n}\n)\n\n```  \nPro Tips:  \n1. Start with one or two URLs to test your schema and prompt\n2. Use clear field names in your schemas (like `product_name` instead of just `name`)\n3. Break complex data into smaller, nested schemas\n4. Add descriptions to your schema fields to help the AI understand what to look for  \nIn the next section, weâ€™ll explore best practices for schema definition, including how to create advanced Pydantic models, design effective schemas, improve extraction accuracy with field descriptions, and see complex schema examples in action.",
      "metadata": {
        "title": "Mastering the Extract Endpoint in Firecrawl",
        "url": "https://www.firecrawl.dev/blog/mastering-firecrawl-extract-endpoint"
      }
    },
    {
      "id": "fd51a3a9-3dc1-4281-abbc-00d381e98764",
      "source": "firecrawl/blog/mastering-firecrawl-extract-endpoint.md",
      "content": "Advanced Extraction Patterns\n\nLetâ€™s look at a couple of advanced patterns for extracting data effectively from different types of websites.",
      "metadata": {
        "title": "Mastering the Extract Endpoint in Firecrawl",
        "url": "https://www.firecrawl.dev/blog/mastering-firecrawl-extract-endpoint"
      }
    },
    {
      "id": "ff95ec53-5d25-43bf-8ec3-7d8a52134e00",
      "source": "firecrawl/blog/mastering-firecrawl-extract-endpoint.md",
      "content": "Advanced Extraction Patterns > Asynchronous Extraction\n\nIn practice, you will usually scrape dozens if not hundreds of URLs with `extract`. For these larger jobs, you can use `async_extract` which processes URLs concurrently, significantly reducing the total execution time. This asynchronous version returns the same results as `extract` but can be up to 10x faster when processing multiple URLs. Simply use `app.async_extract()` instead of `app.extract()` and the async version returns a job object with status and progress information that you can use to track the extraction progress. This allows your program to continue executing other tasks while the extraction runs in the background, rather than blocking until completion.  \n```python\nclass HackerNewsArticle(BaseModel):\ntitle: str = Field(description=\"The title of the article\")\nurl: Optional[str] = Field(description=\"The URL of the article if present\")\npoints: int = Field(description=\"Number of upvotes/points\")\nauthor: str = Field(description=\"Username of the submitter\")\ncomments_count: int = Field(description=\"Number of comments on the post\")\nposted_time: str = Field(description=\"When the article was posted\")\n\nclass HackerNewsResponse(BaseModel):\narticles: List[HackerNewsArticle]\n\nextract_job = app.async_extract(\nurls=[\\\n\"https://news.ycombinator.com/\",\\\n# your other URLs here ...\\\n],\nparams={\n\"schema\": HackerNewsResponse.model_json_schema(),\n\"prompt\": \"\"\"\nExtract articles from the Hacker News front page. Each article should include:\n- The title of the post\n- The linked URL (if present)\n- Number of points/upvotes\n- Username of who posted it\n- Number of comments\n- When it was posted\n\"\"\"}\n)\n\n```  \nLetâ€™s break down whatâ€™s happening above:  \n1. First, we define a `HackerNewsArticle` Pydantic model that specifies the structure for each article:\n- `title`: The articleâ€™s headline\n- `url`: The link to the full article (optional)\n- `points`: Number of upvotes\n- `author`: Username who posted it\n- `comments_count`: Number of comments\n- `posted_time`: When it was posted\n2. We then create a `HackerNewsResponse` model that contains a list of these articles.  \n3. Instead of using the synchronous `extract()`, we use `async_extract()`.  \n4. The extraction job is configured with:\n- A list of URLs to process (in this case, the Hacker News homepage)\n- Parameters including our schema and a prompt that guides the AI in extracting the correct information  \nThis approach is particularly useful when you need to scrape multiple pages, as it prevents your application from blocking while waiting for each URL to be processed sequentially.  \n`async_extract` returns an extraction job object that includes a `job_id`. You can periodically pass this job ID to `get_extract_status` method to check on the jobâ€™s progress:  \n```python\njob_status = app.get_extract_status(extract_job.job_id)\n\nprint(job_status)\n{\n\"status\": \"pending\",\n\"progress\": 36,\n\"results\": [{\\\n\"url\": \"https://news.ycombinator.com/\",\\\n\"data\": { ... }\\\n}]\n}\n\n```  \nPossible states include:  \n- `completed`: The extraction finished successfully.\n- `pending`: Firecrawl is still processing your request.\n- `failed`: An error occurred; data was not fully extracted.\n- `cancelled`: The job was cancelled by the user.  \nA completed output of `async_extract` will be the same as plain `extract` with additional `status` of `completed` key-value pair.",
      "metadata": {
        "title": "Mastering the Extract Endpoint in Firecrawl",
        "url": "https://www.firecrawl.dev/blog/mastering-firecrawl-extract-endpoint"
      }
    },
    {
      "id": "151a87d0-321d-4672-adc2-29c3811580f0",
      "source": "firecrawl/blog/mastering-firecrawl-extract-endpoint.md",
      "content": "Advanced Extraction Patterns > Using Web Search\n\nAnother powerful feature of the extract endpoint is the ability to search the related pages to the URL you are scraping for additional information. For example, letâ€™s say we are scraping GitHubâ€™s trending page and we want to get the following information:  \n```python\nclass GithubRepository(BaseModel):\nname: str = Field(description=\"The name of the repository\")\nurl: str = Field(description=\"The URL of the repository\")\nstars: int = Field(description=\"Number of stars/favorites\")\nlicense: str = Field(description=\"The open-source license of the repository\")\nn_releases: str = Field(description=\"The number of releases of the repo if any\")\n\nclass GithubTrendingResponse(BaseModel):\nrepositories: List[GithubRepository]\n\n```  \nIf you look at the GitHub trending page, you will see that the first page doesnâ€™t contain any information about the license or number of releases. Those pieces of info are on each repositoryâ€™s page.  \nTo get this information, we can set the `enableWebSearch` parameter in the `extract` endpoint. This parameter will search the related pages to the URL you are scraping for additional information.  \n```python\ndata = app.extract(\nurls=[\"https://github.com/trending\"],\nparams={\n\"schema\": GithubTrendingResponse.model_json_schema(),\n\"prompt\": \"Extract information based on the schema provided. If there are missing fields, search the related pages for the missing information.\",\n\"enableWebSearch\": True,\n},\n)\n\ndata['data']['repositories'][0]\n\n```  \n```python\n{\n'url': 'https://github.com/yt-dlp/yt-dlp',\n'name': 'YT-DLP',\n'license': 'Unlicense license',\n'n_stars': 98854,\n'n_releases': '103'\n}\n\n```  \nThe web search feature is particularly powerful when you need to gather comprehensive information that might be spread across multiple pages, saving you from having to write complex crawling logic or multiple extraction calls.",
      "metadata": {
        "title": "Mastering the Extract Endpoint in Firecrawl",
        "url": "https://www.firecrawl.dev/blog/mastering-firecrawl-extract-endpoint"
      }
    },
    {
      "id": "e2889d20-3b72-4296-a4d7-13a834186fe1",
      "source": "firecrawl/blog/mastering-firecrawl-extract-endpoint.md",
      "content": "Best Practices For Schema Definition\n\nWhen designing schemas for the extract endpoint, following these best practices will help you get better, more reliable results:",
      "metadata": {
        "title": "Mastering the Extract Endpoint in Firecrawl",
        "url": "https://www.firecrawl.dev/blog/mastering-firecrawl-extract-endpoint"
      }
    },
    {
      "id": "aaef4dc7-1553-471b-86e4-25b5a2fa3e6d",
      "source": "firecrawl/blog/mastering-firecrawl-extract-endpoint.md",
      "content": "Best Practices For Schema Definition > 1. Start simple, then expand\n\n```python\n# Start with basic fields\nclass Product(BaseModel):\nname: str\nprice: float\n\n# Then gradually add complexity\nclass Product(BaseModel):\nname: str = Field(description=\"Full product name including brand\")\nprice: float = Field(description=\"Current price in USD\")\nvariants: List[str] = Field(description=\"List of variants of the product\")\nspecifications: str = Field(description=\"Specifications of the product\")\n\n```  \nThe simple schema focuses on just the essential fields (name and price) which is great for:  \n- Initial testing and validation\n- Cases where you only need basic product information\n- Faster development and debugging  \nThe complex schema adds more detailed fields and descriptions which helps when:  \n- You need comprehensive product data\n- The source pages have varied formats\n- You want to ensure consistent data quality\n- You need specific variants or specifications  \nBy starting simple and expanding gradually, you can validate your extraction works before adding complexity. The field descriptions in the complex schema also help guide the AI to extract the right information in the right format.",
      "metadata": {
        "title": "Mastering the Extract Endpoint in Firecrawl",
        "url": "https://www.firecrawl.dev/blog/mastering-firecrawl-extract-endpoint"
      }
    },
    {
      "id": "1abaaa60-6633-43be-952f-8714d8ded9ee",
      "source": "firecrawl/blog/mastering-firecrawl-extract-endpoint.md",
      "content": "Best Practices For Schema Definition > 2. Use clear, descriptive field names and types\n\n```python\n# Poor naming and typing\nclass BadSchema(BaseModel):\nn: str # Unclear name\np: float # Unclear name\ndata: Any # Too flexible\n\n# Better naming and typing\nclass GoodSchema(BaseModel):\nproduct_name: str\nprice_usd: float\ntechnical_specifications: str\n\n```  \nUse type hints that match your expected data. This helps both the AI model and other developers understand your schema. The only exception is that Firecrawl doesnâ€™t support a `datetime` data type, so if you are scraping temporal information, use `str`.",
      "metadata": {
        "title": "Mastering the Extract Endpoint in Firecrawl",
        "url": "https://www.firecrawl.dev/blog/mastering-firecrawl-extract-endpoint"
      }
    },
    {
      "id": "0b1e7912-c033-4655-9cfd-4146fcadcdda",
      "source": "firecrawl/blog/mastering-firecrawl-extract-endpoint.md",
      "content": "Best Practices For Schema Definition > 3. Structure Complex Data Hierarchically\n\n```python\nclass Author(BaseModel):\nname: str\nbio: Optional[str]\nsocial_links: List[str]\n\nclass Article(BaseModel):\ntitle: str\nauthor: Author\ncontent: str\ntags: List[str]\n\nclass Blog(BaseModel):\narticles: List[Article]\nsite_name: str\nlast_updated: datetime\n\n```  \nBreaking down complex data into nested models makes the schema more maintainable and helps the AI understand relationships between data points. In the above example, the `Blog` model contains a list of `Article` objects, which in turn contain `Author` objects. This hierarchical structure clearly shows how the data is organized - a blog has many articles, and each article has one author with their own properties.",
      "metadata": {
        "title": "Mastering the Extract Endpoint in Firecrawl",
        "url": "https://www.firecrawl.dev/blog/mastering-firecrawl-extract-endpoint"
      }
    },
    {
      "id": "95e7e9fe-f667-4012-8fd4-d28f8e28a234",
      "source": "firecrawl/blog/mastering-firecrawl-extract-endpoint.md",
      "content": "Best Practices For Schema Definition > 4. Include Example Data\n\n```python\nclass ProductReview(BaseModel):\nrating: int = Field(\ndescription=\"Customer rating from 1-5 stars\"\n)\ncomment: str\nreviewer_name: str\nverified_purchase: bool\n\nclass Config:\njson_schema_extra = {\n\"example\": {\n\"rating\": 4,\n\"comment\": \"Great product, fast shipping!\",\n\"reviewer_name\": \"John D.\",\n\"verified_purchase\": True\n}\n}\n\n```  \nProviding examples in your schema helps the AI understand exactly what format you expect, especially for complex or ambiguous fields.  \nPro tips:  \n- Use `Optional` fields when data might not always be present\n- Include unit information in field descriptions (e.g., â€œprice in USDâ€, â€œweight in kgâ€)\n- Use enums for fields with a fixed set of possible values",
      "metadata": {
        "title": "Mastering the Extract Endpoint in Firecrawl",
        "url": "https://www.firecrawl.dev/blog/mastering-firecrawl-extract-endpoint"
      }
    },
    {
      "id": "061f1bef-2550-44e4-ba0e-bba9e4f07a20",
      "source": "firecrawl/blog/mastering-firecrawl-extract-endpoint.md",
      "content": "A Real-World Example: Scraping GitHub Trending Repositories\n\nLetâ€™s build a complete example that extracts trending repository data from [GitHubâ€™s trending page](https://github.com/trending). This example will show how to:  \n1. Design a robust schema\n2. Handle nested data\n3. Process multiple items\n4. Use field descriptions effectively  \nLetâ€™s start by making fresh imports and setup:  \n```python\nfrom firecrawl import FirecrawlApp\nfrom pydantic import BaseModel, Field\nfrom typing import List, Optional\nfrom datetime import datetime\nimport os\nfrom dotenv import load_dotenv\n\n# Load environment variables\nload_dotenv()\n\n# Initialize Firecrawl\napp = FirecrawlApp()\n\n```  \nNow, letâ€™s define our nested schema for the data:  \n```python\nclass Developer(BaseModel):\nusername: str = Field(description=\"GitHub username of the developer\")\nprofile_url: str = Field(description=\"URL to the developer's GitHub profile\")\n\nclass Repository(BaseModel):\nname: str = Field(description=\"Repository name in format 'username/repo'\")\ndescription: Optional[str] = Field(\ndescription=\"Repository description if available\"\n)\nurl: str = Field(description=\"Full URL to the repository\")\nlanguage: Optional[str] = Field(description=\"Primary programming language used\")\nstars: int = Field(description=\"Total number of stars\")\nforks: int = Field(description=\"Total number of forks\")\nstars_today: int = Field(description=\"Number of stars gained today\")\ndevelopers: List[Developer] = Field(\ndefault_factory=list, description=\"List of contributors to the repository\"\n)\n\nclass TrendingData(BaseModel):\nrepositories: List[Repository] = Field(description=\"List of trending repositories\")\n\n```  \nThe schema above defines three Pydantic models for structured GitHub trending data:  \n- `Developer`: Represents a contributor with their GitHub username and profile URL\n- `Repository`: Contains details about a GitHub repo including name, description, URL, language, stars count, forks count, stars gained today, and a list of contributors\n- `TrendingData`: The root model that contains a list of trending repositories  \nNext, letâ€™s define a function for running a scraper using our schema:  \n```python\ndef scrape_trending_repos():\n# Extract the data\nresult = app.extract(\nurls=[\"https://github.com/trending\"],\nparams={\n\"prompt\": \"\"\"\nExtract information about trending GitHub repositories.\nFor each repository:\n- Get the full repository name (username/repo)\n- Get the description if available\n- Extract the primary programming language\n- Get the total stars and forks\n- Get the number of stars gained today\n- Get information about contributors including their usernames and profile URLs\n\nNote:\n- Stars and forks should be converted to numbers (e.g., '1.2k' â†’ 1200)\n- Stars today should be extracted from the \"X stars today\" text\n- Ensure all URLs are complete (add https://github.com if needed)\n\"\"\",\n\"schema\": TrendingData.model_json_schema()\n}\n)\n\nreturn result\n\n```  \nNow, letâ€™s print the details of some of the scraped repos:  \n```python\n# Process and display the results\nif result[\"success\"]:\ntrending = TrendingData(**result[\"data\"])\n\n# Print the top 3 repositories\nprint(\"ðŸ”¥ Top 3 Trending Repositories:\")\nfor repo in trending.repositories[:3]:\nprint(f\"nðŸ“¦ {repo.name}\")\nprint(f\"ðŸ“ {repo.description[:100]}...\" if repo.description else \"No description\")\nprint(f\"ðŸŒŸ {repo.stars:,} total stars ({repo.stars_today:,} today)\")\nprint(f\"ðŸ”¤ {repo.language or 'Unknown language'}\")\nprint(f\"ðŸ”— {repo.url}\")\n\n```  \nExample output:  \n```bash\nðŸ”¥ Top 3 Trending Repositories:\n\nðŸ“¦ microsoft/garnet\nðŸ“ Garnet: A Remote Cache-Store for Distributed Applications...\nðŸŒŸ 12,483 total stars (1,245 today)\nðŸ”¤ C#\nðŸ”— https://github.com/microsoft/garnet\n\nðŸ“¦ openai/whisper\nðŸ“ Robust Speech Recognition via Large-Scale Weak Supervision...\nðŸŒŸ 54,321 total stars (523 today)\nðŸ”¤ Python\nðŸ”— https://github.com/openai/whisper\n\nðŸ“¦ lencx/ChatGPT\nðŸ“ ChatGPT Desktop Application (Mac, Windows and Linux)...\nðŸŒŸ 43,210 total stars (342 today)\nðŸ”¤ TypeScript\nðŸ”— https://github.com/lencx/ChatGPT\n\n```  \nThis example demonstrates several best practices:  \n1. **Nested Models**: Using separate models for repositories and developers\n2. **Smart Defaults**: Using `default_factory` for lists and timestamps\n3. **Clear Descriptions**: Each field has a detailed description\n4. **Type Safety**: Using proper types like `HttpUrl` for URLs\n5. **Detailed Prompt**: The prompt clearly explains how to handle special cases  \nYou can extend this example by:  \n- Adding time period filtering (daily, weekly, monthly)\n- Including more repository metadata\n- Saving results to a database\n- Setting up automated daily tracking\n- Adding error handling and retries",
      "metadata": {
        "title": "Mastering the Extract Endpoint in Firecrawl",
        "url": "https://www.firecrawl.dev/blog/mastering-firecrawl-extract-endpoint"
      }
    },
    {
      "id": "e28b9874-9689-427a-a7f0-60a992c6adbd",
      "source": "firecrawl/blog/mastering-firecrawl-extract-endpoint.md",
      "content": "Extract vs Other Endpoints\n\nFirecrawl offers three main endpoints for gathering web data: `/extract`, `/scrape`, and `/crawl`. Each serves different use cases and has unique strengths. Letâ€™s explore when to use each one.",
      "metadata": {
        "title": "Mastering the Extract Endpoint in Firecrawl",
        "url": "https://www.firecrawl.dev/blog/mastering-firecrawl-extract-endpoint"
      }
    },
    {
      "id": "a1dc8dba-d5f0-4308-a892-4858188d39fd",
      "source": "firecrawl/blog/mastering-firecrawl-extract-endpoint.md",
      "content": "Extract vs Other Endpoints > Extract vs Scrape\n\nThe `/scrape` endpoint is best for:  \n- Single-page detailed extraction\n- Getting multiple output formats (HTML, markdown, screenshots)\n- JavaScript-heavy websites requiring browser rendering\n- Capturing visual elements or taking screenshots  \n```python\n# Scrape endpoint example - multiple formats\nresult = app.scrape_url(\n\"https://example.com\",\nparams={\n\"formats\": [\"html\", \"markdown\", \"screenshot\", \"extract\"],\n\"extract\": {\n\"schema\": MySchema.model_json_schema(),\n\"prompt\": \"Extract product information\"\n}\n}\n)\n\n```  \nThe `/extract` endpoint is better for:  \n- Processing multiple URLs efficiently\n- Website-wide data extraction\n- Complex structured data extraction\n- Building data enrichment pipelines  \n```python\n# Extract endpoint example - multiple URLs\nresult = app.extract(\nurls=[\"https://example.com/*\"],\nparams={\n\"prompt\": \"Extract all product information\",\n\"schema\": ProductSchema.model_json_schema()\n}\n)\n\n```  \nNote that the _extract_ functionality is built into the `scrape_url` function and usually returns identical structured results like `extract`. The difference is that `scrape_url` is designed for single pages or when other extraction options are required.",
      "metadata": {
        "title": "Mastering the Extract Endpoint in Firecrawl",
        "url": "https://www.firecrawl.dev/blog/mastering-firecrawl-extract-endpoint"
      }
    },
    {
      "id": "4eefda9e-c0dc-4e5a-b67c-edb6711ebebf",
      "source": "firecrawl/blog/mastering-firecrawl-extract-endpoint.md",
      "content": "Extract vs Other Endpoints > Extract vs Crawl\n\nThe `/crawl` endpoint excels at:  \n- Discovering and following links automatically\n- Complete website archiving\n- Sitemap generation\n- Sequential page processing  \n```python\n# Crawl endpoint example\nresult = app.crawl_url(\n\"https://example.com\",\nparams={\n\"limit\": 5,\n\"includePaths\": \"*/blog/*\",\n\"excludePaths\": \"*/author/*\",\n\"scrapeOptions\": {\n\"formats\": [\"markdown\", \"html\"],\n\"includeTags\": [\"code\", \"#page-header\"],\n\"excludeTags\": [\"h1\", \"h2\", \".main-content\"]\n}\n}\n)\n\n```  \nThe `/extract` endpoint is preferable when:  \n- You know exactly which URLs to process\n- You need specific structured data from known pages\n- You want parallel processing of multiple URLs",
      "metadata": {
        "title": "Mastering the Extract Endpoint in Firecrawl",
        "url": "https://www.firecrawl.dev/blog/mastering-firecrawl-extract-endpoint"
      }
    },
    {
      "id": "e12827d1-22c2-44de-9c6e-7469f1d0ca27",
      "source": "firecrawl/blog/mastering-firecrawl-extract-endpoint.md",
      "content": "Extract vs Other Endpoints > Choosing the Right Endpoint\n\nHereâ€™s a decision framework for choosing between endpoints:  \n1. **Use `/extract` when:**\n- You need structured data from known URLs\n- You want to process multiple pages in parallel\n- You need to combine data from different websites\n- You have a specific schema for the data you want\n2. **Use `/scrape` when:**\n- You need multiple output formats\n- The website requires JavaScript rendering\n- You need screenshots or visual elements\n- Youâ€™re dealing with single pages that need detailed extraction\n3. **Use `/crawl` when:**\n- You need to discover pages automatically\n- You want to archive entire websites\n- You need to follow specific URL patterns\n- Youâ€™re building a complete site map  \nUltimately, the extraction functionality is built into all endpoints, allowing you to choose the most appropriate one for your specific use case while maintaining consistent data extraction capabilities.  \nReference our detailed guides for more information:  \n- [Mastering the Scrape Endpoint](https://www.firecrawl.dev/blog/mastering-firecrawl-scrape-endpoint)\n- [Complete Crawl Endpoint Guide](https://www.firecrawl.dev/blog/mastering-the-crawl-endpoint-in-firecrawl)",
      "metadata": {
        "title": "Mastering the Extract Endpoint in Firecrawl",
        "url": "https://www.firecrawl.dev/blog/mastering-firecrawl-extract-endpoint"
      }
    },
    {
      "id": "240ad8f4-6395-4e4d-8ffe-f1e6d7a0e262",
      "source": "firecrawl/blog/mastering-firecrawl-extract-endpoint.md",
      "content": "Conclusion\n\nThe `/extract` endpoint combines AI-driven understanding with structured data validation to solve common web scraping challenges. Instead of maintaining brittle HTML selectors or writing custom parsing logic, developers can describe their data needs in plain English while ensuring consistent output through schema validation. This approach works particularly well for projects that need to gather structured data from multiple sources or websites that frequently change their layouts.  \nFor those new to the endpoint, we recommend starting with simple schemas and single URLs before tackling more complex extractions. This helps in understanding how the AI interprets your prompts and how different schema designs affect the extraction results. As you become more comfortable with the basics, you can explore advanced features like website-wide extraction, nested schemas, and data enrichment patterns to build more sophisticated data pipelines.",
      "metadata": {
        "title": "Mastering the Extract Endpoint in Firecrawl",
        "url": "https://www.firecrawl.dev/blog/mastering-firecrawl-extract-endpoint"
      }
    },
    {
      "id": "7b0bd6fb-ed37-4d76-87f2-6ab55ccdc4a0",
      "source": "firecrawl/blog/mastering-firecrawl-extract-endpoint.md",
      "content": "Further Reading\n\n- [15 Python Web Scraping Projects](https://www.firecrawl.dev/blog/python-web-scraping-projects)\n- [How to Deploy Web Scrapers](https://www.firecrawl.dev/blog/deploy-web-scrapers)\n- [Automated Amazon Price Tracking](https://www.firecrawl.dev/blog/automated-price-tracking-tutorial-python)\n- [How to Run Scrapers on Schedule](https://www.firecrawl.dev/blog/automated-web-scraping-free-2025)  \nArticle updated recently  \n[ðŸ”¥](https://www.firecrawl.dev/)",
      "metadata": {
        "title": "Mastering the Extract Endpoint in Firecrawl",
        "url": "https://www.firecrawl.dev/blog/mastering-firecrawl-extract-endpoint"
      }
    },
    {
      "id": "8c753f93-6275-4eff-a2f2-4f6f66fa5124",
      "source": "firecrawl/blog/mastering-firecrawl-extract-endpoint.md",
      "content": "Ready to _Build?_\n\nStart scraping web data for your AI apps today.  \nNo credit card needed.  \nGet Started",
      "metadata": {
        "title": "Mastering the Extract Endpoint in Firecrawl",
        "url": "https://www.firecrawl.dev/blog/mastering-firecrawl-extract-endpoint"
      }
    },
    {
      "id": "d7965399-4794-466d-a838-f4c380a11b95",
      "source": "firecrawl/blog/mastering-firecrawl-extract-endpoint.md",
      "content": "About the Author\n\n[![Bex Tuychiev image](https://www.firecrawl.dev/bex.jpg)\\\nBex Tuychiev@bextuychiev](https://x.com/bextuychiev)  \nBex is a Top 10 AI writer on Medium and a Kaggle Master with over 15k followers. He loves writing detailed guides, tutorials, and notebooks on complex data science and machine learning topics",
      "metadata": {
        "title": "Mastering the Extract Endpoint in Firecrawl",
        "url": "https://www.firecrawl.dev/blog/mastering-firecrawl-extract-endpoint"
      }
    },
    {
      "id": "1362a499-7dcc-4bdd-8073-13b9ce494b47",
      "source": "firecrawl/blog/mastering-firecrawl-extract-endpoint.md",
      "content": "About the Author > More articles by Bex Tuychiev\n\n[Building an Automated Price Tracking Tool\\\n\\\nBuild an automated e-commerce price tracker in Python. Learn web scraping, price monitoring, and automated alerts using Firecrawl, Streamlit, PostgreSQL.](https://www.firecrawl.dev/blog/automated-price-tracking-tutorial-python) [Web Scraping Automation: How to Run Scrapers on a Schedule\\\n\\\nLearn how to automate web scraping in Python using free tools like schedule, asyncio, cron jobs and GitHub Actions. This comprehensive guide covers local and cloud-based scheduling methods to run scrapers reliably in 2025.](https://www.firecrawl.dev/blog/automated-web-scraping-free-2025) [Automated Data Collection - A Comprehensive Guide\\\n\\\nLearn how to build robust automated data collection systems using modern tools and best practices. This guide covers everything from selecting the right tools to implementing scalable collection pipelines.](https://www.firecrawl.dev/blog/automated-data-collection-guide) [BeautifulSoup4 vs. Scrapy - A Comprehensive Comparison for Web Scraping in Python\\\n\\\nLearn the key differences between BeautifulSoup4 and Scrapy for web scraping in Python. Compare their features, performance, and use cases to choose the right tool for your web scraping needs.](https://www.firecrawl.dev/blog/beautifulsoup4-vs-scrapy-comparison) [How to Build an Automated Competitor Price Monitoring System with Python\\\n\\\nLearn how to build an automated competitor price monitoring system in Python that tracks prices across e-commerce sites, provides real-time comparisons, and maintains price history using Firecrawl, Streamlit, and GitHub Actions.](https://www.firecrawl.dev/blog/automated-competitor-price-scraping) [Scraping Company Data and Funding Information in Bulk With Firecrawl and Claude\\\n\\\nLearn how to build a web scraper in Python that gathers company details, funding rounds, and investor information from public sources like Crunchbase using Firecrawl and Claude for automated data collection and analysis.](https://www.firecrawl.dev/blog/crunchbase-scraping-with-firecrawl-claude) [Data Enrichment: A Complete Guide to Enhancing Your Data Quality\\\n\\\nLearn how to enrich your data quality with a comprehensive guide covering data enrichment tools, best practices, and real-world examples. Discover how to leverage modern solutions like Firecrawl to automate data collection, validation, and integration for better business insights.](https://www.firecrawl.dev/blog/complete-guide-to-data-enrichment) [Building an Intelligent Code Documentation RAG Assistant with DeepSeek and Firecrawl\\\n\\\nLearn how to build an intelligent documentation assistant powered by DeepSeek and RAG (Retrieval Augmented Generation) that can answer questions about any documentation website by combining language models with efficient information retrieval.](https://www.firecrawl.dev/blog/deepseek-rag-documentation-assistant)",
      "metadata": {
        "title": "Mastering the Extract Endpoint in Firecrawl",
        "url": "https://www.firecrawl.dev/blog/mastering-firecrawl-extract-endpoint"
      }
    },
    {
      "id": "d1d472f9-97ff-447a-b0a7-5c18e4d349f0",
      "source": "firecrawl/blog/category-tutorials.md",
      "content": "---\ntitle: Blog\nurl: https://www.firecrawl.dev/blog/category/tutorials\n---  \nIntroducing /extract - Get web data with a prompt [Try now](https://www.firecrawl.dev/extract)",
      "metadata": {
        "title": "Blog",
        "url": "https://www.firecrawl.dev/blog/category/tutorials"
      }
    },
    {
      "id": "17a630ab-09f2-4818-a316-b885c2aa24e9",
      "source": "firecrawl/blog/category-tutorials.md",
      "content": "Explore Articles\n\n[All](https://www.firecrawl.dev/blog) [Product Updates](https://www.firecrawl.dev/blog/category/product) [Tutorials](https://www.firecrawl.dev/blog/category/tutorials) [Customer Stories](https://www.firecrawl.dev/blog/category/customer-stories) [Tips & Resources](https://www.firecrawl.dev/blog/category/tips-and-resources)  \n[![Building an Intelligent Code Documentation RAG Assistant with DeepSeek and Firecrawl](https://www.firecrawl.dev/images/blog/deepseek_rag/deepseek-rag-documentation-assistant.jpg)\\\n**Building an Intelligent Code Documentation RAG Assistant with DeepSeek and Firecrawl** \\\nLearn how to build an intelligent documentation assistant powered by DeepSeek and RAG (Retrieval Augmented Generation) that can answer questions about any documentation website by combining language models with efficient information retrieval.\\\n\\\nBy Bex TuychievFeb 10, 2025](https://www.firecrawl.dev/blog/deepseek-rag-documentation-assistant)  \n[![Automated Data Collection - A Comprehensive Guide](https://www.firecrawl.dev/images/blog/automated_data_collection/automated-data-collection.jpg)\\\n**Automated Data Collection - A Comprehensive Guide** \\\nLearn how to build robust automated data collection systems using modern tools and best practices. This guide covers everything from selecting the right tools to implementing scalable collection pipelines.\\\n\\\nBy Bex TuychievFeb 2, 2025](https://www.firecrawl.dev/blog/automated-data-collection-guide)  \n[![Building an AI Resume Job Matching App With Firecrawl And Claude](https://www.firecrawl.dev/images/blog/resume_parser/ai-resume-parser.jpg)\\\n**Building an AI Resume Job Matching App With Firecrawl And Claude** \\\nLearn how to build an AI-powered job matching system that automatically scrapes job postings, parses resumes, evaluates opportunities using Claude, and sends Discord alerts for matching positions using Firecrawl, Streamlit, and Supabase.\\\n\\\nBy Bex TuychievFeb 1, 2025](https://www.firecrawl.dev/blog/ai-resume-parser-job-matcher-python)  \n[![Scraping Company Data and Funding Information in Bulk With Firecrawl and Claude](https://www.firecrawl.dev/images/blog/company-data-scraping/company-data-scraping.jpg)\\\n**Scraping Company Data and Funding Information in Bulk With Firecrawl and Claude** \\\nLearn how to build a web scraper in Python that gathers company details, funding rounds, and investor information from public sources like Crunchbase using Firecrawl and Claude for automated data collection and analysis.\\\n\\\nBy Bex TuychievJan 31, 2025](https://www.firecrawl.dev/blog/crunchbase-scraping-with-firecrawl-claude)  \n[![Mastering the Extract Endpoint in Firecrawl](https://www.firecrawl.dev/images/blog/extract_endpoint/mastering-extract-endpoint.jpg)\\\n**Mastering the Extract Endpoint in Firecrawl** \\\nLearn how to use Firecrawl's extract endpoint to automatically gather structured data from any website using AI. Build powerful web scrapers, create training datasets, and enrich your data without writing complex code.\\\n\\\nBy Bex TuychievJan 23, 2025](https://www.firecrawl.dev/blog/mastering-firecrawl-extract-endpoint)  \n[![How to Build a Bulk Sales Lead Extractor in Python Using AI](https://www.firecrawl.dev/images/blog/sales_lead_extractor/sales-lead-extractor.jpg)\\\n**How to Build a Bulk Sales Lead Extractor in Python Using AI** \\\nLearn how to build an automated sales lead extraction tool in Python that uses AI to scrape company information from websites, exports data to Excel, and streamlines the lead generation process using Firecrawl and Streamlit.\\\n\\\nBy Bex TuychievJan 12, 2025](https://www.firecrawl.dev/blog/sales-lead-extractor-python-ai)  \n[![Building a Trend Detection System with AI in TypeScript: A Step-by-Step Guide](https://www.firecrawl.dev/images/blog/trend_finder/trend-finder-typescript.jpg)\\\n**Building a Trend Detection System with AI in TypeScript: A Step-by-Step Guide** \\\nLearn how to build an automated trend detection system in TypeScript that monitors social media and news sites, analyzes content with AI, and sends real-time Slack alerts using Firecrawl, Together AI, and GitHub Actions.\\\n\\\nBy Bex TuychievJan 11, 2025](https://www.firecrawl.dev/blog/trend-finder-typescript)  \n[![How to Build an Automated Competitor Price Monitoring System with Python](https://www.firecrawl.dev/images/blog/competitor_price_scraping/competitor-price-scraping.jpg)\\\n**How to Build an Automated Competitor Price Monitoring System with Python** \\\nLearn how to build an automated competitor price monitoring system in Python that tracks prices across e-commerce sites, provides real-time comparisons, and maintains price history using Firecrawl, Streamlit, and GitHub Actions.\\\n\\\nBy Bex TuychievJan 6, 2025](https://www.firecrawl.dev/blog/automated-competitor-price-scraping)  \n[![BeautifulSoup4 vs. Scrapy - A Comprehensive Comparison for Web Scraping in Python](https://www.firecrawl.dev/images/blog/bs4_scrapy/bs4-vs-scrapy-comparison.jpg)\\\n**BeautifulSoup4 vs. Scrapy - A Comprehensive Comparison for Web Scraping in Python** \\\nLearn the key differences between BeautifulSoup4 and Scrapy for web scraping in Python. Compare their features, performance, and use cases to choose the right tool for your web scraping needs.\\\n\\\nBy Bex TuychievDec 24, 2024](https://www.firecrawl.dev/blog/beautifulsoup4-vs-scrapy-comparison)  \n[![15 Python Web Scraping Projects: From Beginner to Advanced](https://www.firecrawl.dev/images/blog/web_scraping_projects/python-web-scraping-projects.jpg)\\\n**15 Python Web Scraping Projects: From Beginner to Advanced** \\\nExplore 15 hands-on web scraping projects in Python, from beginner to advanced level. Learn essential concepts like data extraction, concurrent processing, and distributed systems while building real-world applications.\\\n\\\nBy Bex TuychievDec 17, 2024](https://www.firecrawl.dev/blog/python-web-scraping-projects)  \n[![How to Deploy Python Web Scrapers](https://www.firecrawl.dev/images/blog/deploying-web-scrapers/deploy-web-scrapers.jpg)\\\n**How to Deploy Python Web Scrapers** \\\nLearn how to deploy Python web scrapers using GitHub Actions, Heroku, PythonAnywhere and more.\\\n\\\nBy Bex TuychievDec 16, 2024](https://www.firecrawl.dev/blog/deploy-web-scrapers)  \n[![Why Companies Need a Data Strategy for Generative AI](https://www.firecrawl.dev/images/blog/data-strategy.jpg)\\\n**Why Companies Need a Data Strategy for Generative AI** \\\nLearn why a well-defined data strategy is essential for building robust, production-ready generative AI systems, and discover practical steps for curation, maintenance, and integration.\\\n\\\nBy Eric CiarlaDec 15, 2024](https://www.firecrawl.dev/blog/why-companies-need-a-data-strategy-for-generative-ai)  \n[![Data Enrichment: A Complete Guide to Enhancing Your Data Quality](https://www.firecrawl.dev/images/blog/data_enrichment_guide/complete-data-enrichment-guide.jpg)\\\n**Data Enrichment: A Complete Guide to Enhancing Your Data Quality** \\\nLearn how to enrich your data quality with a comprehensive guide covering data enrichment tools, best practices, and real-world examples. Discover how to leverage modern solutions like Firecrawl to automate data collection, validation, and integration for better business insights.\\\n\\\nBy Bex TuychievDec 14, 2024](https://www.firecrawl.dev/blog/complete-guide-to-data-enrichment)  \n[![A Complete Guide Scraping Authenticated Websites with cURL and Firecrawl](https://www.firecrawl.dev/images/blog/complete-guide-to-curl-authentication-firecrawl-api.jpg)\\\n**A Complete Guide Scraping Authenticated Websites with cURL and Firecrawl** \\\nLearn how to scrape login-protected websites using cURL and Firecrawl API. Step-by-step guide covering basic auth, tokens, and cookies with real examples.\\\n\\\nBy Rudrank RiyamDec 13, 2024](https://www.firecrawl.dev/blog/complete-guide-to-curl-authentication-firecrawl-api)  \n[![Building an Automated Price Tracking Tool](https://www.firecrawl.dev/images/blog/price-tracking/price-tracking.jpg)\\\n**Building an Automated Price Tracking Tool** \\\nBuild an automated e-commerce price tracker in Python. Learn web scraping, price monitoring, and automated alerts using Firecrawl, Streamlit, PostgreSQL.\\\n\\\nBy Bex TuychievDec 9, 2024](https://www.firecrawl.dev/blog/automated-price-tracking-tutorial-python)  \n[![Web Scraping Automation: How to Run Scrapers on a Schedule](https://www.firecrawl.dev/images/blog/scheduling-scrapers-images/automated-web-scraping-free-2025.jpg)\\\n**Web Scraping Automation: How to Run Scrapers on a Schedule** \\\nLearn how to automate web scraping in Python using free tools like schedule, asyncio, cron jobs and GitHub Actions. This comprehensive guide covers local and cloud-based scheduling methods to run scrapers reliably in 2025.\\\n\\\nBy Bex TuychievDec 5, 2024](https://www.firecrawl.dev/blog/automated-web-scraping-free-2025)  \n[![How to Generate Sitemaps Using Firecrawl's /map Endpoint: A Complete Guide](https://www.firecrawl.dev/images/blog/generating-sitemaps/how-to-generate-sitemap-using-firecrawl-map-endpoint.jpg)\\\n**How to Generate Sitemaps Using Firecrawl's /map Endpoint: A Complete Guide** \\\nLearn how to generate XML and visual sitemaps using Firecrawl's /map endpoint. Step-by-step guide with Python code examples, performance comparisons, and interactive visualization techniques for effective website mapping.\\\n\\\nBy Bex TuychievNov 29, 2024](https://www.firecrawl.dev/blog/how-to-generate-sitemaps-using-firecrawl-map-endpoint)  \n[![How to Use Firecrawl's Scrape API: Complete Web Scraping Tutorial](https://www.firecrawl.dev/images/blog/scrape-masterclass/mastering-scrape.jpg)\\\n**How to Use Firecrawl's Scrape API: Complete Web Scraping Tutorial** \\\nLearn how to scrape websites using Firecrawl's /scrape endpoint. Master JavaScript rendering, structured data extraction, and batch operations with Python code examples.\\\n\\\nBy Bex TuychievNov 25, 2024](https://www.firecrawl.dev/blog/mastering-firecrawl-scrape-endpoint)  \n[![How to Create an llms.txt File for Any Website](https://www.firecrawl.dev/images/blog/How-to-Create-an-llms-txt-File-for-Any-Website.jpg)\\\n**How to Create an llms.txt File for Any Website** \\\nLearn how to generate an llms.txt file for any website using the llms.txt Generator and Firecrawl.\\\n\\\nBy Eric CiarlaNov 22, 2024](https://www.firecrawl.dev/blog/How-to-Create-an-llms-txt-File-for-Any-Website)  \n[![Mastering Firecrawl's Crawl Endpoint: A Complete Web Scraping Guide](https://www.firecrawl.dev/images/blog/crawl-masterclass/images/mastering-crawl.jpg)\\\n**Mastering Firecrawl's Crawl Endpoint: A Complete Web Scraping Guide** \\\nLearn how to use Firecrawl's /crawl endpoint for efficient web scraping. Master URL control, performance optimization, and integration with LangChain for AI-powered data extraction.\\\n\\\nBy Bex TuychievNov 18, 2024](https://www.firecrawl.dev/blog/mastering-the-crawl-endpoint-in-firecrawl)  \n[![Getting Started with OpenAI's Predicted Outputs for Faster LLM Responses](https://www.firecrawl.dev/images/blog/openai-predicted-outputs.jpg)\\\n**Getting Started with OpenAI's Predicted Outputs for Faster LLM Responses** \\\nA guide to leveraging Predicted Outputs to speed up LLM tasks with GPT-4o models.\\\n\\\nBy Eric CiarlaNov 5, 2024](https://www.firecrawl.dev/blog/getting-started-with-predicted-outputs-openai)  \n[![Getting Started with Grok-2: Setup and Web Crawler Example](https://www.firecrawl.dev/images/blog/grok-2-web-crawler.jpg)\\\n**Getting Started with Grok-2: Setup and Web Crawler Example** \\\nA detailed guide on setting up Grok-2 and building a web crawler using Firecrawl.\\\n\\\nBy Nicolas CamaraOct 21, 2024](https://www.firecrawl.dev/blog/grok-2-setup-and-web-crawler-example)  \n[![OpenAI Swarm Tutorial: Create Marketing Campaigns for Any Website](https://www.firecrawl.dev/images/blog/openai-swarm.png)\\\n**OpenAI Swarm Tutorial: Create Marketing Campaigns for Any Website** \\\nA guide to building a multi-agent system using OpenAI Swarm and Firecrawl for AI-driven marketing strategies\\\n\\\nBy Nicolas CamaraOct 12, 2024](https://www.firecrawl.dev/blog/openai-swarm-agent-tutorial)  \n[![Using OpenAI's Realtime API and Firecrawl to Talk with Any Website](https://www.firecrawl.dev/images/blog/How-to-Talk-with-Any-Website-Using-OpenAIs-Realtime-API-and-Firecrawl.jpg)\\\n**Using OpenAI's Realtime API and Firecrawl to Talk with Any Website** \\\nBuild a real-time conversational agent that interacts with any website using OpenAI's Realtime API and Firecrawl.\\\n\\\nBy Nicolas CamaraOct 11, 2024](https://www.firecrawl.dev/blog/How-to-Talk-with-Any-Website-Using-OpenAIs-Realtime-API-and-Firecrawl)  \n[![Scraping Job Boards Using Firecrawl Actions and OpenAI](https://www.firecrawl.dev/images/blog/firecrawl-openai-job-scraping.jpg)\\\n**Scraping Job Boards Using Firecrawl Actions and OpenAI** \\\nA step-by-step guide to scraping job boards and extracting structured data using Firecrawl and OpenAI.\\\n\\\nBy Eric CiarlaSept 27, 2024](https://www.firecrawl.dev/blog/scrape-job-boards-firecrawl-openai)  \n[![Build a Full-Stack AI Web App in 12 Minutes](https://www.firecrawl.dev/images/blog/Build-a-Full-Stack-AI-Web-App-in-12-Minutes.png)\\\n**Build a Full-Stack AI Web App in 12 Minutes** \\\nBuild a Full-Stack AI Web App in 12 minutes with Cursor, OpenAI o1, V0, Firecrawl & Patched\\\n\\\nBy Dev DigestSep 18, 2024](https://www.firecrawl.dev/blog/Build-a-Full-Stack-AI-Web-App-in-12-Minutes)  \n[![How to Use OpenAI's o1 Reasoning Models in Your Applications](https://www.firecrawl.dev/images/blog/how-to-use-openai-o1-reasoning-models-in-applications.jpg)\\\n**How to Use OpenAI's o1 Reasoning Models in Your Applications** \\\nLearn how to harness OpenAI's latest o1 series models for complex reasoning tasks in your apps.\\\n\\\nBy Eric CiarlaSep 16, 2024](https://www.firecrawl.dev/blog/how-to-use-openai-o1-reasoning-models-in-applications)  \n[![How to Use Prompt Caching and Cache Control with Anthropic Models](https://www.firecrawl.dev/images/blog/anthropic-prompt-caching.png)\\\n**How to Use Prompt Caching and Cache Control with Anthropic Models** \\\nLearn how to cache large context prompts with Anthropic Models like Opus, Sonnet, and Haiku for faster and cheaper chats that analyze website data.\\\n\\\nBy Eric CiarlaAug 14, 2024](https://www.firecrawl.dev/blog/using-prompt-caching-with-anthropic)  \n[![Building Knowledge Graphs from Web Data using CAMEL-AI and Firecrawl](https://www.firecrawl.dev/images/blog/knowledge-graph.jpg)\\\n**Building Knowledge Graphs from Web Data using CAMEL-AI and Firecrawl** \\\nA guide on constructing knowledge graphs from web pages using CAMEL-AI and Firecrawl\\\n\\\nBy Wendong FanAug 13, 2024](https://www.firecrawl.dev/blog/building-knowledge-graphs-from-web-data-camelai-firecrawl)  \n[![How to Use OpenAI's Structured Outputs and JSON Strict Mode](https://www.firecrawl.dev/images/blog/openai-structured-output.png)\\\n**How to Use OpenAI's Structured Outputs and JSON Strict Mode** \\\nA guide for getting structured data from the latest OpenAI models.\\\n\\\nBy Eric CiarlaAug 7, 2024](https://www.firecrawl.dev/blog/using-structured-output-and-json-strict-mode-openai)  \n[![Scrape and Analyze Airbnb Data with Firecrawl and E2B](https://www.firecrawl.dev/images/blog/firecrawl-e2b-airbnb.png)\\\n**Scrape and Analyze Airbnb Data with Firecrawl and E2B** \\\nLearn how to scrape and analyze Airbnb data using Firecrawl and E2B in a few lines of code.\\\n\\\nBy Nicolas CamaraMay 23, 2024](https://www.firecrawl.dev/blog/scrape-analyze-airbnb-data-with-e2b)  \n[![Build a 'Chat with website' using Groq Llama 3](https://www.firecrawl.dev/images/blog/g4.png)\\\n**Build a 'Chat with website' using Groq Llama 3** \\\nLearn how to use Firecrawl, Groq Llama 3, and Langchain to build a 'Chat with your website' bot.\\\n\\\nBy Nicolas CamaraMay 22, 2024](https://www.firecrawl.dev/blog/chat-with-website)  \n[![Using LLM Extraction for Customer Insights](https://www.firecrawl.dev/images/blog/g3.png)\\\n**Using LLM Extraction for Customer Insights** \\\nUsing LLM Extraction for Insights and Lead Generation using Make and Firecrawl.\\\n\\\nBy Caleb PefferMay 21, 2024](https://www.firecrawl.dev/blog/lead-gen-business-insights-make-firecrawl)  \n[![Build an agent that checks for website contradictions](https://www.firecrawl.dev/images/blog/g1.png)\\\n**Build an agent that checks for website contradictions** \\\nUsing Firecrawl and Claude to scrape your website's data and look for contradictions.\\\n\\\nBy Eric CiarlaMay 19, 2024](https://www.firecrawl.dev/blog/contradiction-agent)  \n[ðŸ”¥](https://www.firecrawl.dev/)",
      "metadata": {
        "title": "Blog",
        "url": "https://www.firecrawl.dev/blog/category/tutorials"
      }
    },
    {
      "id": "f62bcaeb-8625-4f93-ae9b-bd94c5827f83",
      "source": "firecrawl/blog/category-tutorials.md",
      "content": "Ready to _Build?_\n\nStart scraping web data for your AI apps today.  \nNo credit card needed.  \nGet Started",
      "metadata": {
        "title": "Blog",
        "url": "https://www.firecrawl.dev/blog/category/tutorials"
      }
    },
    {
      "id": "bc71a198-f753-4020-b5fb-f7ba57ea8054",
      "source": "firecrawl/blog/python-web-scraping-projects.md",
      "content": "---\ntitle: 15 Python Web Scraping Projects: From Beginner to Advanced\nurl: https://www.firecrawl.dev/blog/python-web-scraping-projects\n---  \nIntroducing /extract - Get web data with a prompt [Try now](https://www.firecrawl.dev/extract)  \nDec 17, 2024  \nâ€¢  \n[![Bex Tuychiev image](https://www.firecrawl.dev/bex.jpg)Bex Tuychiev](https://x.com/bextuychiev)  \n# 15 Python Web Scraping Projects: From Beginner to Advanced  \n![15 Python Web Scraping Projects: From Beginner to Advanced image](https://www.firecrawl.dev/images/blog/web_scraping_projects/python-web-scraping-projects.jpg)",
      "metadata": {
        "title": "15 Python Web Scraping Projects: From Beginner to Advanced",
        "url": "https://www.firecrawl.dev/blog/python-web-scraping-projects"
      }
    },
    {
      "id": "5d91c602-23d3-48f2-982f-29d0ab75d3fe",
      "source": "firecrawl/blog/python-web-scraping-projects.md",
      "content": "Introduction\n\nWeb scraping is one of the most powerful tools in a programmerâ€™s arsenal, allowing you to gather data from across the internet automatically. It has countless applications like market research, competitive analysis, [price monitoring](https://www.firecrawl.dev/blog/automated-price-tracking-tutorial-python), and data-driven decision making. The ability to extract structured data from web pages opens up endless possibilities for automation and analysis.  \nThis guide outlines 15 web scraping project ideas in Python that progress from basic concepts to advanced techniques. Each project includes learning objectives, key technical concepts, and a structured development roadmap. While this guide doesnâ€™t provide complete code implementations, it serves as a blueprint for your web scraping journey - helping you understand what to build and how to approach each challenge systematically.  \nLetâ€™s begin by understanding the available tools and setting up our development environment. Then weâ€™ll explore each project outline in detail, giving you a solid foundation to start building your own web scraping solutions.",
      "metadata": {
        "title": "15 Python Web Scraping Projects: From Beginner to Advanced",
        "url": "https://www.firecrawl.dev/blog/python-web-scraping-projects"
      }
    },
    {
      "id": "2c06671f-0808-41e6-a631-b3816e37cf54",
      "source": "firecrawl/blog/python-web-scraping-projects.md",
      "content": "Table of Contents\n\n1. [Introduction](https://www.firecrawl.dev/blog/python-web-scraping-projects#introduction)\n2. [Prerequisites](https://www.firecrawl.dev/blog/python-web-scraping-projects#prerequisites)\n- [Required Skills](https://www.firecrawl.dev/blog/python-web-scraping-projects#required-skills)\n- [Technical Requirements](https://www.firecrawl.dev/blog/python-web-scraping-projects#technical-requirements)\n- [Optional but Helpful](https://www.firecrawl.dev/blog/python-web-scraping-projects#optional-but-helpful)\n- [Time Commitment](https://www.firecrawl.dev/blog/python-web-scraping-projects#time-commitment)\n3. [Comparing Python Web Scraping Frameworks](https://www.firecrawl.dev/blog/python-web-scraping-projects#comparing-python-web-scraping-frameworks-for-your-projects)\n- [BeautifulSoup4](https://www.firecrawl.dev/blog/python-web-scraping-projects#beautifulsoup4)\n- [Selenium](https://www.firecrawl.dev/blog/python-web-scraping-projects#selenium)\n- [Scrapy](https://www.firecrawl.dev/blog/python-web-scraping-projects#scrapy)\n- [Firecrawl](https://www.firecrawl.dev/blog/python-web-scraping-projects#firecrawl)\n4. [Setting Up Your Web Scraping Environment](https://www.firecrawl.dev/blog/python-web-scraping-projects#setting-up-your-web-scraping-environment) 5. [Beginner Web Scraping Projects](https://www.firecrawl.dev/blog/python-web-scraping-projects#beginner-web-scraping-projects)\n1. [Weather Data Scraper](https://www.firecrawl.dev/blog/python-web-scraping-projects#1-weather-data-scraper)\n2. [News Headlines Aggregator](https://www.firecrawl.dev/blog/python-web-scraping-projects#2-news-headlines-aggregator)\n3. [Book Price Tracker](https://www.firecrawl.dev/blog/python-web-scraping-projects#3-book-price-tracker)\n4. [Recipe Collector](https://www.firecrawl.dev/blog/python-web-scraping-projects#4-recipe-collector)\n5. [Job Listing Monitor](https://www.firecrawl.dev/blog/python-web-scraping-projects#5-job-listing-monitor)\n5. [Intermediate Web Scraping Projects](https://www.firecrawl.dev/blog/python-web-scraping-projects#intermediate-web-scraping-projects)\n1. [E-commerce Price Comparison Tool](https://www.firecrawl.dev/blog/python-web-scraping-projects#1-e-commerce-price-comparison-tool)\n2. [Social Media Analytics Tool](https://www.firecrawl.dev/blog/python-web-scraping-projects#2-social-media-analytics-tool)\n3. [Real Estate Market Analyzer](https://www.firecrawl.dev/blog/python-web-scraping-projects#3-real-estate-market-analyzer)\n4. [Academic Research Aggregator](https://www.firecrawl.dev/blog/python-web-scraping-projects#4-academic-research-aggregator)\n5. [Financial Market Data Analyzer](https://www.firecrawl.dev/blog/python-web-scraping-projects#5-financial-market-data-analyzer)\n6. [Advanced Web Scraping Projects](https://www.firecrawl.dev/blog/python-web-scraping-projects#advanced-web-scraping-projects)\n1. [Multi-threaded News Aggregator](https://www.firecrawl.dev/blog/python-web-scraping-projects#1-multi-threaded-news-aggregator)\n2. [Distributed Web Archive System](https://www.firecrawl.dev/blog/python-web-scraping-projects#2-distributed-web-archive-system)\n3. [Automated Market Research Tool](https://www.firecrawl.dev/blog/python-web-scraping-projects#3-automated-market-research-tool)\n4. [Competitive Intelligence Dashboard](https://www.firecrawl.dev/blog/python-web-scraping-projects#4-competitive-intelligence-dashboard)\n5. [Full-Stack Scraping Platform](https://www.firecrawl.dev/blog/python-web-scraping-projects#5-full-stack-scraping-platform)\n7. [Conclusion](https://www.firecrawl.dev/blog/python-web-scraping-projects#conclusion)",
      "metadata": {
        "title": "15 Python Web Scraping Projects: From Beginner to Advanced",
        "url": "https://www.firecrawl.dev/blog/python-web-scraping-projects"
      }
    },
    {
      "id": "604c42ad-62f2-44d1-a5ee-5c428dca82e0",
      "source": "firecrawl/blog/python-web-scraping-projects.md",
      "content": "Prerequisites > Required Skills\n\n- Basic Python programming experience:\n- Variables, data types, and operators\n- Control structures (if/else, loops)\n- Functions and basic error handling\n- Working with lists and dictionaries\n- Reading/writing files\n- Installing and importing packages\n- Basic web knowledge:\n- Understanding of HTML structure\n- Ability to use browser developer tools (inspect elements)\n- Basic CSS selectors (class, id, tag selection)\n- Understanding of URLs and query parameters\n- Development environment:\n- Python 3.x installed\n- Ability to use command line/terminal\n- Experience with pip package manager\n- Text editor or IDE (VS Code, PyCharm, etc.)",
      "metadata": {
        "title": "15 Python Web Scraping Projects: From Beginner to Advanced",
        "url": "https://www.firecrawl.dev/blog/python-web-scraping-projects"
      }
    },
    {
      "id": "f16e62d8-dfe0-440c-a956-38a0935689c8",
      "source": "firecrawl/blog/python-web-scraping-projects.md",
      "content": "Prerequisites > Technical Requirements\n\n- Computer with internet connection\n- Modern web browser with developer tools\n- Python 3.7+ installed\n- Ability to install Python packages via pip\n- Basic understanding of virtual environments",
      "metadata": {
        "title": "15 Python Web Scraping Projects: From Beginner to Advanced",
        "url": "https://www.firecrawl.dev/blog/python-web-scraping-projects"
      }
    },
    {
      "id": "6be935db-9ca0-4f19-9271-f0533417ef7c",
      "source": "firecrawl/blog/python-web-scraping-projects.md",
      "content": "Prerequisites > Optional but Helpful\n\n- Understanding of:\n- HTTP methods (GET, POST)\n- JSON and CSV data formats\n- Basic regular expressions\n- Simple database concepts\n- Git version control\n- Experience with:\n- pandas library for data manipulation\n- Basic data visualization\n- API interactions\n- Web browser automation",
      "metadata": {
        "title": "15 Python Web Scraping Projects: From Beginner to Advanced",
        "url": "https://www.firecrawl.dev/blog/python-web-scraping-projects"
      }
    },
    {
      "id": "c3a6d417-ebd6-4ce4-a64d-32ef0dcb7cf9",
      "source": "firecrawl/blog/python-web-scraping-projects.md",
      "content": "Prerequisites > Time Commitment\n\n- 2-4 hours for setup and environment configuration\n- 4-8 hours per beginner project\n- Regular practice for skill improvement  \nIf youâ€™re new to web scraping, we recommend starting with the Weather Data Scraper or Recipe Collector projects, as they involve simpler website structures and basic data extraction patterns. The News Headlines Aggregator and Job Listing Monitor projects are more complex and might require additional learning about handling multiple data sources and pagination.",
      "metadata": {
        "title": "15 Python Web Scraping Projects: From Beginner to Advanced",
        "url": "https://www.firecrawl.dev/blog/python-web-scraping-projects"
      }
    },
    {
      "id": "55c125f7-9069-4e56-afc4-903c62c08791",
      "source": "firecrawl/blog/python-web-scraping-projects.md",
      "content": "Comparing Python Web Scraping Frameworks For Your Projects\n\nWhen starting with web scraping in Python, youâ€™ll encounter several popular frameworks. Each has its strengths and ideal use cases. Letâ€™s compare the main options to help you choose the right tool for your needs.",
      "metadata": {
        "title": "15 Python Web Scraping Projects: From Beginner to Advanced",
        "url": "https://www.firecrawl.dev/blog/python-web-scraping-projects"
      }
    },
    {
      "id": "7108df33-ebdf-4bd6-90ef-4a7a5d37aa70",
      "source": "firecrawl/blog/python-web-scraping-projects.md",
      "content": "Comparing Python Web Scraping Frameworks For Your Projects > BeautifulSoup4\n\nBeautifulSoup4 (BS4) is one of the most popular Python libraries for web scraping. It provides a simple and intuitive way to parse HTML and XML documents by creating a parse tree that can be navigated and searched. BS4 excels at extracting data from static web pages where JavaScript rendering isnâ€™t required. The library works by transforming HTML code into a tree of Python objects, making it easy to locate and extract specific elements using methods like `find()` and `find_all()`. While it lacks some advanced features found in other frameworks, its simplicity and ease of use make it an excellent choice for beginners and straightforward scraping tasks.  \nPros:  \n- Easy to learn and use\n- Excellent documentation\n- Great for parsing HTML/XML\n- Lightweight and minimal dependencies  \nCons:  \n- No JavaScript rendering\n- Limited to basic HTML parsing\n- No built-in download features\n- Can be slow for large-scale scraping  \nExample usage:  \n```python\nfrom bs4 import BeautifulSoup\nimport requests\n\nresponse = requests.get('https://example.com')\nsoup = BeautifulSoup(response.text, 'html.parser')\ntitles = soup.find_all('h1')\n\n```",
      "metadata": {
        "title": "15 Python Web Scraping Projects: From Beginner to Advanced",
        "url": "https://www.firecrawl.dev/blog/python-web-scraping-projects"
      }
    },
    {
      "id": "9a60068d-fef6-4355-b1c2-13002180d53b",
      "source": "firecrawl/blog/python-web-scraping-projects.md",
      "content": "Comparing Python Web Scraping Frameworks For Your Projects > Selenium\n\nSelenium is a powerful web automation framework that can control web browsers programmatically. Originally designed for web application testing, it has become a popular choice for web scraping, especially when dealing with dynamic websites that require JavaScript rendering. Selenium works by automating a real web browser, allowing it to interact with web pages just like a human user would - clicking buttons, filling forms, and handling dynamic content. This makes it particularly useful for scraping modern web applications where content is loaded dynamically through JavaScript.  \nPros:  \n- Handles JavaScript-rendered content\n- Supports browser automation\n- Can interact with web elements\n- Good for testing and scraping  \nCons:  \n- Resource-intensive\n- Slower than other solutions\n- Requires browser drivers\n- Complex setup and maintenance  \nExample Usage:  \n```python\nfrom selenium import webdriver\nfrom selenium.webdriver.common.by import By\n\ndriver = webdriver.Chrome()\ndriver.get(\"https://example.com\")\nelements = driver.find_elements(By.CLASS_NAME, \"product-title\")\n\n```",
      "metadata": {
        "title": "15 Python Web Scraping Projects: From Beginner to Advanced",
        "url": "https://www.firecrawl.dev/blog/python-web-scraping-projects"
      }
    },
    {
      "id": "f768230b-cfaf-4c25-bffe-a644e062e725",
      "source": "firecrawl/blog/python-web-scraping-projects.md",
      "content": "Comparing Python Web Scraping Frameworks For Your Projects > Scrapy\n\nScrapy is a comprehensive web scraping framework that provides a complete solution for extracting data from websites at scale. Itâ€™s designed as a fast, powerful, and extensible framework that can handle complex scraping tasks efficiently. Unlike simpler libraries, Scrapy provides a full suite of features including a crawling engine, data processing pipelines, and middleware components. It follows the principle of â€œbatteries includedâ€ while remaining highly customizable for specific needs. Scrapy is particularly well-suited for large-scale scraping projects where performance and reliability are crucial.  \nPros:  \n- High performance\n- Built-in pipeline processing\n- Extensive middleware support\n- Robust error handling  \nCons:  \n- Steep learning curve\n- Complex configuration\n- Limited JavaScript support\n- Overkill for simple projects  \nExample Usage:  \n```python\nimport scrapy\n\nclass ProductSpider(scrapy.Spider):\nname = \"products\"\nstart_urls = [\"https://example.com\"]\n\ndef parse(self, response):\nfor product in response.css(\".product\"):\nyield {\n\"name\": product.css(\".title::text\").get(),\n\"price\": product.css(\".price::text\").get(),\n}\n\n```",
      "metadata": {
        "title": "15 Python Web Scraping Projects: From Beginner to Advanced",
        "url": "https://www.firecrawl.dev/blog/python-web-scraping-projects"
      }
    },
    {
      "id": "3b722bf3-69cf-4e99-8ba2-7037bf180912",
      "source": "firecrawl/blog/python-web-scraping-projects.md",
      "content": "Comparing Python Web Scraping Frameworks For Your Projects > Firecrawl\n\nFirecrawl represents a paradigm shift in web scraping by using AI to eliminate traditional scraping bottlenecks. Unlike conventional frameworks that require manual selector maintenance, Firecrawl uses natural language understanding to automatically identify and extract HTML element content based on semantic descriptions. This approach directly addresses the primary challenges faced in the projects outlined in this guide:  \n1. Development speed\n- Traditional approach: Writing selectors, handling JavaScript, managing anti-bot measures (~2-3 days per site)\n- Firecrawl approach: Define data schema, let AI handle extraction (~30 minutes per site)\n2. Maintenance requirements\n- Traditional approach: Regular updates when sites change, selector fixes, anti-bot adaptations\n- Firecrawl approach: Schema remains stable, AI adapts to site changes automatically\n3. Project implementation\n- For the e-commerce projects: Built-in handling of dynamic pricing, AJAX requests, and anti-bot measures\n- For news aggregation: Automatic content classification and extraction across different layouts\n- For market research: Seamless handling of multiple site structures and authentication flows  \nPros:  \n- AI-powered content extraction eliminates selector maintenance\n- Automatic handling of JavaScript-rendered content\n- Built-in anti-bot measures with enterprise-grade reliability\n- Multiple output formats (JSON, CSV, structured objects)\n- Site change resilience through semantic understanding\n- Consistent extraction across different page layouts  \nCons:  \n- Paid service (consider ROI vs. development time)\n- API-dependent architecture\n- Less granular control over parsing process\n- May be overkill for simple, static sites\n- Slower for large-scale operations  \nExample Implementation:  \n```python\nfrom firecrawl import FirecrawlApp\nfrom pydantic import BaseModel, Field\n\nclass Product(BaseModel):\nname: str = Field(description=\"The product name and title\")\nprice: float = Field(description=\"The current price in USD\")\ndescription: str = Field(description=\"The product description text\")\nrating: float = Field(description=\"The average customer rating out of 5 stars\")\nnum_reviews: int = Field(description=\"The total number of customer reviews\")\navailability: str = Field(description=\"The current availability status\")\nbrand: str = Field(description=\"The product manufacturer or brand\")\ncategory: str = Field(description=\"The product category or department\")\nasin: str = Field(description=\"The Amazon Standard Identification Number\")\n\napp = FirecrawlApp()\ndata = app.scrape_url(\n'https://www.amazon.com/gp/product/1718501900', # A sample Amazon product\nparams={\n\"formats\": ['extract'],\n\"extract\": {\n\"schema\": Product.model_json_schema()\n}\n}\n)\n\n```  \nThis example demonstrates how Firecrawl reduces complex e-commerce scraping to a simple schema definition. The same approach applies to all projects in this guide, potentially reducing development time from weeks to days. For production environments where reliability and maintenance efficiency are crucial, this automated approach often proves more cost-effective than maintaining custom scraping infrastructure.  \n* * *  \nHere is a table summarizing the differences between these tools:  \n| Tool | Best For | Learning Curve | Key Features |\n| --- | --- | --- | --- |\n| BeautifulSoup4 | Static websites, Beginners | Easy | Simple API, Great documentation |\n| Selenium | Dynamic websites, Browser automation | Moderate | Full browser control, JavaScript support |\n| Scrapy | Large-scale projects | Steep | High performance, Extensive features |\n| Firecrawl | Production use, AI-powered scraping | Easy | Low maintenance, Built-in anti-bot |  \nUseful Resources:  \n- [BeautifulSoup4 documentation](https://www.crummy.com/software/BeautifulSoup/bs4/doc/)\n- [Selenium documentation](https://www.selenium.dev/documentation/)\n- [Scrapy documentation](https://docs.scrapy.org/)\n- [Firecrawl documentation](https://firecrawl.dev/)\n- [Introduction to web scraping in Python tutorial](https://realpython.com/python-web-scraping-practical-introduction/)  \nWith these tools and resources at your disposal, youâ€™re ready to start exploring web scraping in Python. Letâ€™s move on to setting up your environment.",
      "metadata": {
        "title": "15 Python Web Scraping Projects: From Beginner to Advanced",
        "url": "https://www.firecrawl.dev/blog/python-web-scraping-projects"
      }
    },
    {
      "id": "4d8bb155-c1de-41d5-b885-91581e8a01da",
      "source": "firecrawl/blog/python-web-scraping-projects.md",
      "content": "Setting Up Your Web Scraping Environment\n\nBefore diving into the projects, letâ€™s set up our Python environment with the necessary tools and libraries. Weâ€™ll create a virtual environment and install the required packages.  \n1. Create and activate a virtual environment  \n```bash\n# Create a new virtual environment\npython -m venv scraping-env\n\n# Activate virtual environment\n# On Windows:\nscraping-envScriptsactivate\n\n# On macOS/Linux:\nsource scraping-env/bin/activate\n\n```  \n2. Install Required Packages  \n```bash\npip install requests beautifulsoup4 selenium scrapy firecrawl-py pandas\n\n```  \n3. Additional Setup for Selenium  \nIf you plan to use Selenium, youâ€™ll need to install a webdriver. For Chrome:  \n```bash\npip install webdriver-manager\n\n```  \n4. Basic Project Structure  \nCreate a basic project structure to organize your code:  \n```bash\nmkdir web_scraping_projects\ncd web_scraping_projects\ntouch requirements.txt\n\n```  \nAdd the dependencies to `requirements.txt`:  \n```text\nrequests>=2.31.0\nbeautifulsoup4>=4.12.2\nselenium>=4.15.2\nscrapy>=2.11.0\nfirecrawl-py>=0.1.0\npandas>=2.1.3\nwebdriver-manager>=4.0.1\n\n```  \n5. Important Notes  \n- Always check a websiteâ€™s robots.txt file before scraping\n- Implement proper delays between requests (rate limiting)\n- Consider using a user agent string to identify your scraper\n- Handle errors and exceptions appropriately\n- Store your API keys and sensitive data in environment variables  \nWith this environment set up, youâ€™ll be ready to tackle any of the projects in this tutorial, from beginner to advanced level. Each project may require additional specific setup steps, which will be covered in their respective sections.",
      "metadata": {
        "title": "15 Python Web Scraping Projects: From Beginner to Advanced",
        "url": "https://www.firecrawl.dev/blog/python-web-scraping-projects"
      }
    },
    {
      "id": "e69dc903-363b-44e5-a52f-8aa9c382eb37",
      "source": "firecrawl/blog/python-web-scraping-projects.md",
      "content": "Beginner Web Scraping Projects\n\nLetâ€™s start with some beginner-friendly web scraping projects that will help you build foundational skills.",
      "metadata": {
        "title": "15 Python Web Scraping Projects: From Beginner to Advanced",
        "url": "https://www.firecrawl.dev/blog/python-web-scraping-projects"
      }
    },
    {
      "id": "61006589-0df5-4ad7-97b5-8b18f5614d89",
      "source": "firecrawl/blog/python-web-scraping-projects.md",
      "content": "Beginner Web Scraping Projects > 1. Weather Data Scraper\n\nA real-time weather data scraper for weather.com extracts temperature, humidity, wind speed and precipitation forecasts. The project serves as an introduction to fundamental web scraping concepts including HTTP requests, HTML parsing, and error handling.  \nThis beginner-friendly project demonstrates proper web scraping practices through practical application, with opportunities to expand into historical trend analysis and multi-location comparisons. The core focus is on DOM navigation, rate limiting implementation, and efficient data storage techniques.  \n**Learning objectives**:  \n- Understanding HTML structure and basic DOM elements\n- Making HTTP requests\n- Parsing simple HTML responses\n- Handling basic error cases  \n**Proposed project steps**:  \n1. Set up your development environment:\n- Install required libraries (requests, beautifulsoup4)\n- Create a new Python script file\n- Configure your IDE/editor\n2. Analyze the weather website structure:\n- Open browser developer tools (F12)\n- Inspect HTML elements for weather data\n- Document CSS selectors for key elements\n- Check robots.txt for scraping permissions\n3. Build the basic scraper structure:\n- Create a WeatherScraper class\n- Add methods for making HTTP requests\n- Implement user agent rotation\n- Add request delay functionality\n4. Implement data extraction:\n- Write methods to parse temperature\n- Extract humidity percentage\n- Get wind speed and direction\n- Collect precipitation forecast\n- Parse â€œfeels likeâ€ temperature\n- Get weather condition description\n5. Add error handling and validation:\n- Implement request timeout handling\n- Add retry logic for failed requests\n- Validate extracted data types\n- Handle missing data scenarios\n- Log errors and exceptions\n6. Create data storage functionality:\n- Design CSV file structure\n- Implement data cleaning\n- Add timestamp to records\n- Create append vs overwrite options\n- Include location information\n7. Test and refine:\n- Test with multiple locations\n- Verify data accuracy\n- Optimize request patterns\n- Add data validation checks\n- Document known limitations  \n**Key concepts to learn**:  \n- HTTP requests and responses\n- HTML parsing basics\n- CSS selectors and HTML class/id attributes\n- Data extraction patterns\n- Basic error handling  \n**Website suggestions**:  \n- [weather.com](https://weather.com/) - Main weather data source with comprehensive information\n- [accuweather.com](https://accuweather.com/) - Alternative source with detailed forecasts\n- [weatherunderground.com](https://weatherunderground.com/) - Community-driven weather data\n- [openweathermap.org](https://openweathermap.org/) - Free API available for learning\n- [forecast.weather.gov](https://forecast.weather.gov/) - Official US weather data source",
      "metadata": {
        "title": "15 Python Web Scraping Projects: From Beginner to Advanced",
        "url": "https://www.firecrawl.dev/blog/python-web-scraping-projects"
      }
    },
    {
      "id": "f24c0660-fa43-4633-9fff-53aac31ee976",
      "source": "firecrawl/blog/python-web-scraping-projects.md",
      "content": "Beginner Web Scraping Projects > 2. News Headlines Aggregator\n\nA news headline aggregation system that pulls together breaking stories and trending content from multiple online news sources. The automated scraping engine visits major news websites on a schedule, extracting headlines, metadata, and key details into a unified data stream. The consolidated feed gives users a single interface to monitor news across publishers while handling the complexity of different site structures, update frequencies, and content formats behind the scenes.  \n**Learning Objectives**:  \n- Working with multiple data sources\n- Handling different HTML structures\n- Implementing proper delays between requests\n- Basic data deduplication  \n**Project steps**:  \n1. Initial website selection and analysis\n- Choose 2-3 news websites from suggested list\n- Document each siteâ€™s robots.txt rules\n- Identify optimal request intervals\n- Map out common headline patterns\n- Note any access restrictions\n2. HTML structure analysis\n- Inspect headline container elements\n- Document headline text selectors\n- Locate timestamp information\n- Find article category/section tags\n- Map author and source attribution\n- Identify image thumbnail locations\n3. Data model design\n- Define headline object structure\n- Create schema for metadata fields\n- Plan timestamp standardization\n- Design category classification\n- Structure source tracking fields\n- Add URL and unique ID fields\n4. Individual scraper development\n- Build base scraper class\n- Implement site-specific extractors\n- Add request delay handling\n- Include user-agent rotation\n- Set up error logging\n- Add data validation checks\n5. Data processing and storage\n- Implement text cleaning\n- Normalize timestamps\n- Remove duplicate headlines\n- Filter unwanted content\n- Create CSV/JSON export\n- Set up incremental updates\n6. Integration and testing\n- Combine multiple scrapers\n- Add master scheduler\n- Test with different intervals\n- Validate combined output\n- Monitor performance\n- Document limitations  \n**Key concepts to learn**:  \n- Rate limiting and polite scraping\n- Working with multiple websites\n- Text normalization\n- Basic data structures for aggregation\n- Time handling in Python  \n**Website suggestions**:  \n- [reuters.com](https://reuters.com/) - Major international news agency\n- [apnews.com](https://apnews.com/) - Associated Press news wire service\n- [bbc.com/news](https://bbc.com/news) - International news coverage\n- [theguardian.com](https://theguardian.com/) - Global news with good HTML structure\n- [aljazeera.com](https://aljazeera.com/) - International perspective on news",
      "metadata": {
        "title": "15 Python Web Scraping Projects: From Beginner to Advanced",
        "url": "https://www.firecrawl.dev/blog/python-web-scraping-projects"
      }
    },
    {
      "id": "13ead182-2182-49a6-9575-82f8df8989ad",
      "source": "firecrawl/blog/python-web-scraping-projects.md",
      "content": "Beginner Web Scraping Projects > 3. Book Price Tracker\n\nDevelop an automated price monitoring system that continuously scans multiple online bookstores to track price fluctuations for specific books. The tool will maintain a watchlist of titles, periodically check their current prices, and notify users when prices drop below certain thresholds or when significant discounts become available. This enables book enthusiasts to make cost-effective purchasing decisions by capitalizing on temporary price reductions across different retailers.  \n**Learning objectives**:  \n- Persistent data storage\n- Price extraction and normalization\n- Basic automation concepts\n- Simple alert systems  \n**Project steps**:  \n1. Analyze target bookstores  \n- Research and select online bookstores to monitor\n- Study website structures and price display patterns\n- Document required headers and request parameters\n- Test rate limits and access restrictions  \n2. Design data storage  \n- Create database tables for books and price history\n- Define schema for watchlists and price thresholds\n- Plan price tracking and comparison logic\n- Set up automated backups  \n3. Build price extraction system  \n- Implement separate scrapers for each bookstore\n- Extract prices, availability and seller info\n- Handle different currencies and formats\n- Add error handling and retries\n- Validate extracted data  \n4. Implement automation  \n- Set up scheduled price checks\n- Configure appropriate delays between requests\n- Track successful/failed checks\n- Implement retry logic for failures\n- Monitor system performance  \n5. Add notification system  \n- Create price threshold triggers\n- Set up email notifications\n- Add price drop alerts\n- Generate price history reports\n- Allow customizable alert preferences  \n**Key concepts to learn**:  \n- Database basics (SQLite or similar)\n- Regular expressions for price extraction\n- Scheduling with Python\n- Email notifications\n- Data comparison logic  \n**Website suggestions**:  \n- [amazon.com](https://amazon.com/) - Large selection and dynamic pricing\n- [bookdepository.com](https://bookdepository.com/) - International book retailer\n- [barnesandnoble.com](https://barnesandnoble.com/) - Major US book retailer\n- [abebooks.com](https://abebooks.com/) - Used and rare books marketplace\n- [bookfinder.com](https://bookfinder.com/) - Book price comparison site",
      "metadata": {
        "title": "15 Python Web Scraping Projects: From Beginner to Advanced",
        "url": "https://www.firecrawl.dev/blog/python-web-scraping-projects"
      }
    },
    {
      "id": "faca8b37-0788-47eb-8010-3b02755aa29b",
      "source": "firecrawl/blog/python-web-scraping-projects.md",
      "content": "Beginner Web Scraping Projects > 4. Recipe Collector\n\nBuild an automated recipe scraping tool that collects detailed cooking information from food websites. The system will extract comprehensive recipe data including ingredient lists with measurements, step-by-step preparation instructions, cooking durations, serving sizes, and nutritional facts. This tool enables home cooks to easily aggregate and organize recipes from multiple sources into a standardized format.  \n**Learning objectives**:  \n- Handling nested HTML structures\n- Extracting structured data\n- Text cleaning and normalization\n- Working with lists and complex data types  \n**Project steps**:  \n1. Analyze recipe website structures\n- Study HTML structure of target recipe sites\n- Identify common patterns for recipe components\n- Document CSS selectors and XPaths for key elements\n- Map variations between different sites\n2. Design a recipe data model\n- Create database schema for recipes\n- Define fields for ingredients, instructions, metadata\n- Plan data types and relationships\n- Add support for images and rich media\n- Include tags and categories\n3. Implement extraction logic for recipe components\n- Build scrapers for each target website\n- Extract recipe title and description\n- Parse ingredient lists with quantities and units\n- Capture step-by-step instructions\n- Get cooking times and temperatures\n- Collect serving size information\n- Extract nutritional data\n- Download recipe images\n4. Clean and normalize extracted data\n- Standardize ingredient measurements\n- Convert temperature units\n- Normalize cooking durations\n- Clean up formatting and special characters\n- Handle missing or incomplete data\n- Validate data consistency\n- Remove duplicate recipes\n5. Store recipes in a structured format\n- Save to SQL/NoSQL database\n- Export options to JSON/YAML\n- Generate printable recipe cards\n- Add search and filtering capabilities\n- Implement recipe categorization\n- Create backup system  \n**Key concepts to learn**:  \n- Complex HTML navigation\n- Data cleaning techniques\n- JSON/YAML data formats\n- Nested data structures\n- Text processing  \n**Website suggestions**:  \n- [allrecipes.com](https://allrecipes.com/) - Large recipe database\n- [foodnetwork.com](https://foodnetwork.com/) - Professional recipes\n- [epicurious.com](https://epicurious.com/) - Curated recipe collection\n- [simplyrecipes.com](https://simplyrecipes.com/) - Well-structured recipes\n- [food.com](https://food.com/) - User-submitted recipes",
      "metadata": {
        "title": "15 Python Web Scraping Projects: From Beginner to Advanced",
        "url": "https://www.firecrawl.dev/blog/python-web-scraping-projects"
      }
    },
    {
      "id": "f3c6fda0-8e34-45f3-8f08-a84bd44599a2",
      "source": "firecrawl/blog/python-web-scraping-projects.md",
      "content": "Beginner Web Scraping Projects > 5. Job Listing Monitor\n\nCreate an automated job search monitoring tool that continuously scans multiple job listing websites for new positions matching user-defined criteria. The tool will track key details like job titles, companies, locations, salaries, and requirements. Users can specify search filters such as keywords, experience level, job type (remote/hybrid/onsite), and salary range. The system will store listings in a database and notify users of new matches via email or other alerts. This helps job seekers stay on top of opportunities without manually checking multiple sites.  \nThe tool can integrate with major job boards like LinkedIn, Indeed, Glassdoor and company career pages. It will handle different site structures, login requirements, and listing formats while respecting rate limits and terms of service. Advanced features could include sentiment analysis of job descriptions, automatic resume submission, and tracking application status across multiple positions.  \n**Learning objectives**:  \n- Working with search parameters\n- Handling pagination\n- Form submission\n- Data filtering  \n**Project steps**:  \n1. Set up initial project structure and dependencies\n- Create virtual environment\n- Install required libraries\n- Set up database (SQLite/PostgreSQL)\n- Configure logging and error handling\n- Set up email notification system\n2. Implement site-specific scrapers\n- Analyze HTML structure of each job board\n- Handle authentication if required\n- Create separate scraper classes for each site (one is enough if you are using Firecrawl)\n- Implement rate limiting and rotating user agents\n- Add proxy support for avoiding IP blocks\n- Handle JavaScript-rendered content with Selenium (no need if you are using Firecrawl)\n3. Build search parameter system\n- Create configuration for search criteria\n- Implement URL parameter generation\n- Handle different parameter formats per site\n- Add validation for search inputs\n- Support multiple search profiles\n- Implement location-based searching\n4. Develop listing extraction logic\n- Extract job details (title, company, location, etc)\n- Parse salary information\n- Clean and standardize data format\n- Handle missing/incomplete data\n- Extract application requirements\n- Identify remote/hybrid/onsite status\n- Parse required skills and experience\n5. Create storage and monitoring system\n- Design database schema\n- Implement data deduplication\n- Track listing history/changes\n- Set up automated monitoring schedule\n- Create email alert templates\n- Build basic web interface for results\n- Add export functionality  \n**Key concepts to learn**:  \n- URL parameters and query strings\n- HTML forms and POST requests\n- Pagination handling\n- Data filtering techniques\n- Incremental data updates  \n**Website suggestions**:  \n- [linkedin.com](https://linkedin.com/) - Professional networking and job site\n- [indeed.com](https://indeed.com/) - Large job search engine\n- [glassdoor.com](https://glassdoor.com/) - Company reviews and job listings\n- [monster.com](https://monster.com/) - Global job search platform\n- [dice.com](https://dice.com/) - Technology job board\n- [careerbuilder.com](https://careerbuilder.com/) - Major US job site",
      "metadata": {
        "title": "15 Python Web Scraping Projects: From Beginner to Advanced",
        "url": "https://www.firecrawl.dev/blog/python-web-scraping-projects"
      }
    },
    {
      "id": "fdc69708-dc15-414c-9daa-ac99611368cd",
      "source": "firecrawl/blog/python-web-scraping-projects.md",
      "content": "Intermediate Web Scraping Projects\n\nThese projects build upon basic scraping concepts and introduce more complex scenarios and techniques.",
      "metadata": {
        "title": "15 Python Web Scraping Projects: From Beginner to Advanced",
        "url": "https://www.firecrawl.dev/blog/python-web-scraping-projects"
      }
    },
    {
      "id": "407042e7-d241-4da3-bcfa-060f0749d8de",
      "source": "firecrawl/blog/python-web-scraping-projects.md",
      "content": "Intermediate Web Scraping Projects > 1. E-commerce Price Comparison Tool\n\nBuild a sophisticated price comparison system monitoring major e-commerce platforms like Amazon, eBay, Walmart and Best Buy. The tool tracks products via SKUs and model numbers, scraping pricing data at configurable intervals. It normalizes data by mapping equivalent items and standardizing prices, shipping costs, and seller information across platforms.  \nA dashboard interface displays historical price trends, sends price drop alerts via email/SMS, and recommends optimal purchase timing based on seasonal patterns and historical lows. The system handles JavaScript-rendered content, dynamic AJAX requests, and anti-bot measures while maintaining data in both SQL and NoSQL stores.  \nKey technical challenges include managing product variants, currency conversion, and adapting to frequent site layout changes while ensuring data accuracy and consistency.  \nRead our separate guide on [building an Amazon price tracking application](https://www.firecrawl.dev/blog/automated-price-tracking-tutorial-python) using Firecrawl for the basic version of this project.  \n**Learning objectives**:  \n- Multi-site data aggregation\n- Price normalization techniques\n- Advanced rate limiting\n- Proxy rotation\n- Database optimization  \n**Project steps**:  \n1. Design system architecture\n- Plan database schema for products and prices\n- Design API structure for data access\n- Set up proxy management system\n- Configure rate limiting rules\n- Plan data update intervals\n2. Implement core scraping functionality\n- Create base scraper class\n- Add proxy rotation mechanism\n- Implement user agent rotation\n- Set up request queuing\n- Add retry logic\n- Handle JavaScript rendering\n- Configure session management\n3. Build product matching system\n- Implement product identification\n- Create fuzzy matching algorithms\n- Handle variant products\n- Normalize product names\n- Match product specifications\n- Track product availability\n4. Develop price analysis features\n- Track historical prices\n- Calculate price trends\n- Identify price patterns\n- Generate price alerts\n- Create price prediction models\n- Compare shipping costs\n- Track discount patterns\n5. Create reporting system\n- Build price comparison reports\n- Generate trend analysis\n- Create price alert notifications\n- Export data in multiple formats\n- Schedule automated reports\n- Track price history  \n**Key concepts to learn**:  \n- Advanced rate limiting\n- Proxy management\n- Product matching algorithms\n- Price normalization\n- Historical data tracking  \n**Website suggestions**:  \n- [amazon.com](https://amazon.com/) - Large product database\n- [walmart.com](https://walmart.com/) - Major retailer\n- [bestbuy.com](https://bestbuy.com/) - Electronics focus\n- [target.com](https://target.com/) - Retail products\n- [newegg.com](https://newegg.com/) - Tech products",
      "metadata": {
        "title": "15 Python Web Scraping Projects: From Beginner to Advanced",
        "url": "https://www.firecrawl.dev/blog/python-web-scraping-projects"
      }
    },
    {
      "id": "d338a7a0-4d79-41dd-aba3-2cfb20f5c526",
      "source": "firecrawl/blog/python-web-scraping-projects.md",
      "content": "Intermediate Web Scraping Projects > 2. Social Media Analytics Tool\n\nBuild a comprehensive social media analytics platform that combines web scraping, API integration, and real-time monitoring capabilities. The system will aggregate engagement metrics and content across major social networks, process JavaScript-heavy pages, and provide actionable insights through customizable dashboards. Key features include sentiment analysis of comments, competitive benchmarking, and automated trend detection. The tool emphasizes scalable data collection while respecting rate limits and platform terms of service.  \n**Learning objectives**:  \n- JavaScript rendering\n- API integration\n- Real-time monitoring\n- Data visualization\n- Engagement metrics analysis  \n**Project steps**:  \n1. Platform analysis and setup\n- Research API limitations\n- Document scraping restrictions\n- Set up authentication\n- Plan data collection strategy\n- Configure monitoring intervals\n2. Implement data collection\n- Create platform-specific scrapers\n- Handle JavaScript rendering\n- Implement API calls\n- Track rate limits\n- Monitor API quotas\n- Handle pagination\n- Collect media content\n3. Build analytics engine\n- Calculate engagement rates\n- Track follower growth\n- Analyze posting patterns\n- Monitor hashtag performance\n- Measure audience interaction\n- Generate sentiment analysis\n- Track competitor metrics\n4. Develop visualization system\n- Create interactive dashboards\n- Generate trend graphs\n- Build comparison charts\n- Display real-time metrics\n- Create export options\n- Generate automated reports\n5. Add monitoring features\n- Set up real-time tracking\n- Create alert system\n- Monitor competitor activity\n- Track brand mentions\n- Generate periodic reports\n- Implement custom metrics  \n**Key concepts to learn**:  \n- API integration\n- Real-time data collection\n- Engagement metrics\n- Data visualization\n- JavaScript handling  \n**Website suggestions**:  \n- [twitter.com](https://twitter.com/) - Real-time social updates\n- [instagram.com](https://instagram.com/) - Visual content platform\n- [facebook.com](https://facebook.com/) - Social networking\n- [linkedin.com](https://linkedin.com/) - Professional network\n- [reddit.com](https://reddit.com/) - Community discussions",
      "metadata": {
        "title": "15 Python Web Scraping Projects: From Beginner to Advanced",
        "url": "https://www.firecrawl.dev/blog/python-web-scraping-projects"
      }
    },
    {
      "id": "b9a8ab6f-fe83-4f88-8250-80bbd561e398",
      "source": "firecrawl/blog/python-web-scraping-projects.md",
      "content": "Intermediate Web Scraping Projects > 3. Real Estate Market Analyzer\n\nDevelop a comprehensive real estate market analysis tool that collects and analyzes property listings from multiple sources. The system will track prices, property features, market trends, and neighborhood statistics to provide insights into real estate market conditions. This project focuses on handling pagination, geographic data, and large datasets.  \n**Learning objectives**:  \n- Geographic data handling\n- Advanced pagination\n- Data relationships\n- Market analysis\n- Database optimization  \n**Project steps**:  \n1. Set up data collection framework\n- Design database schema\n- Configure geocoding system\n- Set up mapping integration\n- Plan data update frequency\n- Configure backup system\n2. Implement listing collection\n- Create site-specific scrapers\n- Handle dynamic loading\n- Process pagination\n- Extract property details\n- Collect images and media\n- Parse property features\n- Handle location data\n3. Build analysis system\n- Calculate market trends\n- Analyze price per square foot\n- Track inventory levels\n- Monitor days on market\n- Compare neighborhood stats\n- Generate market reports\n- Create price predictions\n4. Develop visualization tools\n- Create interactive maps\n- Build trend graphs\n- Display comparative analysis\n- Show market indicators\n- Generate heat maps\n- Create property reports\n5. Add advanced features\n- Implement search filters\n- Add custom alerts\n- Create watchlists\n- Generate market reports\n- Track favorite properties\n- Monitor price changes  \n**Key concepts to learn**:  \n- Geographic data processing\n- Complex pagination\n- Data relationships\n- Market analysis\n- Mapping integration  \n**Website suggestions**:  \n- [zillow.com](https://zillow.com/) - Real estate listings\n- [realtor.com](https://realtor.com/) - Property database\n- [trulia.com](https://trulia.com/) - Housing market data\n- [redfin.com](https://redfin.com/) - Real estate platform\n- [homes.com](https://homes.com/) - Property listings",
      "metadata": {
        "title": "15 Python Web Scraping Projects: From Beginner to Advanced",
        "url": "https://www.firecrawl.dev/blog/python-web-scraping-projects"
      }
    },
    {
      "id": "2691f76d-e86d-40fa-9ab2-514cfe1c8612",
      "source": "firecrawl/blog/python-web-scraping-projects.md",
      "content": "Intermediate Web Scraping Projects > 4. Academic Research Aggregator\n\nCreate a comprehensive academic research aggregator that collects scholarly articles, papers, and publications from multiple academic databases and repositories. The system will track research papers, citations, author information, and publication metrics to help researchers stay updated with the latest developments in their field.  \n**Learning objectives**:  \n- PDF parsing and extraction\n- Citation network analysis\n- Academic API integration\n- Complex search parameters\n- Large dataset management  \n**Project steps**:  \n1. Source identification and setup\n- Research academic databases\n- Document API access requirements\n- Set up authentication systems\n- Plan data collection strategy\n- Configure access protocols\n- Handle rate limitations\n2. Implement data collection\n- Create database-specific scrapers\n- Handle PDF downloads\n- Extract paper metadata\n- Parse citations\n- Track author information\n- Collect publication dates\n- Handle multiple languages\n3. Build citation analysis system\n- Track citation networks\n- Calculate impact factors\n- Analyze author networks\n- Monitor research trends\n- Generate citation graphs\n- Track paper influence\n- Identify key papers\n4. Develop search and filtering\n- Implement advanced search\n- Add field-specific filters\n- Create topic clustering\n- Enable author tracking\n- Support boolean queries\n- Add relevance ranking\n- Enable export options\n5. Create visualization and reporting\n- Generate citation networks\n- Create author collaboration maps\n- Display research trends\n- Show topic evolution\n- Create custom reports\n- Enable data export  \n**Key concepts to learn**:  \n- PDF text extraction\n- Network analysis\n- Academic APIs\n- Complex search logic\n- Large-scale data processing  \n**Website suggestions**:  \n- [scholar.google.com](https://scholar.google.com/) - Academic search engine\n- [arxiv.org](https://arxiv.org/) - Research paper repository\n- [sciencedirect.com](https://sciencedirect.com/) - Scientific publications\n- [ieee.org](https://ieee.org/) - Technical papers\n- [pubmed.gov](https://pubmed.gov/) - Medical research",
      "metadata": {
        "title": "15 Python Web Scraping Projects: From Beginner to Advanced",
        "url": "https://www.firecrawl.dev/blog/python-web-scraping-projects"
      }
    },
    {
      "id": "18aa21a6-1242-4981-8db9-657af7e0b300",
      "source": "firecrawl/blog/python-web-scraping-projects.md",
      "content": "Intermediate Web Scraping Projects > 5. Financial Market Data Analyzer\n\nBuild a sophisticated financial market analysis tool that collects and processes data from multiple financial sources including stock markets, cryptocurrency exchanges, and forex platforms. The system will track prices, trading volumes, market indicators, and news sentiment to provide comprehensive market insights.  \n**Learning objectives**:  \n- Real-time data handling\n- WebSocket connections\n- Financial calculations\n- Time series analysis\n- News sentiment analysis  \n**Project steps**:  \n1. Data source integration\n- Set up API connections\n- Configure WebSocket feeds\n- Implement rate limiting\n- Handle authentication\n- Manage data streams\n- Plan backup sources\n2. Market data collection\n- Track price movements\n- Monitor trading volume\n- Calculate market indicators\n- Record order book data\n- Track market depth\n- Handle multiple exchanges\n- Process tick data\n3. Build analysis engine\n- Implement technical indicators\n- Calculate market metrics\n- Process trading signals\n- Analyze price patterns\n- Generate market alerts\n- Track correlations\n- Monitor volatility\n4. Develop news analysis\n- Collect financial news\n- Process news sentiment\n- Track market impact\n- Monitor social media\n- Analyze announcement effects\n- Generate news alerts\n5. Create visualization system\n- Build price charts\n- Display market indicators\n- Show volume analysis\n- Create correlation maps\n- Generate trading signals\n- Enable custom dashboards  \n**Key concepts to learn**:  \n- WebSocket programming\n- Real-time data processing\n- Financial calculations\n- Market analysis\n- News sentiment analysis  \n**Website suggestions**:  \n- [finance.yahoo.com](https://finance.yahoo.com/) - Financial data\n- [marketwatch.com](https://marketwatch.com/) - Market news\n- [investing.com](https://investing.com/) - Trading data\n- [tradingview.com](https://tradingview.com/) - Technical analysis\n- [coinmarketcap.com](https://coinmarketcap.com/) - Crypto markets",
      "metadata": {
        "title": "15 Python Web Scraping Projects: From Beginner to Advanced",
        "url": "https://www.firecrawl.dev/blog/python-web-scraping-projects"
      }
    },
    {
      "id": "54837eaf-4538-4b57-92c4-dd1672b47a55",
      "source": "firecrawl/blog/python-web-scraping-projects.md",
      "content": "Advanced Web Scraping Projects\n\nThese projects represent complex, production-grade applications that combine multiple advanced concepts and require sophisticated architecture decisions. Theyâ€™re ideal for developers who have mastered basic and intermediate scraping techniques.",
      "metadata": {
        "title": "15 Python Web Scraping Projects: From Beginner to Advanced",
        "url": "https://www.firecrawl.dev/blog/python-web-scraping-projects"
      }
    },
    {
      "id": "480ff61c-e736-441c-9071-b37b2a355fa6",
      "source": "firecrawl/blog/python-web-scraping-projects.md",
      "content": "Advanced Web Scraping Projects > 1. Multi-threaded News Aggregator\n\nBuild an enterprise-grade news aggregation system that uses concurrent processing to efficiently collect and analyze news from hundreds of sources simultaneously. The system will handle rate limiting, proxy rotation, and load balancing while maintaining high throughput and data accuracy. This project focuses on scalability and performance optimization.  \n**Learning objectives**:  \n- Concurrent programming\n- Thread/Process management\n- Queue systems\n- Load balancing\n- Performance optimization  \n**Project steps**:  \n1. Design concurrent architecture\n- Plan threading strategy\n- Design queue system\n- Configure worker pools\n- Set up load balancing\n- Plan error handling\n- Implement logging system\n- Design monitoring tools\n2. Build core scraping engine\n- Create worker threads\n- Implement task queue\n- Set up proxy rotation\n- Handle rate limiting\n- Manage session pools\n- Configure retries\n- Monitor performance\n3. Develop content processing\n- Implement NLP analysis\n- Extract key information\n- Classify content\n- Detect duplicates\n- Process media content\n- Handle multiple languages\n- Generate summaries\n4. Create storage and indexing\n- Design database sharding\n- Implement caching\n- Set up search indexing\n- Manage data retention\n- Handle data validation\n- Configure backups\n- Optimize queries\n5. Build monitoring system\n- Track worker status\n- Monitor queue health\n- Measure throughput\n- Track error rates\n- Generate alerts\n- Create dashboards\n- Log performance metrics  \n**Key concepts to learn**:  \n- Thread synchronization\n- Queue management\n- Resource pooling\n- Performance monitoring\n- System optimization  \n**Website suggestions**:  \n- [reuters.com](https://reuters.com/) - International news\n- [apnews.com](https://apnews.com/) - News wire service\n- [bloomberg.com](https://bloomberg.com/) - Financial news\n- [nytimes.com](https://nytimes.com/) - News articles\n- [wsj.com](https://wsj.com/) - Business news",
      "metadata": {
        "title": "15 Python Web Scraping Projects: From Beginner to Advanced",
        "url": "https://www.firecrawl.dev/blog/python-web-scraping-projects"
      }
    },
    {
      "id": "9b65102b-de4a-40fe-b32c-07939034b53a",
      "source": "firecrawl/blog/python-web-scraping-projects.md",
      "content": "Advanced Web Scraping Projects > 2. Distributed Web Archive System\n\nBuild a distributed web archiving system that preserves historical versions of websites across a network of nodes. The system will handle massive-scale crawling, content deduplication, versioning, and provide a searchable interface to access archived content. Think of it as building your own Internet Archive Wayback Machine with distributed architecture.  \n**Learning objectives**:  \n- Distributed systems architecture\n- Content-addressable storage\n- Version control concepts\n- Distributed crawling\n- Large-scale search  \n**Project steps**:  \n1. Design distributed architecture\n- Plan node communication\n- Design content addressing\n- Configure storage sharding\n- Implement consensus protocol\n- Set up service discovery\n- Plan failure recovery\n- Design replication strategy\n2. Build core archiving engine\n- Implement snapshot system\n- Handle resource capturing\n- Process embedded content\n- Manage asset dependencies\n- Create versioning system\n- Handle redirects\n- Implement diff detection\n3. Develop distributed crawler\n- Create crawler nodes\n- Implement work distribution\n- Handle URL deduplication\n- Manage crawl frontiers\n- Process robots.txt\n- Configure politeness rules\n- Monitor node health\n4. Create storage and indexing\n- Implement content hashing\n- Build merkle trees\n- Create delta storage\n- Set up distributed index\n- Handle data replication\n- Manage storage quotas\n- Optimize retrieval\n5. Build access interface\n- Create temporal navigation\n- Implement diff viewing\n- Enable full-text search\n- Build API endpoints\n- Create admin dashboard\n- Enable export options\n- Handle access control  \n**Key concepts to learn**:  \n- Distributed systems\n- Content addressing\n- Merkle trees\n- Consensus protocols\n- Temporal data models  \n**Technical requirements**:  \n- Distributed database (e.g., Cassandra)\n- Message queue system (e.g., Kafka)\n- Search engine (e.g., Elasticsearch)\n- Content-addressable storage\n- Load balancers\n- Service mesh\n- Monitoring system  \n**Advanced features**:  \n- Temporal graph analysis\n- Content change detection\n- Link integrity verification\n- Resource deduplication\n- Distributed consensus\n- Automated preservation\n- Access control policies  \nThis project combines distributed systems concepts with web archiving challenges, requiring deep understanding of both scalable architecture and content preservation techniques. Itâ€™s particularly relevant for organizations needing to maintain compliant records of web content or researchers studying web evolution patterns.",
      "metadata": {
        "title": "15 Python Web Scraping Projects: From Beginner to Advanced",
        "url": "https://www.firecrawl.dev/blog/python-web-scraping-projects"
      }
    },
    {
      "id": "c5308623-1561-4852-8c01-aac5716e4735",
      "source": "firecrawl/blog/python-web-scraping-projects.md",
      "content": "Advanced Web Scraping Projects > 3. Automated Market Research Tool\n\nCreate a comprehensive market research platform that combines web scraping, data analysis, and automated reporting to provide competitive intelligence and market insights. The system will track competitors, analyze market trends, and generate detailed reports automatically.  \n**Learning objectives**:  \n- Large-scale data collection\n- Advanced analytics\n- Automated reporting\n- Competitive analysis\n- Market intelligence  \n**Project steps**:  \n1. Design research framework\n- Define data sources\n- Plan collection strategy\n- Design analysis pipeline\n- Configure reporting system\n- Set up monitoring\n- Plan data storage\n- Configure backup systems\n2. Implement data collection\n- Create source scrapers\n- Handle authentication\n- Manage rate limits\n- Process structured data\n- Extract unstructured content\n- Track changes\n- Validate data quality\n3. Build analysis engine\n- Process market data\n- Analyze trends\n- Track competitors\n- Generate insights\n- Calculate metrics\n- Identify patterns\n- Create predictions\n4. Develop reporting system\n- Generate automated reports\n- Create visualizations\n- Build interactive dashboards\n- Enable customization\n- Schedule updates\n- Handle distribution\n- Track engagement\n5. Add intelligence features\n- Implement trend detection\n- Create alerts system\n- Enable custom analysis\n- Build recommendation engine\n- Generate insights\n- Track KPIs\n- Monitor competition  \n**Key concepts to learn**:  \n- Market analysis\n- Report automation\n- Data visualization\n- Competitive intelligence\n- Trend analysis  \n**Website suggestions**:  \n- Company websites\n- Industry news sites\n- Government databases\n- Social media platforms\n- Review sites",
      "metadata": {
        "title": "15 Python Web Scraping Projects: From Beginner to Advanced",
        "url": "https://www.firecrawl.dev/blog/python-web-scraping-projects"
      }
    },
    {
      "id": "cad29ddd-7d54-4e4a-9e87-856df4eb7c35",
      "source": "firecrawl/blog/python-web-scraping-projects.md",
      "content": "Advanced Web Scraping Projects > 4. Competitive Intelligence Dashboard\n\nBuild a real-time competitive intelligence platform that monitors competitor activities across multiple channels including websites, social media, and news sources. The system will provide automated alerts and analysis of competitive movements in the market.  \n**Learning objectives**:  \n- Real-time monitoring\n- Complex automation\n- Data warehousing\n- Dashboard development\n- Alert systems  \n**Project steps**:  \n1. Set up monitoring system\n- Configure data sources\n- Set up real-time tracking\n- Implement change detection\n- Design alert system\n- Plan data storage\n- Configure monitoring rules\n- Handle authentication\n2. Build data collection\n- Create source scrapers\n- Handle dynamic content\n- Process structured data\n- Extract unstructured content\n- Track changes\n- Monitor social media\n- Collect news mentions\n3. Develop analysis engine\n- Process competitor data\n- Analyze market position\n- Track product changes\n- Monitor pricing\n- Analyze marketing\n- Track customer sentiment\n- Generate insights\n4. Create dashboard interface\n- Build real-time displays\n- Create interactive charts\n- Enable custom views\n- Implement filtering\n- Add search functionality\n- Enable data export\n- Configure alerts\n5. Implement alert system\n- Set up notification rules\n- Create custom triggers\n- Handle priority levels\n- Enable user preferences\n- Track alert history\n- Generate summaries\n- Monitor effectiveness  \n**Key concepts to learn**:  \n- Real-time monitoring\n- Change detection\n- Alert systems\n- Dashboard design\n- Competitive analysis  \n**Website suggestions**:  \n- Competitor websites\n- Social media platforms\n- News aggregators\n- Review sites\n- Industry forums",
      "metadata": {
        "title": "15 Python Web Scraping Projects: From Beginner to Advanced",
        "url": "https://www.firecrawl.dev/blog/python-web-scraping-projects"
      }
    },
    {
      "id": "22bc22ff-0c37-4dd5-a84e-5605a00fe884",
      "source": "firecrawl/blog/python-web-scraping-projects.md",
      "content": "Advanced Web Scraping Projects > 5. Full-Stack Scraping Platform\n\nDevelop a complete web scraping platform with a user interface that allows non-technical users to create and manage scraping tasks. The system will include visual scraping tools, scheduling, monitoring, and data export capabilities.  \n**Learning objectives**:  \n- Full-stack development\n- API design\n- Frontend development\n- System architecture\n- User management  \n**Project steps**:  \n1. Design system architecture\n- Plan component structure\n- Design API endpoints\n- Configure databases\n- Set up authentication\n- Plan scaling strategy\n- Design monitoring\n- Configure deployment\n2. Build backend system\n- Create API endpoints\n- Implement authentication\n- Handle task management\n- Process scheduling\n- Manage user data\n- Handle file storage\n- Configure security\n3. Develop scraping engine\n- Create scraper framework\n- Handle different sites\n- Manage sessions\n- Process rate limits\n- Handle errors\n- Validate data\n- Monitor performance\n4. Create frontend interface\n- Build user dashboard\n- Create task manager\n- Implement scheduling\n- Show monitoring data\n- Enable configuration\n- Handle data export\n- Display results\n5. Add advanced features\n- Visual scraper builder\n- Template system\n- Export options\n- Notification system\n- User management\n- Usage analytics\n- API documentation  \n**Key concepts to learn**:  \n- System architecture\n- API development\n- Frontend frameworks\n- User management\n- Deployment  \n**Website suggestions**:  \n- Any website (platform should be generic)\n- Test sites for development\n- Documentation resources\n- API references\n- Example targets",
      "metadata": {
        "title": "15 Python Web Scraping Projects: From Beginner to Advanced",
        "url": "https://www.firecrawl.dev/blog/python-web-scraping-projects"
      }
    },
    {
      "id": "39574506-10c2-4101-8b51-962af73e12a6",
      "source": "firecrawl/blog/python-web-scraping-projects.md",
      "content": "Conclusion\n\nWeb scraping is a powerful skill that opens up endless possibilities for data collection and analysis. Through these 15 projects, ranging from basic weather scrapers to advanced AI-powered content extraction systems, youâ€™ve seen how web scraping can be applied to solve real-world problems across different domains.  \nKey takeaways from these projects include:  \n- Start with simpler projects to build foundational skills\n- Progress gradually to more complex architectures\n- Focus on ethical scraping practices and website policies\n- Use appropriate tools based on project requirements\n- Implement proper error handling and data validation\n- Consider scalability and maintenance from the start  \nWhether youâ€™re building a simple price tracker or a full-scale market intelligence platform, the principles and techniques covered in these projects will serve as a solid foundation for your web scraping journey. Remember to always check robots.txt files, implement appropriate delays, and respect website terms of service while scraping.  \nFor your next steps, pick a project that aligns with your current skill level and start building. The best way to learn web scraping is through hands-on practice and real-world applications. As you gain confidence, gradually tackle more complex projects and keep exploring new tools and techniques in this ever-evolving field.  \nArticle updated recently  \n[ðŸ”¥](https://www.firecrawl.dev/)",
      "metadata": {
        "title": "15 Python Web Scraping Projects: From Beginner to Advanced",
        "url": "https://www.firecrawl.dev/blog/python-web-scraping-projects"
      }
    },
    {
      "id": "1b8edb46-f0ac-4d6b-bbf9-400469498b93",
      "source": "firecrawl/blog/python-web-scraping-projects.md",
      "content": "Ready to _Build?_\n\nStart scraping web data for your AI apps today.  \nNo credit card needed.  \nGet Started",
      "metadata": {
        "title": "15 Python Web Scraping Projects: From Beginner to Advanced",
        "url": "https://www.firecrawl.dev/blog/python-web-scraping-projects"
      }
    },
    {
      "id": "87c0e997-c9fa-44c1-8283-159c11d89ccb",
      "source": "firecrawl/blog/python-web-scraping-projects.md",
      "content": "About the Author\n\n[![Bex Tuychiev image](https://www.firecrawl.dev/bex.jpg)\\\nBex Tuychiev@bextuychiev](https://x.com/bextuychiev)  \nBex is a Top 10 AI writer on Medium and a Kaggle Master with over 15k followers. He loves writing detailed guides, tutorials, and notebooks on complex data science and machine learning topics",
      "metadata": {
        "title": "15 Python Web Scraping Projects: From Beginner to Advanced",
        "url": "https://www.firecrawl.dev/blog/python-web-scraping-projects"
      }
    },
    {
      "id": "1ab10e9b-0a69-4223-9435-56e3a51ec83f",
      "source": "firecrawl/blog/python-web-scraping-projects.md",
      "content": "About the Author > More articles by Bex Tuychiev\n\n[Building an Automated Price Tracking Tool\\\n\\\nBuild an automated e-commerce price tracker in Python. Learn web scraping, price monitoring, and automated alerts using Firecrawl, Streamlit, PostgreSQL.](https://www.firecrawl.dev/blog/automated-price-tracking-tutorial-python) [Web Scraping Automation: How to Run Scrapers on a Schedule\\\n\\\nLearn how to automate web scraping in Python using free tools like schedule, asyncio, cron jobs and GitHub Actions. This comprehensive guide covers local and cloud-based scheduling methods to run scrapers reliably in 2025.](https://www.firecrawl.dev/blog/automated-web-scraping-free-2025) [Automated Data Collection - A Comprehensive Guide\\\n\\\nLearn how to build robust automated data collection systems using modern tools and best practices. This guide covers everything from selecting the right tools to implementing scalable collection pipelines.](https://www.firecrawl.dev/blog/automated-data-collection-guide) [BeautifulSoup4 vs. Scrapy - A Comprehensive Comparison for Web Scraping in Python\\\n\\\nLearn the key differences between BeautifulSoup4 and Scrapy for web scraping in Python. Compare their features, performance, and use cases to choose the right tool for your web scraping needs.](https://www.firecrawl.dev/blog/beautifulsoup4-vs-scrapy-comparison) [How to Build an Automated Competitor Price Monitoring System with Python\\\n\\\nLearn how to build an automated competitor price monitoring system in Python that tracks prices across e-commerce sites, provides real-time comparisons, and maintains price history using Firecrawl, Streamlit, and GitHub Actions.](https://www.firecrawl.dev/blog/automated-competitor-price-scraping) [Scraping Company Data and Funding Information in Bulk With Firecrawl and Claude\\\n\\\nLearn how to build a web scraper in Python that gathers company details, funding rounds, and investor information from public sources like Crunchbase using Firecrawl and Claude for automated data collection and analysis.](https://www.firecrawl.dev/blog/crunchbase-scraping-with-firecrawl-claude) [Data Enrichment: A Complete Guide to Enhancing Your Data Quality\\\n\\\nLearn how to enrich your data quality with a comprehensive guide covering data enrichment tools, best practices, and real-world examples. Discover how to leverage modern solutions like Firecrawl to automate data collection, validation, and integration for better business insights.](https://www.firecrawl.dev/blog/complete-guide-to-data-enrichment) [Building an Intelligent Code Documentation RAG Assistant with DeepSeek and Firecrawl\\\n\\\nLearn how to build an intelligent documentation assistant powered by DeepSeek and RAG (Retrieval Augmented Generation) that can answer questions about any documentation website by combining language models with efficient information retrieval.](https://www.firecrawl.dev/blog/deepseek-rag-documentation-assistant)",
      "metadata": {
        "title": "15 Python Web Scraping Projects: From Beginner to Advanced",
        "url": "https://www.firecrawl.dev/blog/python-web-scraping-projects"
      }
    },
    {
      "id": "84d656d3-e99b-43c3-a1a4-8546a754eb13",
      "source": "firecrawl/blog/category-customer-stories.md",
      "content": "---\ntitle: Blog\nurl: https://www.firecrawl.dev/blog/category/customer-stories\n---  \nIntroducing /extract - Get web data with a prompt [Try now](https://www.firecrawl.dev/extract)",
      "metadata": {
        "title": "Blog",
        "url": "https://www.firecrawl.dev/blog/category/customer-stories"
      }
    },
    {
      "id": "90f4e745-a01c-4aeb-97f1-83bd339ad0d6",
      "source": "firecrawl/blog/category-customer-stories.md",
      "content": "Explore Articles\n\n[All](https://www.firecrawl.dev/blog) [Product Updates](https://www.firecrawl.dev/blog/category/product) [Tutorials](https://www.firecrawl.dev/blog/category/tutorials) [Customer Stories](https://www.firecrawl.dev/blog/category/customer-stories) [Tips & Resources](https://www.firecrawl.dev/blog/category/tips-and-resources)  \n[![How Stack AI Uses Firecrawl to Power AI Agents](https://www.firecrawl.dev/images/blog/customer-story-stackai.jpg)\\\n**How Stack AI Uses Firecrawl to Power AI Agents** \\\nDiscover how Stack AI leverages Firecrawl to seamlessly feed agentic AI workflows with high-quality web data.\\\n\\\nBy Jonathan KleimanJan 3, 2025](https://www.firecrawl.dev/blog/how-stack-ai-uses-firecrawl-to-power-ai-agents)  \n[![How Cargo Empowers GTM Teams with Firecrawl](https://www.firecrawl.dev/images/blog/customer-story-cargo.jpg)\\\n**How Cargo Empowers GTM Teams with Firecrawl** \\\nSee how Cargo uses Firecrawl to instantly analyze webpage content and power Go-To-Market workflows for their users.\\\n\\\nBy Tariq MinhasDec 6, 2024](https://www.firecrawl.dev/blog/how-cargo-empowers-gtm-teams-with-firecrawl)  \n[![How Athena Intelligence Empowers Enterprise Analysts with Firecrawl](https://www.firecrawl.dev/images/blog/customer-story-athena-intelligence.jpg)\\\n**How Athena Intelligence Empowers Enterprise Analysts with Firecrawl** \\\nDiscover how Athena Intelligence leverages Firecrawl to fuel its AI-native analytics platform for enterprise analysts.\\\n\\\nBy Ben ReillySep 10, 2024](https://www.firecrawl.dev/blog/how-athena-intelligence-empowers-analysts-with-firecrawl)  \n[![How Gamma Supercharges Onboarding with Firecrawl](https://www.firecrawl.dev/images/blog/customer-story-gamma.jpg)\\\n**How Gamma Supercharges Onboarding with Firecrawl** \\\nSee how Gamma uses Firecrawl to instantly generate websites and presentations to 20+ million users.\\\n\\\nBy Jon NoronhaAug 8, 2024](https://www.firecrawl.dev/blog/how-gamma-supercharges-onboarding-with-firecrawl)  \n[ðŸ”¥](https://www.firecrawl.dev/)",
      "metadata": {
        "title": "Blog",
        "url": "https://www.firecrawl.dev/blog/category/customer-stories"
      }
    },
    {
      "id": "c6ad9b1a-7d05-47e5-b35e-1287c88a0bab",
      "source": "firecrawl/blog/category-customer-stories.md",
      "content": "Ready to _Build?_\n\nStart scraping web data for your AI apps today.  \nNo credit card needed.  \nGet Started",
      "metadata": {
        "title": "Blog",
        "url": "https://www.firecrawl.dev/blog/category/customer-stories"
      }
    },
    {
      "id": "e0528881-6f51-445d-8287-868036b7f79e",
      "source": "firecrawl/blog/your-ip-has-been-temporarily-blocked-or-banned.md",
      "content": "---\ntitle: Your IP got blocked or banned? Here is how to unblock it\nurl: https://www.firecrawl.dev/blog/your-ip-has-been-temporarily-blocked-or-banned\n---  \nIntroducing /extract - Get web data with a prompt [Try now](https://www.firecrawl.dev/extract)  \nAug 6, 2024  \nâ€¢  \n[![Eric Ciarla image](https://www.firecrawl.dev/eric-img.jpeg)Eric Ciarla](https://x.com/ericciarla)  \n# Your IP got blocked or banned? Here is how to unblock it  \nIP blocks can occur due to various reasons, including spamming, unauthorized link posting, excessive web scraping, or accessing geo-restricted content.\nIf your IP was temporarily blocked or banned, all you might need to do is wait for the block to be lifted. If you donâ€™t know when the block will be lifted or if youâ€™re in a position where you canâ€™t wait, you can try the following methods:",
      "metadata": {
        "title": "Your IP got blocked or banned? Here is how to unblock it",
        "url": "https://www.firecrawl.dev/blog/your-ip-has-been-temporarily-blocked-or-banned"
      }
    },
    {
      "id": "a604ec93-4a01-4722-beea-b33db1077cc3",
      "source": "firecrawl/blog/your-ip-has-been-temporarily-blocked-or-banned.md",
      "content": "Clear Digital Footprint\n\n1. Clear your browserâ€™s cache and cookies.\n2. Delete any residual program data from relevant folders.\n3. Clean up registry entries related to the blocked application or website.",
      "metadata": {
        "title": "Your IP got blocked or banned? Here is how to unblock it",
        "url": "https://www.firecrawl.dev/blog/your-ip-has-been-temporarily-blocked-or-banned"
      }
    },
    {
      "id": "2e6866f2-7d26-476f-b0cb-d1bb376b1453",
      "source": "firecrawl/blog/your-ip-has-been-temporarily-blocked-or-banned.md",
      "content": "IP Blocking while Web Scraping?\n\n1. Use a proxy. We recommend using ISP proxies (for speed) or mobile proxy services (for reliability).\n2. Use a VPN.\n3. Use a web scraper service that takes care of rotating IP addresses for you like [Firecrawl](https://www.firecrawl.dev/).  \nProxies are often considered the most effective tool for bypassing blocks due to their ability to mask IP addresses and provide access to geo-restricted content. They offer a wide range of IP addresses and locations, making it difficult for websites to detect and block automated activities.  \nTo avoid the hassle of IP blocks during web scraping, consider using a service like [Firecrawl](https://www.firecrawl.dev/). It automatically rotates proxies, eliminating the need to manage the complexities of web scraping yourself.  \nArticle updated recently  \n[ðŸ”¥](https://www.firecrawl.dev/)",
      "metadata": {
        "title": "Your IP got blocked or banned? Here is how to unblock it",
        "url": "https://www.firecrawl.dev/blog/your-ip-has-been-temporarily-blocked-or-banned"
      }
    },
    {
      "id": "27f334b0-03e5-44ac-911e-fc9d043ac995",
      "source": "firecrawl/blog/your-ip-has-been-temporarily-blocked-or-banned.md",
      "content": "Ready to _Build?_\n\nStart scraping web data for your AI apps today.  \nNo credit card needed.  \nGet Started",
      "metadata": {
        "title": "Your IP got blocked or banned? Here is how to unblock it",
        "url": "https://www.firecrawl.dev/blog/your-ip-has-been-temporarily-blocked-or-banned"
      }
    },
    {
      "id": "495ca67a-63c7-4e75-8505-239049cf2922",
      "source": "firecrawl/blog/your-ip-has-been-temporarily-blocked-or-banned.md",
      "content": "About the Author\n\n[![Eric Ciarla image](https://www.firecrawl.dev/eric-img.jpeg)\\\nEric Ciarla@ericciarla](https://x.com/ericciarla)  \nEric Ciarla is the Chief Operating Officer (COO) of Firecrawl and leads marketing. He also worked on Mendable.ai and sold it to companies like Snapchat, Coinbase, and MongoDB.\nPreviously worked at Ford and Fracta as a Data Scientist. Eric also co-founded SideGuide, a tool for learning code within VS Code with 50,000 users.",
      "metadata": {
        "title": "Your IP got blocked or banned? Here is how to unblock it",
        "url": "https://www.firecrawl.dev/blog/your-ip-has-been-temporarily-blocked-or-banned"
      }
    },
    {
      "id": "4e2778ea-d895-4401-b4ae-0d1dd885a6c9",
      "source": "firecrawl/blog/your-ip-has-been-temporarily-blocked-or-banned.md",
      "content": "About the Author > More articles by Eric Ciarla\n\n[How to Create an llms.txt File for Any Website\\\n\\\nLearn how to generate an llms.txt file for any website using the llms.txt Generator and Firecrawl.](https://www.firecrawl.dev/blog/How-to-Create-an-llms-txt-File-for-Any-Website) [Cloudflare Error 1015: How to solve it?\\\n\\\nCloudflare Error 1015 is a rate limiting error that occurs when Cloudflare detects that you are exceeding the request limit set by the website owner.](https://www.firecrawl.dev/blog/cloudflare-error-1015-how-to-solve-it) [Build an agent that checks for website contradictions\\\n\\\nUsing Firecrawl and Claude to scrape your website's data and look for contradictions.](https://www.firecrawl.dev/blog/contradiction-agent) [Why Companies Need a Data Strategy for Generative AI\\\n\\\nLearn why a well-defined data strategy is essential for building robust, production-ready generative AI systems, and discover practical steps for curation, maintenance, and integration.](https://www.firecrawl.dev/blog/why-companies-need-a-data-strategy-for-generative-ai) [Getting Started with OpenAI's Predicted Outputs for Faster LLM Responses\\\n\\\nA guide to leveraging Predicted Outputs to speed up LLM tasks with GPT-4o models.](https://www.firecrawl.dev/blog/getting-started-with-predicted-outputs-openai) [How to easily install requests with pip and python\\\n\\\nA tutorial on installing the requests library in Python using various methods, with usage examples and troubleshooting tips](https://www.firecrawl.dev/blog/how-to-easily-install-requests-with-pip-and-python) [How to quickly install BeautifulSoup with Python\\\n\\\nA guide on installing the BeautifulSoup library in Python using various methods, with usage examples and troubleshooting tips](https://www.firecrawl.dev/blog/how-to-quickly-install-beautifulsoup-with-python) [How to Use OpenAI's o1 Reasoning Models in Your Applications\\\n\\\nLearn how to harness OpenAI's latest o1 series models for complex reasoning tasks in your apps.](https://www.firecrawl.dev/blog/how-to-use-openai-o1-reasoning-models-in-applications)",
      "metadata": {
        "title": "Your IP got blocked or banned? Here is how to unblock it",
        "url": "https://www.firecrawl.dev/blog/your-ip-has-been-temporarily-blocked-or-banned"
      }
    },
    {
      "id": "9b7113af-64ad-46fd-a5d2-f382813a9269",
      "source": "firecrawl/blog/launch-week-i-day-3-introducing-map-endpoint.md",
      "content": "---\ntitle: Launch Week I / Day 3: Introducing the Map Endpoint\nurl: https://www.firecrawl.dev/blog/launch-week-i-day-3-introducing-map-endpoint\n---  \nIntroducing /extract - Get web data with a prompt [Try now](https://www.firecrawl.dev/extract)  \nAugust 28, 2024  \nâ€¢  \n[![Eric Ciarla image](https://www.firecrawl.dev/eric-img.jpeg)Eric Ciarla](https://x.com/ericciarla)  \n# Launch Week I / Day 3: Introducing the Map Endpoint  \n![Launch Week I / Day 3: Introducing the Map Endpoint image](https://www.firecrawl.dev/images/blog/firecrawl-map-endpoint.png)  \nWelcome to Day 3 of Firecrawlâ€™s first-ever Launch Week! Weâ€™re thrilled to unveil the Map Endpoint.  \n**Introducing the Map Endpoint (Alpha)**  \nThis powerful new endpoint allows you to transform a single URL into a comprehensive map of an entire website in record time. Itâ€™s the fastest and easiest way to gather all the URLs on a website, opening up new possibilities for your web scraping projects.  \nThe Map endpoint is extremely useful when you need to quickly know the links on a website or when you need to scrape pages of a website that are related to a specific topic (using the `search` parameter).  \n**Getting Started with the Map Endpoint**  \nUsing the new Map endpoint is straightforward. Hereâ€™s a quick example using our Python SDK:  \n```python\nfrom firecrawl.firecrawl import FirecrawlApp\n\napp = FirecrawlApp(api_key=\"fc-YOUR_API_KEY\")\n\n# Map a website:\nmap_result = app.map_url('https://firecrawl.dev')\nprint(map_result)\n\n```  \nThe response will include a list of URLs found on the website:  \n```json\n{\n\"status\": \"success\",\n\"links\": [\\\n\"https://firecrawl.dev\",\\\n\"https://www.firecrawl.dev/pricing\",\\\n\"https://www.firecrawl.dev/blog\",\\\n\"https://www.firecrawl.dev/playground\",\\\n\"https://www.firecrawl.dev/smart-crawl\",\\\n...\\\n]\n}\n\n```  \n**Whatâ€™s Next?**  \nWeâ€™re just getting started! The Map endpoint opens up exciting possibilities for future features and integrations. Stay tuned for the remaining two days of Launch Week, where weâ€™ll be unveiling even more tools to supercharge your web scraping projects.  \nWe canâ€™t wait to see how youâ€™ll use the Map endpoint in your projects. As always, we welcome your feedback and suggestions to help us improve and refine this new feature.  \nHappy mapping, and see you tomorrow for Day 4 of Launch Week!  \nArticle updated recently  \n[ðŸ”¥](https://www.firecrawl.dev/)",
      "metadata": {
        "title": "Launch Week I / Day 3: Introducing the Map Endpoint",
        "url": "https://www.firecrawl.dev/blog/launch-week-i-day-3-introducing-map-endpoint"
      }
    },
    {
      "id": "49d7b64b-0438-4c43-b655-3b513ca13efc",
      "source": "firecrawl/blog/launch-week-i-day-3-introducing-map-endpoint.md",
      "content": "Ready to _Build?_\n\nStart scraping web data for your AI apps today.  \nNo credit card needed.  \nGet Started",
      "metadata": {
        "title": "Launch Week I / Day 3: Introducing the Map Endpoint",
        "url": "https://www.firecrawl.dev/blog/launch-week-i-day-3-introducing-map-endpoint"
      }
    },
    {
      "id": "a65b9314-e978-4f23-9698-707a89fc61f2",
      "source": "firecrawl/blog/launch-week-i-day-3-introducing-map-endpoint.md",
      "content": "About the Author\n\n[![Eric Ciarla image](https://www.firecrawl.dev/eric-img.jpeg)\\\nEric Ciarla@ericciarla](https://x.com/ericciarla)  \nEric Ciarla is the Chief Operating Officer (COO) of Firecrawl and leads marketing. He also worked on Mendable.ai and sold it to companies like Snapchat, Coinbase, and MongoDB.\nPreviously worked at Ford and Fracta as a Data Scientist. Eric also co-founded SideGuide, a tool for learning code within VS Code with 50,000 users.",
      "metadata": {
        "title": "Launch Week I / Day 3: Introducing the Map Endpoint",
        "url": "https://www.firecrawl.dev/blog/launch-week-i-day-3-introducing-map-endpoint"
      }
    },
    {
      "id": "b14ff2b2-ba02-4b01-8efb-639b1031249c",
      "source": "firecrawl/blog/launch-week-i-day-3-introducing-map-endpoint.md",
      "content": "About the Author > More articles by Eric Ciarla\n\n[How to Create an llms.txt File for Any Website\\\n\\\nLearn how to generate an llms.txt file for any website using the llms.txt Generator and Firecrawl.](https://www.firecrawl.dev/blog/How-to-Create-an-llms-txt-File-for-Any-Website) [Cloudflare Error 1015: How to solve it?\\\n\\\nCloudflare Error 1015 is a rate limiting error that occurs when Cloudflare detects that you are exceeding the request limit set by the website owner.](https://www.firecrawl.dev/blog/cloudflare-error-1015-how-to-solve-it) [Build an agent that checks for website contradictions\\\n\\\nUsing Firecrawl and Claude to scrape your website's data and look for contradictions.](https://www.firecrawl.dev/blog/contradiction-agent) [Why Companies Need a Data Strategy for Generative AI\\\n\\\nLearn why a well-defined data strategy is essential for building robust, production-ready generative AI systems, and discover practical steps for curation, maintenance, and integration.](https://www.firecrawl.dev/blog/why-companies-need-a-data-strategy-for-generative-ai) [Getting Started with OpenAI's Predicted Outputs for Faster LLM Responses\\\n\\\nA guide to leveraging Predicted Outputs to speed up LLM tasks with GPT-4o models.](https://www.firecrawl.dev/blog/getting-started-with-predicted-outputs-openai) [How to easily install requests with pip and python\\\n\\\nA tutorial on installing the requests library in Python using various methods, with usage examples and troubleshooting tips](https://www.firecrawl.dev/blog/how-to-easily-install-requests-with-pip-and-python) [How to quickly install BeautifulSoup with Python\\\n\\\nA guide on installing the BeautifulSoup library in Python using various methods, with usage examples and troubleshooting tips](https://www.firecrawl.dev/blog/how-to-quickly-install-beautifulsoup-with-python) [How to Use OpenAI's o1 Reasoning Models in Your Applications\\\n\\\nLearn how to harness OpenAI's latest o1 series models for complex reasoning tasks in your apps.](https://www.firecrawl.dev/blog/how-to-use-openai-o1-reasoning-models-in-applications)",
      "metadata": {
        "title": "Launch Week I / Day 3: Introducing the Map Endpoint",
        "url": "https://www.firecrawl.dev/blog/launch-week-i-day-3-introducing-map-endpoint"
      }
    },
    {
      "id": "5cd4cceb-7d7e-441f-9d23-301e3a69571e",
      "source": "firecrawl/blog/category-product.md",
      "content": "---\ntitle: Blog\nurl: https://www.firecrawl.dev/blog/category/product\n---  \nIntroducing /extract - Get web data with a prompt [Try now](https://www.firecrawl.dev/extract)",
      "metadata": {
        "title": "Blog",
        "url": "https://www.firecrawl.dev/blog/category/product"
      }
    },
    {
      "id": "52d98771-bcdb-4770-aa76-d78d170818fe",
      "source": "firecrawl/blog/category-product.md",
      "content": "Explore Articles\n\n[All](https://www.firecrawl.dev/blog) [Product Updates](https://www.firecrawl.dev/blog/category/product) [Tutorials](https://www.firecrawl.dev/blog/category/tutorials) [Customer Stories](https://www.firecrawl.dev/blog/category/customer-stories) [Tips & Resources](https://www.firecrawl.dev/blog/category/tips-and-resources)  \n[![Introducing /extract: Get structured web data with just a prompt](https://www.firecrawl.dev/images/blog/firecrawl-extract-endpoint.png)\\\n**Introducing /extract: Get structured web data with just a prompt** \\\nOur new /extract endpoint harnesses AI to turn any website into structured data for your applications seamlessly.\\\n\\\nBy Eric CiarlaJanuary 20, 2025](https://www.firecrawl.dev/blog/introducing-extract-open-beta)  \n[![Handling 300k requests per day: an adventure in scaling](https://www.firecrawl.dev/images/blog/an-adventure-in-scaling.jpg)\\\n**Handling 300k requests per day: an adventure in scaling** \\\nPutting out fires was taking up all our time, and we had to scale fast. This is how we did it.\\\n\\\nBy GergÅ‘ MÃ³ricz (mogery)Sep 13, 2024](https://www.firecrawl.dev/blog/an-adventure-in-scaling)  \n[![Launch Week I Recap](https://www.firecrawl.dev/images/blog/launch-week-1-recap.png)\\\n**Launch Week I Recap** \\\nA look back at the new features and updates introduced during Firecrawl's inaugural Launch Week.\\\n\\\nBy Eric CiarlaSeptember 2, 2024](https://www.firecrawl.dev/blog/firecrawl-launch-week-1-recap)  \n[![Launch Week I / Day 7: Crawl Webhooks (v1)](https://www.firecrawl.dev/images/blog/webhooks.png)\\\n**Launch Week I / Day 7: Crawl Webhooks (v1)** \\\nNew /crawl webhook support. Send notifications to your apps during a crawl.\\\n\\\nBy Nicolas CamaraSeptember 1, 2024](https://www.firecrawl.dev/blog/launch-week-i-day-7-webhooks)  \n[![Launch Week I / Day 6: LLM Extract (v1)](https://www.firecrawl.dev/images/blog/firecrawl-llm-extract.png)\\\n**Launch Week I / Day 6: LLM Extract (v1)** \\\nExtract structured data from your web pages using the extract format in /scrape.\\\n\\\nBy Nicolas CamaraAugust 31, 2024](https://www.firecrawl.dev/blog/launch-week-i-day-6-llm-extract)  \n[![Launch Week I / Day 5: Real-Time Crawling with WebSockets](https://www.firecrawl.dev/images/blog/firecrawl-websockets.png)\\\n**Launch Week I / Day 5: Real-Time Crawling with WebSockets** \\\nOur new WebSocket-based method for real-time data extraction and monitoring.\\\n\\\nBy Eric CiarlaAugust 30, 2024](https://www.firecrawl.dev/blog/launch-week-i-day-5-real-time-crawling-websockets)  \n[![Launch Week I / Day 4: Introducing Firecrawl /v1](https://www.firecrawl.dev/images/blog/firecrawl-v1-release.png)\\\n**Launch Week I / Day 4: Introducing Firecrawl /v1** \\\nOur biggest release yet - v1, a more reliable and developer-friendly API for seamless web data gathering.\\\n\\\nBy Eric CiarlaAugust 29, 2024](https://www.firecrawl.dev/blog/launch-week-i-day-4-introducing-firecrawl-v1)  \n[![Launch Week I / Day 3: Introducing the Map Endpoint](https://www.firecrawl.dev/images/blog/firecrawl-map-endpoint.png)\\\n**Launch Week I / Day 3: Introducing the Map Endpoint** \\\nOur new Map endpoint enables lightning-fast website mapping for enhanced web scraping projects.\\\n\\\nBy Eric CiarlaAugust 28, 2024](https://www.firecrawl.dev/blog/launch-week-i-day-3-introducing-map-endpoint)  \n[![Launch Week I / Day 2: 2x Rate Limits](https://www.firecrawl.dev/images/blog/firecrawl-rate-limits.png)\\\n**Launch Week I / Day 2: 2x Rate Limits** \\\nFirecrawl doubles rate limits across all plans, supercharging your web scraping capabilities.\\\n\\\nBy Eric CiarlaAugust 27, 2024](https://www.firecrawl.dev/blog/launch-week-i-day-2-doubled-rate-limits)  \n[![Launch Week I / Day 1: Introducing Teams](https://www.firecrawl.dev/images/blog/firecrawl-teams.png)\\\n**Launch Week I / Day 1: Introducing Teams** \\\nOur new Teams feature, enabling seamless collaboration on web scraping projects.\\\n\\\nBy Eric CiarlaAugust 26, 2024](https://www.firecrawl.dev/blog/launch-week-i-day-1-introducing-teams)  \n[![Introducing Fire Engine for Firecrawl](https://www.firecrawl.dev/images/blog/fire-engine-launch.png)\\\n**Introducing Fire Engine for Firecrawl** \\\nThe most scalable, reliable, and fast way to get web data for Firecrawl.\\\n\\\nBy Eric CiarlaAug 6, 2024](https://www.firecrawl.dev/blog/introducing-fire-engine-for-firecrawl)  \n[![Firecrawl July 2024 Updates](https://www.firecrawl.dev/images/blog/launch-yc-firecrawl.png)\\\n**Firecrawl July 2024 Updates** \\\nDiscover the latest features, integrations, and improvements in Firecrawl for July 2024.\\\n\\\nBy Eric CiarlaJuly 31, 2024](https://www.firecrawl.dev/blog/firecrawl-july-2024-updates)  \n[![Firecrawl June 2024 Updates](https://www.firecrawl.dev/images/blog/dashboard2.png)\\\n**Firecrawl June 2024 Updates** \\\nDiscover the latest features, integrations, and improvements in Firecrawl for June 2024.\\\n\\\nBy Nicolas CamaraJune 30, 2024](https://www.firecrawl.dev/blog/firecrawl-june-2024-updates)  \n[![Extract website data using LLMs](https://www.firecrawl.dev/images/blog/g2.png)\\\n**Extract website data using LLMs** \\\nLearn how to use Firecrawl and Groq to extract structured data from a web page in a few lines of code.\\\n\\\nBy Nicolas CamaraMay 20, 2024](https://www.firecrawl.dev/blog/data-extraction-using-llms)  \n[ðŸ”¥](https://www.firecrawl.dev/)",
      "metadata": {
        "title": "Blog",
        "url": "https://www.firecrawl.dev/blog/category/product"
      }
    },
    {
      "id": "a229629b-5d77-43bf-81e7-ded1b13abe4d",
      "source": "firecrawl/blog/category-product.md",
      "content": "Ready to _Build?_\n\nStart scraping web data for your AI apps today.  \nNo credit card needed.  \nGet Started",
      "metadata": {
        "title": "Blog",
        "url": "https://www.firecrawl.dev/blog/category/product"
      }
    },
    {
      "id": "1c329730-8716-4129-a055-bd9042677890",
      "source": "firecrawl/blog/grok-2-setup-and-web-crawler-example.md",
      "content": "---\ntitle: Getting Started with Grok-2: Setup and Web Crawler Example\nurl: https://www.firecrawl.dev/blog/grok-2-setup-and-web-crawler-example\n---  \nIntroducing /extract - Get web data with a prompt [Try now](https://www.firecrawl.dev/extract)  \nOct 21, 2024  \nâ€¢  \n[![Nicolas Camara image](https://www.firecrawl.dev/nick-img.jpeg)Nicolas Camara](https://x.com/nickscamara_)  \n# Getting Started with Grok-2: Setup and Web Crawler Example  \n![Getting Started with Grok-2: Setup and Web Crawler Example image](https://www.firecrawl.dev/images/blog/grok-2-web-crawler.jpg)  \nGrok-2, the latest language model from x.ai, brings advanced language understanding capabilities to developers, enabling the creation of intelligent applications with ease. In this tutorial, weâ€™ll walk you through setting up Grok-2, obtaining an API key, and then building a web crawler using Firecrawl to extract structured data from any website.",
      "metadata": {
        "title": "Getting Started with Grok-2: Setup and Web Crawler Example",
        "url": "https://www.firecrawl.dev/blog/grok-2-setup-and-web-crawler-example"
      }
    },
    {
      "id": "6125c53e-03c7-4e5b-b56e-7ad22a740077",
      "source": "firecrawl/blog/grok-2-setup-and-web-crawler-example.md",
      "content": "Part 1: Setting Up Grok-2\n\nBefore diving into coding, we need to set up Grok-2 and get an API key.",
      "metadata": {
        "title": "Getting Started with Grok-2: Setup and Web Crawler Example",
        "url": "https://www.firecrawl.dev/blog/grok-2-setup-and-web-crawler-example"
      }
    },
    {
      "id": "e0b4d5ea-0336-467a-8909-545e8d8a0024",
      "source": "firecrawl/blog/grok-2-setup-and-web-crawler-example.md",
      "content": "Part 1: Setting Up Grok-2 > Step 1: Sign Up for an x.ai Account\n\nTo access the Grok-2 API, youâ€™ll need an x.ai account.  \n1. **Visit the Sign-Up Page:** Go to [x.ai Sign-Up](https://accounts.x.ai/sign-up?redirect=cloud-console).\n2. **Register:** Fill out the registration form with your email and create a password.\n3. **Verify Your Email:** Check your inbox for a verification email from x.ai and click the link to verify your account.",
      "metadata": {
        "title": "Getting Started with Grok-2: Setup and Web Crawler Example",
        "url": "https://www.firecrawl.dev/blog/grok-2-setup-and-web-crawler-example"
      }
    },
    {
      "id": "7086c7b6-9ef7-4561-8590-3cf4b446a3ba",
      "source": "firecrawl/blog/grok-2-setup-and-web-crawler-example.md",
      "content": "Part 1: Setting Up Grok-2 > Step 2: Fund Your Account\n\nTo use the Grok-2 API, your account must have funds.  \n1. **Access the Cloud Console:** After logging in, youâ€™ll be directed to the x.ai Cloud Console.\n2. **Navigate to Billing:** Click on the **Billing** tab in the sidebar.\n3. **Add Payment Method:** Provide your payment details to add credits to your account.",
      "metadata": {
        "title": "Getting Started with Grok-2: Setup and Web Crawler Example",
        "url": "https://www.firecrawl.dev/blog/grok-2-setup-and-web-crawler-example"
      }
    },
    {
      "id": "b105fd7c-fbc6-4b61-9eac-014f45fc33f9",
      "source": "firecrawl/blog/grok-2-setup-and-web-crawler-example.md",
      "content": "Part 1: Setting Up Grok-2 > Step 3: Obtain Your API Key\n\nWith your account funded, you can now generate an API key.  \n1. **Go to API Keys:** Click on the **API Keys** tab in the Cloud Console.\n2. **Create a New API Key:** Click on **Create New API Key** and give it a descriptive name.\n3. **Copy Your API Key:** Make sure to copy your API key now, as it wonâ€™t be displayed again for security reasons.  \n_Note: Keep your API key secure and do not share it publicly._",
      "metadata": {
        "title": "Getting Started with Grok-2: Setup and Web Crawler Example",
        "url": "https://www.firecrawl.dev/blog/grok-2-setup-and-web-crawler-example"
      }
    },
    {
      "id": "62ec7ec3-fbc6-4979-b48c-f6535f774291",
      "source": "firecrawl/blog/grok-2-setup-and-web-crawler-example.md",
      "content": "Part 2: Building a Web Crawler with Grok-2 and Firecrawl\n\nNow that Grok-2 is set up, letâ€™s build a web crawler to extract structured data from websites.",
      "metadata": {
        "title": "Getting Started with Grok-2: Setup and Web Crawler Example",
        "url": "https://www.firecrawl.dev/blog/grok-2-setup-and-web-crawler-example"
      }
    },
    {
      "id": "4127ea6f-5edc-4b9b-8928-377682af2441",
      "source": "firecrawl/blog/grok-2-setup-and-web-crawler-example.md",
      "content": "Part 2: Building a Web Crawler with Grok-2 and Firecrawl > Prerequisites\n\n- **Python 3.6+**\n- **Firecrawl Python Library**\n- **Requests Library**\n- **dotenv Library**  \nInstall the required packages:  \n```bash\npip install firecrawl-py requests python-dotenv\n\n```",
      "metadata": {
        "title": "Getting Started with Grok-2: Setup and Web Crawler Example",
        "url": "https://www.firecrawl.dev/blog/grok-2-setup-and-web-crawler-example"
      }
    },
    {
      "id": "87ef2e0c-a002-4c01-a671-05aeddb12e68",
      "source": "firecrawl/blog/grok-2-setup-and-web-crawler-example.md",
      "content": "Part 2: Building a Web Crawler with Grok-2 and Firecrawl > Step 1: Set Up Environment Variables\n\nCreate a `.env` file in your project directory to store your API keys securely.  \n```env\nGROK_API_KEY=your_grok_api_key\nFIRECRAWL_API_KEY=your_firecrawl_api_key\n\n```  \n_Replace `your_grok_api_key` and `your_firecrawl_api_key` with your actual API keys._",
      "metadata": {
        "title": "Getting Started with Grok-2: Setup and Web Crawler Example",
        "url": "https://www.firecrawl.dev/blog/grok-2-setup-and-web-crawler-example"
      }
    },
    {
      "id": "91794734-2fdb-4017-9a25-f543cac0d1c1",
      "source": "firecrawl/blog/grok-2-setup-and-web-crawler-example.md",
      "content": "Part 2: Building a Web Crawler with Grok-2 and Firecrawl > Step 2: Initialize Your Script\n\nCreate a new Python script (e.g., `web_crawler.py`) and start by importing the necessary libraries and loading your environment variables.  \n```python\nimport os\nimport json\nimport requests\nfrom dotenv import load_dotenv\nfrom firecrawl import FirecrawlApp\n\n# Load environment variables from .env file\nload_dotenv()\n\n# Retrieve API keys\ngrok_api_key = os.getenv(\"GROK_API_KEY\")\nfirecrawl_api_key = os.getenv(\"FIRECRAWL_API_KEY\")\n\n# Initialize FirecrawlApp\napp = FirecrawlApp(api_key=firecrawl_api_key)\n\n```",
      "metadata": {
        "title": "Getting Started with Grok-2: Setup and Web Crawler Example",
        "url": "https://www.firecrawl.dev/blog/grok-2-setup-and-web-crawler-example"
      }
    },
    {
      "id": "d001d656-07fe-4cbb-ba6a-0d1d1fddd48a",
      "source": "firecrawl/blog/grok-2-setup-and-web-crawler-example.md",
      "content": "Part 2: Building a Web Crawler with Grok-2 and Firecrawl > Step 3: Define the Grok-2 API Interaction Function\n\nWe need a function to interact with the Grok-2 API.  \n```python\ndef grok_completion(prompt):\nurl = \"https://api.x.ai/v1/chat/completions\"\nheaders = {\n\"Content-Type\": \"application/json\",\n\"Authorization\": f\"Bearer {grok_api_key}\"\n}\ndata = {\n\"messages\": [\\\n{\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\\\n{\"role\": \"user\", \"content\": prompt}\\\n],\n\"model\": \"grok-2\",\n\"stream\": False,\n\"temperature\": 0\n}\nresponse = requests.post(url, headers=headers, json=data)\nresponse_data = response.json()\nreturn response_data['choices'][0]['message']['content']\n\n```",
      "metadata": {
        "title": "Getting Started with Grok-2: Setup and Web Crawler Example",
        "url": "https://www.firecrawl.dev/blog/grok-2-setup-and-web-crawler-example"
      }
    },
    {
      "id": "6861a032-38a0-46cd-83c6-630932616919",
      "source": "firecrawl/blog/grok-2-setup-and-web-crawler-example.md",
      "content": "Part 2: Building a Web Crawler with Grok-2 and Firecrawl > Step 4: Identify Relevant Pages on the Website\n\nDefine a function to find pages related to our objective.  \n```python\ndef find_relevant_pages(objective, url):\nprompt = f\"Based on the objective '{objective}', suggest a 1-2 word search term to locate relevant information on the website.\"\nsearch_term = grok_completion(prompt).strip()\nmap_result = app.map_url(url, params={\"search\": search_term})\nreturn map_result.get(\"links\", [])\n\n```",
      "metadata": {
        "title": "Getting Started with Grok-2: Setup and Web Crawler Example",
        "url": "https://www.firecrawl.dev/blog/grok-2-setup-and-web-crawler-example"
      }
    },
    {
      "id": "849f15af-4f0b-4def-b8df-928d10cbe4db",
      "source": "firecrawl/blog/grok-2-setup-and-web-crawler-example.md",
      "content": "Part 2: Building a Web Crawler with Grok-2 and Firecrawl > Step 5: Extract Data from the Pages\n\nCreate a function to scrape the pages and extract the required data.  \n```python\ndef extract_data_from_pages(links, objective):\nfor link in links[:3]: # Limit to top 3 links\nscrape_result = app.scrape_url(link, params={'formats': ['markdown']})\ncontent = scrape_result.get('markdown', '')\nprompt = f\"\"\"Given the following content, extract the information related to the objective '{objective}' in JSON format. If not found, reply 'Objective not met'.\n\nContent: {content}\n\nRemember:\n- Only return JSON if the objective is met.\n- Do not include any extra text.\n\"\"\"\nresult = grok_completion(prompt).strip()\nif result != \"Objective not met\":\ntry:\ndata = json.loads(result)\nreturn data\nexcept json.JSONDecodeError:\ncontinue # Try the next link if JSON parsing fails\nreturn None\n\n```",
      "metadata": {
        "title": "Getting Started with Grok-2: Setup and Web Crawler Example",
        "url": "https://www.firecrawl.dev/blog/grok-2-setup-and-web-crawler-example"
      }
    },
    {
      "id": "af9e5816-4c2f-48a0-b13b-7472b2a920e3",
      "source": "firecrawl/blog/grok-2-setup-and-web-crawler-example.md",
      "content": "Part 2: Building a Web Crawler with Grok-2 and Firecrawl > Step 6: Implement the Main Function\n\nCombine everything into a main function.  \n```python\ndef main():\nurl = input(\"Enter the website URL to crawl: \")\nobjective = input(\"Enter your data extraction objective: \")\n\nprint(\"nFinding relevant pages...\")\nlinks = find_relevant_pages(objective, url)\n\nif not links:\nprint(\"No relevant pages found.\")\nreturn\n\nprint(\"Extracting data from pages...\")\ndata = extract_data_from_pages(links, objective)\n\nif data:\nprint(\"nData extracted successfully:\")\nprint(json.dumps(data, indent=2))\nelse:\nprint(\"Could not find data matching the objective.\")\n\nif __name__ == \"__main__\":\nmain()\n\n```",
      "metadata": {
        "title": "Getting Started with Grok-2: Setup and Web Crawler Example",
        "url": "https://www.firecrawl.dev/blog/grok-2-setup-and-web-crawler-example"
      }
    },
    {
      "id": "38a6b340-e6f0-4ab6-845f-552ba4ddc464",
      "source": "firecrawl/blog/grok-2-setup-and-web-crawler-example.md",
      "content": "Part 2: Building a Web Crawler with Grok-2 and Firecrawl > Step 7: Run the Script\n\nSave your script and run it from the command line.  \n```bash\npython web_crawler.py\n\n```  \n**Example Interaction:**  \n```\nEnter the website URL to crawl: https://example.com\nEnter your data extraction objective: Retrieve the list of services offered.\n\nFinding relevant pages...\nExtracting data from pages...\n\nData extracted successfully:\n{\n\"services\": [\\\n\"Web Development\",\\\n\"SEO Optimization\",\\\n\"Digital Marketing\"\\\n]\n}\n\n```",
      "metadata": {
        "title": "Getting Started with Grok-2: Setup and Web Crawler Example",
        "url": "https://www.firecrawl.dev/blog/grok-2-setup-and-web-crawler-example"
      }
    },
    {
      "id": "543c4564-3419-4a30-8c06-e2525ec7cdbd",
      "source": "firecrawl/blog/grok-2-setup-and-web-crawler-example.md",
      "content": "Conclusion\n\nIn this tutorial, weâ€™ve successfully set up Grok-2, obtained an API key, and built a web crawler using Firecrawl. This powerful combination allows you to automate the process of extracting structured data from websites, making it a valuable tool for various applications.",
      "metadata": {
        "title": "Getting Started with Grok-2: Setup and Web Crawler Example",
        "url": "https://www.firecrawl.dev/blog/grok-2-setup-and-web-crawler-example"
      }
    },
    {
      "id": "213ac509-47ce-4c67-83ac-4e0c77f28ff0",
      "source": "firecrawl/blog/grok-2-setup-and-web-crawler-example.md",
      "content": "Next Steps\n\n- **Explore More Features:** Check out the Grok-2 and Firecrawl documentation to learn about additional functionalities.\n- **Enhance Error Handling:** Improve the script with better error handling and logging.\n- **Customize Data Extraction:** Modify the extraction logic to suit different objectives or data types.",
      "metadata": {
        "title": "Getting Started with Grok-2: Setup and Web Crawler Example",
        "url": "https://www.firecrawl.dev/blog/grok-2-setup-and-web-crawler-example"
      }
    },
    {
      "id": "2af1a08c-7807-44c1-bb93-ec17dc8dcc37",
      "source": "firecrawl/blog/grok-2-setup-and-web-crawler-example.md",
      "content": "References\n\n- [x.ai Grok-2 API Documentation](https://docs.x.ai/docs)\n- [Firecrawl Python Library Documentation](https://docs.firecrawl.dev/sdks/python)\n- [x.ai Cloud Console](https://accounts.x.ai/cloud-console)\n- [GitHub Repository with Full Code](https://github.com/mendableai/firecrawl/tree/main/examples/grok_web_crawler)  \nArticle updated recently  \n[ðŸ”¥](https://www.firecrawl.dev/)",
      "metadata": {
        "title": "Getting Started with Grok-2: Setup and Web Crawler Example",
        "url": "https://www.firecrawl.dev/blog/grok-2-setup-and-web-crawler-example"
      }
    },
    {
      "id": "9909168f-d0ab-4fc9-bf7b-32e9c8516fe0",
      "source": "firecrawl/blog/grok-2-setup-and-web-crawler-example.md",
      "content": "Ready to _Build?_\n\nStart scraping web data for your AI apps today.  \nNo credit card needed.  \nGet Started",
      "metadata": {
        "title": "Getting Started with Grok-2: Setup and Web Crawler Example",
        "url": "https://www.firecrawl.dev/blog/grok-2-setup-and-web-crawler-example"
      }
    },
    {
      "id": "36333a62-3d6d-498e-9333-25675885614d",
      "source": "firecrawl/blog/grok-2-setup-and-web-crawler-example.md",
      "content": "About the Author\n\n[![Nicolas Camara image](https://www.firecrawl.dev/nick-img.jpeg)\\\nNicolas Camara@nickscamara_](https://x.com/nickscamara_)  \nNicolas Camara is the Chief Technology Officer (CTO) at Firecrawl.\nHe previously built and scaled Mendable, one of the pioneering \"chat with your documents\" apps,\nwhich had major Fortune 500 customers like Snapchat, Coinbase, and MongoDB.\nPrior to that, Nicolas built SideGuide, the first code-learning tool inside VS Code,\nand grew a community of 50,000 users. Nicolas studied Computer Science and has over 10 years of experience in building software.",
      "metadata": {
        "title": "Getting Started with Grok-2: Setup and Web Crawler Example",
        "url": "https://www.firecrawl.dev/blog/grok-2-setup-and-web-crawler-example"
      }
    },
    {
      "id": "16030605-9187-43e4-822e-a45151e4cbd6",
      "source": "firecrawl/blog/grok-2-setup-and-web-crawler-example.md",
      "content": "About the Author > More articles by Nicolas Camara\n\n[Using OpenAI's Realtime API and Firecrawl to Talk with Any Website\\\n\\\nBuild a real-time conversational agent that interacts with any website using OpenAI's Realtime API and Firecrawl.](https://www.firecrawl.dev/blog/How-to-Talk-with-Any-Website-Using-OpenAIs-Realtime-API-and-Firecrawl) [Extract website data using LLMs\\\n\\\nLearn how to use Firecrawl and Groq to extract structured data from a web page in a few lines of code.](https://www.firecrawl.dev/blog/data-extraction-using-llms) [Getting Started with Grok-2: Setup and Web Crawler Example\\\n\\\nA detailed guide on setting up Grok-2 and building a web crawler using Firecrawl.](https://www.firecrawl.dev/blog/grok-2-setup-and-web-crawler-example) [Launch Week I / Day 6: LLM Extract (v1)\\\n\\\nExtract structured data from your web pages using the extract format in /scrape.](https://www.firecrawl.dev/blog/launch-week-i-day-6-llm-extract) [Launch Week I / Day 7: Crawl Webhooks (v1)\\\n\\\nNew /crawl webhook support. Send notifications to your apps during a crawl.](https://www.firecrawl.dev/blog/launch-week-i-day-7-webhooks) [OpenAI Swarm Tutorial: Create Marketing Campaigns for Any Website\\\n\\\nA guide to building a multi-agent system using OpenAI Swarm and Firecrawl for AI-driven marketing strategies](https://www.firecrawl.dev/blog/openai-swarm-agent-tutorial) [Build a 'Chat with website' using Groq Llama 3\\\n\\\nLearn how to use Firecrawl, Groq Llama 3, and Langchain to build a 'Chat with your website' bot.](https://www.firecrawl.dev/blog/chat-with-website) [Scrape and Analyze Airbnb Data with Firecrawl and E2B\\\n\\\nLearn how to scrape and analyze Airbnb data using Firecrawl and E2B in a few lines of code.](https://www.firecrawl.dev/blog/scrape-analyze-airbnb-data-with-e2b)",
      "metadata": {
        "title": "Getting Started with Grok-2: Setup and Web Crawler Example",
        "url": "https://www.firecrawl.dev/blog/grok-2-setup-and-web-crawler-example"
      }
    },
    {
      "id": "7b2ce693-b96b-4783-b7ed-5996191322c8",
      "source": "firecrawl/blog/how-stack-ai-uses-firecrawl-to-power-ai-agents.md",
      "content": "---\ntitle: How Stack AI Uses Firecrawl to Power AI Agents\nurl: https://www.firecrawl.dev/blog/how-stack-ai-uses-firecrawl-to-power-ai-agents\n---  \nIntroducing /extract - Get web data with a prompt [Try now](https://www.firecrawl.dev/extract)  \nJan 3, 2025  \nâ€¢  \n[![Jonathan Kleiman image](https://www.firecrawl.dev/customers/jonathan.jpeg)Jonathan Kleiman](https://x.com/KleimanJonathan)  \n# How Stack AI Uses Firecrawl to Power AI Agents  \n![How Stack AI Uses Firecrawl to Power AI Agents image](https://www.firecrawl.dev/images/blog/customer-story-stackai.jpg)  \nAt [Stack AI](https://www.stack-ai.com/), we use web scraping to ingest website content as a knowledge base that can then feed an agentic AI workflow. Our users can apply this to automate retrieval, research, and content generation tasks with AI Agents. When we started looking for a reliable solution, we came across Firecrawl and decided to give it a try.  \nIntegrating Firecrawl was seamless thanks to their lightweight API and onboarding resources. Their team provided clear guidelines that aligned with our tech stack. This allowed us to integrate their scraping API into our existing data ingestion pipeline in less than 15 minutes. That quick turnaround really stood out, and it immediately became clear weâ€™d found a solid partner.  \nIf we ever had to move away from Firecrawl, weâ€™d miss its scalability and quality. Firecrawl has consistently provided high-quality data without downtime, enabling us to feed AI agents with accurate web data. Their ability to handle complex scraping tasks while ensuring compliance with website terms has been invaluable. Itâ€™s not easy to find a scraping solution thatâ€™s both reliable and fully compliant, so we appreciate the peace of mind Firecrawl brings.  \nTheir support has also been top-notch. The Firecrawl team has been exceptional in providing support. When we encountered technical hurdles or needed advice on optimizing performance, their response was swift and solutions were actionable. Their dedication to customer success stood out at every interaction.  \nWeâ€™re excited to see where this partnership goes as our AI platform continues to grow.  \nArticle updated recently",
      "metadata": {
        "title": "How Stack AI Uses Firecrawl to Power AI Agents",
        "url": "https://www.firecrawl.dev/blog/how-stack-ai-uses-firecrawl-to-power-ai-agents"
      }
    },
    {
      "id": "c8f1e546-e753-499d-bc01-2f8e6a01f904",
      "source": "firecrawl/blog/how-stack-ai-uses-firecrawl-to-power-ai-agents.md",
      "content": "About the Author\n\n[![Jonathan Kleiman image](https://www.firecrawl.dev/customers/jonathan.jpeg)\\\nJonathan Kleiman@KleimanJonathan](https://x.com/KleimanJonathan)  \nJonathan Kleiman is the Head of Growth at Stack AI.  \n[ðŸ”¥](https://www.firecrawl.dev/)",
      "metadata": {
        "title": "How Stack AI Uses Firecrawl to Power AI Agents",
        "url": "https://www.firecrawl.dev/blog/how-stack-ai-uses-firecrawl-to-power-ai-agents"
      }
    },
    {
      "id": "9bdec4c3-5da5-466a-923f-8813dbb4a3b6",
      "source": "firecrawl/blog/how-stack-ai-uses-firecrawl-to-power-ai-agents.md",
      "content": "Ready to _Build?_\n\nStart scraping web data for your AI apps today.  \nNo credit card needed.  \nGet Started",
      "metadata": {
        "title": "How Stack AI Uses Firecrawl to Power AI Agents",
        "url": "https://www.firecrawl.dev/blog/how-stack-ai-uses-firecrawl-to-power-ai-agents"
      }
    },
    {
      "id": "b140e0f8-e930-4cea-b85c-9d6ab1237041",
      "source": "firecrawl/blog/launch-week-ii-day-6-introducing-mobile-scraping.md",
      "content": "---\ntitle: Launch Week II - Day 6: Introducing Mobile Scraping and Mobile Screenshots\nurl: https://www.firecrawl.dev/blog/launch-week-ii-day-6-introducing-mobile-scraping\n---  \nIntroducing /extract - Get web data with a prompt [Try now](https://www.firecrawl.dev/extract)  \nNovember 2, 2024  \nâ€¢  \n[![Eric Ciarla image](https://www.firecrawl.dev/eric-img.jpeg)Eric Ciarla](https://x.com/ericciarla)  \n# Launch Week II - Day 6: Introducing Mobile Scraping and Mobile Screenshots  \n![Launch Week II - Day 6: Introducing Mobile Scraping and Mobile Screenshots image](https://www.firecrawl.dev/images/blog/firecrawl-mobile-scraping.jpg)  \nWelcome to Day 6 of Firecrawlâ€™s second Launch Week! Today, weâ€™re excited to announce a highly requested feature: **Mobile Scraping and Mobile Screenshots**.  \n**Introducing Mobile Device Emulation**  \nFirecrawl now introduces **mobile device emulation** for both scraping and screenshots, empowering you to interact with sites as if from a mobile device. This feature is essential for testing mobile-specific content, understanding responsive design, and gaining insights from mobile-specific elements.",
      "metadata": {
        "title": "Launch Week II - Day 6: Introducing Mobile Scraping and Mobile Screenshots",
        "url": "https://www.firecrawl.dev/blog/launch-week-ii-day-6-introducing-mobile-scraping"
      }
    },
    {
      "id": "232beb23-342e-4223-888a-66ec473e07ac",
      "source": "firecrawl/blog/launch-week-ii-day-6-introducing-mobile-scraping.md",
      "content": "Why Mobile Scraping?\n\nMobile-first experiences are increasingly common, and this feature enables you to:  \n- **High-Fidelity Mobile Screenshots**: Take accurate mobile screenshots for a true representation of how a site appears on mobile devices.\n- **Test Mobile Layouts and UI Elements**: Verify mobile-specific designs and ensure your scraping results are accurate for responsive websites.\n- **Access Mobile-Only Content**: Scrape content exclusive to mobile users, gaining insights that vary from desktop versions.",
      "metadata": {
        "title": "Launch Week II - Day 6: Introducing Mobile Scraping and Mobile Screenshots",
        "url": "https://www.firecrawl.dev/blog/launch-week-ii-day-6-introducing-mobile-scraping"
      }
    },
    {
      "id": "1a7d3e43-1318-49a3-9824-f0e404cc884a",
      "source": "firecrawl/blog/launch-week-ii-day-6-introducing-mobile-scraping.md",
      "content": "Usage\n\nTo activate mobile scraping, simply add `\"mobile\": true` in your request, which will enable Firecrawlâ€™s mobile emulation mode.  \nFor further details, including additional configuration options, visit the [API Reference](https://docs.firecrawl.dev/api-reference/endpoint/scrape).  \n**Whatâ€™s Next?**  \nThat wraps up Day 6 of Launch Week II! We canâ€™t wait to see how youâ€™ll utilize mobile scraping and screenshots in your applications. Happy scraping, and stay tuned for the final day of [Launch Week II](https://www.firecrawl.dev/launch-week) tomorrow!  \nArticle updated recently  \n[ðŸ”¥](https://www.firecrawl.dev/)",
      "metadata": {
        "title": "Launch Week II - Day 6: Introducing Mobile Scraping and Mobile Screenshots",
        "url": "https://www.firecrawl.dev/blog/launch-week-ii-day-6-introducing-mobile-scraping"
      }
    },
    {
      "id": "f9bcda7f-8e0d-405b-ac84-c4931facd7f8",
      "source": "firecrawl/blog/launch-week-ii-day-6-introducing-mobile-scraping.md",
      "content": "Ready to _Build?_\n\nStart scraping web data for your AI apps today.  \nNo credit card needed.  \nGet Started",
      "metadata": {
        "title": "Launch Week II - Day 6: Introducing Mobile Scraping and Mobile Screenshots",
        "url": "https://www.firecrawl.dev/blog/launch-week-ii-day-6-introducing-mobile-scraping"
      }
    },
    {
      "id": "13228983-35fd-4dcc-9a93-73ad2d5b4862",
      "source": "firecrawl/blog/launch-week-ii-day-6-introducing-mobile-scraping.md",
      "content": "About the Author\n\n[![Eric Ciarla image](https://www.firecrawl.dev/eric-img.jpeg)\\\nEric Ciarla@ericciarla](https://x.com/ericciarla)  \nEric Ciarla is the Chief Operating Officer (COO) of Firecrawl and leads marketing. He also worked on Mendable.ai and sold it to companies like Snapchat, Coinbase, and MongoDB.\nPreviously worked at Ford and Fracta as a Data Scientist. Eric also co-founded SideGuide, a tool for learning code within VS Code with 50,000 users.",
      "metadata": {
        "title": "Launch Week II - Day 6: Introducing Mobile Scraping and Mobile Screenshots",
        "url": "https://www.firecrawl.dev/blog/launch-week-ii-day-6-introducing-mobile-scraping"
      }
    },
    {
      "id": "73e0002b-5dcd-42d4-9fa2-e81770bdd44b",
      "source": "firecrawl/blog/launch-week-ii-day-6-introducing-mobile-scraping.md",
      "content": "About the Author > More articles by Eric Ciarla\n\n[How to Create an llms.txt File for Any Website\\\n\\\nLearn how to generate an llms.txt file for any website using the llms.txt Generator and Firecrawl.](https://www.firecrawl.dev/blog/How-to-Create-an-llms-txt-File-for-Any-Website) [Cloudflare Error 1015: How to solve it?\\\n\\\nCloudflare Error 1015 is a rate limiting error that occurs when Cloudflare detects that you are exceeding the request limit set by the website owner.](https://www.firecrawl.dev/blog/cloudflare-error-1015-how-to-solve-it) [Build an agent that checks for website contradictions\\\n\\\nUsing Firecrawl and Claude to scrape your website's data and look for contradictions.](https://www.firecrawl.dev/blog/contradiction-agent) [Why Companies Need a Data Strategy for Generative AI\\\n\\\nLearn why a well-defined data strategy is essential for building robust, production-ready generative AI systems, and discover practical steps for curation, maintenance, and integration.](https://www.firecrawl.dev/blog/why-companies-need-a-data-strategy-for-generative-ai) [Getting Started with OpenAI's Predicted Outputs for Faster LLM Responses\\\n\\\nA guide to leveraging Predicted Outputs to speed up LLM tasks with GPT-4o models.](https://www.firecrawl.dev/blog/getting-started-with-predicted-outputs-openai) [How to easily install requests with pip and python\\\n\\\nA tutorial on installing the requests library in Python using various methods, with usage examples and troubleshooting tips](https://www.firecrawl.dev/blog/how-to-easily-install-requests-with-pip-and-python) [How to quickly install BeautifulSoup with Python\\\n\\\nA guide on installing the BeautifulSoup library in Python using various methods, with usage examples and troubleshooting tips](https://www.firecrawl.dev/blog/how-to-quickly-install-beautifulsoup-with-python) [How to Use OpenAI's o1 Reasoning Models in Your Applications\\\n\\\nLearn how to harness OpenAI's latest o1 series models for complex reasoning tasks in your apps.](https://www.firecrawl.dev/blog/how-to-use-openai-o1-reasoning-models-in-applications)",
      "metadata": {
        "title": "Launch Week II - Day 6: Introducing Mobile Scraping and Mobile Screenshots",
        "url": "https://www.firecrawl.dev/blog/launch-week-ii-day-6-introducing-mobile-scraping"
      }
    },
    {
      "id": "f4bc490b-7ce2-40b7-90cd-635ab7181cd2",
      "source": "firecrawl/blog/automated-competitor-price-scraping.md",
      "content": "---\ntitle: How to Build an Automated Competitor Price Monitoring System with Python\nurl: https://www.firecrawl.dev/blog/automated-competitor-price-scraping\n---  \nIntroducing /extract - Get web data with a prompt [Try now](https://www.firecrawl.dev/extract)  \nJan 6, 2025  \nâ€¢  \n[![Bex Tuychiev image](https://www.firecrawl.dev/bex.jpg)Bex Tuychiev](https://x.com/bextuychiev)  \n# How to Build an Automated Competitor Price Monitoring System with Python  \n![How to Build an Automated Competitor Price Monitoring System with Python image](https://www.firecrawl.dev/images/blog/competitor_price_scraping/competitor-price-scraping.jpg)",
      "metadata": {
        "title": "How to Build an Automated Competitor Price Monitoring System with Python",
        "url": "https://www.firecrawl.dev/blog/automated-competitor-price-scraping"
      }
    },
    {
      "id": "91e26323-4bf4-4136-8630-4c55d5b4e5b4",
      "source": "firecrawl/blog/automated-competitor-price-scraping.md",
      "content": "Introduction\n\nIn todayâ€™s competitive e-commerce landscape, staying on top of competitor pricing is crucial for maintaining market share and optimizing your pricing strategy. Manual price checking is time-consuming and error-prone. This tutorial will show you how to build an automated system that tracks competitor prices across multiple websites and provides real-time comparisons.  \nHere is what weâ€™ll build:  \n![Screenshot of the competitor price monitoring dashboard showing tracked products with their prices and competitors in a clean web interface](https://www.firecrawl.dev/images/blog/competitor_price_scraping/finalproduct.png)  \nKey features:  \n- Track your products and their prices\n- Monitor multiple competitor URLs per product\n- AI-based scraping of competitor prices\n- Automated price updates every six hours\n- Clean, intuitive dashboard interface\n- Price comparison metrics\n- One-click competitor website access  \nTo build this app, we will use Python and these key libraries:  \n- Streamlit for building the web interface\n- Firecrawl for AI-powered web scraping\n- SQLAlchemy for database management  \nFor infrastructure, we will use:  \n- Supabase for PostgreSQL database hosting\n- GitHub actions for automated price updates  \nPrerequisites:  \n- Python 3.8+ installed\n- Understanding of Python programming\n- GitHub account for hosting and automation\n- Supabase account (free tier available)\n- Text editor or IDE of your choice\n- Basic understanding of web scraping concepts\n- Familiarity with SQL and databases  \nWith that said, letâ€™s get started!  \n> The complete code for the project can be found in [this GitHub repository](https://github.com/FirstClassML/firecrawl_projects/tree/main/competitor-price-monitor).",
      "metadata": {
        "title": "How to Build an Automated Competitor Price Monitoring System with Python",
        "url": "https://www.firecrawl.dev/blog/automated-competitor-price-scraping"
      }
    },
    {
      "id": "08bf221e-d62b-4a4b-9a48-af54b904fb2a",
      "source": "firecrawl/blog/automated-competitor-price-scraping.md",
      "content": "Table of Contents\n\n- [Introduction](https://www.firecrawl.dev/blog/automated-competitor-price-scraping#introduction)\n- [Step-by-Step Implementation](https://www.firecrawl.dev/blog/automated-competitor-price-scraping#step-by-step-implementation)\n- [Step 1: Project setup](https://www.firecrawl.dev/blog/automated-competitor-price-scraping#step-1-project-setup)\n- [Step 2: Add a sidebar to the UI for product input](https://www.firecrawl.dev/blog/automated-competitor-price-scraping#step-2-add-a-sidebar-to-the-ui-for-product-input)\n- [Step 3: Store new products in PostgreSQL table](https://www.firecrawl.dev/blog/automated-competitor-price-scraping#step-3-store-new-products-in-postgresql-table)\n- [Step 4: Display products in the main dashboard](https://www.firecrawl.dev/blog/automated-competitor-price-scraping#step-4-display-products-in-the-main-dashboard)\n- [Step 5: Add a form to capture competing product URLs](https://www.firecrawl.dev/blog/automated-competitor-price-scraping#step-5-add-a-form-to-capture-competing-product-urls)\n- [Step 6: Scrape competitor product details](https://www.firecrawl.dev/blog/automated-competitor-price-scraping#step-6-scrape-competitor-product-details)\n- [Step 7: Store competitor product details in the database](https://www.firecrawl.dev/blog/automated-competitor-price-scraping#step-7-store-competitor-product-details-in-the-database)\n- [Step 8: Display competitors for each product](https://www.firecrawl.dev/blog/automated-competitor-price-scraping#step-8-display-competitors-for-each-product)\n- [Step 9: Write a script to update prices for all items](https://www.firecrawl.dev/blog/automated-competitor-price-scraping#step-9-write-a-script-to-update-prices-for-all-items)\n- [Step 10: Checking competitor prices regularly with GitHub Actions](https://www.firecrawl.dev/blog/automated-competitor-price-scraping#step-10-checking-competitor-prices-regularly-with-github-actions)\n- [Limitations and Considerations](https://www.firecrawl.dev/blog/automated-competitor-price-scraping#limitations-and-considerations)\n- [Conclusion](https://www.firecrawl.dev/blog/automated-competitor-price-scraping#conclusion)",
      "metadata": {
        "title": "How to Build an Automated Competitor Price Monitoring System with Python",
        "url": "https://www.firecrawl.dev/blog/automated-competitor-price-scraping"
      }
    },
    {
      "id": "3625ca00-ebfa-44e2-87f3-8fb4940afb5e",
      "source": "firecrawl/blog/automated-competitor-price-scraping.md",
      "content": "Step-by-Step Implementation > Step 1: Project setup\n\nFirst, letâ€™s set up our project environment. Create a new directory and initialize it with Poetry:  \n```bash\nmkdir competitor-price-monitor\ncd competitor-price-monitor\npoetry init\n\n```  \n[Poetry](https://www.python-poetry.org/) is a modern dependency management and packaging tool for Python that makes it easy to manage project dependencies, virtual environments, and package distribution. It provides a simpler and more intuitive way to handle Python project dependencies compared to `pip` and `requirements.txt` files. Read this [Poetry starter guide](https://www.datacamp.com/tutorial/python-poetry) if you are unfamiliar with the tool.  \nWhen you run the `poetry init` command, Poetry asks some questions to set up your project. When asked for the Python version, type `^3.10`. When asked for specifying dependencies interactively, type â€œnoâ€. You can press ENTER for other questions.  \nNext, you should create this basic project structure:  \n```bash\ncompetitor-price-monitor/\nâ”œâ”€â”€ src/\nâ”‚ â”œâ”€â”€ __init__.py\nâ”‚ â”œâ”€â”€ app.py\nâ”‚ â”œâ”€â”€ database.py\nâ”‚ â”œâ”€â”€ scraper.py\nâ”‚ â””â”€â”€ check_prices.py\nâ”œâ”€â”€ .env\nâ”œâ”€â”€ .gitignore\nâ”œâ”€â”€ pyproject.toml # Automatically created by poetry\nâ””â”€â”€ README.md\n\n```  \nHere are the commands:  \n```bash\nmkdir -p src\ntouch src/{__init__.py,app.py,database.py,scraper.py,check_prices.py}\ntouch .env .gitignore README.md\n\n# Create .gitignore content\ncat << 'EOF' > .gitignore\n__pycache__/\n*.py[cod]\n*.egg-info/\n.env\n.venv/\n.idea/\n.vscode/\n*.db\n*.log\n.DS_Store\nEOF\n\n```  \nNext, you should install the necessary dependencies with Poetry:  \n```bash\npoetry add streamlit firecrawl-py sqlalchemy psycopg2-binary python-dotenv pandas plotly\n\n```  \nThis command automatically resolves dependency versions and adds them to `pyproject.toml` file, which will be crucial later on.  \nLetâ€™s initialize Git and commit the changes:  \n```bash\ngit add .\ngit commit -m \"Initial commit\"\n\n```  \nFinally, start the Streamlit server to see app updates as you change the `src/app.py` file in the coming sections.  \n```bash\npoetry run streamlit run src/app.py\n\n```",
      "metadata": {
        "title": "How to Build an Automated Competitor Price Monitoring System with Python",
        "url": "https://www.firecrawl.dev/blog/automated-competitor-price-scraping"
      }
    },
    {
      "id": "a81a6b9f-e13e-45da-8f19-6ea10584b29a",
      "source": "firecrawl/blog/automated-competitor-price-scraping.md",
      "content": "Step-by-Step Implementation > Step 2: Add a sidebar to the UI for product input\n\nLetâ€™s start building the UI inside `src/app.py`:  \n```python\nimport streamlit as st\n\ndef main():\nst.title(\"Competitor Price Monitor\")\nst.markdown(\n\"##### Compare your product prices to competitors' prices. Input your product details and competitors' URLs to get started.\"\n)\nst.markdown(\"### Tracked Products\")\nst.markdown(\"---\")\n\n# Sidebar for adding new products\nwith st.sidebar:\nst.header(\"Add New Product\")\nadd_product()\n\ndef add_product():\npass\n\nif __name__ == \"__main__\":\nmain()\n\n```  \nWe begin by defining a `main` function that sets up the core UI components - a title, description, and section for tracked products in the main area, along with a sidebar containing a form to add new products using the `add_product` function. Letâ€™s define it next:  \n```python\n# src/app.py\n...\n\ndef add_product():\n\"\"\"Form to add a new product\"\"\"\nwith st.form(\"add_product\"):\nname = st.text_input(\"Product name\")\nprice = st.number_input(\"Your price\", min_value=0)\nurl = st.text_input(\"Product URL (optional)\")\n\nif st.form_submit_button(\"Add product\"):\nst.success(f\"Added product: {name}\")\nreturn True\nreturn False\n\n```  \nThe `add_product` function creates a form in the sidebar that allows users to input details for a new product they want to track. It collects the product name, price, and an optional URL. For now, when submitted, it displays a success message and returns True. In the next step, we will set up a database to store products added through this form.  \nFor now, your app must be looking like this:  \n![Screenshot of the initial Streamlit app showing a title \"Competitor Price Monitor\", a description, and a sidebar with a form to add new products](https://www.firecrawl.dev/images/blog/competitor_price_scraping/step1.png)  \nLetâ€™s commit the latest changes now:  \n```bash\ngit add .\ngit commit -m \"Add a form to collect new products\"\n\n```",
      "metadata": {
        "title": "How to Build an Automated Competitor Price Monitoring System with Python",
        "url": "https://www.firecrawl.dev/blog/automated-competitor-price-scraping"
      }
    },
    {
      "id": "d233a88e-d277-41f4-8e09-a070e99bf808",
      "source": "firecrawl/blog/automated-competitor-price-scraping.md",
      "content": "Step-by-Step Implementation > Step 3: Store new products in PostgreSQL table\n\nTo capture products entered through the form, we need an online database. In this case, Postgres is the best option since itâ€™s reliable and scalable.  \nThere are many platforms for hosting Postgres instances but the one I find the easiest and fastest to set up is Supabase. So, please head over to [the Supabase website](https://supabase.com/) and create your free account. During the sign-up process, you will be given a password, which you should save somewhere safe on your machine.  \nThen, in a few minutes, your free Postgres instance comes online. To connect to this instance, click on Home in the left sidebar and then, â€œConnectâ€:  \n![Screenshot of Supabase dashboard showing the database connection menu with various connection options including the Transaction pooler URI](https://www.firecrawl.dev/images/blog/competitor_price_scraping/supabase.png)  \nThe menu displays different ways to connect to your database. Weâ€™ll use the connection URI with transaction pooling enabled, as this provides the best balance of performance and reliability. So, copy the â€œTransaction poolerâ€ URI and return to your working environment.  \nOpen the `.env` file you created in the project setup step and paste the following variable:  \n```bash\nPOSTGRES_URL=\"THE-SUPABASE-URL-STRING-WITH-YOUR-PASSWORD-ADDED\"\n\n```  \nThe connection string contains a `[YOUR-PASSWORD]` placeholder which you should replace with the password you copied when creating your Supabase account and project (remove the brackets).  \nThen, open `src/database.py` and paste the following code:  \n```python\n# src/database.py\nfrom sqlalchemy import create_engine, Column, String, Float, DateTime, ForeignKey\nfrom sqlalchemy.orm import sessionmaker, relationship, declarative_base\nfrom datetime import datetime\nimport uuid\n\nBase = declarative_base()\n\nclass Product(Base):\n__tablename__ = \"products\"\n\nid = Column(String, primary_key=True, default=lambda: str(uuid.uuid4()))\nname = Column(String, nullable=False)\nyour_price = Column(Float, nullable=False)\nurl = Column(String)\n\n```  \nThe code above defines a SQLAlchemy model for storing product information in a PostgreSQL database. SQLAlchemy is a powerful Python SQL toolkit and Object-Relational Mapping (ORM) library that provides a high-level, Pythonic interface for interacting with databases.  \nKey benefits of SQLAlchemy include:  \n- Writing database operations using Python classes and methods instead of raw SQL\n- Automatic handling of database connections and transactions\n- Database-agnostic code that works across different SQL databases\n- Built-in security features to prevent SQL injection  \nThe `Product` model defines a table with the following columns:  \n- `id`: A unique identifier generated using UUID4\n- `name`: The product name (required)\n- `your_price`: The productâ€™s price in your store (required)\n- `url`: The product URL (optional)  \nThe model uses SQLAlchemyâ€™s declarative base system which automatically maps Python classes to database tables. When we create the tables, SQLAlchemy will generate the appropriate SQL schema based on these class definitions.  \nNow, return `src/app.py` and make the following below.  \n1. Change the imports:  \n```python\nimport os\nfrom database import Product, Base\nfrom dotenv import load_dotenv\nfrom sqlalchemy import create_engine\nfrom sqlalchemy.orm import sessionmaker\nimport streamlit as st\n\n```  \n2. Load environment variables from `.env` and set up the database:  \n```python\n# Load environment variables\nload_dotenv()\n\n# Database setup - creates the tables we specify\nengine = create_engine(os.getenv(\"POSTGRES_URL\"))\nBase.metadata.create_all(engine)\nSession = sessionmaker(bind=engine)\n\n```  \nThe code above sets up the core database infrastructure for our application. First, it loads environment variables from a `.env` file using `python-dotenv`, which allows us to securely store sensitive information like database credentials.  \nThen it initializes the `SQLAlchemy` engine by creating a connection to our PostgreSQL database using the `POSTGRES_URL` from our environment variables. The `create_all()` call creates any database tables that donâ€™t already exist, based on the models we defined (like the `Product` model).  \nFinally, it creates a Session factory using `sessionmaker`. Sessions handle database transactions and provide an interface for querying and modifying data. Each database operation will create a new `Session` instance to ensure thread safety.  \n3. Update the `add_product()` function:  \n```python\ndef add_product():\n\"\"\"Form to add a new product\"\"\"\nwith st.form(\"add_product\"):\nname = st.text_input(\"Product name\")\nprice = st.number_input(\"Your price\", min_value=0)\nurl = st.text_input(\"Product URL (optional)\")\n\nif st.form_submit_button(\"Add product\"):\nsession = Session()\nproduct = Product(name=name, your_price=price, url=url)\nsession.add(product)\nsession.commit()\nsession.close()\nst.success(f\"Added product: {name}\")\nreturn True\nreturn False\n\n```  \nThis time, when the form is submitted, a new Product instance is created and saved to the database using SQLAlchemyâ€™s session management.  \n* * *  \nThe rest of the script stays the same. Now, try adding a few products through the form - the functionality stays the same but the products are captured in the database.  \nThen, commit the latest changes to Git:  \n```bash\ngit add .\ngit commit -m \"Connect the sidebar form to a database table\"\n\n```",
      "metadata": {
        "title": "How to Build an Automated Competitor Price Monitoring System with Python",
        "url": "https://www.firecrawl.dev/blog/automated-competitor-price-scraping"
      }
    },
    {
      "id": "7a6f7d5d-bd29-468a-9c11-effa306b5118",
      "source": "firecrawl/blog/automated-competitor-price-scraping.md",
      "content": "Step-by-Step Implementation > Step 4: Display products in the main dashboard\n\nWhen the user opens our app, products existing in the database must be shown automatically like below:  \n![Screenshot showing the main dashboard with tracked products. Each product has its name, price, and buttons for visiting and deleting the product.](https://www.firecrawl.dev/images/blog/competitor_price_scraping/step4.png)  \nRight below the â€œTracked Productsâ€ text, we want to display each productâ€™s name, its price, and two buttons for visiting and deleting the product. Letâ€™s implement these features in `src/app.py`.  \n1. Import the `webbrowser` module:  \n```python\n# src/app.py\n... # the rest of the imports\nimport webbrowser\n\n```  \nThis module provides functions to open URLs in the default web browser, which weâ€™ll use for the â€œVisit productâ€ button.  \n2. Add a function to delete a product from the Products table:  \n```python\ndef delete_product(product_id: str):\n\"\"\"Delete a product and all its competitors\"\"\"\nsession = Session()\nproduct = session.query(Product).filter_by(id=product_id).first()\nif product:\nsession.delete(product)\nsession.commit()\nsession.close()\n\n```  \nThe `delete_product` function takes a product ID as a string parameter and removes the corresponding product from the database. It first creates a new database session, then queries the Products table to find the product with the matching ID. If found, it deletes the product and commits the change to persist it. Finally, it closes the database session to free up resources. This function will be called when the user clicks the delete button for a specific product.  \n3. Add a function to display existing products with Streamlit `container` components:  \n```python\ndef display_product_details(product):\n\"\"\"Display details for a single product\"\"\"\nst.subheader(product.name)\ncols = st.columns([1, 2])\nwith cols[0]:\nst.metric(\nlabel=\"Your Price\",\nvalue=f\"${product.your_price:.2f}\",\n)\nwith cols[1]:\ncol1, col2 = st.columns(2)\nwith col1:\nif product.url:\nst.button(\n\"Visit product\",\nkey=f\"visit_btn_{product.id}\",\nuse_container_width=True,\non_click=lambda: webbrowser.open_new_tab(product.url),\n)\nelse:\nst.text(\"No URL provided\")\nwith col2:\nst.button(\n\"ðŸ—‘ï¸ Delete\",\nkey=f\"delete_btn_{product.id}\",\ntype=\"primary\",\nuse_container_width=True,\non_click=lambda: delete_product(product.id),\n)\n\n```  \nThe `display_product_details` function takes a product object as input and creates a nicely formatted display layout using Streamlit components. It shows the product name as a subheader and splits the display into two main columns.  \nIn the first column (1/3 width), it displays the productâ€™s price using Streamlitâ€™s metric component, which shows the price formatted with a dollar sign and 2 decimal places.  \nThe second column (2/3 width) is further divided into two equal sub-columns that contain action buttons:  \n1. A â€œVisit productâ€ button that opens the product URL in a new browser tab (if a URL exists), otherwise displays â€œNo URL providedâ€.\n2. A â€œDeleteâ€ button with a trash can emoji that calls the `delete_product` function to remove the product from the database.  \nThe function uses Streamlitâ€™s column layout system to create a responsive grid layout, and the buttons are configured to use the full width of their containers. Each button is given a unique key based on the product ID to ensure proper rendering.  \n4. Update the `main` function to use the `display_product_details` function when the app is loaded:  \n```python\ndef main():\nst.title(\"Competitor Price Monitor\")\nst.markdown(\n\"##### Compare your product prices to competitors' prices. Input your product details and competitors' URLs to get started.\"\n)\nst.markdown(\"### Tracked Products\")\nst.markdown(\"---\")\n\n# Sidebar for adding new products\nwith st.sidebar:\nst.header(\"Add New Product\")\nadd_product()\n\n# Main content area\nsession = Session()\nproducts = session.query(Product).all()\n\nif not products:\nst.info(\"No products added yet. Use the sidebar to add your first product.\")\nelse:\nfor product in products:\nwith st.container():\ndisplay_product_details(product)\n\nsession.close()\n\n```  \nThe updated main function adds several key improvements:  \nThe main content area now displays all tracked products in an organized way. It first queries the database to get all products, then handles two scenarios:  \n- If no products exist, it shows a helpful message guiding users to add their first product\n- If products exist, it displays each one in its own container using the `display_product_details` function, which creates a consistent and professional layout for each product  \nThe function also properly manages database connections by creating a session at the start and ensuring itâ€™s closed when done, following best practices for database handling.  \nAfter this step, the fake products you added in the last step must be visible in the dashboard:  \n![Screenshot showing the product dashboard with sample products displayed in a grid layout, including product names, prices, and action buttons](https://www.firecrawl.dev/images/blog/competitor_price_scraping/step4_2.png)  \nCheck to see if the buttons work (which they should!), then remove all fake products to verify that the app correctly displays a message when there are no products in the database.  \nThen, commit the latest changes in your terminal:  \n```bash\ngit add .\ngit commit -m \"Display existing products in the database\"\n\n```",
      "metadata": {
        "title": "How to Build an Automated Competitor Price Monitoring System with Python",
        "url": "https://www.firecrawl.dev/blog/automated-competitor-price-scraping"
      }
    },
    {
      "id": "6f984b1b-d370-4ae6-b1a1-9125a5e59b97",
      "source": "firecrawl/blog/automated-competitor-price-scraping.md",
      "content": "Step-by-Step Implementation > Step 5: Add a form to capture competing product URLs\n\nNow, the app must allow users to add any number of competing products to each product. To enable this functionality, we will add a small form to capture competing product URLs in each product container.  \nInside `src/app.py`, add this new function:  \n```python\ndef add_competitor_form(product, session):\n\"\"\"Form to add a new competitor\"\"\"\nwith st.expander(\"Add new competitor\", expanded=False):\nwith st.form(f\"add_competitor_{product.id}\"):\ncompetitor_url = st.text_input(\"ðŸ”— Competitor product URL\")\ncol1, col2 = st.columns([3, 1])\nwith col2:\nsubmit = st.form_submit_button(\n\"Add competitor\", use_container_width=True\n)\n\nif submit:\n# TODO: Add competitor to the database\nst.success(\"Competitor added successfully!\")\n\n```  \nThe `add_competitor_form` function creates a collapsible form for each product that allows users to add competitor URLs. It takes a product and database session as parameters.  \nInside the form, it displays a text input field for the competitorâ€™s URL and a submit button in a two-column layout. When submitted, it currently just shows a success message since the database functionality is not yet implemented.  \nThe form is hidden by default but can be expanded by clicking. Each form has a unique key based on the product ID to avoid conflicts when multiple products are displayed.  \nTo add the form to the app, update the `main` function again:  \n```python\ndef main():\n... # The rest of the function\n\nif not products:\nst.info(\"No products added yet. Use the sidebar to add your first product.\")\nelse:\nfor product in products:\nwith st.container():\ndisplay_product_details(product)\nadd_competitor_form(product, session)\n\nsession.close()\n\n```  \nHere, we are adding a single line of code that calls the `add_competitor_form` function. After this change, the app must be looking like this:  \n![Screenshot showing a product container with an expanded \"Add new competitor\" form. The form contains a URL input field and an \"Add competitor\" button.](https://www.firecrawl.dev/images/blog/competitor_price_scraping/step5.png)  \nCapture the changes again with Git:  \n```bash\ngit add .\ngit commit -m \"Add a form to capture competitor product URLs\"\n\n```",
      "metadata": {
        "title": "How to Build an Automated Competitor Price Monitoring System with Python",
        "url": "https://www.firecrawl.dev/blog/automated-competitor-price-scraping"
      }
    },
    {
      "id": "d1f589f9-27e1-4fe3-92f9-0532461ab376",
      "source": "firecrawl/blog/automated-competitor-price-scraping.md",
      "content": "Step-by-Step Implementation > Step 6: Scrape competitor product details\n\nWhen a user enters a competing product URL, the app must automatically scrape that products details like name, brand, and most importantly its price. Then, this information must be saved to a database, all under the hood. In this step, we will implement the scraping part using [Firecrawl](https://firecrawl.dev/).  \nFirecrawl is an AI-powered scraping API that can automatically extract the information you need without relying on HTML and CSS selectors. So, you need to [sign up for a free Firecrawl account](https://firecrawl.dev/) for this step. After the sign up, you will be given an API key, which you should store as an environment variable in your `.env` file like below:  \n```bash\nPOSTGRES_URL=\"YOUR-POSTGRES-URL\"\nFIRECRAWL_API_KEY=\"fc-YOUR-FC-API-KEY\"\n\n```  \nThe next step is to add the following imports to `src/scraper.py`:  \n```python\n# src/scraper.py\nimport warnings\n\nwarnings.filterwarnings(\"ignore\")\n\nfrom datetime import datetime\nfrom firecrawl import FirecrawlApp\nfrom pydantic import BaseModel, Field\nfrom dotenv import load_dotenv\n\nload_dotenv()\napp = FirecrawlApp()\n\n```  \nThe imports above set up the scraping functionality:  \n- `warnings` is used to suppress unnecessary warning messages\n- `datetime` for handling timestamps\n- `FirecrawlApp` class provides the AI-powered scraping capabilities\n- `BaseModel` and `Field` from `pydantic` for data validation\n- `load_dotenv` to load environment variables\n- The `FirecrawlApp` instance is initialized and ready to use  \nNext, we create a new Pydantic model to specify the structure of scraped information about each competitor product:  \n```python\n# src/scraper.py\nclass CompetitorProduct(BaseModel):\n\"\"\"Schema for extracting competitor product data\"\"\"\n\nname: str = Field(description=\"The name/title of the product\")\nprice: float = Field(description=\"The current price of the product\")\nimage_url: str | None = Field(None, description=\"URL of the main product image\")\n\n```  \nPydantic models are data validation classes that help define the structure and types of data you want to extract. When used with Firecrawl, they act as a schema that tells the AI what information to look for and extract from web pages.  \nThe `CompetitorProduct` model above defines three fields that we want to extract from competitor product pages:  \n1. `name`: A required string field for the product title/name\n2. `price`: A required float field for the productâ€™s price\n3. `image_url`: An optional string field for the productâ€™s main image URL  \nThe `Field` descriptions provide hints to Firecrawlâ€™s AI about what each field represents, helping it accurately identify and extract the right information from diverse webpage layouts. This model will be used to automatically parse competitor product pages into a consistent format.  \nNext, we define a function that performs the scraping process with Firecrawl:  \n```python\ndef scrape_competitor_product(url: str) -> dict:\n\"\"\"\nScrape product information from a competitor's webpage\n\"\"\"\nextracted_data = app.scrape_url(\nurl,\nparams={\n\"formats\": [\"extract\"],\n\"extract\": {\n\"schema\": CompetitorProduct.model_json_schema(),\n},\n},\n)\n\n# Add timestamp to the extracted data\ndata = extracted_data[\"extract\"]\ndata[\"last_checked\"] = datetime.utcnow()\n\nreturn data\n\n```  \nThe `scrape_competitor_product` function takes a URL as input and uses Firecrawlâ€™s AI-powered scraping capabilities to extract product information from competitor websites. Hereâ€™s how it works:  \n1. The function accepts a single parameter `url` which is the webpage to scrape\n2. It calls Firecrawlâ€™s `scrape_url` method with two key parameters:\n- `formats`: Specifies we want to use the â€œextractâ€ format for structured data extraction\n- `extract`: Provides the `Pydantic` schema that defines what data to extract\n3. The `CompetitorProduct` schema is converted to JSON format and passed as the extraction template\n4. After scraping, it pulls out just the extracted data from the response\n5. A timestamp is added to track when the price was checked\n6. Finally, it returns a dictionary containing the product name, price, image URL and timestamp  \nThis function abstracts away the complexity of web scraping by using Firecrawlâ€™s AI to automatically locate and extract the relevant product details, regardless of the websiteâ€™s specific structure or layout.  \nIn the next step, we will capture the information returned by `scrape_competitor_product` to a database table.  \nYou can commit the changes now:  \n```bash\ngit add .\ngit commit -m \"Create a function to scrape competitor product details\"\n\n```",
      "metadata": {
        "title": "How to Build an Automated Competitor Price Monitoring System with Python",
        "url": "https://www.firecrawl.dev/blog/automated-competitor-price-scraping"
      }
    },
    {
      "id": "a5f066b3-340e-40e2-8fe7-1df0d8ff76d0",
      "source": "firecrawl/blog/automated-competitor-price-scraping.md",
      "content": "Step-by-Step Implementation > Step 7: Store competitor product details in the database\n\nWhen a user clicks the â€œAdd competitorâ€ button, the app must pass the entered URL to the `scrape_competitor_product` function. The function will scrape the competitorâ€™s product details and passes them to a dedicated database table for competitor products. So, letâ€™s create that table in `src/database.py`:  \n```python\n# Update the `Product` table to add a link to the `competitors` table\nclass Product(Base):\n__tablename__ = \"products\"\n\nid = Column(String, primary_key=True, default=lambda: str(uuid.uuid4()))\nname = Column(String, nullable=False)\nyour_price = Column(Float, nullable=False)\nurl = Column(String)\ncompetitors = relationship(\n\"Competitor\", back_populates=\"product\", cascade=\"all, delete-orphan\"\n)\n\nclass Competitor(Base):\n__tablename__ = \"competitors\"\n\nid = Column(String, primary_key=True, default=lambda: str(uuid.uuid4()))\nproduct_id = Column(String, ForeignKey(\"products.id\"))\nurl = Column(String, nullable=False)\nname = Column(String)\ncurrent_price = Column(Float)\nlast_checked = Column(DateTime, default=datetime.utcnow)\nimage_url = Column(String)\nproduct = relationship(\"Product\", back_populates=\"competitors\")\n\n```  \nThe above code snippet first adds a new `competitors` attribute to the `Products` table. This creates a one-to-many relationship between products and competitors. It also creates a new `Competitor` table to store details about competitor products, including their URL, name, current price, last checked timestamp, and image URL. The relationship is set up with cascading deletes, so when a product is removed through the delete button in the UI, all its competitor entries are automatically deleted as well.  \nNow, letâ€™s piece together the `scrape_competitor_product` function from the last step and the database table in the UI so that `Add competitor` button works as expected.  \n1. Add the following imports to `src/app.py`:  \n```python\nimport time\nfrom scraper import scrape_competitor_product\nfrom database import Competitor\n\n```  \n2. Replace the `add_competitor_form` function with this new version:  \n```python\ndef add_competitor_form(product, session):\n\"\"\"Form to add a new competitor\"\"\"\nwith st.expander(\"Add new competitor\", expanded=False):\nwith st.form(f\"add_competitor_{product.id}\"):\ncompetitor_url = st.text_input(\"ðŸ”— Competitor product URL\")\ncol1, col2 = st.columns([3, 1])\nwith col2:\nsubmit = st.form_submit_button(\n\"Add competitor\", use_container_width=True\n)\n\nif submit:\ntry:\nwith st.spinner(\"Fetching competitor data...\"):\ndata = scrape_competitor_product(competitor_url)\ncompetitor = Competitor(\nproduct_id=product.id,\nurl=competitor_url,\nname=data[\"name\"],\ncurrent_price=data[\"price\"],\nimage_url=data.get(\"image_url\"),\nlast_checked=data[\"last_checked\"],\n)\nsession.add(competitor)\nsession.commit()\nst.success(\"âœ… Competitor added successfully!\")\n\n# Refresh the page\ntime.sleep(1)\nst.rerun()\nexcept Exception as e:\nst.error(f\"âŒ Error adding competitor: {str(e)}\")\n\n```  \nThis new version of the add_competitor_form function adds several key improvements:  \n1. It adds error handling with `try/except` to gracefully handle scraping failures\n2. It shows a loading spinner while scraping the competitor URL\n3. It creates a proper Competitor database record linked to the product\n4. It displays success/error messages to give feedback to the user\n5. It automatically refreshes the page after adding a competitor using `st.rerun()`\n6. It properly commits the new competitor to the database session  \nThe function now provides a much better user experience with visual feedback and proper error handling while integrating with both the scraper and database components.  \nAfter making these changes, try adding a few fake competitors through the app. Any product link from Amazon, eBay, BestBuy or other e-commerce stores will work.  \nThen, return to your terminal to commit the latest changes:  \n```bash\ngit add .\ngit commit -m \"Link the Add competitor button to the database\"\n\n```",
      "metadata": {
        "title": "How to Build an Automated Competitor Price Monitoring System with Python",
        "url": "https://www.firecrawl.dev/blog/automated-competitor-price-scraping"
      }
    },
    {
      "id": "3991efec-efcf-4d0b-81a0-25a88c061bf3",
      "source": "firecrawl/blog/automated-competitor-price-scraping.md",
      "content": "Step-by-Step Implementation > Step 8: Display competitors for each product\n\nNow that the competitor form is working properly, we need to update the UI again so that existing competitors are shown below each product in our database. Here is our target layout after these changes are implemented:  \n![Screenshot showing a product container with expanded competitor details. Each competitor has their price, price difference percentage, last checked timestamp, and action buttons for visiting and deleting the competitor.](https://www.firecrawl.dev/images/blog/competitor_price_scraping/step8.png)  \nLooking at the screenshot, we can see several key changes to the product layout:  \n1. Each product now has a collapsible â€œView competitorsâ€ section that expands to show all tracked competitors\n2. For each competitor, we display:\n- The competitorâ€™s price (e.g. $799.99)\n- The price difference percentage compared to our price (e.g. -20.0%)\n- When the price was last checked (e.g. 2025-01-02 16:34)\n- A â€œVisit productâ€ link to view the competitorâ€™s page\n- A delete button to remove the competitor\n3. The competitor prices are formatted with currency symbols and proper decimal places\n4. The price difference is color-coded (red for higher prices, green for lower)\n5. The â€œAdd new competitorâ€ section remains as an expandable form below the competitors list\n6. Products with no competitors show a â€œNo competitors added yetâ€ message  \nLetâ€™s implement these UI improvements to match this new layout.  \nFirst, add this single import statement to `src/app.py`:  \n```python\nfrom urllib.parse import urlparse\n\n```  \nThe `urlparse` function allows us to extract the domain of a URL.  \nNext, letâ€™s create a function that will later be linked to a button to delete a competitor:  \n```python\n# src/app.py\ndef delete_competitor(competitor_id: str):\n\"\"\"Delete a competitor\"\"\"\nsession = Session()\ncompetitor = session.query(Competitor).filter_by(id=competitor_id).first()\nif competitor:\nsession.delete(competitor)\nsession.commit()\nsession.close()\n\n```  \nThe `delete_competitor` function takes a competitor ID as input and removes that competitor from the database. It opens a database session, queries for the competitor with the given ID, deletes it if found, commits the change, and closes the session.  \nNext, create a function to display competitor details:  \n```python\ndef display_competitor_metrics(product, comp):\n\"\"\"Display competitor price comparison metrics\"\"\"\nst.markdown(f\"#### {urlparse(comp.url).netloc}\")\ncols = st.columns([1, 2, 1, 1])\n\ndiff = ((comp.current_price - product.your_price) / product.your_price) * 100\n\nwith cols[0]:\nst.metric(\nlabel=\"ðŸ’° Competitor price\",\nvalue=f\"${comp.current_price:.2f}\",\ndelta=f\"{diff:+.1f}%\",\ndelta_color=\"normal\",\n)\nwith cols[1]:\nst.markdown(f\"**ðŸ•’ Checked:** {comp.last_checked.strftime('%Y-%m-%d %H:%M')}\")\nwith cols[2]:\nst.button(\n\"Visit product\",\nkey=f\"visit_btn_{comp.id}\",\nuse_container_width=True,\non_click=lambda: webbrowser.open_new_tab(comp.url),\n)\nwith cols[3]:\nst.button(\n\"ðŸ—‘ï¸\",\nkey=f\"delete_comp_btn_{comp.id}\",\ntype=\"primary\",\nuse_container_width=True,\non_click=lambda: delete_competitor(comp.id),\n)\n\n```  \nThe `display_competitor_metrics` function is responsible for showing competitor price information in a structured layout using Streamlit components. Hereâ€™s what it does:  \n1. Takes two parameters:\n- `product`: The userâ€™s product object containing price and details\n- `comp`: The competitor product object with pricing data\n2. Creates a header showing the competitorâ€™s domain name (extracted from their URL)  \n3. Sets up a 4-column layout to display:\n- Column 1: Shows the competitorâ€™s price with a percentage difference from your price\n- Column 2: Shows when the price was last checked/updated\n- Column 3: Contains a button to visit the competitorâ€™s product page\n- Column 4: Contains a delete button to remove this competitor\n4. Calculates the price difference percentage between your price and competitorâ€™s price  \n5. Uses Streamlit components:\n- `st.metric`: Displays the competitor price with the percentage difference\n- `st.markdown`: Shows the last checked timestamp\n- `st.button`: Creates interactive buttons for visiting and deleting\n6. Integrates with other functions:\n- Uses `webbrowser.open_new_tab()` for the visit button\n- Calls `delete_competitor()` when delete button is clicked  \nThis function creates an interactive and informative UI element that helps users monitor and manage competitor pricing data effectively.  \nThen, create another function to display all competitors using `display_competitor_metrics`:  \n```python\ndef display_competitors(product):\n\"\"\"Display all competitors for a product\"\"\"\nif product.competitors:\nwith st.expander(\"View competitors\", expanded=False):\nfor comp in product.competitors:\ndisplay_competitor_metrics(product, comp)\nelse:\nst.info(\"No competitors added yet\")\n\n```  \nThe `display_competitors` function takes a product object as input and displays all competitors associated with that product. If the product has competitors, it creates an expandable section using Streamlitâ€™s expander component. Inside this section, it iterates through each competitor and calls the `display_competitor_metrics` function to show detailed pricing information and controls for that competitor. If no competitors exist for the product, it shows an informational message indicating that no competitors have been added yet.  \nFinally, letâ€™s update the `main` function to use this last `display_competitors` function:  \n```python\ndef main():\n# ... the rest of the function\n\nif not products:\nst.info(\"No products added yet. Use the sidebar to add your first product.\")\nelse:\nfor product in products:\nwith st.container():\ndisplay_product_details(product)\ndisplay_competitors(product) # This line is new\nadd_competitor_form(product, session)\n\nsession.close()\n\n```  \nNow, the app is almost ready! We just need to implement a feature to update competitor product prices regularly. Letâ€™s tackle that in the last two sections.  \n```bash\ngit add .\ngit commit -m \"Display competitors for each product\"\n\n```",
      "metadata": {
        "title": "How to Build an Automated Competitor Price Monitoring System with Python",
        "url": "https://www.firecrawl.dev/blog/automated-competitor-price-scraping"
      }
    },
    {
      "id": "99160604-7bd8-4090-90b4-c8bd77209d37",
      "source": "firecrawl/blog/automated-competitor-price-scraping.md",
      "content": "Step-by-Step Implementation > Step 9: Write a script to update prices for all items\n\nCurrently, once a competing product is added to our database, its details is never updated. Obviously, we have to fix this issue as websites regularly change product prices and run flash sales and discounts. Thatâ€™s where the `src/check_prices.py` script comes into play.  \nOnce ready, it must rerun the scraper on all existing competing products in the database and fetch their latest details. Letâ€™s start by making the following imports and setup:  \n```python\nimport os\nfrom database import Base, Product, Competitor\nfrom sqlalchemy import create_engine\nfrom sqlalchemy.orm import sessionmaker\nfrom scraper import scrape_competitor_product\nfrom dotenv import load_dotenv\n\nload_dotenv()\n\n# Database setup\nengine = create_engine(os.getenv(\"POSTGRES_URL\"))\nBase.metadata.create_all(engine)\nSession = sessionmaker(bind=engine)\n\n```  \nThen, we create a single function called `update_competitor_prices`:  \n```python\ndef update_competitor_prices():\n\"\"\"Update all competitor prices\"\"\"\nsession = Session()\ncompetitors = session.query(Competitor).all()\n\nfor competitor in competitors:\ntry:\n# Scrape updated data\ndata = scrape_competitor_product(competitor.url)\n\n# Update competitor\ncompetitor.current_price = data[\"price\"]\ncompetitor.last_checked = data[\"last_checked\"]\n\nprint(f\"Updated price for {competitor.name}: ${data['price']}\")\nexcept Exception as e:\nprint(f\"Error updating {competitor.name}: {str(e)}\")\n\nsession.commit()\nsession.close()\n\n```  \nThe function iterates through all competitor products in the database and fetches their latest prices and details. For each competitor, it:  \n1. Makes a new web request to scrape the current price using `scrape_competitor_product` function\n2. Updates the competitor record in the database with the new price and timestamp\n3. Prints success/failure messages for monitoring\n4. Commits all changes to persist the updates  \nThis allows us to keep our competitor price data fresh and track price changes over time. The function handles errors gracefully by catching and logging exceptions for any competitors that fail to update.  \nFinally, we add the following code to the end of the script to allow running the script directly:  \n```python\nif __name__ == \"__main__\":\nupdate_competitor_prices()\n\n```  \nTry testing the function and refreshing your local Streamlit instance to see the â€œLast checkedâ€ timestamp change for each competitor. Then, you can commit the changes:  \n```bash\ngit add .\ngit commit -m \"Create a script to check all competitor prices\"\n\n```",
      "metadata": {
        "title": "How to Build an Automated Competitor Price Monitoring System with Python",
        "url": "https://www.firecrawl.dev/blog/automated-competitor-price-scraping"
      }
    },
    {
      "id": "c2616f75-027b-4271-a6e5-765ce54efcd8",
      "source": "firecrawl/blog/automated-competitor-price-scraping.md",
      "content": "Step-by-Step Implementation > Step 10: Checking competitor prices regularly with GitHub Actions\n\nNow that we have a script to update competitor prices, we need to run it automatically every few hours. GitHub Actions is perfect for this task since itâ€™s free and integrates well with our Git repository.  \nFirst, you should create a new GitHub repository to push your code and then, add it as the remote origin:  \n```bash\ngit add remote origin url-to-your-repo\ngit push\n\n```  \nThen, create a new `.github/workflows` directory structure in your project root:  \n```bash\nmkdir -p .github/workflows\n\n```  \nThe `.github/workflows` directory is where GitHub looks for workflow definition files. These files define automated tasks (called Actions) that GitHub can run for your repository. By creating this directory structure, weâ€™re setting up the foundation for automated competitor price checking using GitHub Actions.  \n> Note: if you are unfamiliar with GitHub Actions, check out our [deploying web scrapers tutorial](https://www.firecrawl.dev/blog/deploy-web-scrapers) to learn more about it.  \nNext step is exporting our project dependencies to a standard `requirements.txt` file as GitHub Actions need it to create the correct virtual environment to execute `src/check_prices.py`:  \n```bash\npoetry export -f requirements.txt --output requirements.txt\n\n```  \nThen, create a new file at `.github/workflows/update-prices.yml` with the following contents:  \n```yaml\nname: Check competitor prices\n\non:\nschedule:\n# Runs every 6 hours\n- cron: \"0 0,6,12,18 * * *\"\nworkflow_dispatch: # Allows manual triggering\n\njobs:\ncheck-prices:\nruns-on: ubuntu-latest\n\nsteps:\n- name: Checkout code\nuses: actions/checkout@v4\n\n- name: Set up Python\nuses: actions/setup-python@v5\nwith:\npython-version: \"3.10\"\ncache: \"pip\"\n\n- name: Install dependencies\nrun: |\npython -m pip install --upgrade pip\npip install -r requirements.txt\n\n- name: Run price checker\nenv:\nFIRECRAWL_API_KEY: ${{ secrets.FIRECRAWL_API_KEY }}\nPOSTGRES_URL: ${{ secrets.POSTGRES_URL }}\nrun: python src/check_prices.py\n\n```  \nThis workflow does several things:  \n1. Runs automatically every 6 hours using a cron schedule\n2. Can be triggered manually through GitHubâ€™s UI\n3. Sets up Python and Poetry\n4. Installs project dependencies\n5. Runs our price update script with the necessary environment variables  \nTo make this work, you need to add your environment variables as GitHub repository secrets:  \n1. Go to your GitHub repository\n2. Click Settings â†’ Secrets and variables â†’ Actions\n3. Click â€œNew repository secretâ€\n4. Add both `POSTGRES_URL` and `FIRECRAWL_API_KEY` with their values  \nAfter committing this file, GitHub Actions will start updating your competitor prices automatically every 6 hours UTC time:  \n```bash\ngit add .\ngit commit -m \"Add GitHub Actions workflow to update prices\"\ngit push\n\n```  \nYou can monitor the workflow runs in the â€œActionsâ€ tab of your GitHub repository. Each run will show you which competitor prices were successfully updated and any errors that occurred.  \nThatâ€™s it! You now have a fully automated competitor price monitoring system. The app will keep track of your products, let you add competitors easily, and automatically update their prices every 6 hours using GitHub Actions.",
      "metadata": {
        "title": "How to Build an Automated Competitor Price Monitoring System with Python",
        "url": "https://www.firecrawl.dev/blog/automated-competitor-price-scraping"
      }
    },
    {
      "id": "1b503873-e095-4a89-aaa9-b4cf82f445f2",
      "source": "firecrawl/blog/automated-competitor-price-scraping.md",
      "content": "Step-by-Step Implementation > Limitations and Considerations > 1. Free Tier Restrictions\n\n- **GitHub Actions Minutes**: GitHub provides 2,000 minutes/month of free Actions runtime for public repositories. With our 6-hour schedule (4 runs daily), this averages around 120-240 minutes per month depending on script runtime. Monitor your usage to avoid exceeding limits.  \n- **Firecrawl API Limits**: The free tier of Firecrawl has request limitations. Consider implementing retry logic and error handling for rate limit responses.",
      "metadata": {
        "title": "How to Build an Automated Competitor Price Monitoring System with Python",
        "url": "https://www.firecrawl.dev/blog/automated-competitor-price-scraping"
      }
    },
    {
      "id": "12cf1761-1ebf-4405-a9df-d8c89fb4e6d7",
      "source": "firecrawl/blog/automated-competitor-price-scraping.md",
      "content": "Step-by-Step Implementation > Limitations and Considerations > 2. Legal Considerations\n\n- Always review and respect websitesâ€™ `robots.txt` files and Terms of Service\n- Some e-commerce sites explicitly prohibit automated price monitoring\n- Consider implementing delays between requests to avoid overwhelming target servers\n- Store only essential data to comply with data protection regulations",
      "metadata": {
        "title": "How to Build an Automated Competitor Price Monitoring System with Python",
        "url": "https://www.firecrawl.dev/blog/automated-competitor-price-scraping"
      }
    },
    {
      "id": "a4cc5eb7-e582-4572-8e2e-657c1a1a8f7c",
      "source": "firecrawl/blog/automated-competitor-price-scraping.md",
      "content": "Step-by-Step Implementation > Limitations and Considerations > 3. Data Accuracy\n\n- Price scraping can be affected by:\n- Regional pricing differences\n- Dynamic pricing systems\n- Special offers and discounts\n- Currency variations\n- Product variants/options",
      "metadata": {
        "title": "How to Build an Automated Competitor Price Monitoring System with Python",
        "url": "https://www.firecrawl.dev/blog/automated-competitor-price-scraping"
      }
    },
    {
      "id": "28d98ec2-974c-47d5-a3dc-100ede59f787",
      "source": "firecrawl/blog/automated-competitor-price-scraping.md",
      "content": "Step-by-Step Implementation > Limitations and Considerations > 4. Technical Limitations\n\n- **Database Growth**: Regular price checks will continuously grow your database. Implement data retention policies or archiving strategies for older price records.  \n- **Error Handling**: The current implementation provides basic error handling. In production, consider adding:  \n```python\ndef scrape_competitor_product(url: str) -> dict:\nretries = 3\nwhile retries > 0:\ntry:\n# Existing scraping code\nexcept Exception as e:\nretries -= 1\nif retries == 0:\nraise e\ntime.sleep(5) # Wait before retry\n\n```  \n- **Maintenance Requirements**: Regular monitoring is needed to ensure:\n- Database connection stability\n- API key validity\n- Scraping accuracy\n- GitHub Actions workflow status",
      "metadata": {
        "title": "How to Build an Automated Competitor Price Monitoring System with Python",
        "url": "https://www.firecrawl.dev/blog/automated-competitor-price-scraping"
      }
    },
    {
      "id": "64e1fc1f-4524-4634-a752-ec90731ea104",
      "source": "firecrawl/blog/automated-competitor-price-scraping.md",
      "content": "Step-by-Step Implementation > Limitations and Considerations > 6. Monitoring and Alerting\n\n- The current implementation lacks comprehensive monitoring\n- Consider adding:\n- Scraping success/failure metrics\n- Database performance monitoring\n- API usage tracking\n- System health checks",
      "metadata": {
        "title": "How to Build an Automated Competitor Price Monitoring System with Python",
        "url": "https://www.firecrawl.dev/blog/automated-competitor-price-scraping"
      }
    },
    {
      "id": "266fb576-c950-47fe-a1d9-174a5c3a89cd",
      "source": "firecrawl/blog/automated-competitor-price-scraping.md",
      "content": "Step-by-Step Implementation > Limitations and Considerations > 7. Cost Implications for Scaling\n\nAs your monitoring needs grow, consider the costs of:  \n- Upgraded database plans\n- Additional GitHub Actions minutes\n- Premium API tiers\n- Monitoring tools\n- Error tracking services  \nBy understanding these limitations and planning accordingly, you can build a more robust and reliable price monitoring system that scales with your needs while staying within technical and legal boundaries.",
      "metadata": {
        "title": "How to Build an Automated Competitor Price Monitoring System with Python",
        "url": "https://www.firecrawl.dev/blog/automated-competitor-price-scraping"
      }
    },
    {
      "id": "4ae0e1f9-b3c7-468d-99bf-524728bafffe",
      "source": "firecrawl/blog/automated-competitor-price-scraping.md",
      "content": "Conclusion\n\nIn this tutorial, weâ€™ve built a complete competitor price monitoring system that automatically tracks product prices across multiple e-commerce websites. Hereâ€™s what we accomplished:  \n1. Created a user-friendly web interface with Streamlit for managing products and competitors\n2. Implemented AI-powered web scraping using Firecrawl to handle diverse website layouts\n3. Set up a PostgreSQL database with SQLAlchemy for reliable data storage\n4. Automated regular price updates using GitHub Actions\n5. Added error handling and basic monitoring capabilities",
      "metadata": {
        "title": "How to Build an Automated Competitor Price Monitoring System with Python",
        "url": "https://www.firecrawl.dev/blog/automated-competitor-price-scraping"
      }
    },
    {
      "id": "f859fe7c-ebe8-4b9d-8205-50324dcbc158",
      "source": "firecrawl/blog/automated-competitor-price-scraping.md",
      "content": "Conclusion > Next Steps\n\nTo build upon this foundation:  \n1. Deploy the Streamlit app to a cloud platform\n2. Set up automated backups for your database\n3. Implement a more comprehensive monitoring system\n4. Add user authentication for multi-user support\n5. Create an API for programmatic access to price data",
      "metadata": {
        "title": "How to Build an Automated Competitor Price Monitoring System with Python",
        "url": "https://www.firecrawl.dev/blog/automated-competitor-price-scraping"
      }
    },
    {
      "id": "15ad6685-2505-4293-97e5-f63b5a77dbb6",
      "source": "firecrawl/blog/automated-competitor-price-scraping.md",
      "content": "Conclusion > Additional Resources\n\n- [Streamlit Documentation](https://docs.streamlit.io/)\n- [Firecrawl API Reference](https://docs.firecrawl.dev/)\n- [SQLAlchemy Tutorial](https://docs.sqlalchemy.org/en/20/tutorial/)\n- [GitHub Actions Documentation](https://docs.github.com/en/actions)\n- [Supabase Documentation](https://supabase.com/docs)  \nThe complete source code for this project is available on [GitHub](https://github.com/FirstClassML/firecrawl_projects/tree/main/competitor-price-monitor).  \nArticle updated recently  \n[ðŸ”¥](https://www.firecrawl.dev/)",
      "metadata": {
        "title": "How to Build an Automated Competitor Price Monitoring System with Python",
        "url": "https://www.firecrawl.dev/blog/automated-competitor-price-scraping"
      }
    },
    {
      "id": "99072dfb-e68a-4860-8d14-08c6d776eea7",
      "source": "firecrawl/blog/automated-competitor-price-scraping.md",
      "content": "Ready to _Build?_\n\nStart scraping web data for your AI apps today.  \nNo credit card needed.  \nGet Started",
      "metadata": {
        "title": "How to Build an Automated Competitor Price Monitoring System with Python",
        "url": "https://www.firecrawl.dev/blog/automated-competitor-price-scraping"
      }
    },
    {
      "id": "b1e0139b-f30d-486c-b19b-0c78a7646cf8",
      "source": "firecrawl/blog/automated-competitor-price-scraping.md",
      "content": "About the Author\n\n[![Bex Tuychiev image](https://www.firecrawl.dev/bex.jpg)\\\nBex Tuychiev@bextuychiev](https://x.com/bextuychiev)  \nBex is a Top 10 AI writer on Medium and a Kaggle Master with over 15k followers. He loves writing detailed guides, tutorials, and notebooks on complex data science and machine learning topics",
      "metadata": {
        "title": "How to Build an Automated Competitor Price Monitoring System with Python",
        "url": "https://www.firecrawl.dev/blog/automated-competitor-price-scraping"
      }
    },
    {
      "id": "d28f0afe-0c54-45e5-b6b5-2ad0e6c62f1f",
      "source": "firecrawl/blog/automated-competitor-price-scraping.md",
      "content": "About the Author > More articles by Bex Tuychiev\n\n[Building an Automated Price Tracking Tool\\\n\\\nBuild an automated e-commerce price tracker in Python. Learn web scraping, price monitoring, and automated alerts using Firecrawl, Streamlit, PostgreSQL.](https://www.firecrawl.dev/blog/automated-price-tracking-tutorial-python) [Web Scraping Automation: How to Run Scrapers on a Schedule\\\n\\\nLearn how to automate web scraping in Python using free tools like schedule, asyncio, cron jobs and GitHub Actions. This comprehensive guide covers local and cloud-based scheduling methods to run scrapers reliably in 2025.](https://www.firecrawl.dev/blog/automated-web-scraping-free-2025) [Automated Data Collection - A Comprehensive Guide\\\n\\\nLearn how to build robust automated data collection systems using modern tools and best practices. This guide covers everything from selecting the right tools to implementing scalable collection pipelines.](https://www.firecrawl.dev/blog/automated-data-collection-guide) [BeautifulSoup4 vs. Scrapy - A Comprehensive Comparison for Web Scraping in Python\\\n\\\nLearn the key differences between BeautifulSoup4 and Scrapy for web scraping in Python. Compare their features, performance, and use cases to choose the right tool for your web scraping needs.](https://www.firecrawl.dev/blog/beautifulsoup4-vs-scrapy-comparison) [How to Build an Automated Competitor Price Monitoring System with Python\\\n\\\nLearn how to build an automated competitor price monitoring system in Python that tracks prices across e-commerce sites, provides real-time comparisons, and maintains price history using Firecrawl, Streamlit, and GitHub Actions.](https://www.firecrawl.dev/blog/automated-competitor-price-scraping) [Scraping Company Data and Funding Information in Bulk With Firecrawl and Claude\\\n\\\nLearn how to build a web scraper in Python that gathers company details, funding rounds, and investor information from public sources like Crunchbase using Firecrawl and Claude for automated data collection and analysis.](https://www.firecrawl.dev/blog/crunchbase-scraping-with-firecrawl-claude) [Data Enrichment: A Complete Guide to Enhancing Your Data Quality\\\n\\\nLearn how to enrich your data quality with a comprehensive guide covering data enrichment tools, best practices, and real-world examples. Discover how to leverage modern solutions like Firecrawl to automate data collection, validation, and integration for better business insights.](https://www.firecrawl.dev/blog/complete-guide-to-data-enrichment) [Building an Intelligent Code Documentation RAG Assistant with DeepSeek and Firecrawl\\\n\\\nLearn how to build an intelligent documentation assistant powered by DeepSeek and RAG (Retrieval Augmented Generation) that can answer questions about any documentation website by combining language models with efficient information retrieval.](https://www.firecrawl.dev/blog/deepseek-rag-documentation-assistant)",
      "metadata": {
        "title": "How to Build an Automated Competitor Price Monitoring System with Python",
        "url": "https://www.firecrawl.dev/blog/automated-competitor-price-scraping"
      }
    },
    {
      "id": "557737db-4221-4aa1-894c-6b44cbe3ea1a",
      "source": "firecrawl/blog/Build-a-Full-Stack-AI-Web-App-in-12-Minutes.md",
      "content": "---\ntitle: Build a Full-Stack AI Web App in 12 Minutes\nurl: https://www.firecrawl.dev/blog/Build-a-Full-Stack-AI-Web-App-in-12-Minutes\n---  \nIntroducing /extract - Get web data with a prompt [Try now](https://www.firecrawl.dev/extract)  \nSep 18, 2024  \nâ€¢  \n[![Dev Digest image](https://www.firecrawl.dev/devdigest.jpg)Dev Digest](https://x.com/dev__digest)  \n# Build a Full-Stack AI Web App in 12 Minutes  \nBuild a Full-Stack AI Web App in 12 Minutes: Cursor, OpenAI o1, V0, Firecrawl & Patched - YouTube  \nDevelopers Digest  \n35.3K subscribers  \n[Build a Full-Stack AI Web App in 12 Minutes: Cursor, OpenAI o1, V0, Firecrawl & Patched](https://www.youtube.com/watch?v=utEzCj5ASPY)  \nDevelopers Digest  \nSearch  \nInfo  \nShopping  \nTap to unmute  \nIf playback doesn't begin shortly, try restarting your device.  \nYou're signed out  \nVideos you watch may be added to the TV's watch history and influence TV recommendations. To avoid this, cancel and sign in to YouTube on your computer.  \nCancelConfirm  \nShare  \nInclude playlist  \nAn error occurred while retrieving sharing information. Please try again later.  \nWatch later  \nShare  \nCopy link  \nWatch on  \n0:00  \n/ â€¢Live  \nâ€¢  \n[Watch on YouTube](https://www.youtube.com/watch?v=utEzCj5ASPY \"Watch on YouTube\")  \nIn this video, I share an app idea inspired by a viral tweet and demonstrate how to build a consultant report generator using the latest AI tools. Weâ€™ll leverage OpenAIâ€™s new O1 model, Firecrawl for web crawling, TipTap for text editing, Cursor for development, and Patched for automating workflows. Follow along as I walk through each step, from idea of the application to deploying the app on Vercel.  \nArticle updated recently",
      "metadata": {
        "title": "Build a Full-Stack AI Web App in 12 Minutes",
        "url": "https://www.firecrawl.dev/blog/Build-a-Full-Stack-AI-Web-App-in-12-Minutes"
      }
    },
    {
      "id": "b4324d64-6985-4e86-a5ff-9b48cf78cd0f",
      "source": "firecrawl/blog/Build-a-Full-Stack-AI-Web-App-in-12-Minutes.md",
      "content": "About the Author\n\n[![Dev Digest image](https://www.firecrawl.dev/devdigest.jpg)\\\nDev Digest@dev__digest](https://x.com/dev__digest)  \nDev Digest is a YouTube channel that creates educational content on software development.  \n[ðŸ”¥](https://www.firecrawl.dev/)",
      "metadata": {
        "title": "Build a Full-Stack AI Web App in 12 Minutes",
        "url": "https://www.firecrawl.dev/blog/Build-a-Full-Stack-AI-Web-App-in-12-Minutes"
      }
    },
    {
      "id": "20e8ffd9-5a95-46c2-8c9c-5935fd05abae",
      "source": "firecrawl/blog/Build-a-Full-Stack-AI-Web-App-in-12-Minutes.md",
      "content": "Ready to _Build?_\n\nStart scraping web data for your AI apps today.  \nNo credit card needed.  \nGet Started",
      "metadata": {
        "title": "Build a Full-Stack AI Web App in 12 Minutes",
        "url": "https://www.firecrawl.dev/blog/Build-a-Full-Stack-AI-Web-App-in-12-Minutes"
      }
    },
    {
      "id": "ef1f8f5a-58d9-47c6-88ab-bb44327b9cab",
      "source": "firecrawl/blog/scrape-job-boards-firecrawl-openai.md",
      "content": "---\ntitle: Scraping Job Boards Using Firecrawl Actions and OpenAI\nurl: https://www.firecrawl.dev/blog/scrape-job-boards-firecrawl-openai\n---  \nIntroducing /extract - Get web data with a prompt [Try now](https://www.firecrawl.dev/extract)  \nSept 27, 2024  \nâ€¢  \n[![Eric Ciarla image](https://www.firecrawl.dev/eric-img.jpeg)Eric Ciarla](https://x.com/ericciarla)  \n# Scraping Job Boards Using Firecrawl Actions and OpenAI  \n![Scraping Job Boards Using Firecrawl Actions and OpenAI image](https://www.firecrawl.dev/images/blog/firecrawl-openai-job-scraping.jpg)  \nScraping job boards to extract structured data can be a complex task, especially when dealing with dynamic websites and unstructured content. In this guide, weâ€™ll walk through how to use [Firecrawl Actions](https://firecrawl.dev/) and OpenAI models to efficiently scrape job listings and extract valuable information.",
      "metadata": {
        "title": "Scraping Job Boards Using Firecrawl Actions and OpenAI",
        "url": "https://www.firecrawl.dev/blog/scrape-job-boards-firecrawl-openai"
      }
    },
    {
      "id": "5ac982a2-ac40-4e61-bcfd-0ad01c2d4e3d",
      "source": "firecrawl/blog/scrape-job-boards-firecrawl-openai.md",
      "content": "Why Use Firecrawl and OpenAI?\n\n- **Firecrawl** simplifies web scraping by handling dynamic content and providing actions like clicking and scrolling.\n- **OpenAIâ€™s `o1` and `4o` models** excel at understanding and extracting structured data from unstructured text. `o1` is best for more complex reasoning tasks while `4o` is best for speed and cost.",
      "metadata": {
        "title": "Scraping Job Boards Using Firecrawl Actions and OpenAI",
        "url": "https://www.firecrawl.dev/blog/scrape-job-boards-firecrawl-openai"
      }
    },
    {
      "id": "72103191-6985-4602-86e8-55397def7f76",
      "source": "firecrawl/blog/scrape-job-boards-firecrawl-openai.md",
      "content": "Prerequisites\n\n- Python 3.7 or higher\n- API keys for both [Firecrawl](https://firecrawl.dev/) and [OpenAI](https://openai.com/)\n- Install required libraries:  \n```bash\npip install requests python-dotenv openai\n\n```",
      "metadata": {
        "title": "Scraping Job Boards Using Firecrawl Actions and OpenAI",
        "url": "https://www.firecrawl.dev/blog/scrape-job-boards-firecrawl-openai"
      }
    },
    {
      "id": "527c17c5-44a3-44a9-b7c3-2079b687e997",
      "source": "firecrawl/blog/scrape-job-boards-firecrawl-openai.md",
      "content": "Step 1: Set Up Your Environment\n\nCreate a `.env` file in your project directory and add your API keys:  \n```\nFIRECRAWL_API_KEY=your_firecrawl_api_key\nOPENAI_API_KEY=your_openai_api_key\n\n```",
      "metadata": {
        "title": "Scraping Job Boards Using Firecrawl Actions and OpenAI",
        "url": "https://www.firecrawl.dev/blog/scrape-job-boards-firecrawl-openai"
      }
    },
    {
      "id": "baf4d76d-ab1c-4585-92d0-86422960b57b",
      "source": "firecrawl/blog/scrape-job-boards-firecrawl-openai.md",
      "content": "Step 2: Initialize API Clients\n\n```python\nimport os\nimport requests\nimport json\nfrom dotenv import load_dotenv\nimport openai\n\n# Load environment variables\nload_dotenv()\n\n# Initialize API keys\nfirecrawl_api_key = os.getenv(\"FIRECRAWL_API_KEY\")\nopenai.api_key = os.getenv(\"OPENAI_API_KEY\")\n\n```",
      "metadata": {
        "title": "Scraping Job Boards Using Firecrawl Actions and OpenAI",
        "url": "https://www.firecrawl.dev/blog/scrape-job-boards-firecrawl-openai"
      }
    },
    {
      "id": "bc449ac7-d232-4c29-b2e6-c9994ecb813e",
      "source": "firecrawl/blog/scrape-job-boards-firecrawl-openai.md",
      "content": "Step 3: Define the Jobs Page URL and Resume\n\nSpecify the URL of the jobs page you want to scrape and provide your resume for matching.  \n```python\n# URL of the jobs page to scrape\njobs_page_url = \"https://openai.com/careers/search\"\n\n# Candidate's resume (as a string)\nresume_paste = \"\"\"\n[Your resume content here]\n\"\"\"\n\n```",
      "metadata": {
        "title": "Scraping Job Boards Using Firecrawl Actions and OpenAI",
        "url": "https://www.firecrawl.dev/blog/scrape-job-boards-firecrawl-openai"
      }
    },
    {
      "id": "da61a72d-68cf-4c56-8adf-d46f2a54c0d7",
      "source": "firecrawl/blog/scrape-job-boards-firecrawl-openai.md",
      "content": "Step 4: Scrape the Jobs Page Using Firecrawl\n\nWe use Firecrawl to scrape the jobs page and extract the HTML content.  \n```python\ntry:\nresponse = requests.post(\n\"https://api.firecrawl.dev/v1/scrape\",\nheaders={\n\"Content-Type\": \"application/json\",\n\"Authorization\": f\"Bearer {firecrawl_api_key}\"\n},\njson={\n\"url\": jobs_page_url,\n\"formats\": [\"markdown\"]\n}\n)\nif response.status_code == 200:\nresult = response.json()\nif result.get('success'):\nhtml_content = result['data']['markdown']\n# Prepare the prompt for OpenAI\nprompt = f\"\"\"\nExtract up to 30 job application links from the given markdown content.\nReturn the result as a JSON object with a single key 'apply_links' containing an array of strings (the links).\nThe output should be a valid JSON object, with no additional text.\n\nMarkdown content:\n{html_content[:100000]}\n\"\"\"\nelse:\nhtml_content = \"\"\nelse:\nhtml_content = \"\"\nexcept Exception as e:\nhtml_content = \"\"\n\n```",
      "metadata": {
        "title": "Scraping Job Boards Using Firecrawl Actions and OpenAI",
        "url": "https://www.firecrawl.dev/blog/scrape-job-boards-firecrawl-openai"
      }
    },
    {
      "id": "89b605df-3b66-4613-873e-880b77dc28cd",
      "source": "firecrawl/blog/scrape-job-boards-firecrawl-openai.md",
      "content": "Step 5: Extract Apply Links Using OpenAIâ€™s `gpt-4o` Model\n\nWe use OpenAIâ€™s `gpt-4o` model to parse the scraped content and extract application links.  \n```python\n# Extract apply links using OpenAI\napply_links = []\nif html_content:\ntry:\ncompletion = openai.ChatCompletion.create(\nmodel=\"gpt-4o\",\nmessages=[\\\n{\\\n\"role\": \"user\",\\\n\"content\": prompt\\\n}\\\n]\n)\nif completion.choices:\nresult = json.loads(completion.choices[0].message.content.strip())\napply_links = result['apply_links']\nexcept Exception as e:\npass\n\n```",
      "metadata": {
        "title": "Scraping Job Boards Using Firecrawl Actions and OpenAI",
        "url": "https://www.firecrawl.dev/blog/scrape-job-boards-firecrawl-openai"
      }
    },
    {
      "id": "cb3779a7-1a59-4586-961a-865583a269a3",
      "source": "firecrawl/blog/scrape-job-boards-firecrawl-openai.md",
      "content": "Step 6: Extract Job Details from Each Apply Link\n\nWe iterate over each apply link and use Firecrawlâ€™s extraction capabilities to get job details.  \n```python\n# Initialize a list to store job data\nextracted_data = []\n\n# Define the extraction schema\nschema = {\n\"type\": \"object\",\n\"properties\": {\n\"job_title\": {\"type\": \"string\"},\n\"sub_division_of_organization\": {\"type\": \"string\"},\n\"key_skills\": {\"type\": \"array\", \"items\": {\"type\": \"string\"}},\n\"compensation\": {\"type\": \"string\"},\n\"location\": {\"type\": \"string\"},\n\"apply_link\": {\"type\": \"string\"}\n},\n\"required\": [\"job_title\", \"sub_division_of_organization\", \"key_skills\", \"compensation\", \"location\", \"apply_link\"]\n}\n\n# Extract job details for each link\nfor link in apply_links:\ntry:\nresponse = requests.post(\n\"https://api.firecrawl.dev/v1/scrape\",\nheaders={\n\"Content-Type\": \"application/json\",\n\"Authorization\": f\"Bearer {firecrawl_api_key}\"\n},\njson={\n\"url\": link,\n\"formats\": [\"extract\"],\n\"actions\": [{\\\n\"type\": \"click\",\\\n\"selector\": \"#job-overview\"\\\n}],\n\"extract\": {\n\"schema\": schema\n}\n}\n)\nif response.status_code == 200:\nresult = response.json()\nif result.get('success'):\nextracted_data.append(result['data']['extract'])\nexcept Exception as e:\npass\n\n```",
      "metadata": {
        "title": "Scraping Job Boards Using Firecrawl Actions and OpenAI",
        "url": "https://www.firecrawl.dev/blog/scrape-job-boards-firecrawl-openai"
      }
    },
    {
      "id": "bcd56586-c720-4ccb-bd67-e03d41cffc08",
      "source": "firecrawl/blog/scrape-job-boards-firecrawl-openai.md",
      "content": "Step 7: Match Jobs to Your Resume Using OpenAIâ€™s `o1` Model\n\nWe use OpenAIâ€™s `o1` model to analyze your resume and recommend the top 3 job listings.  \n```python\n# Prepare the prompt\nprompt = f\"\"\"\nPlease analyze the resume and job listings, and return a JSON list of the top 3 roles that best fit the candidate's experience and skills. Include only the job title, compensation, and apply link for each recommended role. The output should be a valid JSON array of objects in the following format:\n\n[\\\n{\\\n\"job_title\": \"Job Title\",\\\n\"compensation\": \"Compensation\",\\\n\"apply_link\": \"Application URL\"\\\n},\\\n...\\\n]\n\nBased on the following resume:\n{resume_paste}\n\nAnd the following job listings:\n{json.dumps(extracted_data, indent=2)}\n\"\"\"\n\n# Get recommendations from OpenAI\ncompletion = openai.ChatCompletion.create(\nmodel=\"o1-preview\",\nmessages=[\\\n{\\\n\"role\": \"user\",\\\n\"content\": prompt\\\n}\\\n]\n)\n\n# Extract recommended jobs\nrecommended_jobs = json.loads(completion.choices[0].message.content.strip())\n\n```",
      "metadata": {
        "title": "Scraping Job Boards Using Firecrawl Actions and OpenAI",
        "url": "https://www.firecrawl.dev/blog/scrape-job-boards-firecrawl-openai"
      }
    },
    {
      "id": "bb6180f2-70cd-4374-b204-2a2023d8043c",
      "source": "firecrawl/blog/scrape-job-boards-firecrawl-openai.md",
      "content": "Step 8: Output the Recommended Jobs\n\nFinally, we can print or save the recommended jobs.  \n```python\n# Output the recommended jobs\nprint(json.dumps(recommended_jobs, indent=2))\n\n```",
      "metadata": {
        "title": "Scraping Job Boards Using Firecrawl Actions and OpenAI",
        "url": "https://www.firecrawl.dev/blog/scrape-job-boards-firecrawl-openai"
      }
    },
    {
      "id": "73a31cee-d983-4011-899a-fca1602a64f1",
      "source": "firecrawl/blog/scrape-job-boards-firecrawl-openai.md",
      "content": "Full Code Example on GitHub\n\nYou can find the full code example [on GitHub](https://github.com/mendableai/firecrawl/tree/main/examples/o1_job_recommender).",
      "metadata": {
        "title": "Scraping Job Boards Using Firecrawl Actions and OpenAI",
        "url": "https://www.firecrawl.dev/blog/scrape-job-boards-firecrawl-openai"
      }
    },
    {
      "id": "a1fe5191-7f28-4bf9-9826-650ff4da9b1e",
      "source": "firecrawl/blog/scrape-job-boards-firecrawl-openai.md",
      "content": "Conclusion\n\nBy following this guide, youâ€™ve learned how to:  \n- **Scrape dynamic job boards** using Firecrawl.\n- **Extract structured data** from web pages with custom schemas.\n- **Leverage OpenAIâ€™s models** to parse content and make intelligent recommendations.  \nThis approach can be extended to other websites and data extraction tasks, providing a powerful toolset for automating data collection and analysis.",
      "metadata": {
        "title": "Scraping Job Boards Using Firecrawl Actions and OpenAI",
        "url": "https://www.firecrawl.dev/blog/scrape-job-boards-firecrawl-openai"
      }
    },
    {
      "id": "ea630faa-c4e4-40d6-af4d-8f438053d486",
      "source": "firecrawl/blog/scrape-job-boards-firecrawl-openai.md",
      "content": "References\n\n- [Firecrawl Documentation](https://docs.firecrawl.dev/)\n- [OpenAI API Reference](https://platform.openai.com/docs/api-reference/introduction)  \nThatâ€™s it! Youâ€™ve now built a pipeline to scrape job boards and find the best job matches using Firecrawl and OpenAI. Happy coding!  \nArticle updated recently  \n[ðŸ”¥](https://www.firecrawl.dev/)",
      "metadata": {
        "title": "Scraping Job Boards Using Firecrawl Actions and OpenAI",
        "url": "https://www.firecrawl.dev/blog/scrape-job-boards-firecrawl-openai"
      }
    },
    {
      "id": "a3a5377b-439d-4472-82a7-b654fbc819b7",
      "source": "firecrawl/blog/scrape-job-boards-firecrawl-openai.md",
      "content": "Ready to _Build?_\n\nStart scraping web data for your AI apps today.  \nNo credit card needed.  \nGet Started",
      "metadata": {
        "title": "Scraping Job Boards Using Firecrawl Actions and OpenAI",
        "url": "https://www.firecrawl.dev/blog/scrape-job-boards-firecrawl-openai"
      }
    },
    {
      "id": "2c71df85-83fc-4290-9df8-125bc71ad496",
      "source": "firecrawl/blog/scrape-job-boards-firecrawl-openai.md",
      "content": "About the Author\n\n[![Eric Ciarla image](https://www.firecrawl.dev/eric-img.jpeg)\\\nEric Ciarla@ericciarla](https://x.com/ericciarla)  \nEric Ciarla is the Chief Operating Officer (COO) of Firecrawl and leads marketing. He also worked on Mendable.ai and sold it to companies like Snapchat, Coinbase, and MongoDB.\nPreviously worked at Ford and Fracta as a Data Scientist. Eric also co-founded SideGuide, a tool for learning code within VS Code with 50,000 users.",
      "metadata": {
        "title": "Scraping Job Boards Using Firecrawl Actions and OpenAI",
        "url": "https://www.firecrawl.dev/blog/scrape-job-boards-firecrawl-openai"
      }
    },
    {
      "id": "9e79fa2e-7560-488b-940c-dfdfbc4bc0c3",
      "source": "firecrawl/blog/scrape-job-boards-firecrawl-openai.md",
      "content": "About the Author > More articles by Eric Ciarla\n\n[How to Create an llms.txt File for Any Website\\\n\\\nLearn how to generate an llms.txt file for any website using the llms.txt Generator and Firecrawl.](https://www.firecrawl.dev/blog/How-to-Create-an-llms-txt-File-for-Any-Website) [Cloudflare Error 1015: How to solve it?\\\n\\\nCloudflare Error 1015 is a rate limiting error that occurs when Cloudflare detects that you are exceeding the request limit set by the website owner.](https://www.firecrawl.dev/blog/cloudflare-error-1015-how-to-solve-it) [Build an agent that checks for website contradictions\\\n\\\nUsing Firecrawl and Claude to scrape your website's data and look for contradictions.](https://www.firecrawl.dev/blog/contradiction-agent) [Why Companies Need a Data Strategy for Generative AI\\\n\\\nLearn why a well-defined data strategy is essential for building robust, production-ready generative AI systems, and discover practical steps for curation, maintenance, and integration.](https://www.firecrawl.dev/blog/why-companies-need-a-data-strategy-for-generative-ai) [Getting Started with OpenAI's Predicted Outputs for Faster LLM Responses\\\n\\\nA guide to leveraging Predicted Outputs to speed up LLM tasks with GPT-4o models.](https://www.firecrawl.dev/blog/getting-started-with-predicted-outputs-openai) [How to easily install requests with pip and python\\\n\\\nA tutorial on installing the requests library in Python using various methods, with usage examples and troubleshooting tips](https://www.firecrawl.dev/blog/how-to-easily-install-requests-with-pip-and-python) [How to quickly install BeautifulSoup with Python\\\n\\\nA guide on installing the BeautifulSoup library in Python using various methods, with usage examples and troubleshooting tips](https://www.firecrawl.dev/blog/how-to-quickly-install-beautifulsoup-with-python) [How to Use OpenAI's o1 Reasoning Models in Your Applications\\\n\\\nLearn how to harness OpenAI's latest o1 series models for complex reasoning tasks in your apps.](https://www.firecrawl.dev/blog/how-to-use-openai-o1-reasoning-models-in-applications)",
      "metadata": {
        "title": "Scraping Job Boards Using Firecrawl Actions and OpenAI",
        "url": "https://www.firecrawl.dev/blog/scrape-job-boards-firecrawl-openai"
      }
    },
    {
      "id": "85ce05d1-12f6-49be-b4c2-e65a37435b5a",
      "source": "firecrawl/blog/launch-week-i-day-2-doubled-rate-limits.md",
      "content": "---\ntitle: Launch Week I / Day 2: 2x Rate Limits\nurl: https://www.firecrawl.dev/blog/launch-week-i-day-2-doubled-rate-limits\n---  \nIntroducing /extract - Get web data with a prompt [Try now](https://www.firecrawl.dev/extract)  \nAugust 27, 2024  \nâ€¢  \n[![Eric Ciarla image](https://www.firecrawl.dev/eric-img.jpeg)Eric Ciarla](https://x.com/ericciarla)  \n# Launch Week I / Day 2: 2x Rate Limits  \n![Launch Week I / Day 2: 2x Rate Limits image](https://www.firecrawl.dev/images/blog/firecrawl-rate-limits.png)  \nWelcome to Day 2 of Firecrawlâ€™s inaugural Launch Week! Weâ€™re thrilled to keep the momentum going with another exciting announcement that will significantly boost your scraping capabilities.  \n**Introducing: Doubled Rate Limits**  \nToday, weâ€™re doubling the rate limits for our /scrape endpoint across all plans. This means you can now collect twice as much data in the same amount of time, supercharging your data collection projects.  \nHereâ€™s a breakdown of the new limits:  \n- Free Plan: Now 10 requests/minute\n- Hobby Plan: Now 20 requests/minute\n- Standard Plan: Now 100 requests/minute\n- Growth Plan: Now 1000 requests/minute  \n**Why Weâ€™re Doing This**  \nWeâ€™ve heard your feedback loud and clear. Many of you have been pushing the boundaries of whatâ€™s possible with scraping, and we want to support your ambitions. By doubling our rate limits, weâ€™re enabling you to tackle larger projects, scrape more frequently, and ultimately, derive more value from our platform.  \n**Important Note**  \nThese increased limits apply to our main scraping endpoint. The crawling endpoint limits remain unchanged at 1, 3, 10, and 50 requests per minute for Free, Hobby, Standard, and Growth plans respectively.  \n**Whatâ€™s Next?**  \nWeâ€™re just getting started! Keep an eye out for more exciting announcements in the coming days of Launch Week.  \nHappy scraping, and see you tomorrow for Day 3 of Launch Week!  \nArticle updated recently  \n[ðŸ”¥](https://www.firecrawl.dev/)",
      "metadata": {
        "title": "Launch Week I / Day 2: 2x Rate Limits",
        "url": "https://www.firecrawl.dev/blog/launch-week-i-day-2-doubled-rate-limits"
      }
    },
    {
      "id": "3169d009-70c8-4b26-b40d-2708e718dcee",
      "source": "firecrawl/blog/launch-week-i-day-2-doubled-rate-limits.md",
      "content": "Ready to _Build?_\n\nStart scraping web data for your AI apps today.  \nNo credit card needed.  \nGet Started",
      "metadata": {
        "title": "Launch Week I / Day 2: 2x Rate Limits",
        "url": "https://www.firecrawl.dev/blog/launch-week-i-day-2-doubled-rate-limits"
      }
    },
    {
      "id": "0c4ce02a-90f6-4b53-a772-64c26971eece",
      "source": "firecrawl/blog/launch-week-i-day-2-doubled-rate-limits.md",
      "content": "About the Author\n\n[![Eric Ciarla image](https://www.firecrawl.dev/eric-img.jpeg)\\\nEric Ciarla@ericciarla](https://x.com/ericciarla)  \nEric Ciarla is the Chief Operating Officer (COO) of Firecrawl and leads marketing. He also worked on Mendable.ai and sold it to companies like Snapchat, Coinbase, and MongoDB.\nPreviously worked at Ford and Fracta as a Data Scientist. Eric also co-founded SideGuide, a tool for learning code within VS Code with 50,000 users.",
      "metadata": {
        "title": "Launch Week I / Day 2: 2x Rate Limits",
        "url": "https://www.firecrawl.dev/blog/launch-week-i-day-2-doubled-rate-limits"
      }
    },
    {
      "id": "500d2897-3995-4934-92a1-fedbb664d8e4",
      "source": "firecrawl/blog/launch-week-i-day-2-doubled-rate-limits.md",
      "content": "About the Author > More articles by Eric Ciarla\n\n[How to Create an llms.txt File for Any Website\\\n\\\nLearn how to generate an llms.txt file for any website using the llms.txt Generator and Firecrawl.](https://www.firecrawl.dev/blog/How-to-Create-an-llms-txt-File-for-Any-Website) [Cloudflare Error 1015: How to solve it?\\\n\\\nCloudflare Error 1015 is a rate limiting error that occurs when Cloudflare detects that you are exceeding the request limit set by the website owner.](https://www.firecrawl.dev/blog/cloudflare-error-1015-how-to-solve-it) [Build an agent that checks for website contradictions\\\n\\\nUsing Firecrawl and Claude to scrape your website's data and look for contradictions.](https://www.firecrawl.dev/blog/contradiction-agent) [Why Companies Need a Data Strategy for Generative AI\\\n\\\nLearn why a well-defined data strategy is essential for building robust, production-ready generative AI systems, and discover practical steps for curation, maintenance, and integration.](https://www.firecrawl.dev/blog/why-companies-need-a-data-strategy-for-generative-ai) [Getting Started with OpenAI's Predicted Outputs for Faster LLM Responses\\\n\\\nA guide to leveraging Predicted Outputs to speed up LLM tasks with GPT-4o models.](https://www.firecrawl.dev/blog/getting-started-with-predicted-outputs-openai) [How to easily install requests with pip and python\\\n\\\nA tutorial on installing the requests library in Python using various methods, with usage examples and troubleshooting tips](https://www.firecrawl.dev/blog/how-to-easily-install-requests-with-pip-and-python) [How to quickly install BeautifulSoup with Python\\\n\\\nA guide on installing the BeautifulSoup library in Python using various methods, with usage examples and troubleshooting tips](https://www.firecrawl.dev/blog/how-to-quickly-install-beautifulsoup-with-python) [How to Use OpenAI's o1 Reasoning Models in Your Applications\\\n\\\nLearn how to harness OpenAI's latest o1 series models for complex reasoning tasks in your apps.](https://www.firecrawl.dev/blog/how-to-use-openai-o1-reasoning-models-in-applications)",
      "metadata": {
        "title": "Launch Week I / Day 2: 2x Rate Limits",
        "url": "https://www.firecrawl.dev/blog/launch-week-i-day-2-doubled-rate-limits"
      }
    },
    {
      "id": "1b94bde8-1160-4eec-bfab-d47bb4c5efba",
      "source": "firecrawl/blog/introducing-extract-open-beta.md",
      "content": "---\ntitle: Introducing /extract: Get structured web data with just a prompt\nurl: https://www.firecrawl.dev/blog/introducing-extract-open-beta\n---  \nIntroducing /extract - Get web data with a prompt [Try now](https://www.firecrawl.dev/extract)  \nJanuary 20, 2025  \nâ€¢  \n[![Eric Ciarla image](https://www.firecrawl.dev/eric-img.jpeg)Eric Ciarla](https://x.com/ericciarla)  \n# Introducing /extract: Get structured web data with just a prompt  \n/extract by Firecrawl - Get structured web data with just a prompt (Open Beta) - YouTube  \nFirecrawl  \n472 subscribers  \n[/extract by Firecrawl - Get structured web data with just a prompt (Open Beta)](https://www.youtube.com/watch?v=Qq1pFm8enZo)  \nFirecrawl  \nSearch  \nInfo  \nShopping  \nTap to unmute  \nIf playback doesn't begin shortly, try restarting your device.  \nYou're signed out  \nVideos you watch may be added to the TV's watch history and influence TV recommendations. To avoid this, cancel and sign in to YouTube on your computer.  \nCancelConfirm  \nShare  \nInclude playlist  \nAn error occurred while retrieving sharing information. Please try again later.  \nWatch later  \nShare  \nCopy link  \nWatch on  \n0:00  \n/ â€¢Live  \nâ€¢  \n[Watch on YouTube](https://www.youtube.com/watch?v=Qq1pFm8enZo \"Watch on YouTube\")",
      "metadata": {
        "title": "Introducing /extract: Get structured web data with just a prompt",
        "url": "https://www.firecrawl.dev/blog/introducing-extract-open-beta"
      }
    },
    {
      "id": "cab502b8-3bc2-4622-91f7-80c1833439ff",
      "source": "firecrawl/blog/introducing-extract-open-beta.md",
      "content": "The era of writing web scrapers is over\n\nToday weâ€™re releasing [/extract](https://www.firecrawl.dev/extract) - write a prompt, get structured data from any website. No scrapers. No pipelines. Just results.",
      "metadata": {
        "title": "Introducing /extract: Get structured web data with just a prompt",
        "url": "https://www.firecrawl.dev/blog/introducing-extract-open-beta"
      }
    },
    {
      "id": "aa39ddf7-7170-4fa6-a34c-3f944cda8fdf",
      "source": "firecrawl/blog/introducing-extract-open-beta.md",
      "content": "Getting web data is hard\n\nIf youâ€™ve ever needed structured data from websitesâ€”whether to enrich your CRM, monitor competitors, or power various applicationsâ€”youâ€™re probably familiar with the frustrating options available today:  \n- Manually researching and copy-pasting data from multiple sources, consuming countless hours\n- Writing and maintaining fragile web scrapers that break at the slightest site change\n- Using scraping services and building complex LLM pipelines with limited context windows that force you to break down data manually  \nFortunately, with our /extract endpoint, you can leave these cumbersome approaches in the past and focus on what matters - getting the data you need.",
      "metadata": {
        "title": "Introducing /extract: Get structured web data with just a prompt",
        "url": "https://www.firecrawl.dev/blog/introducing-extract-open-beta"
      }
    },
    {
      "id": "2cc7e5a0-8572-4023-887f-f32eec9bc580",
      "source": "firecrawl/blog/introducing-extract-open-beta.md",
      "content": "What You Can Build With /extract\n\nCompanies are already using /extract to:  \n- Enrich thousands of CRM leads with company data\n- Automate KYB processes with structured business information\n- Track competitor prices and feature changes in real-time\n- Build targeted prospecting lists at scale  \nHereâ€™s how it works:  \n1. Write a prompt describing the data you need\n2. Point us at any website (use wildcards like example.com/*)\n3. Get back clean, structured JSON  \nNo more broken scrapers. No more complex pipelines. Just the data you need to build.",
      "metadata": {
        "title": "Introducing /extract: Get structured web data with just a prompt",
        "url": "https://www.firecrawl.dev/blog/introducing-extract-open-beta"
      }
    },
    {
      "id": "ac5792c0-6726-41d4-9f21-aecb7a0faed2",
      "source": "firecrawl/blog/introducing-extract-open-beta.md",
      "content": "Current Limitations\n\nWhile /extract handles most web data needs effectively, there are some edge cases weâ€™re actively improving:  \n1. Scale Limitations: Very large sites (think Amazonâ€™s entire catalog) require breaking requests into smaller chunks\n2. Advanced Filtering: Complex queries like time-based filtering are still in development\n3. Consistency: Multiple runs may return slightly different results as we refine our extraction model  \nWeâ€™re actively working on these areas. Our goal is to make web data as accessible as an API - and weâ€™re getting closer every day.",
      "metadata": {
        "title": "Introducing /extract: Get structured web data with just a prompt",
        "url": "https://www.firecrawl.dev/blog/introducing-extract-open-beta"
      }
    },
    {
      "id": "df2c3bf5-9965-4247-ab91-164f78831e15",
      "source": "firecrawl/blog/introducing-extract-open-beta.md",
      "content": "Get Started\n\n1. **Try it Now**\n- Get 500,000 free tokens in our [playground](https://www.firecrawl.dev/playground?mode=extract)\n- See examples and experiment with different prompts\n- No credit card required\n2. **Build Something Real**\n- Read the [technical docs](https://docs.firecrawl.dev/features/extract)\n- Connect with [Zapier](https://zapier.com/apps/firecrawl/integrations) for no-code workflows  \nReady to turn web data into your competitive advantage? Get started in less than 5 minutes.  \nâ€” Caleb, Eric, Nick and the Firecrawl team ðŸ”¥  \nArticle updated recently  \n[ðŸ”¥](https://www.firecrawl.dev/)",
      "metadata": {
        "title": "Introducing /extract: Get structured web data with just a prompt",
        "url": "https://www.firecrawl.dev/blog/introducing-extract-open-beta"
      }
    },
    {
      "id": "67731a1b-2a88-4804-8c82-e2f5960c36e5",
      "source": "firecrawl/blog/introducing-extract-open-beta.md",
      "content": "Ready to _Build?_\n\nStart scraping web data for your AI apps today.  \nNo credit card needed.  \nGet Started",
      "metadata": {
        "title": "Introducing /extract: Get structured web data with just a prompt",
        "url": "https://www.firecrawl.dev/blog/introducing-extract-open-beta"
      }
    },
    {
      "id": "4cf0569f-76dc-46ae-bceb-bf94490b89bc",
      "source": "firecrawl/blog/introducing-extract-open-beta.md",
      "content": "About the Author\n\n[![Eric Ciarla image](https://www.firecrawl.dev/eric-img.jpeg)\\\nEric Ciarla@ericciarla](https://x.com/ericciarla)  \nEric Ciarla is the Chief Operating Officer (COO) of Firecrawl and leads marketing. He also worked on Mendable.ai and sold it to companies like Snapchat, Coinbase, and MongoDB.\nPreviously worked at Ford and Fracta as a Data Scientist. Eric also co-founded SideGuide, a tool for learning code within VS Code with 50,000 users.",
      "metadata": {
        "title": "Introducing /extract: Get structured web data with just a prompt",
        "url": "https://www.firecrawl.dev/blog/introducing-extract-open-beta"
      }
    },
    {
      "id": "7deb2214-be1f-4e74-9756-847bed3043b9",
      "source": "firecrawl/blog/introducing-extract-open-beta.md",
      "content": "About the Author > More articles by Eric Ciarla\n\n[How to Create an llms.txt File for Any Website\\\n\\\nLearn how to generate an llms.txt file for any website using the llms.txt Generator and Firecrawl.](https://www.firecrawl.dev/blog/How-to-Create-an-llms-txt-File-for-Any-Website) [Cloudflare Error 1015: How to solve it?\\\n\\\nCloudflare Error 1015 is a rate limiting error that occurs when Cloudflare detects that you are exceeding the request limit set by the website owner.](https://www.firecrawl.dev/blog/cloudflare-error-1015-how-to-solve-it) [Build an agent that checks for website contradictions\\\n\\\nUsing Firecrawl and Claude to scrape your website's data and look for contradictions.](https://www.firecrawl.dev/blog/contradiction-agent) [Why Companies Need a Data Strategy for Generative AI\\\n\\\nLearn why a well-defined data strategy is essential for building robust, production-ready generative AI systems, and discover practical steps for curation, maintenance, and integration.](https://www.firecrawl.dev/blog/why-companies-need-a-data-strategy-for-generative-ai) [Getting Started with OpenAI's Predicted Outputs for Faster LLM Responses\\\n\\\nA guide to leveraging Predicted Outputs to speed up LLM tasks with GPT-4o models.](https://www.firecrawl.dev/blog/getting-started-with-predicted-outputs-openai) [How to easily install requests with pip and python\\\n\\\nA tutorial on installing the requests library in Python using various methods, with usage examples and troubleshooting tips](https://www.firecrawl.dev/blog/how-to-easily-install-requests-with-pip-and-python) [How to quickly install BeautifulSoup with Python\\\n\\\nA guide on installing the BeautifulSoup library in Python using various methods, with usage examples and troubleshooting tips](https://www.firecrawl.dev/blog/how-to-quickly-install-beautifulsoup-with-python) [How to Use OpenAI's o1 Reasoning Models in Your Applications\\\n\\\nLearn how to harness OpenAI's latest o1 series models for complex reasoning tasks in your apps.](https://www.firecrawl.dev/blog/how-to-use-openai-o1-reasoning-models-in-applications)",
      "metadata": {
        "title": "Introducing /extract: Get structured web data with just a prompt",
        "url": "https://www.firecrawl.dev/blog/introducing-extract-open-beta"
      }
    },
    {
      "id": "a6fa36af-aedc-4720-bfda-385c3956de06",
      "source": "firecrawl/blog/launch-week-i-day-1-introducing-teams.md",
      "content": "---\ntitle: Launch Week I / Day 1: Introducing Teams\nurl: https://www.firecrawl.dev/blog/launch-week-i-day-1-introducing-teams\n---  \nIntroducing /extract - Get web data with a prompt [Try now](https://www.firecrawl.dev/extract)  \nAugust 26, 2024  \nâ€¢  \n[![Eric Ciarla image](https://www.firecrawl.dev/eric-img.jpeg)Eric Ciarla](https://x.com/ericciarla)  \n# Launch Week I / Day 1: Introducing Teams  \n![Launch Week I / Day 1: Introducing Teams image](https://www.firecrawl.dev/images/blog/firecrawl-teams.png)  \nWelcome to Firecrawlâ€™s first ever Launch Week! Over the course of the next five days, weâ€™ll be bringing you an exciting new feature every day.  \nWeâ€™re kicking off Day 1 with the launch of Teams - one of our most highly requested features.  \n**What is Teams?**  \nNo one wants to work on web scraping projects alone. Teams enables you to collaborate with your co-workers and transform the way you approach data collection.  \nEver wanted to work on complex scraping projects with your entire team? With Teams, you can achieve this with just a few clicks.  \n![Invite Team Member Form](https://www.firecrawl.dev/images/blog/teams-demo.png)  \n**New Pricing Plans to Support Teams**  \nTo accommodate teams of all sizes, weâ€™ve updated our pricing structure. Our Hobby plan now includes 2 seats, perfect for small collaborations. The Standard plan offers 4 seats for growing teams, while our Growth plan supports larger groups with 8 seats. For enterprise-level needs, we offer custom seating options to fit any organization.  \nStay tuned for more exciting announcements throughout Launch Week. We canâ€™t wait to show you whatâ€™s next!  \nArticle updated recently  \n[ðŸ”¥](https://www.firecrawl.dev/)",
      "metadata": {
        "title": "Launch Week I / Day 1: Introducing Teams",
        "url": "https://www.firecrawl.dev/blog/launch-week-i-day-1-introducing-teams"
      }
    },
    {
      "id": "adeed4eb-624a-478c-a3d7-2188432c022b",
      "source": "firecrawl/blog/launch-week-i-day-1-introducing-teams.md",
      "content": "Ready to _Build?_\n\nStart scraping web data for your AI apps today.  \nNo credit card needed.  \nGet Started",
      "metadata": {
        "title": "Launch Week I / Day 1: Introducing Teams",
        "url": "https://www.firecrawl.dev/blog/launch-week-i-day-1-introducing-teams"
      }
    },
    {
      "id": "9997bfd5-a7fd-4761-8ec2-8df7672f95ff",
      "source": "firecrawl/blog/launch-week-i-day-1-introducing-teams.md",
      "content": "About the Author\n\n[![Eric Ciarla image](https://www.firecrawl.dev/eric-img.jpeg)\\\nEric Ciarla@ericciarla](https://x.com/ericciarla)  \nEric Ciarla is the Chief Operating Officer (COO) of Firecrawl and leads marketing. He also worked on Mendable.ai and sold it to companies like Snapchat, Coinbase, and MongoDB.\nPreviously worked at Ford and Fracta as a Data Scientist. Eric also co-founded SideGuide, a tool for learning code within VS Code with 50,000 users.",
      "metadata": {
        "title": "Launch Week I / Day 1: Introducing Teams",
        "url": "https://www.firecrawl.dev/blog/launch-week-i-day-1-introducing-teams"
      }
    },
    {
      "id": "86266b2d-e881-4e12-87fa-ddcd872814a9",
      "source": "firecrawl/blog/launch-week-i-day-1-introducing-teams.md",
      "content": "About the Author > More articles by Eric Ciarla\n\n[How to Create an llms.txt File for Any Website\\\n\\\nLearn how to generate an llms.txt file for any website using the llms.txt Generator and Firecrawl.](https://www.firecrawl.dev/blog/How-to-Create-an-llms-txt-File-for-Any-Website) [Cloudflare Error 1015: How to solve it?\\\n\\\nCloudflare Error 1015 is a rate limiting error that occurs when Cloudflare detects that you are exceeding the request limit set by the website owner.](https://www.firecrawl.dev/blog/cloudflare-error-1015-how-to-solve-it) [Build an agent that checks for website contradictions\\\n\\\nUsing Firecrawl and Claude to scrape your website's data and look for contradictions.](https://www.firecrawl.dev/blog/contradiction-agent) [Why Companies Need a Data Strategy for Generative AI\\\n\\\nLearn why a well-defined data strategy is essential for building robust, production-ready generative AI systems, and discover practical steps for curation, maintenance, and integration.](https://www.firecrawl.dev/blog/why-companies-need-a-data-strategy-for-generative-ai) [Getting Started with OpenAI's Predicted Outputs for Faster LLM Responses\\\n\\\nA guide to leveraging Predicted Outputs to speed up LLM tasks with GPT-4o models.](https://www.firecrawl.dev/blog/getting-started-with-predicted-outputs-openai) [How to easily install requests with pip and python\\\n\\\nA tutorial on installing the requests library in Python using various methods, with usage examples and troubleshooting tips](https://www.firecrawl.dev/blog/how-to-easily-install-requests-with-pip-and-python) [How to quickly install BeautifulSoup with Python\\\n\\\nA guide on installing the BeautifulSoup library in Python using various methods, with usage examples and troubleshooting tips](https://www.firecrawl.dev/blog/how-to-quickly-install-beautifulsoup-with-python) [How to Use OpenAI's o1 Reasoning Models in Your Applications\\\n\\\nLearn how to harness OpenAI's latest o1 series models for complex reasoning tasks in your apps.](https://www.firecrawl.dev/blog/how-to-use-openai-o1-reasoning-models-in-applications)",
      "metadata": {
        "title": "Launch Week I / Day 1: Introducing Teams",
        "url": "https://www.firecrawl.dev/blog/launch-week-i-day-1-introducing-teams"
      }
    },
    {
      "id": "35d6cf5f-662b-4f07-8343-a498a3fd28a7",
      "source": "firecrawl/blog/complete-guide-to-curl-authentication-firecrawl-api.md",
      "content": "---\ntitle: A Complete Guide Scraping Authenticated Websites with cURL and Firecrawl\nurl: https://www.firecrawl.dev/blog/complete-guide-to-curl-authentication-firecrawl-api\n---  \nIntroducing /extract - Get web data with a prompt [Try now](https://www.firecrawl.dev/extract)  \nDec 13, 2024  \nâ€¢  \n[![Rudrank Riyam image](https://www.firecrawl.dev/rudrank.jpg)Rudrank Riyam](https://x.com/rudrankriyam)  \n# A Complete Guide Scraping Authenticated Websites with cURL and Firecrawl  \n![A Complete Guide Scraping Authenticated Websites with cURL and Firecrawl image](https://www.firecrawl.dev/images/blog/complete-guide-to-curl-authentication-firecrawl-api.jpg)  \nScraping authenticated websites is often a key requirement for developers and data analysts. While many graphical tools exist, using **cURL**, a powerful command-line utility, gives you granular control over HTTP requests. Coupled with **Firecrawl**, a scraping API that can handle dynamic browser interactions and complex authentication flows, you can seamlessly extract data from behind login forms, protected dashboards, and other restricted content. Before we get started, we only recommend scraping behind authentication if you have permission from the resources owner.  \nIn this guide, weâ€™ll first introduce cURL and common authentication methods. Then, weâ€™ll show how to combine these approaches with Firecrawlâ€™s API, enabling you to scrape authenticated pages that would otherwise be challenging to access. Youâ€™ll learn everything from basic authentication to custom headers, bearer tokens, cookies, and even multi-step logins using Firecrawlâ€™s action sequences.",
      "metadata": {
        "title": "A Complete Guide Scraping Authenticated Websites with cURL and Firecrawl",
        "url": "https://www.firecrawl.dev/blog/complete-guide-to-curl-authentication-firecrawl-api"
      }
    },
    {
      "id": "98455356-5352-4866-97e1-1f00a86c78b7",
      "source": "firecrawl/blog/complete-guide-to-curl-authentication-firecrawl-api.md",
      "content": "What is cURL?\n\n**cURL (Client URL)** is a command-line tool for transferring data using various network protocols, commonly HTTP and HTTPS. Itâ€™s usually pre-installed on Unix-like systems (macOS, Linux) and easily available for Windows. With cURL, you can quickly test APIs, debug endpoints, and automate repetitive tasks.  \nCheck if cURL is installed by running:  \n```bash\ncurl --version\n\n```  \nIf installed, youâ€™ll see version details. If not, follow your operating systemâ€™s instructions to install it.  \ncURL is lightweight and script-friendlyâ€”an excellent choice for integrating with tools like Firecrawl. With cURL at your fingertips, you can seamlessly orchestrate authenticated scraping sessions by combining cURLâ€™s request capabilities with Firecrawlâ€™s browser-powered scraping.",
      "metadata": {
        "title": "A Complete Guide Scraping Authenticated Websites with cURL and Firecrawl",
        "url": "https://www.firecrawl.dev/blog/complete-guide-to-curl-authentication-firecrawl-api"
      }
    },
    {
      "id": "7702911d-506f-4924-b3a5-25f2a0e04cfe",
      "source": "firecrawl/blog/complete-guide-to-curl-authentication-firecrawl-api.md",
      "content": "Why Use Firecrawl for Authenticated Scraping?\n\n**Firecrawl** is an API designed for scraping websites that might be hard to handle with a simple HTTP client. While cURL can handle direct requests, Firecrawl provides the ability to:  \n- Interact with websites that require JavaScript execution.\n- Navigate multiple steps of login forms.\n- Manage cookies, headers, and tokens easily.\n- Extract content in structured formats like Markdown or JSON.  \nBy pairing cURLâ€™s command-line power with Firecrawlâ€™s scraping engine, you can handle complex authentication scenariosâ€”like logging into a site with a username/password form, or including custom headers and tokensâ€”that would be difficult to script using cURL alone.",
      "metadata": {
        "title": "A Complete Guide Scraping Authenticated Websites with cURL and Firecrawl",
        "url": "https://www.firecrawl.dev/blog/complete-guide-to-curl-authentication-firecrawl-api"
      }
    },
    {
      "id": "2c11a295-c234-44b4-8863-08b2eb93458f",
      "source": "firecrawl/blog/complete-guide-to-curl-authentication-firecrawl-api.md",
      "content": "Authentication Methods\n\nAuthenticated scraping means you must prove your identity or authorization to the target server before accessing protected content. Common methods include:  \n1. **Basic Authentication**\n2. **Bearer Token (OAuth 2.0)**\n3. **Custom Header Authentication**\n4. **Cookie-Based (Session) Authentication**  \nWeâ€™ll look at each method in the context of cURL, and then integrate them with Firecrawl for real-world scraping scenarios.",
      "metadata": {
        "title": "A Complete Guide Scraping Authenticated Websites with cURL and Firecrawl",
        "url": "https://www.firecrawl.dev/blog/complete-guide-to-curl-authentication-firecrawl-api"
      }
    },
    {
      "id": "95a5a379-b697-477b-bce5-3ffa1af214e5",
      "source": "firecrawl/blog/complete-guide-to-curl-authentication-firecrawl-api.md",
      "content": "Authentication Methods > 1. Basic Authentication\n\n**Basic Auth** sends a username and password encoded in Base64 with each request. Itâ€™s simple but should always be used over HTTPS to protect credentials.  \n**cURL Syntax:**  \n```bash\ncurl -u username:password https://api.example.com/securedata\n\n```  \nFor APIs requiring only an API key (as username):  \n```bash\ncurl -u my_api_key: https://api.example.com/data\n\n```  \n**With Firecrawl:**  \nIf Firecrawlâ€™s endpoint itself requires Basic Auth (or if the site youâ€™re scraping uses Basic Auth), you can include this in your request:  \n```bash\ncurl -u YOUR_API_KEY: https://api.firecrawl.dev/v1/scrape\n\n```  \nThis authenticates you to the Firecrawl API using Basic Auth, and you can then direct Firecrawl to scrape authenticated targets.",
      "metadata": {
        "title": "A Complete Guide Scraping Authenticated Websites with cURL and Firecrawl",
        "url": "https://www.firecrawl.dev/blog/complete-guide-to-curl-authentication-firecrawl-api"
      }
    },
    {
      "id": "9f7d1b2f-4f20-42fd-ad47-16b26d6e7af0",
      "source": "firecrawl/blog/complete-guide-to-curl-authentication-firecrawl-api.md",
      "content": "Authentication Methods > 2. Bearer Token Authentication (OAuth 2.0)\n\n**Bearer Tokens** (often from OAuth 2.0 flows) are secure, time-limited keys that you include in the `Authorization` header.  \n**cURL Syntax:**  \n```bash\ncurl -H \"Authorization: Bearer YOUR_TOKEN\" https://api.example.com/profile\n\n```  \n**With Firecrawl:**  \nTo scrape a site requiring a bearer token, you can instruct Firecrawl to use it:  \n```bash\ncurl -X POST https://api.firecrawl.dev/v1/scrape \\\n-H \"Authorization: Bearer fc_your_api_key_here\" \\\n-H \"Content-Type: application/json\" \\\n-d '{\n\"url\": \"https://example.com\",\n\"formats\": [\"markdown\"]\n}'\n\n```  \nHere, `fc_your_api_key_here` is your Firecrawl API token. Firecrawl will handle the scraping behind the scenes, and you can also add target-specific headers or actions if needed.",
      "metadata": {
        "title": "A Complete Guide Scraping Authenticated Websites with cURL and Firecrawl",
        "url": "https://www.firecrawl.dev/blog/complete-guide-to-curl-authentication-firecrawl-api"
      }
    },
    {
      "id": "ac3dee3f-47c7-47e5-8f9b-bbaef3478aed",
      "source": "firecrawl/blog/complete-guide-to-curl-authentication-firecrawl-api.md",
      "content": "Authentication Methods > 3. Custom Header Authentication\n\nSome APIs require custom headers for authentication (e.g., `X-API-Key: value`). These headers are sent alongside requests to prove authorization.  \n**cURL Syntax:**  \n```bash\ncurl -H \"X-API-Key: your_api_key_here\" https://api.example.com/data\n\n```  \n**With Firecrawl:**  \nTo scrape a page requiring a custom header, just include it in the POST data:  \n```bash\ncurl -X POST https://api.firecrawl.dev/v1/scrape \\\n-H \"Authorization: Bearer YOUR_FIRECRAWL_KEY\" \\\n-H \"Content-Type: application/json\" \\\n-d '{\n\"url\": \"https://protected.example.com\",\n\"headers\": {\n\"X-Custom-Auth\": \"token123\"\n}\n}'\n\n```  \nFirecrawl will use the custom header `X-Custom-Auth` when loading the page.",
      "metadata": {
        "title": "A Complete Guide Scraping Authenticated Websites with cURL and Firecrawl",
        "url": "https://www.firecrawl.dev/blog/complete-guide-to-curl-authentication-firecrawl-api"
      }
    },
    {
      "id": "30e19a84-9484-432d-aff7-286d1129deea",
      "source": "firecrawl/blog/complete-guide-to-curl-authentication-firecrawl-api.md",
      "content": "Authentication Methods > 4. Cookie-Based Authentication\n\nWebsites often rely on sessions and cookies for authentication. After logging in via a form, a cookie is set, allowing subsequent authenticated requests.  \n**cURL for Cookie Handling:**  \nSave cookies after login:  \n```bash\ncurl -c cookies.txt -X POST https://example.com/login \\\n-d \"username=yourusername&password=yourpassword\"\n\n```  \nUse these cookies for subsequent requests:  \n```bash\ncurl -b cookies.txt https://example.com/protected\n\n```  \n**With Firecrawl:**  \nIf you need to scrape a protected page that uses cookies for authentication, you can first obtain the cookies using cURL, then pass them to Firecrawl:  \n1. **Obtain Cookies:**  \n```bash\ncurl -c cookies.txt -X POST https://example.com/login \\\n-d \"username=yourusername&password=yourpassword\"\n\n```  \n2. **Use Cookies with Firecrawl:**  \n```bash\ncurl -b cookies.txt -X POST https://api.firecrawl.dev/v1/scrape \\\n-H \"Authorization: Bearer YOUR_FIRECRAWL_KEY\" \\\n-H \"Content-Type: application/json\" \\\n-d '{\n\"url\": \"https://example.com/protected\",\n\"formats\": [\"markdown\"]\n}'\n\n```  \nFirecrawl will then request the protected URL using the cookies youâ€™ve supplied.",
      "metadata": {
        "title": "A Complete Guide Scraping Authenticated Websites with cURL and Firecrawl",
        "url": "https://www.firecrawl.dev/blog/complete-guide-to-curl-authentication-firecrawl-api"
      }
    },
    {
      "id": "b44b9311-af19-4573-8e64-f2b481609705",
      "source": "firecrawl/blog/complete-guide-to-curl-authentication-firecrawl-api.md",
      "content": "Real-World Examples > GitHub API\n\nGitHubâ€™s API supports token-based auth:  \n```bash\ncurl -H \"Authorization: token ghp_YOUR_TOKEN\" https://api.github.com/user/repos\n\n```  \nScraping authenticated GitHub pages (like private profiles) with Firecrawl:  \n```bash\ncurl -X POST https://api.firecrawl.dev/v1/scrape \\\n-H \"Authorization: Bearer YOUR_FIRECRAWL_KEY\" \\\n-H \"Content-Type: application/json\" \\\n-d '{\n\"url\": \"https://github.com/settings/profile\",\n\"headers\": {\n\"Cookie\": \"user_session=YOUR_SESSION_COOKIE; tz=UTC\",\n\"User-Agent\": \"Mozilla/5.0 (Windows NT 10.0; Win64; x64)\"\n}\n}'\n\n```",
      "metadata": {
        "title": "A Complete Guide Scraping Authenticated Websites with cURL and Firecrawl",
        "url": "https://www.firecrawl.dev/blog/complete-guide-to-curl-authentication-firecrawl-api"
      }
    },
    {
      "id": "fe823f8a-20ab-450e-8de1-b891e0c304f4",
      "source": "firecrawl/blog/complete-guide-to-curl-authentication-firecrawl-api.md",
      "content": "Real-World Examples > Dev.to Authentication\n\nDev.to uses API keys as headers:  \n```bash\ncurl -H \"api-key: YOUR_DEV_TO_API_KEY\" https://dev.to/api/articles/me\n\n```  \nTo scrape behind login forms, leverage Firecrawl actions:  \n```bash\ncurl -X POST https://api.firecrawl.dev/v1/scrape \\\n-H \"Authorization: Bearer YOUR_FIRECRAWL_KEY\" \\\n-H \"Content-Type: application/json\" \\\n-d '{\n\"url\": \"https://dev.to/enter\",\n\"actions\": [\\\n{\"type\": \"wait\", \"milliseconds\": 2000},\\\n{\"type\": \"click\", \"selector\": \"input[type=email]\"},\\\n{\"type\": \"write\", \"text\": \"your@email.com\"},\\\n{\"type\": \"click\", \"selector\": \"input[type=password]\"},\\\n{\"type\": \"write\", \"text\": \"your_password\"},\\\n{\"type\": \"click\", \"selector\": \"button[type=submit]\"},\\\n{\"type\": \"wait\", \"milliseconds\": 3000},\\\n{\"type\": \"navigate\", \"url\": \"https://dev.to/dashboard\"},\\\n{\"type\": \"scrape\"}\\\n]\n}'\n\n```  \nFirecrawl can interact with the page dynamically, just like a browser, to submit forms and then scrape the resulting authenticated content.",
      "metadata": {
        "title": "A Complete Guide Scraping Authenticated Websites with cURL and Firecrawl",
        "url": "https://www.firecrawl.dev/blog/complete-guide-to-curl-authentication-firecrawl-api"
      }
    },
    {
      "id": "f7a1ca04-c8f7-4f9c-b93d-b1e09c83a23e",
      "source": "firecrawl/blog/complete-guide-to-curl-authentication-firecrawl-api.md",
      "content": "Conclusion\n\nWhen combined, **cURL and Firecrawl** provide a powerful toolkit for scraping authenticated websites. cURLâ€™s flexibility in handling HTTP requests pairs perfectly with Firecrawlâ€™s ability to navigate, interact, and extract data from pages that require authentication. Whether you need to pass API keys in headers, handle OAuth tokens, emulate sessions, or fill out login forms, these tools make the process efficient and repeatable.  \nTry the examples provided, check out [Firecrawlâ€™s documentation](https://docs.firecrawl.dev/introduction) for more advanced use cases, and start confidently scraping authenticated websites today!  \n**Happy cURLing and Firecrawling!**  \nArticle updated recently",
      "metadata": {
        "title": "A Complete Guide Scraping Authenticated Websites with cURL and Firecrawl",
        "url": "https://www.firecrawl.dev/blog/complete-guide-to-curl-authentication-firecrawl-api"
      }
    },
    {
      "id": "a22e11d5-25cb-4f8b-8f6f-89110331df4e",
      "source": "firecrawl/blog/complete-guide-to-curl-authentication-firecrawl-api.md",
      "content": "About the Author\n\n[![Rudrank Riyam image](https://www.firecrawl.dev/rudrank.jpg)\\\nRudrank Riyam@rudrankriyam](https://x.com/rudrankriyam)  \nRudrank Riyam is a Technical Writer & Author.  \n[ðŸ”¥](https://www.firecrawl.dev/)",
      "metadata": {
        "title": "A Complete Guide Scraping Authenticated Websites with cURL and Firecrawl",
        "url": "https://www.firecrawl.dev/blog/complete-guide-to-curl-authentication-firecrawl-api"
      }
    },
    {
      "id": "bc5085f8-09c6-49c2-9baa-23f68925225e",
      "source": "firecrawl/blog/complete-guide-to-curl-authentication-firecrawl-api.md",
      "content": "Ready to _Build?_\n\nStart scraping web data for your AI apps today.  \nNo credit card needed.  \nGet Started",
      "metadata": {
        "title": "A Complete Guide Scraping Authenticated Websites with cURL and Firecrawl",
        "url": "https://www.firecrawl.dev/blog/complete-guide-to-curl-authentication-firecrawl-api"
      }
    },
    {
      "id": "25804dce-39fc-49ee-a62a-6f82a55375b3",
      "source": "firecrawl/blog/how-cargo-empowers-gtm-teams-with-firecrawl.md",
      "content": "---\ntitle: How Cargo Empowers GTM Teams with Firecrawl\nurl: https://www.firecrawl.dev/blog/how-cargo-empowers-gtm-teams-with-firecrawl\n---  \nIntroducing /extract - Get web data with a prompt [Try now](https://www.firecrawl.dev/extract)  \nDec 6, 2024  \nâ€¢  \n[![Tariq Minhas image](https://www.firecrawl.dev/customers/tariq-minhas.jpg)Tariq Minhas](https://x.com/cargo_hq)  \n# How Cargo Empowers GTM Teams with Firecrawl  \n![How Cargo Empowers GTM Teams with Firecrawl image](https://www.firecrawl.dev/images/blog/customer-story-cargo.jpg)  \nAt Cargo, weâ€™re all about helping our users build efficient Go-To-Market workflows. A key part of this involves analyzing webpage content for classification, personalization, and enrichment. Thatâ€™s where Firecrawl came in and made a real difference for us.  \nWe needed a reliable way for our users to scrape web pages to gather information like industry classification, tailor messages to specific needs, and enrich their data with valuable details. Firecrawl made this process effortless. Users can now easily scrape the content they need and integrate it into their workflows, passing the data to AI-powered nodes for analysis. This generates structured outputs that are used in CRMs, outreach campaigns, or internal alerts like Slack notifications.  \nWhat we appreciate most about Firecrawl is the reliably clean and structured output it provides every time. It also offers an easy way for our non-technical users to perform multi-page crawling without any hassle. If we had to stop using Firecrawl, weâ€™d definitely miss these features. In one sentence: Firecrawl is the most reliable scraping tool weâ€™ve used, capable of handling any web page at scale.  \nWeâ€™re excited to continue working with Firecrawl and see how it helps our users achieve even more.  \nArticle updated recently",
      "metadata": {
        "title": "How Cargo Empowers GTM Teams with Firecrawl",
        "url": "https://www.firecrawl.dev/blog/how-cargo-empowers-gtm-teams-with-firecrawl"
      }
    },
    {
      "id": "60a4b4ce-bca7-4fea-aa2b-98f899cccd2b",
      "source": "firecrawl/blog/how-cargo-empowers-gtm-teams-with-firecrawl.md",
      "content": "About the Author\n\n[![Tariq Minhas image](https://www.firecrawl.dev/customers/tariq-minhas.jpg)\\\nTariq Minhas@cargo_hq](https://x.com/cargo_hq)  \nTariq Minhas is the Commando-in-chief of Cargo.  \n[ðŸ”¥](https://www.firecrawl.dev/)",
      "metadata": {
        "title": "How Cargo Empowers GTM Teams with Firecrawl",
        "url": "https://www.firecrawl.dev/blog/how-cargo-empowers-gtm-teams-with-firecrawl"
      }
    },
    {
      "id": "691a30e0-9fd5-48d8-8784-3e5d73d313cb",
      "source": "firecrawl/blog/how-cargo-empowers-gtm-teams-with-firecrawl.md",
      "content": "Ready to _Build?_\n\nStart scraping web data for your AI apps today.  \nNo credit card needed.  \nGet Started",
      "metadata": {
        "title": "How Cargo Empowers GTM Teams with Firecrawl",
        "url": "https://www.firecrawl.dev/blog/how-cargo-empowers-gtm-teams-with-firecrawl"
      }
    },
    {
      "id": "a902c612-9ad8-4aa9-9615-72c0721c3c9d",
      "source": "firecrawl/blog/how-to-quickly-install-beautifulsoup-with-python.md",
      "content": "---\ntitle: How to quickly install BeautifulSoup with Python\nurl: https://www.firecrawl.dev/blog/how-to-quickly-install-beautifulsoup-with-python\n---  \nIntroducing /extract - Get web data with a prompt [Try now](https://www.firecrawl.dev/extract)  \nAug 9, 2024  \nâ€¢  \n[![Eric Ciarla image](https://www.firecrawl.dev/eric-img.jpeg)Eric Ciarla](https://x.com/ericciarla)  \n# How to quickly install BeautifulSoup with Python  \n[BeautifulSoup](https://www.crummy.com/software/BeautifulSoup/bs4/doc/) is a Python library for pulling data out of HTML and XML files. It provides simple methods for navigating, searching, and modifying the parse tree, saving you hours of work. Beautiful Soup is great for web scraping projects where you need to extract specific pieces of information from web pages.  \nSome common use cases for BeautifulSoup include extracting article text or metadata from news sites, scraping product details and pricing from e-commerce stores, gathering data for machine learning datasets, and more.  \nIn this tutorial, weâ€™ll walk through several ways to get BeautifulSoup installed on your system and show you some basic usage examples to get started.",
      "metadata": {
        "title": "How to quickly install BeautifulSoup with Python",
        "url": "https://www.firecrawl.dev/blog/how-to-quickly-install-beautifulsoup-with-python"
      }
    },
    {
      "id": "251fdca3-4da2-4c56-bfae-b22de7a46107",
      "source": "firecrawl/blog/how-to-quickly-install-beautifulsoup-with-python.md",
      "content": "Troubleshooting\n\nHere are a few things to check if you run into issues installing BeautifulSoup:  \n- Make sure your Python version is 3.6 or higher\n- Upgrade pip to the latest version: `python -m pip install --upgrade pip`\n- If using conda, ensure your Anaconda installation is up-to-date\n- Verify you have proper permissions to install packages. Use `sudo` or run the command prompt as an administrator if needed.  \nCheck the BeautifulSoup documentation or post on Stack Overflow if you need further assistance.",
      "metadata": {
        "title": "How to quickly install BeautifulSoup with Python",
        "url": "https://www.firecrawl.dev/blog/how-to-quickly-install-beautifulsoup-with-python"
      }
    },
    {
      "id": "1d5cb00a-7bdc-48cb-b8d4-9e517c5b263e",
      "source": "firecrawl/blog/how-to-quickly-install-beautifulsoup-with-python.md",
      "content": "Usage Examples\n\nLetâ€™s look at a couple quick examples of how to use BeautifulSoup once you have it installed.",
      "metadata": {
        "title": "How to quickly install BeautifulSoup with Python",
        "url": "https://www.firecrawl.dev/blog/how-to-quickly-install-beautifulsoup-with-python"
      }
    },
    {
      "id": "efc6f812-4e9f-48e8-b699-1ab5215a23f3",
      "source": "firecrawl/blog/how-to-quickly-install-beautifulsoup-with-python.md",
      "content": "Usage Examples > Parsing HTML\n\nHereâ€™s how you can use BeautifulSoup to parse HTML retrieved from a web page:  \n```python\nfrom bs4 import BeautifulSoup\nimport requests\n\nurl = \"https://mendable.ai\"\nresponse = requests.get(url)\n\nsoup = BeautifulSoup(response.text, 'html.parser')\n\nprint(soup.title.text)\n# 'Example Domain'\n\n```  \nWe use the requests library to fetch the HTML from a URL, then pass it to BeautifulSoup to parse. This allows us to navigate and search the HTML using methods like `find()` and `select()`.",
      "metadata": {
        "title": "How to quickly install BeautifulSoup with Python",
        "url": "https://www.firecrawl.dev/blog/how-to-quickly-install-beautifulsoup-with-python"
      }
    },
    {
      "id": "b2a43fff-e25e-487c-9e6e-0b77daf5b5b0",
      "source": "firecrawl/blog/how-to-quickly-install-beautifulsoup-with-python.md",
      "content": "Usage Examples > Extracting Data\n\nBeautifulSoup makes it easy to extract data buried deep within nested HTML tags. For example, to get all the links from a page:  \n```python\nlinks = soup.find_all('a')\n\nfor link in links:\nprint(link.get('href'))\n# 'https://www.firecrawl.dev/'\n\n```  \nThe `find_all()` method retrieves all `<a>` tag elements. We can then iterate through them and access attributes like the `href` URL using `get()`.  \nBy chaining together `find()` and `select()` methods, you can precisely target elements and attributes to scrape from the messiest of HTML pages. BeautifulSoup is an indispensable tool for any Python web scraping project.  \nFor more advanced web scraping projects, consider using a dedicated scraping service like [Firecrawl](https://firecrawl.dev/). Firecrawl takes care of the tedious parts of web scraping, like proxy rotation, JavaScript rendering, and avoiding detection, allowing you to focus your efforts on working with the data itself. Check out the [Python SDK](https://docs.firecrawl.dev/sdks/python) here.",
      "metadata": {
        "title": "How to quickly install BeautifulSoup with Python",
        "url": "https://www.firecrawl.dev/blog/how-to-quickly-install-beautifulsoup-with-python"
      }
    },
    {
      "id": "8f29f2c8-9e53-496d-a14a-b931f8485f04",
      "source": "firecrawl/blog/how-to-quickly-install-beautifulsoup-with-python.md",
      "content": "References\n\n- BeautifulSoup documentation: [https://www.crummy.com/software/BeautifulSoup/bs4/doc/](https://www.crummy.com/software/BeautifulSoup/bs4/doc/)\n- Real Pythonâ€™s BeautifulSoup Tutorial: [https://realpython.com/beautiful-soup-web-scraper-python/](https://realpython.com/beautiful-soup-web-scraper-python/)\n- Firecrawl web scraping service: [https://firecrawl.dev/](https://firecrawl.dev/)  \nArticle updated recently  \n[ðŸ”¥](https://www.firecrawl.dev/)",
      "metadata": {
        "title": "How to quickly install BeautifulSoup with Python",
        "url": "https://www.firecrawl.dev/blog/how-to-quickly-install-beautifulsoup-with-python"
      }
    },
    {
      "id": "16068c8e-7d4c-4504-b23e-444c35947898",
      "source": "firecrawl/blog/how-to-quickly-install-beautifulsoup-with-python.md",
      "content": "Ready to _Build?_\n\nStart scraping web data for your AI apps today.  \nNo credit card needed.  \nGet Started",
      "metadata": {
        "title": "How to quickly install BeautifulSoup with Python",
        "url": "https://www.firecrawl.dev/blog/how-to-quickly-install-beautifulsoup-with-python"
      }
    },
    {
      "id": "ff8f973d-7dfa-42eb-ac07-8c335bf65ecd",
      "source": "firecrawl/blog/how-to-quickly-install-beautifulsoup-with-python.md",
      "content": "About the Author\n\n[![Eric Ciarla image](https://www.firecrawl.dev/eric-img.jpeg)\\\nEric Ciarla@ericciarla](https://x.com/ericciarla)  \nEric Ciarla is the Chief Operating Officer (COO) of Firecrawl and leads marketing. He also worked on Mendable.ai and sold it to companies like Snapchat, Coinbase, and MongoDB.\nPreviously worked at Ford and Fracta as a Data Scientist. Eric also co-founded SideGuide, a tool for learning code within VS Code with 50,000 users.",
      "metadata": {
        "title": "How to quickly install BeautifulSoup with Python",
        "url": "https://www.firecrawl.dev/blog/how-to-quickly-install-beautifulsoup-with-python"
      }
    },
    {
      "id": "417ec46f-1440-41b4-b4f2-16abb6d41970",
      "source": "firecrawl/blog/how-to-quickly-install-beautifulsoup-with-python.md",
      "content": "About the Author > More articles by Eric Ciarla\n\n[How to Create an llms.txt File for Any Website\\\n\\\nLearn how to generate an llms.txt file for any website using the llms.txt Generator and Firecrawl.](https://www.firecrawl.dev/blog/How-to-Create-an-llms-txt-File-for-Any-Website) [Cloudflare Error 1015: How to solve it?\\\n\\\nCloudflare Error 1015 is a rate limiting error that occurs when Cloudflare detects that you are exceeding the request limit set by the website owner.](https://www.firecrawl.dev/blog/cloudflare-error-1015-how-to-solve-it) [Build an agent that checks for website contradictions\\\n\\\nUsing Firecrawl and Claude to scrape your website's data and look for contradictions.](https://www.firecrawl.dev/blog/contradiction-agent) [Why Companies Need a Data Strategy for Generative AI\\\n\\\nLearn why a well-defined data strategy is essential for building robust, production-ready generative AI systems, and discover practical steps for curation, maintenance, and integration.](https://www.firecrawl.dev/blog/why-companies-need-a-data-strategy-for-generative-ai) [Getting Started with OpenAI's Predicted Outputs for Faster LLM Responses\\\n\\\nA guide to leveraging Predicted Outputs to speed up LLM tasks with GPT-4o models.](https://www.firecrawl.dev/blog/getting-started-with-predicted-outputs-openai) [How to easily install requests with pip and python\\\n\\\nA tutorial on installing the requests library in Python using various methods, with usage examples and troubleshooting tips](https://www.firecrawl.dev/blog/how-to-easily-install-requests-with-pip-and-python) [How to quickly install BeautifulSoup with Python\\\n\\\nA guide on installing the BeautifulSoup library in Python using various methods, with usage examples and troubleshooting tips](https://www.firecrawl.dev/blog/how-to-quickly-install-beautifulsoup-with-python) [How to Use OpenAI's o1 Reasoning Models in Your Applications\\\n\\\nLearn how to harness OpenAI's latest o1 series models for complex reasoning tasks in your apps.](https://www.firecrawl.dev/blog/how-to-use-openai-o1-reasoning-models-in-applications)",
      "metadata": {
        "title": "How to quickly install BeautifulSoup with Python",
        "url": "https://www.firecrawl.dev/blog/how-to-quickly-install-beautifulsoup-with-python"
      }
    },
    {
      "id": "f59a560c-68b6-4e9a-83c3-1fe0c21ed434",
      "source": "firecrawl/blog/how-to-easily-install-requests-with-pip-and-python.md",
      "content": "---\ntitle: How to easily install requests with pip and python\nurl: https://www.firecrawl.dev/blog/how-to-easily-install-requests-with-pip-and-python\n---  \nIntroducing /extract - Get web data with a prompt [Try now](https://www.firecrawl.dev/extract)  \nAug 9, 2024  \nâ€¢  \n[![Eric Ciarla image](https://www.firecrawl.dev/eric-img.jpeg)Eric Ciarla](https://x.com/ericciarla)  \n# How to easily install requests with pip and python  \nThe [requests](https://requests.readthedocs.io/) library is the de facto standard for making HTTP requests in Python. It abstracts the complexities of making requests behind a beautiful, simple API so that you can focus on interacting with services and consuming data in your application.  \nSome use cases for requests include Web Scraping, API integration, data retrieval from web services, automated testing of web applications, and sending data to remote servers.  \nIn this tutorial, weâ€™ll cover several ways to install requests and demonstrate basic usage with some examples.",
      "metadata": {
        "title": "How to easily install requests with pip and python",
        "url": "https://www.firecrawl.dev/blog/how-to-easily-install-requests-with-pip-and-python"
      }
    },
    {
      "id": "a787d92f-22c4-4f0c-9405-170c4992cc53",
      "source": "firecrawl/blog/how-to-easily-install-requests-with-pip-and-python.md",
      "content": "Troubleshooting\n\nIf you encounter issues when trying to install requests, here are a few things to check:  \n- Make sure youâ€™re using a recent version of Python (3.6+).\n- If youâ€™re using pip, make sure youâ€™ve upgraded to the latest version with `python -m pip install --upgrade pip`.\n- If youâ€™re using conda, make sure your Anaconda distribution is up-to-date.\n- Check that you have permissions to install packages on your system. You may need to use `sudo` or run your command shell as Administrator.  \nIf youâ€™re still having trouble, consult the requests documentation or ask for help on the requests GitHub issues page.",
      "metadata": {
        "title": "How to easily install requests with pip and python",
        "url": "https://www.firecrawl.dev/blog/how-to-easily-install-requests-with-pip-and-python"
      }
    },
    {
      "id": "2d2e24f8-82eb-48ce-8285-dfe3dc828a1a",
      "source": "firecrawl/blog/how-to-easily-install-requests-with-pip-and-python.md",
      "content": "Usage Examples\n\nWith requests installed, letâ€™s look at a couple of basic usage examples.",
      "metadata": {
        "title": "How to easily install requests with pip and python",
        "url": "https://www.firecrawl.dev/blog/how-to-easily-install-requests-with-pip-and-python"
      }
    },
    {
      "id": "616c7dc0-94f2-4e3e-847f-2ad3c64c4a03",
      "source": "firecrawl/blog/how-to-easily-install-requests-with-pip-and-python.md",
      "content": "Usage Examples > Making a Request\n\nLetâ€™s make a basic GET request to the GitHub API:  \n```python\nimport requests\n\nresponse = requests.get('https://api.github.com')\n\nprint(response.status_code)\n# 200\n\nprint(response.text)\n# '{\"current_user_url\":\"https://api.github.com/user\",\"authorizations_url\":\"https://api.github.com/authorizations\", ...}'\n\n```  \nWe can see the response status code is 200, indicating the request was successful. The response text contains the JSON data returned by the API.",
      "metadata": {
        "title": "How to easily install requests with pip and python",
        "url": "https://www.firecrawl.dev/blog/how-to-easily-install-requests-with-pip-and-python"
      }
    },
    {
      "id": "dd6de19d-d66c-4a10-b83a-f87df5c4d14e",
      "source": "firecrawl/blog/how-to-easily-install-requests-with-pip-and-python.md",
      "content": "Usage Examples > Web Scraping\n\nrequests is commonly used for web scraping. Letâ€™s try scraping the HTML from a Wikipedia page:  \n```python\nimport requests\n\nurl = 'https://en.wikipedia.org/wiki/Web_scraping'\n\nresponse = requests.get(url)\n\nprint(response.status_code)\n# 200\n\nprint(response.text)\n# <!DOCTYPE html>\n# <html class=\"client-nojs\" lang=\"en\" dir=\"ltr\">\n# <head>\n# <meta charset=\"UTF-8\"/>\n# <title>Web scraping - Wikipedia</title>\n# ...\n# </body>\n# </html>\n\n```  \nWe make a GET request to the Wikipedia article on web scraping and print out the complete HTML content of the page by accessing `response.text`. You can then use a library like BeautifulSoup to parse and extract information from this HTML.  \nFor more advanced web scraping needs, consider using a dedicated scraping service like [Firecrawl](https://firecrawl.dev/). Firecrawl handles the complexities of web scraping, including proxy rotation, JavaScript rendering, and avoiding detection, so you can focus on working with the data. Check out the [Python SDK](https://docs.firecrawl.dev/sdks/python) here.",
      "metadata": {
        "title": "How to easily install requests with pip and python",
        "url": "https://www.firecrawl.dev/blog/how-to-easily-install-requests-with-pip-and-python"
      }
    },
    {
      "id": "bda34be6-c55d-4817-bb18-0c88b7c8385e",
      "source": "firecrawl/blog/how-to-easily-install-requests-with-pip-and-python.md",
      "content": "References\n\n- Requests documentation: [https://requests.readthedocs.io/](https://requests.readthedocs.io/)\n- Requests GitHub repository: [https://github.com/psf/requests](https://github.com/psf/requests)\n- BeautifulSoup documentation: [https://www.crummy.com/software/BeautifulSoup/bs4/doc/](https://www.crummy.com/software/BeautifulSoup/bs4/doc/)\n- Firecrawl web scraping service: [https://firecrawl.dev/](https://firecrawl.dev/)  \nArticle updated recently  \n[ðŸ”¥](https://www.firecrawl.dev/)",
      "metadata": {
        "title": "How to easily install requests with pip and python",
        "url": "https://www.firecrawl.dev/blog/how-to-easily-install-requests-with-pip-and-python"
      }
    },
    {
      "id": "df9c3686-8e16-45a9-bafd-a0411faf4d70",
      "source": "firecrawl/blog/how-to-easily-install-requests-with-pip-and-python.md",
      "content": "Ready to _Build?_\n\nStart scraping web data for your AI apps today.  \nNo credit card needed.  \nGet Started",
      "metadata": {
        "title": "How to easily install requests with pip and python",
        "url": "https://www.firecrawl.dev/blog/how-to-easily-install-requests-with-pip-and-python"
      }
    },
    {
      "id": "eb1d6f4c-67e9-4d45-97cb-5ecbc89d5416",
      "source": "firecrawl/blog/how-to-easily-install-requests-with-pip-and-python.md",
      "content": "About the Author\n\n[![Eric Ciarla image](https://www.firecrawl.dev/eric-img.jpeg)\\\nEric Ciarla@ericciarla](https://x.com/ericciarla)  \nEric Ciarla is the Chief Operating Officer (COO) of Firecrawl and leads marketing. He also worked on Mendable.ai and sold it to companies like Snapchat, Coinbase, and MongoDB.\nPreviously worked at Ford and Fracta as a Data Scientist. Eric also co-founded SideGuide, a tool for learning code within VS Code with 50,000 users.",
      "metadata": {
        "title": "How to easily install requests with pip and python",
        "url": "https://www.firecrawl.dev/blog/how-to-easily-install-requests-with-pip-and-python"
      }
    },
    {
      "id": "ba579a27-a26c-4604-873d-08140df93e2a",
      "source": "firecrawl/blog/how-to-easily-install-requests-with-pip-and-python.md",
      "content": "About the Author > More articles by Eric Ciarla\n\n[How to Create an llms.txt File for Any Website\\\n\\\nLearn how to generate an llms.txt file for any website using the llms.txt Generator and Firecrawl.](https://www.firecrawl.dev/blog/How-to-Create-an-llms-txt-File-for-Any-Website) [Cloudflare Error 1015: How to solve it?\\\n\\\nCloudflare Error 1015 is a rate limiting error that occurs when Cloudflare detects that you are exceeding the request limit set by the website owner.](https://www.firecrawl.dev/blog/cloudflare-error-1015-how-to-solve-it) [Build an agent that checks for website contradictions\\\n\\\nUsing Firecrawl and Claude to scrape your website's data and look for contradictions.](https://www.firecrawl.dev/blog/contradiction-agent) [Why Companies Need a Data Strategy for Generative AI\\\n\\\nLearn why a well-defined data strategy is essential for building robust, production-ready generative AI systems, and discover practical steps for curation, maintenance, and integration.](https://www.firecrawl.dev/blog/why-companies-need-a-data-strategy-for-generative-ai) [Getting Started with OpenAI's Predicted Outputs for Faster LLM Responses\\\n\\\nA guide to leveraging Predicted Outputs to speed up LLM tasks with GPT-4o models.](https://www.firecrawl.dev/blog/getting-started-with-predicted-outputs-openai) [How to easily install requests with pip and python\\\n\\\nA tutorial on installing the requests library in Python using various methods, with usage examples and troubleshooting tips](https://www.firecrawl.dev/blog/how-to-easily-install-requests-with-pip-and-python) [How to quickly install BeautifulSoup with Python\\\n\\\nA guide on installing the BeautifulSoup library in Python using various methods, with usage examples and troubleshooting tips](https://www.firecrawl.dev/blog/how-to-quickly-install-beautifulsoup-with-python) [How to Use OpenAI's o1 Reasoning Models in Your Applications\\\n\\\nLearn how to harness OpenAI's latest o1 series models for complex reasoning tasks in your apps.](https://www.firecrawl.dev/blog/how-to-use-openai-o1-reasoning-models-in-applications)",
      "metadata": {
        "title": "How to easily install requests with pip and python",
        "url": "https://www.firecrawl.dev/blog/how-to-easily-install-requests-with-pip-and-python"
      }
    },
    {
      "id": "c0d926b2-bb95-4206-9c41-529e290d968f",
      "source": "firecrawl/blog/launch-week-i-day-7-webhooks.md",
      "content": "---\ntitle: Launch Week I / Day 7: Crawl Webhooks (v1)\nurl: https://www.firecrawl.dev/blog/launch-week-i-day-7-webhooks\n---  \nIntroducing /extract - Get web data with a prompt [Try now](https://www.firecrawl.dev/extract)  \nSeptember 1, 2024  \nâ€¢  \n[![Nicolas Camara image](https://www.firecrawl.dev/nick-img.jpeg)Nicolas Camara](https://x.com/nickscamara_)  \n# Launch Week I / Day 7: Crawl Webhooks (v1)  \n![Launch Week I / Day 7: Crawl Webhooks (v1) image](https://www.firecrawl.dev/images/blog/webhooks.png)  \nWelcome to Day 7 of Firecrawlâ€™s Launch Week! Weâ€™re excited to introduce new /crawl webhook support.",
      "metadata": {
        "title": "Launch Week I / Day 7: Crawl Webhooks (v1)",
        "url": "https://www.firecrawl.dev/blog/launch-week-i-day-7-webhooks"
      }
    },
    {
      "id": "95e4ae7b-3e7c-4b49-b6c0-d64b420595af",
      "source": "firecrawl/blog/launch-week-i-day-7-webhooks.md",
      "content": "Crawl Webhook\n\nYou can now pass a `webhook` parameter to the `/crawl` endpoint. This will send a POST request to the URL you specify when the crawl is started, updated and completed.  \nThe webhook will now trigger for every page crawled and not just the whole result at the end.  \n![Webhook](https://www.firecrawl.dev/images/blog/webhook-v1.png)",
      "metadata": {
        "title": "Launch Week I / Day 7: Crawl Webhooks (v1)",
        "url": "https://www.firecrawl.dev/blog/launch-week-i-day-7-webhooks"
      }
    },
    {
      "id": "56c9bf6f-13b2-41d8-a289-e347798eabab",
      "source": "firecrawl/blog/launch-week-i-day-7-webhooks.md",
      "content": "Crawl Webhook > Webhook Events\n\nThere are now 4 types of events:  \n- `crawl.started` - Triggered when the crawl is started.\n- `crawl.page` - Triggered for every page crawled.\n- `crawl.completed` - Triggered when the crawl is completed to let you know itâ€™s done.\n- `crawl.failed` - Triggered when the crawl fails.",
      "metadata": {
        "title": "Launch Week I / Day 7: Crawl Webhooks (v1)",
        "url": "https://www.firecrawl.dev/blog/launch-week-i-day-7-webhooks"
      }
    },
    {
      "id": "9162a491-1f20-450b-b1ef-ca4ac9ad1a49",
      "source": "firecrawl/blog/launch-week-i-day-7-webhooks.md",
      "content": "Crawl Webhook > Webhook Response\n\n- `success` - If the webhook was successful in crawling the page correctly.\n- `type` - The type of event that occurred.\n- `id` - The ID of the crawl.\n- `data` - The data that was scraped (Array). This will only be non empty on `crawl.page` and will contain 1 item if the page was scraped successfully. The response is the same as the `/scrape` endpoint.\n- `error` - If the webhook failed, this will contain the error message.",
      "metadata": {
        "title": "Launch Week I / Day 7: Crawl Webhooks (v1)",
        "url": "https://www.firecrawl.dev/blog/launch-week-i-day-7-webhooks"
      }
    },
    {
      "id": "18a6e958-ab21-4cad-8fd5-3df2147feaa3",
      "source": "firecrawl/blog/launch-week-i-day-7-webhooks.md",
      "content": "Learn More\n\nLearn more about the webhook in our [documentation](https://docs.firecrawl.dev/features/crawl#crawl-webhook).  \nArticle updated recently  \n[ðŸ”¥](https://www.firecrawl.dev/)",
      "metadata": {
        "title": "Launch Week I / Day 7: Crawl Webhooks (v1)",
        "url": "https://www.firecrawl.dev/blog/launch-week-i-day-7-webhooks"
      }
    },
    {
      "id": "d52812fc-144a-47a4-8355-400e94bf150f",
      "source": "firecrawl/blog/launch-week-i-day-7-webhooks.md",
      "content": "Ready to _Build?_\n\nStart scraping web data for your AI apps today.  \nNo credit card needed.  \nGet Started",
      "metadata": {
        "title": "Launch Week I / Day 7: Crawl Webhooks (v1)",
        "url": "https://www.firecrawl.dev/blog/launch-week-i-day-7-webhooks"
      }
    },
    {
      "id": "c838bb23-d087-43b2-b555-cc9ae0cbd325",
      "source": "firecrawl/blog/launch-week-i-day-7-webhooks.md",
      "content": "About the Author\n\n[![Nicolas Camara image](https://www.firecrawl.dev/nick-img.jpeg)\\\nNicolas Camara@nickscamara_](https://x.com/nickscamara_)  \nNicolas Camara is the Chief Technology Officer (CTO) at Firecrawl.\nHe previously built and scaled Mendable, one of the pioneering \"chat with your documents\" apps,\nwhich had major Fortune 500 customers like Snapchat, Coinbase, and MongoDB.\nPrior to that, Nicolas built SideGuide, the first code-learning tool inside VS Code,\nand grew a community of 50,000 users. Nicolas studied Computer Science and has over 10 years of experience in building software.",
      "metadata": {
        "title": "Launch Week I / Day 7: Crawl Webhooks (v1)",
        "url": "https://www.firecrawl.dev/blog/launch-week-i-day-7-webhooks"
      }
    },
    {
      "id": "85fdef60-b400-49ab-9ce7-dd467b3bef7a",
      "source": "firecrawl/blog/launch-week-i-day-7-webhooks.md",
      "content": "About the Author > More articles by Nicolas Camara\n\n[Using OpenAI's Realtime API and Firecrawl to Talk with Any Website\\\n\\\nBuild a real-time conversational agent that interacts with any website using OpenAI's Realtime API and Firecrawl.](https://www.firecrawl.dev/blog/How-to-Talk-with-Any-Website-Using-OpenAIs-Realtime-API-and-Firecrawl) [Extract website data using LLMs\\\n\\\nLearn how to use Firecrawl and Groq to extract structured data from a web page in a few lines of code.](https://www.firecrawl.dev/blog/data-extraction-using-llms) [Getting Started with Grok-2: Setup and Web Crawler Example\\\n\\\nA detailed guide on setting up Grok-2 and building a web crawler using Firecrawl.](https://www.firecrawl.dev/blog/grok-2-setup-and-web-crawler-example) [Launch Week I / Day 6: LLM Extract (v1)\\\n\\\nExtract structured data from your web pages using the extract format in /scrape.](https://www.firecrawl.dev/blog/launch-week-i-day-6-llm-extract) [Launch Week I / Day 7: Crawl Webhooks (v1)\\\n\\\nNew /crawl webhook support. Send notifications to your apps during a crawl.](https://www.firecrawl.dev/blog/launch-week-i-day-7-webhooks) [OpenAI Swarm Tutorial: Create Marketing Campaigns for Any Website\\\n\\\nA guide to building a multi-agent system using OpenAI Swarm and Firecrawl for AI-driven marketing strategies](https://www.firecrawl.dev/blog/openai-swarm-agent-tutorial) [Build a 'Chat with website' using Groq Llama 3\\\n\\\nLearn how to use Firecrawl, Groq Llama 3, and Langchain to build a 'Chat with your website' bot.](https://www.firecrawl.dev/blog/chat-with-website) [Scrape and Analyze Airbnb Data with Firecrawl and E2B\\\n\\\nLearn how to scrape and analyze Airbnb data using Firecrawl and E2B in a few lines of code.](https://www.firecrawl.dev/blog/scrape-analyze-airbnb-data-with-e2b)",
      "metadata": {
        "title": "Launch Week I / Day 7: Crawl Webhooks (v1)",
        "url": "https://www.firecrawl.dev/blog/launch-week-i-day-7-webhooks"
      }
    },
    {
      "id": "bf5ab4df-f59a-451f-b3d9-347c38965734",
      "source": "firecrawl/blog/beautifulsoup4-vs-scrapy-comparison.md",
      "content": "---\ntitle: BeautifulSoup4 vs. Scrapy - A Comprehensive Comparison for Web Scraping in Python\nurl: https://www.firecrawl.dev/blog/beautifulsoup4-vs-scrapy-comparison\n---  \nIntroducing /extract - Get web data with a prompt [Try now](https://www.firecrawl.dev/extract)  \nDec 24, 2024  \nâ€¢  \n[![Bex Tuychiev image](https://www.firecrawl.dev/bex.jpg)Bex Tuychiev](https://x.com/bextuychiev)  \n# BeautifulSoup4 vs. Scrapy - A Comprehensive Comparison for Web Scraping in Python  \n![BeautifulSoup4 vs. Scrapy - A Comprehensive Comparison for Web Scraping in Python image](https://www.firecrawl.dev/images/blog/bs4_scrapy/bs4-vs-scrapy-comparison.jpg)",
      "metadata": {
        "title": "BeautifulSoup4 vs. Scrapy - A Comprehensive Comparison for Web Scraping in Python",
        "url": "https://www.firecrawl.dev/blog/beautifulsoup4-vs-scrapy-comparison"
      }
    },
    {
      "id": "b03b3ae7-6da4-421c-94fa-a246c71bec09",
      "source": "firecrawl/blog/beautifulsoup4-vs-scrapy-comparison.md",
      "content": "1. Introduction\n\nWeb scraping has become an essential tool for gathering data from the internet. Whether youâ€™re tracking prices, collecting news articles, or building a research dataset, Python offers several popular libraries to help you extract information from websites. Two of the most widely used tools are BeautifulSoup4 (BS4) and Scrapy, each with its own strengths and ideal use cases.  \nChoosing between BS4 and Scrapy isnâ€™t always straightforward. BS4 is known for its simplicity and ease of use, making it perfect for beginners and small projects. Scrapy, on the other hand, offers powerful features for large-scale scraping but comes with a steeper learning curve. Making the right choice can save you time and prevent headaches down the road.  \nIn this guide, weâ€™ll compare BS4 and Scrapy in detail, looking at their features, performance, and best uses. Weâ€™ll also explore practical examples and discuss modern alternatives that solve common scraping challenges. By the end, youâ€™ll have a clear understanding of which tool best fits your needs and how to get started with web scraping in Python.",
      "metadata": {
        "title": "BeautifulSoup4 vs. Scrapy - A Comprehensive Comparison for Web Scraping in Python",
        "url": "https://www.firecrawl.dev/blog/beautifulsoup4-vs-scrapy-comparison"
      }
    },
    {
      "id": "1de7d113-d06d-4831-ba10-757b82d6c39e",
      "source": "firecrawl/blog/beautifulsoup4-vs-scrapy-comparison.md",
      "content": "Prerequisites\n\nBefore diving into the comparison, make sure you have:  \n- Basic knowledge of Python programming\n- Understanding of HTML structure and CSS selectors\n- Python 3.7+ installed on your system\n- Familiarity with command line interface\n- A code editor or IDE of your choice  \nYouâ€™ll also need to install the required libraries:  \n```bash\npip install beautifulsoup4 scrapy firecrawl-py pydantic python-dotenv\n\n```",
      "metadata": {
        "title": "BeautifulSoup4 vs. Scrapy - A Comprehensive Comparison for Web Scraping in Python",
        "url": "https://www.firecrawl.dev/blog/beautifulsoup4-vs-scrapy-comparison"
      }
    },
    {
      "id": "9a9c6613-a719-464b-8595-7e95fdb1a981",
      "source": "firecrawl/blog/beautifulsoup4-vs-scrapy-comparison.md",
      "content": "2. Understanding BeautifulSoup4\n\nBeautifulSoup4, often called BS4, is a Python library that helps developers extract data from HTML and XML files. Think of it as a tool that can read and understand web pages the same way your browser does, but instead of showing you the content, it helps you collect specific information from it. BS4 works by turning messy HTML code into a well-organized structure thatâ€™s easy to navigate and search through.  \nThe library shines in its simplicity. With just a few lines of code, you can pull out specific parts of a webpage like headlines, prices, or product descriptions. Hereâ€™s a quick example:  \n```python\nfrom bs4 import BeautifulSoup\nimport requests\n\n# Get a webpage\nresponse = requests.get('https://news.ycombinator.com')\nsoup = BeautifulSoup(response.text, 'html.parser')\n\n# Find all article titles\ntitles = soup.find_all('span', class_='titleline')\nfor idx, title in enumerate(titles):\nprint(f\"{idx + 1}. {title.text.strip()}\")\nif idx == 4:\nbreak\n\n```  \nThis code demonstrates BeautifulSoup4â€™s straightforward approach to web scraping. It fetches the Hacker News homepage using the requests library, then creates a BeautifulSoup object to parse the HTML. The `find_all()` method searches for `<span>` elements with the class `\"titleline\"`, which contain article titles. The code loops through the first 5 titles, printing each one with its index number. The `strip()` method removes any extra whitespace around the titles.  \nThe output shows real article titles from Hacker News, demonstrating how BS4 can easily extract specific content from a webpage:  \n```out\n1. The GTA III port for the Dreamcast has been released (gitlab.com/skmp)\n2. Arnis: Generate Cities in Minecraft from OpenStreetMap (github.com/louis-e)\n3. Things we learned about LLMs in 2024 (simonwillison.net)\n4. Journey from Entrepreneur to Employee (akshay.co)\n5. Systems ideas that sound good but almost never work (learningbyshipping.com)\n\n```  \nWhile BS4 excels at handling static websites, it does have limitations. It canâ€™t process JavaScript-generated content, which many modern websites use. It also doesnâ€™t handle tasks like managing multiple requests or storing data. However, these limitations are often outweighed by its gentle learning curve and excellent documentation, making it an ideal starting point for anyone new to web scraping.  \nKey Features:  \n- Simple, intuitive API for parsing HTML/XML\n- Powerful searching and filtering methods\n- Forgiving HTML parser that can handle messy code\n- Extensive documentation with clear examples\n- Small memory footprint\n- Compatible with multiple parsers ( `lxml`, `html5lib`)",
      "metadata": {
        "title": "BeautifulSoup4 vs. Scrapy - A Comprehensive Comparison for Web Scraping in Python",
        "url": "https://www.firecrawl.dev/blog/beautifulsoup4-vs-scrapy-comparison"
      }
    },
    {
      "id": "a96e4185-eff0-4990-bd67-cc5d719a6c2d",
      "source": "firecrawl/blog/beautifulsoup4-vs-scrapy-comparison.md",
      "content": "3. Understanding Scrapy\n\n![Scrapy architecture diagram showing components like spiders, engine, scheduler and pipelines](https://www.firecrawl.dev/images/blog/bs4_scrapy/scrapy_architecture.jpeg)  \nSource: [Scrapy documentation](https://docs.scrapy.org/en/latest/topics/architecture.html).  \nScrapy takes a different approach to web scraping by providing a complete framework rather than just a parsing library. Think of it as a Swiss Army knife for web scraping â€“ it includes everything you need to crawl websites, process data, and handle common scraping challenges all in one package. While this makes it more powerful than BS4, it also means thereâ€™s more to learn before you can get started.  \nHereâ€™s a basic example of how Scrapy works:  \n```python\n# hackernews_spider.py\nimport scrapy\n\nclass HackerNewsSpider(scrapy.Spider):\nname = \"hackernews\"\nstart_urls = [\"https://news.ycombinator.com\"]\n\ndef parse(self, response):\n# Get all stories\nfor story in response.css(\"span.titleline\"):\n# Extract story title\nyield {\"title\": story.css(\"a::text\").get()}\n\n# Go to next page if available\n# if next_page := response.css('a.morelink::attr(href)').get():\n# yield response.follow(next_page, self.parse)\n\n# To run the spider, we need to use the Scrapy command line\n# scrapy runspider hackernews_spider.py -o results.json\n\n```  \nThis code defines a simple Scrapy spider that crawls Hacker News. The spider starts at the homepage, extracts story titles from each page, and could optionally follow pagination links (currently commented out). The spider uses CSS selectors to find and extract content, demonstrating Scrapyâ€™s built-in parsing capabilities. The results can be exported to JSON using Scrapyâ€™s command line interface.  \nWhat sets Scrapy apart is its architecture. Instead of making one request at a time like BS4, Scrapy can handle multiple requests simultaneously, making it much faster for large projects. It also includes built-in features that youâ€™d otherwise need to build yourself.  \nScrapyâ€™s key components include:  \nSpider middleware for customizing request/response handling, item pipelines for processing and storing data, and automatic request queuing and scheduling. It provides built-in support for exporting data in formats like JSON, CSV, and XML. The framework also includes robust error handling with retry mechanisms and a command-line interface for project management.",
      "metadata": {
        "title": "BeautifulSoup4 vs. Scrapy - A Comprehensive Comparison for Web Scraping in Python",
        "url": "https://www.firecrawl.dev/blog/beautifulsoup4-vs-scrapy-comparison"
      }
    },
    {
      "id": "b3574099-8b47-4db3-ac3c-bf0cf846c33e",
      "source": "firecrawl/blog/beautifulsoup4-vs-scrapy-comparison.md",
      "content": "4. Head-to-Head Comparison\n\nLetâ€™s break down how BS4 and Scrapy compare in key areas that matter most for web scraping projects.",
      "metadata": {
        "title": "BeautifulSoup4 vs. Scrapy - A Comprehensive Comparison for Web Scraping in Python",
        "url": "https://www.firecrawl.dev/blog/beautifulsoup4-vs-scrapy-comparison"
      }
    },
    {
      "id": "51fe11cb-6b46-4fab-bccd-3521e9b2cf98",
      "source": "firecrawl/blog/beautifulsoup4-vs-scrapy-comparison.md",
      "content": "4. Head-to-Head Comparison > Performance\n\nWhen it comes to speed and efficiency, Scrapy has a clear advantage. Its ability to handle multiple requests at once means it can scrape hundreds of pages while BS4 is still working on its first dozen. Think of BS4 as a solo worker, carefully processing one page at a time, while Scrapy is like a team of workers tackling many pages simultaneously.  \nMemory usage tells a similar story. BS4 is lightweight and uses minimal memory for single pages, making it perfect for small projects. However, Scrapyâ€™s smart memory management shines when dealing with large websites, efficiently handling thousands of pages without slowing down your computer.",
      "metadata": {
        "title": "BeautifulSoup4 vs. Scrapy - A Comprehensive Comparison for Web Scraping in Python",
        "url": "https://www.firecrawl.dev/blog/beautifulsoup4-vs-scrapy-comparison"
      }
    },
    {
      "id": "3b58409f-a6e5-4a4a-b5fb-b4f8f8822a27",
      "source": "firecrawl/blog/beautifulsoup4-vs-scrapy-comparison.md",
      "content": "4. Head-to-Head Comparison > Ease of Use\n\nBS4 takes the lead in simplicity. You can start scraping with just 4-5 lines of code and basic Python knowledge. Hereâ€™s a quick comparison:  \nBS4:  \n```python\nfrom bs4 import BeautifulSoup\nimport requests\n\nresponse = requests.get(\"https://example.com\")\nsoup = BeautifulSoup(response.text, \"html.parser\")\ntitles = soup.find_all(\"h1\")\n\n```  \nScrapy:  \n```python\nimport scrapy\n\nclass MySpider(scrapy.Spider):\nname = 'myspider'\nstart_urls = ['https://example.com']\n\ndef parse(self, response):\ntitles = response.css('h1::text').getall()\nyield {'titles': titles}\n\n# Requires additional setup and command-line usage as seen above\n\n```",
      "metadata": {
        "title": "BeautifulSoup4 vs. Scrapy - A Comprehensive Comparison for Web Scraping in Python",
        "url": "https://www.firecrawl.dev/blog/beautifulsoup4-vs-scrapy-comparison"
      }
    },
    {
      "id": "826e5f20-7401-40d0-a015-d1676d402961",
      "source": "firecrawl/blog/beautifulsoup4-vs-scrapy-comparison.md",
      "content": "4. Head-to-Head Comparison > Features\n\nHereâ€™s a simple breakdown of key features:  \n| Feature | BeautifulSoup4 | Scrapy |\n| --- | --- | --- |\n| JavaScript Support | âŒ | âŒ (needs add-ons) |\n| Multiple Requests | âŒ (manual) | âœ… (automatic) |\n| Data Processing | âŒ (basic) | âœ… (built-in pipelines) |\n| Error Handling | âŒ (manual) | âœ… (automatic retries) |\n| Proxy Support | âŒ (manual) | âœ… (built-in) |",
      "metadata": {
        "title": "BeautifulSoup4 vs. Scrapy - A Comprehensive Comparison for Web Scraping in Python",
        "url": "https://www.firecrawl.dev/blog/beautifulsoup4-vs-scrapy-comparison"
      }
    },
    {
      "id": "7bfba693-7ee6-4c28-9d4d-3033cb31c103",
      "source": "firecrawl/blog/beautifulsoup4-vs-scrapy-comparison.md",
      "content": "4. Head-to-Head Comparison > Use Cases\n\nChoose BS4 when:  \n- Youâ€™re new to web scraping\n- You need to scrape a few simple pages\n- You want to quickly test or prototype\n- The website is mostly static HTML\n- Youâ€™re working within a larger project  \nChoose Scrapy when:  \n- You need to scrape thousands of pages\n- You want built-in data processing\n- You need advanced features like proxy rotation\n- Youâ€™re building a production scraper\n- Performance is critical",
      "metadata": {
        "title": "BeautifulSoup4 vs. Scrapy - A Comprehensive Comparison for Web Scraping in Python",
        "url": "https://www.firecrawl.dev/blog/beautifulsoup4-vs-scrapy-comparison"
      }
    },
    {
      "id": "61652a2a-924d-4470-93d4-0130abc54832",
      "source": "firecrawl/blog/beautifulsoup4-vs-scrapy-comparison.md",
      "content": "5. Common Challenges and Limitations\n\nWeb scraping tools face several hurdles that can make data extraction difficult or unreliable. Understanding these challenges helps you choose the right tool and prepare for potential roadblocks.",
      "metadata": {
        "title": "BeautifulSoup4 vs. Scrapy - A Comprehensive Comparison for Web Scraping in Python",
        "url": "https://www.firecrawl.dev/blog/beautifulsoup4-vs-scrapy-comparison"
      }
    },
    {
      "id": "8615a96f-be3d-4836-abc3-2ce251bef4e5",
      "source": "firecrawl/blog/beautifulsoup4-vs-scrapy-comparison.md",
      "content": "5. Common Challenges and Limitations > Dynamic Content\n\nModern websites often load content using JavaScript after the initial page load. Neither BS4 nor Scrapy can handle this directly. While you can add tools like Selenium or Playwright to either solution, this makes your scraper more complex and slower. A typical example is an infinite scroll page on social media â€“ the content isnâ€™t in the HTML until you scroll down.",
      "metadata": {
        "title": "BeautifulSoup4 vs. Scrapy - A Comprehensive Comparison for Web Scraping in Python",
        "url": "https://www.firecrawl.dev/blog/beautifulsoup4-vs-scrapy-comparison"
      }
    },
    {
      "id": "0f9d7642-f00f-4313-9fc3-0240bca4f159",
      "source": "firecrawl/blog/beautifulsoup4-vs-scrapy-comparison.md",
      "content": "5. Common Challenges and Limitations > Anti-Bot Measures\n\nWebsites are getting smarter at detecting and blocking scrapers. Common protection methods include:  \n- CAPTCHAs and reCAPTCHA challenges\n- IP-based rate limiting\n- Browser fingerprinting\n- Dynamic HTML structure changes\n- Hidden honeypot elements  \nWhile Scrapy offers some built-in tools like proxy support and request delays, both BS4 and Scrapy users often need to implement additional solutions to bypass these protections.",
      "metadata": {
        "title": "BeautifulSoup4 vs. Scrapy - A Comprehensive Comparison for Web Scraping in Python",
        "url": "https://www.firecrawl.dev/blog/beautifulsoup4-vs-scrapy-comparison"
      }
    },
    {
      "id": "4f3a4b59-dd12-4573-9f52-d7669c669047",
      "source": "firecrawl/blog/beautifulsoup4-vs-scrapy-comparison.md",
      "content": "5. Common Challenges and Limitations > Maintenance Burden\n\nPerhaps the biggest challenge is keeping scrapers running over time. Websites frequently change their structure, breaking scrapers that rely on specific HTML patterns. Hereâ€™s a real-world example:  \nBefore website update:  \n```python\n# Working scraper\nsoup.find('div', class_='product-price').text # Returns: \"$99.99\"\n\n```  \nAfter website update, same code now returns None because the structure changed:  \n```python\nsoup.find('span', class_='price-current').text # Returns: None\n\n```  \nThis constant need for updates creates a significant maintenance overhead, especially when managing multiple scrapers. While Scrapyâ€™s more robust architecture helps handle some issues automatically, both tools require regular monitoring and fixes to maintain reliability.",
      "metadata": {
        "title": "BeautifulSoup4 vs. Scrapy - A Comprehensive Comparison for Web Scraping in Python",
        "url": "https://www.firecrawl.dev/blog/beautifulsoup4-vs-scrapy-comparison"
      }
    },
    {
      "id": "74c37426-1199-4c0e-950a-ad6008fa2d4e",
      "source": "firecrawl/blog/beautifulsoup4-vs-scrapy-comparison.md",
      "content": "5. Common Challenges and Limitations > Resource Management\n\nEach tool presents unique resource challenges:  \n- BS4: High memory usage when parsing large pages\n- Scrapy: Complex configuration for optimal performance\n- Both: Network bandwidth limitations\n- Both: Server response time variations  \nThese limitations often require careful planning and optimization, particularly for large-scale scraping projects where efficiency is crucial.",
      "metadata": {
        "title": "BeautifulSoup4 vs. Scrapy - A Comprehensive Comparison for Web Scraping in Python",
        "url": "https://www.firecrawl.dev/blog/beautifulsoup4-vs-scrapy-comparison"
      }
    },
    {
      "id": "eb64da35-5972-4611-9b2f-4b28a4e4ae2c",
      "source": "firecrawl/blog/beautifulsoup4-vs-scrapy-comparison.md",
      "content": "6. Modern Solutions: Introducing Firecrawl\n\nAfter exploring the limitations of traditional scraping tools, letâ€™s look at how modern AI-powered solutions like Firecrawl are changing the web scraping landscape. Firecrawl takes a fundamentally different approach by using natural language understanding to identify and extract content, rather than relying on brittle HTML selectors.",
      "metadata": {
        "title": "BeautifulSoup4 vs. Scrapy - A Comprehensive Comparison for Web Scraping in Python",
        "url": "https://www.firecrawl.dev/blog/beautifulsoup4-vs-scrapy-comparison"
      }
    },
    {
      "id": "172bb952-0954-40ad-a025-208de357dd15",
      "source": "firecrawl/blog/beautifulsoup4-vs-scrapy-comparison.md",
      "content": "6. Modern Solutions: Introducing Firecrawl > AI-Powered Content Extraction\n\n![Firecrawl AI-powered web scraping tool interface showing natural language extraction capabilities and code examples](https://www.firecrawl.dev/images/blog/bs4_scrapy/firecrawl.png)  \nUnlike BS4 and Scrapy which require you to specify exact HTML elements, Firecrawl lets you describe what you want to extract in plain English. This semantic approach means your scrapers keep working even when websites change their structure. Hereâ€™s a practical example of scraping GitHubâ€™s trending repositories:  \n```python\n# Import required libraries\nfrom firecrawl import FirecrawlApp\nfrom pydantic import BaseModel, Field\nfrom dotenv import load_dotenv\nfrom typing import List\n\n# Load environment variables from .env file\nload_dotenv()\n\n# Define Pydantic model for a single GitHub repository\nclass Repository(BaseModel):\n# Each field represents a piece of data we want to extract\nname: str = Field(description=\"The repository name including organization/username\")\ndescription: str = Field(description=\"The repository description\")\nstars: int = Field(description=\"Total number of stars\")\nlanguage: str = Field(description=\"Primary programming language\")\nurl: str = Field(description=\"The repository URL\")\n\n# Define model for the full response containing list of repositories\nclass Repositories(BaseModel):\nrepositories: List[Repository] = Field(description=\"List of trending repositories\")\n\n# Initialize Firecrawl app\napp = FirecrawlApp()\n\n# Scrape GitHub trending page using our defined schema\ntrending_repos = app.scrape_url(\n'https://github.com/trending',\nparams={\n# Specify we want to extract structured data\n\"formats\": [\"extract\"],\n\"extract\": {\n# Use our Pydantic model schema for extraction\n\"schema\": Repositories.model_json_schema(),\n}\n}\n)\n\n# Loop through the first 3 repositories and print their details\nfor idx, repo in enumerate(trending_repos['extract']['repositories']):\nprint(f\"{idx + 1}. {repo['name']}\")\nprint(f\"â­ {repo['stars']} stars\")\nprint(f\"ðŸ’» {repo['language']}\")\nprint(f\"ðŸ“ {repo['description']}\")\nprint(f\"ðŸ”— {repo['url']}n\")\n\n# Break after showing 3 repositories\nif idx == 2:\nbreak\n\n```  \n```python\n1. pathwaycom/pathway\nâ­ 11378 stars\nðŸ’» Python\nðŸ“ Python ETL framework for stream processing, real-time analytics, LLM pipelines, and RAG.\nðŸ”— https://github.com/pathwaycom/pathway\n\n2. EbookFoundation/free-programming-books\nâ­ 345107 stars\nðŸ’» HTML\nðŸ“ ðŸ“š Freely available programming books\nðŸ”— https://github.com/EbookFoundation/free-programming-books\n\n3. DrewThomasson/ebook2audiobook\nâ­ 3518 stars\nðŸ’» Python\nðŸ“ Convert ebooks to audiobooks with chapters and metadata using dynamic AI models and voice cloning. Supports 1,107+ languages!\nðŸ”— https://github.com/DrewThomasson/ebook2audiobook\n\n```  \nFirecrawl addresses the major pain points we discussed earlier:  \n1. **JavaScript Rendering**: Automatically handles dynamic content without additional tools\n2. **Anti-Bot Measures**: Built-in proxy rotation and browser fingerprinting\n3. **Maintenance**: AI adapts to site changes without updating selectors\n4. **Rate Limiting**: Smart request management with automatic retries\n5. **Multiple Formats**: Export data in various formats (JSON, CSV, Markdown)",
      "metadata": {
        "title": "BeautifulSoup4 vs. Scrapy - A Comprehensive Comparison for Web Scraping in Python",
        "url": "https://www.firecrawl.dev/blog/beautifulsoup4-vs-scrapy-comparison"
      }
    },
    {
      "id": "04b159f9-89f8-491b-bc0a-3ca810451492",
      "source": "firecrawl/blog/beautifulsoup4-vs-scrapy-comparison.md",
      "content": "6. Modern Solutions: Introducing Firecrawl > When to Choose Firecrawl\n\nFirecrawl is particularly valuable when:  \n- You need reliable, low-maintenance scrapers\n- Websites frequently change their structure\n- Youâ€™re dealing with JavaScript-heavy sites\n- Anti-bot measures are a concern\n- You need clean, structured data for AI/ML\n- Time-to-market is critical  \nWhile you have to pay for higher usage limits, the reduction in development and maintenance time often makes it more cost-effective than maintaining custom scraping infrastructure with traditional tools.",
      "metadata": {
        "title": "BeautifulSoup4 vs. Scrapy - A Comprehensive Comparison for Web Scraping in Python",
        "url": "https://www.firecrawl.dev/blog/beautifulsoup4-vs-scrapy-comparison"
      }
    },
    {
      "id": "e495880b-e5fa-4eb6-a720-4981be6d3eda",
      "source": "firecrawl/blog/beautifulsoup4-vs-scrapy-comparison.md",
      "content": "7. Making the Right Choice\n\nChoosing the right web scraping tool isnâ€™t a one-size-fits-all decision. Letâ€™s break down a practical framework to help you make the best choice for your specific needs.",
      "metadata": {
        "title": "BeautifulSoup4 vs. Scrapy - A Comprehensive Comparison for Web Scraping in Python",
        "url": "https://www.firecrawl.dev/blog/beautifulsoup4-vs-scrapy-comparison"
      }
    },
    {
      "id": "3f09d16b-4161-46dc-83cf-8169e076cb67",
      "source": "firecrawl/blog/beautifulsoup4-vs-scrapy-comparison.md",
      "content": "7. Making the Right Choice > Decision Framework\n\n1. **Project Scale**\n- Small (1-10 pages): BeautifulSoup4\n- Medium (10-100 pages): BeautifulSoup4 or Scrapy\n- Large (100+ pages): Scrapy or Firecrawl\n2. **Technical Requirements**\n- Static HTML only: BeautifulSoup4\n- Multiple pages & data processing: Scrapy\n- Dynamic content & anti-bot bypass: Firecrawl\n3. **Development Resources**\n- Time available:\n- Hours: BeautifulSoup4\n- Days: Scrapy\n- Minutes: Firecrawl\n- Team expertise:\n- Beginners: BeautifulSoup4\n- Experienced developers: Scrapy\n- Production teams: Firecrawl",
      "metadata": {
        "title": "BeautifulSoup4 vs. Scrapy - A Comprehensive Comparison for Web Scraping in Python",
        "url": "https://www.firecrawl.dev/blog/beautifulsoup4-vs-scrapy-comparison"
      }
    },
    {
      "id": "f105419a-b8ef-4681-9577-65da6274cdd2",
      "source": "firecrawl/blog/beautifulsoup4-vs-scrapy-comparison.md",
      "content": "7. Making the Right Choice > Cost-Benefit Analysis\n\n| Factor | BeautifulSoup4 | Scrapy | Firecrawl |\n| --- | --- | --- | --- |\n| Initial Cost | Free | Free | Paid |\n| Development Time | Low | High | Minimal |\n| Maintenance Cost | High | Medium | Low |\n| Scalability | Limited | Good | Excellent |",
      "metadata": {
        "title": "BeautifulSoup4 vs. Scrapy - A Comprehensive Comparison for Web Scraping in Python",
        "url": "https://www.firecrawl.dev/blog/beautifulsoup4-vs-scrapy-comparison"
      }
    },
    {
      "id": "ba674800-ca43-44b6-b53d-38296d6f9c77",
      "source": "firecrawl/blog/beautifulsoup4-vs-scrapy-comparison.md",
      "content": "7. Making the Right Choice > Future-Proofing Your Choice\n\nConsider these factors for long-term success:  \n1. **Maintainability**\n- Will your team be able to maintain the scraper?\n- How often does the target website change?\n- Whatâ€™s the cost of scraper downtime?\n2. **Scalability Requirements**\n- Do you expect your scraping needs to grow?\n- Will you need to add more websites?\n- Are there seasonal traffic spikes?\n3. **Integration Needs**\n- Does it need to work with existing systems?\n- What format do you need the data in?\n- Are there specific performance requirements?",
      "metadata": {
        "title": "BeautifulSoup4 vs. Scrapy - A Comprehensive Comparison for Web Scraping in Python",
        "url": "https://www.firecrawl.dev/blog/beautifulsoup4-vs-scrapy-comparison"
      }
    },
    {
      "id": "4de64b3c-23a9-4314-822e-831b021fb61f",
      "source": "firecrawl/blog/beautifulsoup4-vs-scrapy-comparison.md",
      "content": "7. Making the Right Choice > Practical Recommendations\n\nStart with BeautifulSoup4 if:  \n- Youâ€™re learning web scraping\n- You need to scrape simple, static websites\n- You have time to handle maintenance\n- Budget is your primary constraint  \nChoose Scrapy when:  \n- You need to scrape at scale\n- You have experienced developers\n- You need fine-grained control\n- Youâ€™re building a long-term solution  \nConsider Firecrawl if:  \n- Time to market is critical\n- You need reliable production scrapers\n- Maintenance costs are a concern\n- Youâ€™re dealing with complex websites\n- You need AI-ready data formats",
      "metadata": {
        "title": "BeautifulSoup4 vs. Scrapy - A Comprehensive Comparison for Web Scraping in Python",
        "url": "https://www.firecrawl.dev/blog/beautifulsoup4-vs-scrapy-comparison"
      }
    },
    {
      "id": "cd373299-4c2f-44a7-8f8d-ea2ccccf7702",
      "source": "firecrawl/blog/beautifulsoup4-vs-scrapy-comparison.md",
      "content": "8. Conclusion\n\nThe web scraping landscape offers distinct tools for different needs. BeautifulSoup4 excels in simplicity, making it ideal for beginners and quick projects. Scrapy provides powerful features for large-scale operations but requires more expertise. Modern solutions like Firecrawl bridge the gap with AI-powered capabilities that address traditional scraping challenges, though at a cost.",
      "metadata": {
        "title": "BeautifulSoup4 vs. Scrapy - A Comprehensive Comparison for Web Scraping in Python",
        "url": "https://www.firecrawl.dev/blog/beautifulsoup4-vs-scrapy-comparison"
      }
    },
    {
      "id": "9f0f011f-411b-45fc-9f56-bff26b443d1d",
      "source": "firecrawl/blog/beautifulsoup4-vs-scrapy-comparison.md",
      "content": "8. Conclusion > Key Takeaways\n\n- BeautifulSoup4: Best for learning and simple, static websites\n- Scrapy: Ideal for large-scale projects needing fine control\n- Firecrawl: Perfect when reliability and low maintenance are priorities\n- Consider long-term costs and scalability in your decision  \nChoose based on your projectâ€™s scale, team expertise, and long-term needs. As websites grow more complex and anti-bot measures evolve, picking the right tool becomes crucial for sustainable web scraping success.",
      "metadata": {
        "title": "BeautifulSoup4 vs. Scrapy - A Comprehensive Comparison for Web Scraping in Python",
        "url": "https://www.firecrawl.dev/blog/beautifulsoup4-vs-scrapy-comparison"
      }
    },
    {
      "id": "214ebc1d-4037-4553-a855-789fce374de3",
      "source": "firecrawl/blog/beautifulsoup4-vs-scrapy-comparison.md",
      "content": "8. Conclusion > Useful links\n\n- [BeautifulSoup4 Documentation](https://www.crummy.com/software/BeautifulSoup/bs4/doc/)\n- [Scrapy Official Website](https://scrapy.org/)\n- [Scrapy Documentation](https://docs.scrapy.org/)\n- [Web Scraping Best Practices](https://www.scrapingbee.com/blog/web-scraping-best-practices/)\n- [Firecrawl Documentation](https://docs.firecrawl.dev/)\n- [Getting Started With Firecrawl](https://www.firecrawl.dev/blog/mastering-firecrawl-scrape-endpoint)  \nArticle updated recently  \n[ðŸ”¥](https://www.firecrawl.dev/)",
      "metadata": {
        "title": "BeautifulSoup4 vs. Scrapy - A Comprehensive Comparison for Web Scraping in Python",
        "url": "https://www.firecrawl.dev/blog/beautifulsoup4-vs-scrapy-comparison"
      }
    },
    {
      "id": "f22596c8-2c9b-4745-aaad-d85bcdeca505",
      "source": "firecrawl/blog/beautifulsoup4-vs-scrapy-comparison.md",
      "content": "Ready to _Build?_\n\nStart scraping web data for your AI apps today.  \nNo credit card needed.  \nGet Started",
      "metadata": {
        "title": "BeautifulSoup4 vs. Scrapy - A Comprehensive Comparison for Web Scraping in Python",
        "url": "https://www.firecrawl.dev/blog/beautifulsoup4-vs-scrapy-comparison"
      }
    },
    {
      "id": "3cd9e3d1-a3fb-46a9-89a2-427e7571bc96",
      "source": "firecrawl/blog/beautifulsoup4-vs-scrapy-comparison.md",
      "content": "About the Author\n\n[![Bex Tuychiev image](https://www.firecrawl.dev/bex.jpg)\\\nBex Tuychiev@bextuychiev](https://x.com/bextuychiev)  \nBex is a Top 10 AI writer on Medium and a Kaggle Master with over 15k followers. He loves writing detailed guides, tutorials, and notebooks on complex data science and machine learning topics",
      "metadata": {
        "title": "BeautifulSoup4 vs. Scrapy - A Comprehensive Comparison for Web Scraping in Python",
        "url": "https://www.firecrawl.dev/blog/beautifulsoup4-vs-scrapy-comparison"
      }
    },
    {
      "id": "83e84b1f-c86a-4b90-8f06-7822e8c73314",
      "source": "firecrawl/blog/beautifulsoup4-vs-scrapy-comparison.md",
      "content": "About the Author > More articles by Bex Tuychiev\n\n[Building an Automated Price Tracking Tool\\\n\\\nBuild an automated e-commerce price tracker in Python. Learn web scraping, price monitoring, and automated alerts using Firecrawl, Streamlit, PostgreSQL.](https://www.firecrawl.dev/blog/automated-price-tracking-tutorial-python) [Web Scraping Automation: How to Run Scrapers on a Schedule\\\n\\\nLearn how to automate web scraping in Python using free tools like schedule, asyncio, cron jobs and GitHub Actions. This comprehensive guide covers local and cloud-based scheduling methods to run scrapers reliably in 2025.](https://www.firecrawl.dev/blog/automated-web-scraping-free-2025) [Automated Data Collection - A Comprehensive Guide\\\n\\\nLearn how to build robust automated data collection systems using modern tools and best practices. This guide covers everything from selecting the right tools to implementing scalable collection pipelines.](https://www.firecrawl.dev/blog/automated-data-collection-guide) [BeautifulSoup4 vs. Scrapy - A Comprehensive Comparison for Web Scraping in Python\\\n\\\nLearn the key differences between BeautifulSoup4 and Scrapy for web scraping in Python. Compare their features, performance, and use cases to choose the right tool for your web scraping needs.](https://www.firecrawl.dev/blog/beautifulsoup4-vs-scrapy-comparison) [How to Build an Automated Competitor Price Monitoring System with Python\\\n\\\nLearn how to build an automated competitor price monitoring system in Python that tracks prices across e-commerce sites, provides real-time comparisons, and maintains price history using Firecrawl, Streamlit, and GitHub Actions.](https://www.firecrawl.dev/blog/automated-competitor-price-scraping) [Scraping Company Data and Funding Information in Bulk With Firecrawl and Claude\\\n\\\nLearn how to build a web scraper in Python that gathers company details, funding rounds, and investor information from public sources like Crunchbase using Firecrawl and Claude for automated data collection and analysis.](https://www.firecrawl.dev/blog/crunchbase-scraping-with-firecrawl-claude) [Data Enrichment: A Complete Guide to Enhancing Your Data Quality\\\n\\\nLearn how to enrich your data quality with a comprehensive guide covering data enrichment tools, best practices, and real-world examples. Discover how to leverage modern solutions like Firecrawl to automate data collection, validation, and integration for better business insights.](https://www.firecrawl.dev/blog/complete-guide-to-data-enrichment) [Building an Intelligent Code Documentation RAG Assistant with DeepSeek and Firecrawl\\\n\\\nLearn how to build an intelligent documentation assistant powered by DeepSeek and RAG (Retrieval Augmented Generation) that can answer questions about any documentation website by combining language models with efficient information retrieval.](https://www.firecrawl.dev/blog/deepseek-rag-documentation-assistant)",
      "metadata": {
        "title": "BeautifulSoup4 vs. Scrapy - A Comprehensive Comparison for Web Scraping in Python",
        "url": "https://www.firecrawl.dev/blog/beautifulsoup4-vs-scrapy-comparison"
      }
    },
    {
      "id": "27e7d9c6-dc89-4286-9494-265c52a9d48a",
      "source": "firecrawl/blog/data-extraction-using-llms.md",
      "content": "---\ntitle: Extract website data using LLMs\nurl: https://www.firecrawl.dev/blog/data-extraction-using-llms\n---  \nIntroducing /extract - Get web data with a prompt [Try now](https://www.firecrawl.dev/extract)  \nMay 20, 2024  \nâ€¢  \n[![Nicolas Camara image](https://www.firecrawl.dev/nick-img.jpeg)Nicolas Camara](https://x.com/nickscamara_)  \n# Extract website data using LLMs  \n![Extract website data using LLMs image](https://www.firecrawl.dev/images/blog/g2.png)",
      "metadata": {
        "title": "Extract website data using LLMs",
        "url": "https://www.firecrawl.dev/blog/data-extraction-using-llms"
      }
    },
    {
      "id": "8f68909f-c3da-4159-aef1-4844cd36b450",
      "source": "firecrawl/blog/data-extraction-using-llms.md",
      "content": "Setup\n\nInstall our python dependencies, including groq and firecrawl-py.  \n```bash\npip install groq firecrawl-py\n\n```",
      "metadata": {
        "title": "Extract website data using LLMs",
        "url": "https://www.firecrawl.dev/blog/data-extraction-using-llms"
      }
    },
    {
      "id": "5aebb849-c0b9-4c02-8aff-59cbbe528ee4",
      "source": "firecrawl/blog/data-extraction-using-llms.md",
      "content": "Getting your Groq and Firecrawl API Keys\n\nTo use Groq and Firecrawl, you will need to get your API keys. You can get your Groq API key from [here](https://groq.com/) and your Firecrawl API key from [here](https://firecrawl.dev/).",
      "metadata": {
        "title": "Extract website data using LLMs",
        "url": "https://www.firecrawl.dev/blog/data-extraction-using-llms"
      }
    },
    {
      "id": "2a36125d-dbf5-4d41-ac95-b5e1009f9287",
      "source": "firecrawl/blog/data-extraction-using-llms.md",
      "content": "Load website with Firecrawl\n\nTo be able to get all the data from a website page and make sure it is in the cleanest format, we will use [Firecrawl](https://firecrawl.dev/). It handles by-passing JS-blocked websites, extracting the main content, and outputting in a LLM-readable format for increased accuracy.  \nHere is how we will scrape a website url using Firecrawl. We will also set a `pageOptions` for only extracting the main content ( `onlyMainContent: True`) of the website page - excluding the navs, footers, etc.  \n```python\nfrom firecrawl import FirecrawlApp # Importing the FireCrawlLoader\n\nurl = \"https://about.fb.com/news/2024/04/introducing-our-open-mixed-reality-ecosystem/\"\n\nfirecrawl = FirecrawlApp(\napi_key=\"fc-YOUR_FIRECRAWL_API_KEY\",\n)\npage_content = firecrawl.scrape_url(url=url, # Target URL to crawl\nparams={\n\"pageOptions\":{\n\"onlyMainContent\": True # Ignore navs, footers, etc.\n}\n})\nprint(page_content)\n\n```  \nPerfect, now we have clean data from the website - ready to be fed to the LLM for data extraction.",
      "metadata": {
        "title": "Extract website data using LLMs",
        "url": "https://www.firecrawl.dev/blog/data-extraction-using-llms"
      }
    },
    {
      "id": "928a3e41-0f24-4474-87fb-de0e49ba29e1",
      "source": "firecrawl/blog/data-extraction-using-llms.md",
      "content": "Extraction and Generation\n\nNow that we have the website data, letâ€™s use Groq to pull out the information we need. Weâ€™ll use Groq Llama 3 model in JSON mode and pick out certain fields from the page content.  \nWe are using LLama 3 8b model for this example. Feel free to use bigger models for improved results.  \n```python\nimport json\nfrom groq import Groq\n\nclient = Groq(\napi_key=\"gsk_YOUR_GROQ_API_KEY\", # Note: Replace 'API_KEY' with your actual Groq API key\n)\n\n# Here we define the fields we want to extract from the page content\nextract = [\"summary\",\"date\",\"companies_building_with_quest\",\"title_of_the_article\",\"people_testimonials\"]\n\ncompletion = client.chat.completions.create(\nmodel=\"llama3-8b-8192\",\nmessages=[\\\n{\\\n\"role\": \"system\",\\\n\"content\": \"You are a legal advisor who extracts information from documents in JSON.\"\\\n},\\\n{\\\n\"role\": \"user\",\\\n# Here we pass the page content and the fields we want to extract\\\n\"content\": f\"Extract the following information from the provided documentation:Page content:nn{page_content}nnInformation to extract: {extract}\"\\\n}\\\n],\ntemperature=0,\nmax_tokens=1024,\ntop_p=1,\nstream=False,\nstop=None,\n# We set the response format to JSON object\nresponse_format={\"type\": \"json_object\"}\n)\n\n# Pretty print the JSON response\ndataExtracted = json.dumps(str(completion.choices[0].message.content), indent=4)\n\nprint(dataExtracted)\n\n```  \nOne pro tip is to use an LLM montioring system like [Traceloop](https://www.traceloop.com/) with these calls. This will allow you to quickly test and monitor output quality.",
      "metadata": {
        "title": "Extract website data using LLMs",
        "url": "https://www.firecrawl.dev/blog/data-extraction-using-llms"
      }
    },
    {
      "id": "d047f8d4-b498-46ff-a137-7386883ee4a5",
      "source": "firecrawl/blog/data-extraction-using-llms.md",
      "content": "And Voila!\n\nYou have now built a data extraction bot using Groq and Firecrawl. You can now use this bot to extract structured data from any website. If you are looking to deploy your own models instead of using the choices Groq gives you, you can try out [Cerebrium](https://www.cerebrium.ai/) which hosts custom models blazingly fast.  \nIf you have any questions or need help, feel free to reach out to us at [Firecrawl](https://firecrawl.dev/).  \nArticle updated recently  \n[ðŸ”¥](https://www.firecrawl.dev/)",
      "metadata": {
        "title": "Extract website data using LLMs",
        "url": "https://www.firecrawl.dev/blog/data-extraction-using-llms"
      }
    },
    {
      "id": "a4a8f90b-c6bc-44ac-976b-09c8b16bfa73",
      "source": "firecrawl/blog/data-extraction-using-llms.md",
      "content": "Ready to _Build?_\n\nStart scraping web data for your AI apps today.  \nNo credit card needed.  \nGet Started",
      "metadata": {
        "title": "Extract website data using LLMs",
        "url": "https://www.firecrawl.dev/blog/data-extraction-using-llms"
      }
    },
    {
      "id": "3b475155-0192-432d-a0a9-12be73f53809",
      "source": "firecrawl/blog/data-extraction-using-llms.md",
      "content": "About the Author\n\n[![Nicolas Camara image](https://www.firecrawl.dev/nick-img.jpeg)\\\nNicolas Camara@nickscamara_](https://x.com/nickscamara_)  \nNicolas Camara is the Chief Technology Officer (CTO) at Firecrawl.\nHe previously built and scaled Mendable, one of the pioneering \"chat with your documents\" apps,\nwhich had major Fortune 500 customers like Snapchat, Coinbase, and MongoDB.\nPrior to that, Nicolas built SideGuide, the first code-learning tool inside VS Code,\nand grew a community of 50,000 users. Nicolas studied Computer Science and has over 10 years of experience in building software.",
      "metadata": {
        "title": "Extract website data using LLMs",
        "url": "https://www.firecrawl.dev/blog/data-extraction-using-llms"
      }
    },
    {
      "id": "83bb7737-c647-4d00-9724-e41100d27c7f",
      "source": "firecrawl/blog/data-extraction-using-llms.md",
      "content": "About the Author > More articles by Nicolas Camara\n\n[Using OpenAI's Realtime API and Firecrawl to Talk with Any Website\\\n\\\nBuild a real-time conversational agent that interacts with any website using OpenAI's Realtime API and Firecrawl.](https://www.firecrawl.dev/blog/How-to-Talk-with-Any-Website-Using-OpenAIs-Realtime-API-and-Firecrawl) [Extract website data using LLMs\\\n\\\nLearn how to use Firecrawl and Groq to extract structured data from a web page in a few lines of code.](https://www.firecrawl.dev/blog/data-extraction-using-llms) [Getting Started with Grok-2: Setup and Web Crawler Example\\\n\\\nA detailed guide on setting up Grok-2 and building a web crawler using Firecrawl.](https://www.firecrawl.dev/blog/grok-2-setup-and-web-crawler-example) [Launch Week I / Day 6: LLM Extract (v1)\\\n\\\nExtract structured data from your web pages using the extract format in /scrape.](https://www.firecrawl.dev/blog/launch-week-i-day-6-llm-extract) [Launch Week I / Day 7: Crawl Webhooks (v1)\\\n\\\nNew /crawl webhook support. Send notifications to your apps during a crawl.](https://www.firecrawl.dev/blog/launch-week-i-day-7-webhooks) [OpenAI Swarm Tutorial: Create Marketing Campaigns for Any Website\\\n\\\nA guide to building a multi-agent system using OpenAI Swarm and Firecrawl for AI-driven marketing strategies](https://www.firecrawl.dev/blog/openai-swarm-agent-tutorial) [Build a 'Chat with website' using Groq Llama 3\\\n\\\nLearn how to use Firecrawl, Groq Llama 3, and Langchain to build a 'Chat with your website' bot.](https://www.firecrawl.dev/blog/chat-with-website) [Scrape and Analyze Airbnb Data with Firecrawl and E2B\\\n\\\nLearn how to scrape and analyze Airbnb data using Firecrawl and E2B in a few lines of code.](https://www.firecrawl.dev/blog/scrape-analyze-airbnb-data-with-e2b)",
      "metadata": {
        "title": "Extract website data using LLMs",
        "url": "https://www.firecrawl.dev/blog/data-extraction-using-llms"
      }
    },
    {
      "id": "63748c3d-4b55-4205-9ab7-54ec1fd74964",
      "source": "firecrawl/blog/mastering-firecrawl-scrape-endpoint.md",
      "content": "---\ntitle: How to Use Firecrawl's Scrape API: Complete Web Scraping Tutorial\nurl: https://www.firecrawl.dev/blog/mastering-firecrawl-scrape-endpoint\n---  \nIntroducing /extract - Get web data with a prompt [Try now](https://www.firecrawl.dev/extract)  \nNov 25, 2024  \nâ€¢  \n[![Bex Tuychiev image](https://www.firecrawl.dev/bex.jpg)Bex Tuychiev](https://x.com/bextuychiev)  \n# How to Use Firecrawl's Scrape API: Complete Web Scraping Tutorial  \n![How to Use Firecrawl's Scrape API: Complete Web Scraping Tutorial image](https://www.firecrawl.dev/images/blog/scrape-masterclass/mastering-scrape.jpg)",
      "metadata": {
        "title": "How to Use Firecrawl's Scrape API: Complete Web Scraping Tutorial",
        "url": "https://www.firecrawl.dev/blog/mastering-firecrawl-scrape-endpoint"
      }
    },
    {
      "id": "5f5c6515-3d13-4ca0-a9cf-4468c044555f",
      "source": "firecrawl/blog/mastering-firecrawl-scrape-endpoint.md",
      "content": "Getting Started with Modern Web Scraping: An Introduction\n\nTraditional web scraping offers unique challenges. Relevant information is often scattered across multiple pages containing complex elements like code blocks, iframes, and media. JavaScript-heavy websites and authentication requirements add additional complexity to the scraping process.  \nEven after successfully scraping, the content requires specific formatting to be useful for downstream processes like data engineering or training AI and machine learning models.  \nFirecrawl addresses these challenges by providing a specialized scraping solution. Its [`/scrape` endpoint](https://docs.firecrawl.dev/features/scrape) offers features like JavaScript rendering, automatic content extraction, bypassing blockers and flexible output formats that make it easier to collect high-quality information and training data at scale.  \nIn this guide, weâ€™ll explore how to effectively use Firecrawlâ€™s `/scrape` endpoint to extract structured data from static and dynamic websites. Weâ€™ll start with basic scraping setup and then dive into a real-world example of scraping weather data from weather.com, demonstrating how to handle JavaScript-based interactions, extract structured data using schemas, and capture screenshots during the scraping process.",
      "metadata": {
        "title": "How to Use Firecrawl's Scrape API: Complete Web Scraping Tutorial",
        "url": "https://www.firecrawl.dev/blog/mastering-firecrawl-scrape-endpoint"
      }
    },
    {
      "id": "7f67a426-5e38-44ae-a433-407828ea9dfa",
      "source": "firecrawl/blog/mastering-firecrawl-scrape-endpoint.md",
      "content": "Table of Contents\n\n- [Getting Started with Modern Web Scraping: An Introduction](https://www.firecrawl.dev/blog/mastering-firecrawl-scrape-endpoint#getting-started-with-modern-web-scraping-an-introduction)\n- [What Is Firecrawlâ€™s `/scrape` Endpoint? The Short Answer](https://www.firecrawl.dev/blog/mastering-firecrawl-scrape-endpoint#what-is-firecrawls-scrape-endpoint-the-short-answer)\n- [Prerequisites: Setting Up Firecrawl](https://www.firecrawl.dev/blog/mastering-firecrawl-scrape-endpoint#prerequisites-setting-up-firecrawl)\n- [Basic Scraping Setup](https://www.firecrawl.dev/blog/mastering-firecrawl-scrape-endpoint#basic-scraping-setup)\n- [Large-scale Scraping With Batch Operations](https://www.firecrawl.dev/blog/mastering-firecrawl-scrape-endpoint#large-scale-scraping-with-batch-operations)\n- [Batch Scraping with `batch_scrape_urls`](https://www.firecrawl.dev/blog/mastering-firecrawl-scrape-endpoint#batch-scraping-with-batch_scrape_urls)\n- [Asynchronous batch scraping with `async_batch_scrape_urls`](https://www.firecrawl.dev/blog/mastering-firecrawl-scrape-endpoint#asynchronous-batch-scraping-with-async_batch_scrape_urls)\n- [How to Scrape Dynamic JavaScript Websites](https://www.firecrawl.dev/blog/mastering-firecrawl-scrape-endpoint#how-to-scrape-dynamic-javascript-websites)\n- [Conclusion](https://www.firecrawl.dev/blog/mastering-firecrawl-scrape-endpoint#conclusion)",
      "metadata": {
        "title": "How to Use Firecrawl's Scrape API: Complete Web Scraping Tutorial",
        "url": "https://www.firecrawl.dev/blog/mastering-firecrawl-scrape-endpoint"
      }
    },
    {
      "id": "634d39f5-16d2-47f9-81de-3edb1e1f7dd8",
      "source": "firecrawl/blog/mastering-firecrawl-scrape-endpoint.md",
      "content": "What Is Firecrawlâ€™s `/scrape` Endpoint? The Short Answer\n\nThe `/scrape` endpoint is Firecrawlâ€™s core web scraping API that enables automated extraction of content from any webpage. It handles common web scraping challenges like:  \n- JavaScript rendering - Executes JavaScript to capture dynamically loaded content\n- Content extraction - Automatically identifies and extracts main content while filtering out noise\n- Format conversion - Converts HTML to clean formats like Markdown or structured JSON\n- Screenshot capture - Takes full or partial page screenshots during scraping\n- Browser automation - Supports clicking, typing and other browser interactions\n- Anti-bot bypass - Uses rotating proxies and browser fingerprinting to avoid blocks  \nThe endpoint accepts a URL and configuration parameters, then returns the scraped content in your desired format. Itâ€™s designed to be flexible enough for both simple static page scraping and complex dynamic site automation.  \nNow that we understand what the endpoint does at a high level, letâ€™s look at how to set it up and start using it in practice.",
      "metadata": {
        "title": "How to Use Firecrawl's Scrape API: Complete Web Scraping Tutorial",
        "url": "https://www.firecrawl.dev/blog/mastering-firecrawl-scrape-endpoint"
      }
    },
    {
      "id": "9d195e58-b4e7-4d4c-9350-df8efca48860",
      "source": "firecrawl/blog/mastering-firecrawl-scrape-endpoint.md",
      "content": "Prerequisites: Setting Up Firecrawl\n\nFirecrawlâ€™s scraping engine is exposed as a REST API, so you can use command-line tools like cURL to use it. However, for a more comfortable experience, better flexibility and control, I recommend using one of its SDKs for Python, Node, Rust or Go. This tutorial will focus on the Python version.  \nTo get started, please make sure to:  \n1. Sign up at [firecrawl.dev](https://www.firecrawl.dev/).\n2. Choose a plan (the free one will work fine for this tutorial).  \nOnce you sign up, you will be given an API token which you can copy from your [dashboard](https://www.firecrawl.dev/app). The best way to save your key is by using a `.env` file, ideal for the purposes of this article:  \n```bash\ntouch .env\necho \"FIRECRAWL_API_KEY='YOUR_API_KEY'\" >> .env\n\n```  \nNow, letâ€™s install Firecrawl Python SDK, `python-dotenv` to read `.env` files, and Pandas for data analysis later:  \n```bash\npip install firecrawl-py python-dotenv pandas\n\n```",
      "metadata": {
        "title": "How to Use Firecrawl's Scrape API: Complete Web Scraping Tutorial",
        "url": "https://www.firecrawl.dev/blog/mastering-firecrawl-scrape-endpoint"
      }
    },
    {
      "id": "b505676c-b09a-4b28-b2f7-4b3e09edfde4",
      "source": "firecrawl/blog/mastering-firecrawl-scrape-endpoint.md",
      "content": "Basic Scraping Setup\n\nScraping with Firecrawl starts by creating an instance of the `FirecrawlApp` class:  \n```python\nfrom firecrawl import FirecrawlApp\nfrom dotenv import load_dotenv\n\nload_dotenv()\n\napp = FirecrawlApp()\n\n```  \nWhen you use the `load_dotenv()` function, the app can automatically use your loaded API key to establish a connection with the scraping engine. Then, scraping any URL takes a single line of code:  \n```python\nurl = \"https://arxiv.org\"\ndata = app.scrape_url(url)\n\n```  \nLetâ€™s take a look at the response format returned by `scrape_url` method:  \n```python\ndata['metadata']\n\n```  \n```json\n{\n\"title\": \"arXiv.org e-Print archiveopen searchopen navigation menucontact arXivsubscribe to arXiv mailings\",\n\"language\": \"en\",\n\"ogLocaleAlternate\": [],\n\"viewport\": \"width=device-width, initial-scale=1\",\n\"msapplication-TileColor\": \"#da532c\",\n\"theme-color\": \"#ffffff\",\n\"sourceURL\": \"[https://arxiv.org](https://arxiv.org)\",\n\"url\": \"[https://arxiv.org/](https://arxiv.org/)\",\n\"statusCode\": 200\n}\n\n```  \nThe response `metadata` includes basic information like the page title, viewport settings and a status code.  \nNow, letâ€™s look at the scraped contents, which is converted into `markdown` by default:  \n```python\nfrom IPython.display import Markdown\n\nMarkdown(data['markdown'][:500])\n\n```  \n```text\narXiv is a free distribution service and an open-access archive for nearly 2.4 million\nscholarly articles in the fields of physics, mathematics, computer science, quantitative biology, quantitative finance, statistics, electrical engineering and systems science, and economics.\nMaterials on this site are not peer-reviewed by arXiv.\n\nSubject search and browse:\n\nPhysics\n\nMathematics\n\nQuantitative Biology\n\nComputer Science\n\nQuantitative Finance\n\nStatistics\n\nElectrical Engineering and Systems Scienc\n\n```  \nThe response can include several other formats that we can request when scraping a URL. Letâ€™s try requesting multiple formats at once to see what additional data we can get back:  \n```python\ndata = app.scrape_url(\nurl,\nparams={\n'formats': [\\\n'html',\\\n'rawHtml',\\\n'links',\\\n'screenshot',\\\n]\n}\n)\n\n```  \nHere is what these formats scrape:  \n- **HTML**: The raw HTML content of the page.\n- **rawHtml**: The unprocessed HTML content, exactly as it appears on the page.\n- **links**: A list of all the hyperlinks found on the page.\n- **screenshot**: An image capture of the page as it appears in a browser.  \nThe HTML format is useful for developers who need to analyze or manipulate the raw structure of a webpage. The `rawHtml` format is ideal for cases where the exact original HTML content is required, such as for archival purposes or detailed comparison. The links format is beneficial for SEO specialists and web crawlers who need to extract and analyze all hyperlinks on a page. The screenshot format is perfect for visual documentation, quality assurance, and capturing the appearance of a webpage at a specific point in time.  \nPassing more than one scraping format to `params` adds additional keys to the response:  \n```python\ndata.keys()\n\n```  \n```text\ndict_keys(['rawHtml', 'screenshot', 'metadata', 'html', 'links'])\n\n```  \nLetâ€™s display the screenshot Firecrawl took of arXiv.org:  \n```python\nfrom IPython.display import Image\n\nImage(data['screenshot'])\n\n```  \n![Screenshot of arXiv.org homepage that was taken with Firecrawl's screenshot feature showing research paper categories like Computer Science, Mathematics, Physics and other scientific disciplines](https://www.firecrawl.dev/images/blog/scrape-masterclass/notebook_files/notebook_20_0.png)  \nNotice how the screenshot is cropped to fit a certain viewport. For most pages, it is better to capture the entire screen by using the `screenshot@fullPage` format:  \n```python\ndata = app.scrape_url(\nurl,\nparams={\n\"formats\": [\\\n\"screenshot@fullPage\",\\\n]\n}\n)\n\nImage(data['screenshot'])\n\n```  \n![Full page screenshot of arXiv.org homepage taken with Firecrawl's full-page screenshot capture feature showing research paper categories, search functionality, and recent submissions in an academic layout](https://www.firecrawl.dev/images/blog/scrape-masterclass/notebook_files/notebook_22_0.png)  \nAs a bonus, the `/scrape` endpoint can handle PDF links as well:  \n```python\npdf_link = \"https://arxiv.org/pdf/2411.09833.pdf\"\ndata = app.scrape_url(pdf_link)\n\nMarkdown(data['markdown'][:500])\n\n```  \n```text\narXiv:2411.09833v1 [math.DG] 14 Nov 2024\nEINSTEIN METRICS ON THE FULL FLAG F(N).\nMIKHAIL R. GUZMAN\nAbstract.LetM=G/Kbe a full flag manifold. In this work, we investigate theG-\nstability of Einstein metrics onMand analyze their stability types, including coindices,\nfor several cases. We specifically focus onF(n) = SU(n)/T, emphasizingn= 5, where\nwe identify four new Einstein metrics in addition to known ones. Stability data, including\ncoindex and Hessian spectrum, confirms that these metrics on\n\n```",
      "metadata": {
        "title": "How to Use Firecrawl's Scrape API: Complete Web Scraping Tutorial",
        "url": "https://www.firecrawl.dev/blog/mastering-firecrawl-scrape-endpoint"
      }
    },
    {
      "id": "355438f1-8c7a-48f1-910c-48f000e40aeb",
      "source": "firecrawl/blog/mastering-firecrawl-scrape-endpoint.md",
      "content": "Basic Scraping Setup > Further Scrape Configuration Options\n\nBy default, `scrape_url` converts everything it sees on a webpage to one of the specified formats. To control this behavior, Firecrawl offers the following parameters:  \n- `onlyMainContent`\n- `includeTags`\n- `excludeTags`  \n`onlyMainContent` excludes the navigation, footers, headers, etc. and is set to True by default.  \n`includeTags` and `excludeTags` can be used to allowlist/blocklist certain HTML elements:  \n```python\nurl = \"https://arxiv.org\"\n\ndata = app.scrape_url(url, params={\"includeTags\": [\"p\"], \"excludeTags\": [\"span\"]})\n\nMarkdown(data['markdown'][:1000])\n\n```  \n```markdown\n[Help](https://info.arxiv.org/help) | [Advanced Search](https://arxiv.org/search/advanced)\n\narXiv is a free distribution service and an open-access archive for nearly 2.4 million\nscholarly articles in the fields of physics, mathematics, computer science, quantitative biology, quantitative finance, statistics, electrical engineering and systems science, and economics.\nMaterials on this site are not peer-reviewed by arXiv.\n\n[arXiv Operational Status](https://status.arxiv.org)\n\nGet status notifications via\n[email](https://subscribe.sorryapp.com/24846f03/email/new)\nor [slack](https://subscribe.sorryapp.com/24846f03/slack/new)\n\n```  \n`includeTags` and `excludeTags` also support referring to HTML elements by their `#id` or `.class-name`.  \nThese configuration options help ensure efficient and precise scraping. While `onlyMainContent` filters out peripheral elements, `includeTags` and `excludeTags` enable surgical targeting of specific HTML elements - particularly valuable when dealing with complex webpage structures or when only certain content types are needed.",
      "metadata": {
        "title": "How to Use Firecrawl's Scrape API: Complete Web Scraping Tutorial",
        "url": "https://www.firecrawl.dev/blog/mastering-firecrawl-scrape-endpoint"
      }
    },
    {
      "id": "872c33b2-6e0c-4531-aca6-0271e3e0258c",
      "source": "firecrawl/blog/mastering-firecrawl-scrape-endpoint.md",
      "content": "Advanced Data Extraction: Structured Techniques\n\nScraping clean, LLM-ready data is the core philosophy of Firecrawl. However, certain web pages with their complex structures can interfere with this philosophy when scraped in their entirety. For this reason, Firecrawl offers two scraping methods for better structured outputs:  \n1. Natural language extraction - Use prompts to extract specific information and have an LLM structure the response\n2. Manual structured data extraction - Define JSON schemas to have an LLM scrape data in a predefined format  \nIn this section, we will cover both methods.",
      "metadata": {
        "title": "How to Use Firecrawl's Scrape API: Complete Web Scraping Tutorial",
        "url": "https://www.firecrawl.dev/blog/mastering-firecrawl-scrape-endpoint"
      }
    },
    {
      "id": "33be5db2-2cf8-419e-9b1c-8ad2a4b0fd39",
      "source": "firecrawl/blog/mastering-firecrawl-scrape-endpoint.md",
      "content": "Advanced Data Extraction: Structured Techniques > Natural Language Extraction - Use AI to Extract Data\n\nTo illustrate natural language scraping, letâ€™s try extracting all news article links that may be related to the 2024 US presidential election from the New York Times:  \n```python\nurl = \"https://nytimes.com\"\n\ndata = app.scrape_url(\nurl,\nparams={\n'formats': ['markdown', 'extract', 'screenshot'],\n'extract': {\n'prompt': \"Return a list of links of news articles that may be about the 2024 US presidential election\"\n}\n}\n)\n\n```  \nTo enable this feature, you are required to pass the `extract` option to the list of `formats` and provide a prompt in a dictionary to a separate `extract` field.  \nOnce scraping finishes, the response will include a new `extract` key:  \n```python\ndata['extract']\n\n```  \n```python\n{'news_articles': [{'title': 'Harris Loss Has Democrats Fighting Over How to Talk About Transgender Rights',\\\n'link': 'https://www.nytimes.com/2024/11/20/us/politics/presidential-campaign-transgender-rights.html'},\\\n{'title': 'As Democrats Question How to Win Back Latinos, Ruben Gallego Offers Answers',\\\n'link': 'https://www.nytimes.com/2024/11/20/us/politics/ruben-gallego-arizona-latino-voters-democrats.html'},\\\n...\\\n{'title': 'The Final Push for Ukraine?',\\\n'link': 'https://www.nytimes.com/2024/11/20/briefing/ukraine-russia-trump.html'}]}\n\n```  \nDue to the nature of this scraping method, the returned output can have arbitrary structure as we can see above. It seems the above output has the following format:  \n```python\n{\n\"news_articles\": [\\\n{\"title\": \"article_title\", \"link\": \"article_url\"},\\\n...\\\n]\n}\n\n```  \nThis LLM-based extraction can have endless applications, from extracting specific data points from complex websites to analyzing sentiment across multiple news sources to gathering structured information from unstructured web content.  \nTo improve the accuracy of the extraction and give additional instructions, you have the option to include a system prompt to the underlying LLM:  \n```python\ndata = app.scrape_url(\nurl,\nparams={\n'formats': ['markdown', 'extract'],\n'extract': {\n'prompt': \"Find any mentions of specific dollar amounts or financial figures and return them with their context and article link.\",\n'systemPrompt': \"You are a helpful assistant that extracts numerical financial data.\"\n}\n}\n)\n\n```  \nAbove, we are dictating that the LLM must act as an assistant that extracts numerical financial data. Letâ€™s look at its response:  \n```python\ndata['extract']\n\n```  \n```python\n{'financial_data': [\\\n{\\\n'amount': 121200000,\\\n'context': 'RenÃ© Magritte became the 16th artist whose work broke the nine-figure '\\\n'threshold at auction when his painting sold for $121.2 million.',\\\n'article_link': 'https://www.nytimes.com/2024/11/19/arts/design/magritte-surrealism-christies-auction.html'\\\n},\\\n{\\\n'amount': 5000000,\\\n'context': 'Benjamin Netanyahu offers $5 million for each hostage freed in Gaza.',\\\n'article_link': 'https://www.nytimes.com/2024/11/19/world/middleeast/israel-5-million-dollars-hostage.html'\\\n}\\\n]}\n\n```  \nThe output shows the LLM successfully extracted two financial data points from the articles.  \nThe LLM not only identified the specific amounts but also provided relevant context and source article links for each figure.",
      "metadata": {
        "title": "How to Use Firecrawl's Scrape API: Complete Web Scraping Tutorial",
        "url": "https://www.firecrawl.dev/blog/mastering-firecrawl-scrape-endpoint"
      }
    },
    {
      "id": "e3f50870-00af-446a-a5fc-cd577279dbd9",
      "source": "firecrawl/blog/mastering-firecrawl-scrape-endpoint.md",
      "content": "Advanced Data Extraction: Structured Techniques > Schema-Based Data Extraction - Building Structured Models\n\nWhile natural language scraping is powerful for exploration and prototyping, production systems typically require more structured and deterministic approaches. LLM responses can vary between runs of the same prompt, making the output format inconsistent and difficult to reliably parse in automated workflows.  \nFor this reason, Firecrawl allows you to pass a predefined schema to guide the LLMâ€™s output when transforming the scraped content. To facilitate this feature, Firecrawl uses Pydantic models.  \nIn the example below, we will extract only news article links, their titles with some additional details from the New York Times:  \n```python\nfrom pydantic import BaseModel, Field\n\nclass IndividualArticle(BaseModel):\ntitle: str = Field(description=\"The title of the news article\")\nsubtitle: str = Field(description=\"The subtitle of the news article\")\nurl: str = Field(description=\"The URL of the news article\")\nauthor: str = Field(description=\"The author of the news article\")\ndate: str = Field(description=\"The date the news article was published\")\nread_duration: int = Field(description=\"The estimated time it takes to read the news article\")\ntopics: list[str] = Field(description=\"A list of topics the news article is about\")\n\nclass NewsArticlesSchema(BaseModel):\nnews_articles: list[IndividualArticle] = Field(\ndescription=\"A list of news articles extracted from the page\"\n)\n\n```  \nAbove, we define a Pydantic schema that specifies the structure of the data we want to extract. The schema consists of two models:  \n`IndividualArticle` defines the structure for individual news articles with fields for:  \n- `title`\n- `subtitle`\n- `url`\n- `author`\n- `date`\n- `read_duration`\n- `topics`  \n`NewsArticlesSchema` acts as a container model that holds a list of `IndividualArticle` objects, representing multiple articles extracted from the page. If we donâ€™t use this container model, Firecrawl will only return the first news article it finds.  \nEach model field uses Pydanticâ€™s `Field` class to provide descriptions that help guide the LLM in correctly identifying and extracting the requested data. This structured approach ensures consistent output formatting.  \nThe next step is passing this schema to the `extract` parameter of `scrape_url`:  \n```python\nurl = \"https://nytimes.com\"\n\nstructured_data = app.scrape_url(\nurl,\nparams={\n\"formats\": [\"extract\", \"screenshot\"],\n\"extract\": {\n\"schema\": NewsArticlesSchema.model_json_schema(),\n\"prompt\": \"Extract the following data from the NY Times homepage: news article title, url, author, date, read_duration for all news articles\",\n\"systemPrompt\": \"You are a helpful assistant that extracts news article data from NY Times.\",\n},\n},\n)\n\n```  \nWhile passing the schema, we call its `model_json_schema()` method to automatically convert it to valid JSON. Letâ€™s look at the output:  \n```python\nstructured_data['extract']\n\n```  \n```python\n{\n'news_articles': [\\\n{\\\n'title': 'How Google Spent 15 Years Creating a Culture of Concealment',\\\n'subtitle': '',\\\n'url': 'https://www.nytimes.com/2024/11/20/technology/google-antitrust-employee-messages.html',\\\n'author': 'David Streitfeld',\\\n'date': '2024-11-20',\\\n'read_duration': 9,\\\n'topics': []\\\n},\\\n# ... additional articles ...\\\n{\\\n'title': 'The Reintroduction of Daniel Craig',\\\n'subtitle': '',\\\n'url': 'https://www.nytimes.com/2024/11/20/movies/daniel-craig-queer.html',\\\n'author': '',\\\n'date': '2024-11-20',\\\n'read_duration': 9,\\\n'topics': []\\\n}\\\n]\n}\n\n```  \nThis time, the response fields exactly match the fields we set during schema definition:  \n```python\n{\n\"news_articles\": [\\\n{...}, # Article 1\\\n{...}, # Article 2,\\\n... # Article n\\\n]\n}\n\n```  \nWhen creating the scraping schema, the following best practices can go a long way in ensuring reliable and accurate data extraction:  \n1. Keep field names simple and descriptive\n2. Use clear field descriptions that guide the LLM\n3. Break complex data into smaller, focused fields\n4. Include validation rules where possible\n5. Consider making optional fields that may not always be present\n6. Test the schema with a variety of content examples\n7. Iterate and refine based on extraction results  \nTo follow these best practices, the following Pydantic tips can help:  \n1. Use `Field(default=None)` to make fields optional\n2. Add validation with `Field(min_length=1, max_length=100)`\n3. Create custom validators with @validator decorator\n4. Use `conlist()` for list fields with constraints\n5. Add example values with `Field(example=\"Sample text\")`\n6. Create nested models for complex data structures\n7. Use computed fields with `@property` decorator  \nIf you follow all these tips, your schema can become quite sophisticated like below:  \n```python\nfrom pydantic import BaseModel, Field\nfrom typing import Optional, List\nfrom datetime import datetime\n\nclass Author(BaseModel):\n# Required field - must be provided when creating an Author\nname: str = Field(\n...,\nmin_length=1,\nmax_length=100,\ndescription=\"The full name of the article author\",\n)\n\n# Optional field - can be None or omitted\ntitle: Optional[str] = Field(\nNone, description=\"Author's title or role, if available\"\n)\n\nclass NewsArticle(BaseModel):\n# Required field - must be provided when creating a NewsArticle\ntitle: str = Field(\n...,\nmin_length=5,\nmax_length=300,\ndescription=\"The main headline or title of the news article\",\nexample=\"Breaking News: Major Scientific Discovery\",\n)\n\n# Required field - must be provided when creating a NewsArticle\nurl: str = Field(\n...,\ndescription=\"The full URL of the article\",\nexample=\"https://www.nytimes.com/2024/01/01/science/discovery.html\",\n)\n\n# Optional field - can be None or omitted\nauthors: Optional[List[Author]] = Field(\ndefault=None, description=\"List of article authors and their details\"\n)\n\n# Optional field - can be None or omitted\npublish_date: Optional[datetime] = Field(\ndefault=None, description=\"When the article was published\"\n)\n\n# Optional field with default empty list\nfinancial_amounts: List[float] = Field(\ndefault_factory=list,\nmax_length=10,\ndescription=\"Any monetary amounts mentioned in the article in USD\",\n)\n\n@property\ndef is_recent(self) -> bool:\nif not self.publish_date:\nreturn False\nreturn (datetime.now() - self.publish_date).days < 7\n\n```  \nThe schema above defines two key data models for news article data:  \nAuthor - Represents article author information with:  \n- `name` (required): The authorâ€™s full name\n- `title` (optional): The authorâ€™s role or title  \nNewsArticle - Represents a news article with:  \n- `title` (required): The article headline (5-300 chars)\n- `url` (required): Full article URL\n- `authors` (optional): List of Author objects\n- `publish_date` (optional): Article publication datetime\n- `financial_amounts` (optional): List of monetary amounts in USD  \nThe `NewsArticle` model includes an `is_recent` property that checks if the article was published within the last 7 days.  \nAs you can see, web scraping process becomes much easier and more powerful if you combine it with structured data models that validate and organize the scraped information. This allows for consistent data formats, type checking, and easy access to properties like checking if an article is recent.",
      "metadata": {
        "title": "How to Use Firecrawl's Scrape API: Complete Web Scraping Tutorial",
        "url": "https://www.firecrawl.dev/blog/mastering-firecrawl-scrape-endpoint"
      }
    },
    {
      "id": "0c57fad1-7a82-4bfc-96aa-7c3973ba06c2",
      "source": "firecrawl/blog/mastering-firecrawl-scrape-endpoint.md",
      "content": "Large-scale Scraping With Batch Operations\n\nUp to this point, we have been focusing on scraping pages one URL at a time. In reality, you will work with multiple, perhaps, thousands of URLs that need to be scraped in parallel. This is where batch operations become essential for efficient web scraping at scale. Batch operations allow you to process multiple URLs simultaneously, significantly reducing the overall time needed to collect data from multiple web pages.",
      "metadata": {
        "title": "How to Use Firecrawl's Scrape API: Complete Web Scraping Tutorial",
        "url": "https://www.firecrawl.dev/blog/mastering-firecrawl-scrape-endpoint"
      }
    },
    {
      "id": "a6d52802-4e18-42fc-97dc-ae169f07053b",
      "source": "firecrawl/blog/mastering-firecrawl-scrape-endpoint.md",
      "content": "Large-scale Scraping With Batch Operations > Batch Scraping with `batch_scrape_urls`\n\nThe `batch_scrape_urls` method lets you scrape multiple URLs at once.  \nLetâ€™s scrape all the news article links we obtained from our previous schema extraction example.  \n```python\narticles = structured_data['extract']['news_articles']\narticle_links = [article['url'] for article in articles]\n\nclass ArticleSummary(BaseModel):\ntitle: str = Field(description=\"The title of the news article\")\nsummary: str = Field(description=\"A short summary of the news article\")\n\nbatch_data = app.batch_scrape_urls(article_links, params={\n\"formats\": [\"extract\"],\n\"extract\": {\n\"schema\": ArticleSummary.model_json_schema(),\n\"prompt\": \"Extract the title of the news article and generate its brief summary\",\n}\n})\n\n```  \nHere is what is happening in the codeblock above:  \n- We extract the list of news articles from our previous structured data result\n- We create a list of article URLs by mapping over the articles and getting their â€˜urlâ€™ field\n- We define an `ArticleSummary` model with title and summary fields to structure our output\n- We use `batch_scrape_urls()` to process all article URLs in parallel, configuring it to:\n- Extract data in structured format\n- Use our `ArticleSummary` schema\n- Generate titles and summaries based on the article content  \nThe response from `batch_scrape_urls()` is a bit different:  \n```python\nbatch_data.keys()\n\n```  \n```python\ndict_keys(['success', 'status', 'completed', 'total', 'creditsUsed', 'expiresAt', 'data'])\n\n```  \nIt contains the following fields:  \n- `success`: Boolean indicating if the batch request succeeded\n- `status`: Current status of the batch job\n- `completed`: Number of URLs processed so far\n- `total`: Total number of URLs in the batch\n- `creditsUsed`: Number of API credits consumed\n- `expiresAt`: When the results will expire\n- `data`: The extracted data for each URL  \nLetâ€™s focus on the `data` key where the actual content is stored:  \n```python\nlen(batch_data['data'])\n\n```  \n```out\n19\n\n```  \nThe batch processing completed successfully with 19 articles. Letâ€™s examine the structure of the first article:  \n```python\nbatch_data['data'][0].keys()\n\n```  \n````out\ndict_keys(['extract', 'metadata'])\n\nThe response format here matches what we get from individual `scrape_url` calls.\n\n```python\nprint(batch_data['data'][0]['extract'])  \n````\n\n```out\n{'title': 'Ukrainian Forces Face Increasing Challenges Amidst Harsh Winter Conditions', 'summary': 'As the war in Ukraine enters its fourth winter, conditions are worsening for Ukrainian soldiers who find themselves trapped on the battlefield, surrounded by Russian forces. Military commanders express concerns over dwindling supplies and increasingly tough situations. The U.S. has recently allowed Ukraine to use American weapons for deeper strikes into Russia, marking a significant development in the ongoing conflict.'}  \n```\n\nThe scraping was performed according to our specifications, extracting the metadata, the title and generating a brief summary.\n\n### Asynchronous batch scraping with `async_batch_scrape_urls`\n\nScraping the 19 NY Times articles in a batch took about 10 seconds on my machine. While thatâ€™s not much, in practice, we cannot wait around as Firecrawl batch-scrapes thousands of URLs. For these larger workloads, Firecrawl provides an asynchronous batch scraping API that lets you submit jobs and check their status later, rather than blocking until completion. This is especially useful when integrating web scraping into automated workflows or processing large URL lists.\n\nThis feature is available through the `async_batch_scrape_urls` method and it works a bit differently:\n\n```python\nbatch_scrape_job = app.async_batch_scrape_urls(\narticle_links,\nparams={\n\"formats\": [\"extract\"],\n\"extract\": {\n\"schema\": ArticleSummary.model_json_schema(),\n\"prompt\": \"Extract the title of the news article and generate its brief summary\",\n},\n},\n)  \n```\n\nWhen using `async_batch_scrape_urls` instead of the synchronous version, the response comes back immediately rather than waiting for all URLs to be scraped. This allows the program to continue executing while the scraping happens in the background.\n\n```python\nbatch_scrape_job  \n```\n\n```python\n{'success': True,\n'id': '77a94b62-c676-4db2-b61b-4681e99f4704',\n'url': 'https://api.firecrawl.dev/v1/batch/scrape/77a94b62-c676-4db2-b61b-4681e99f4704'}  \n```\n\nThe response contains an ID belonging the background task that was initiated to process the URLs under the hood.\n\nYou can use this ID later to check the jobâ€™s status with `check_batch_scrape_status` method:\n\n```python\nbatch_scrape_job_status = app.check_batch_scrape_status(batch_scrape_job['id'])  \nbatch_scrape_job_status.keys()  \n```\n\n```python\ndict_keys(['success', 'status', 'total', 'completed', 'creditsUsed', 'expiresAt', 'data', 'error', 'next'])  \n```\n\nIf the job finished scraping all URLs, its `status` will be set to `completed`:\n\n```python\nbatch_scrape_job_status['status']  \n```\n\n```out\n'completed'  \n```\n\nLetâ€™s look at how many pages were scraped:\n\n```python\nbatch_scrape_job_status['total']  \n```\n\n```python\n19  \n```\n\nThe response always includes the `data` field, whether the job is complete or not, with the content scraped up to that point. It has `error` and `next` fields to indicate if any errors occurred during scraping and whether there are more results to fetch.\n\n## How to Scrape Dynamic JavaScript Websites\n\nOut in the wild, many websites you encounter will be dynamic, meaning their content is generated on-the-fly using JavaScript rather than being pre-rendered on the server. These sites often require user interaction like clicking buttons or typing into forms before displaying their full content. Traditional web scrapers that only look at the initial HTML fail to capture this dynamic content, which is why browser automation capabilities are essential for comprehensive web scraping.\n\nFirecrawl supports dynamic scraping by default. In the parameters of `scrape_url` or `batch_scrape_url`, you can define necessary actions to reach the target state of the page you are scraping. As an example, we will build a scraper that will extract the following information from `https://weather.com`:\n\n- Current Temperature\n- Temperature High\n- Temperature Low\n- Humidity\n- Pressure\n- Visibility\n- Wind Speed\n- Dew Point\n- UV Index\n- Moon Phase\n\nThese details are displayed for every city you search through the website:\n\n![Weather.com interface showing detailed weather forecast for London including temperature, humidity, wind speed and other meteorological data in an interactive dashboard layout](https://www.firecrawl.dev/images/blog/scrape-masterclass/notebook_files/image.png)\n\nUnlike websites such as Amazon where you can simply modify the URLâ€™s search parameter (e.g. `?search=your-query`), weather.com presents a unique challenge. The site generates dynamic and unique IDs for each city, making traditional URL manipulation techniques ineffective. To scrape weather data for any given city, you must simulate the actual user journey: visiting the homepage, interacting with the search bar, entering the city name, and selecting the appropriate result from the dropdown list. This multi-step interaction process is necessary because of how weather.com structures its dynamic content delivery (at this point, I urge to visit the website and visit a few city pages).\n\nFortunately, Firecrawl natively supports such interactions through the `actions` parameter. It accepts a list of dictionaries, where each dictionary represents one of the following interactions:\n\n- Waiting for the page to load\n- Clicking on an element\n- Writing text in input fields\n- Scrolling up/down\n- Take a screenshot at the current state\n- Scrape the current state of the webpage\n\nLetâ€™s define the actions we need for weather.com:\n\n```python\nactions = [\\\n{\"type\": \"wait\", \"milliseconds\": 3000},\\\n{\"type\": \"click\", \"selector\": 'input[id=\"LocationSearch_input\"]'},\\\n{\"type\": \"write\", \"text\": \"London\"},\\\n{\"type\": \"screenshot\"},\\\n{\"type\": \"wait\", \"milliseconds\": 1000},\\\n{\"type\": \"click\", \"selector\": \"button[data-testid='ctaButton']\"},\\\n{\"type\": \"wait\", \"milliseconds\": 3000},\\\n]  \n```\n\nLetâ€™s examine how we choose the selectors, as this is the most technical aspect of the actions. Using browser developer tools, we inspect the webpage elements to find the appropriate selectors. For the search input field, we locate an element with the ID â€œLocationSearch_inputâ€. After entering a city name, we include a 3-second wait to allow the dropdown search results to appear. At this stage, we capture a screenshot for debugging to verify the text input was successful.\n\nThe final step involves clicking the first matching result, which is identified by a button element with the `data-testid` attribute `ctaButton`. Note that if youâ€™re implementing this in the future, these specific attribute names may have changed - youâ€™ll need to use browser developer tools to find the current correct selectors.\n\nNow, letâ€™s define a Pydantic schema to guide the LLM:\n\n```python\nclass WeatherData(BaseModel):\nlocation: str = Field(description=\"The name of the city\")\ntemperature: str = Field(description=\"The current temperature in degrees Fahrenheit\")\ntemperature_high: str = Field(description=\"The high temperature for the day in degrees Fahrenheit\")\ntemperature_low: str = Field(description=\"The low temperature for the day in degrees Fahrenheit\")\nhumidity: str = Field(description=\"The current humidity as a percentage\")\npressure: str = Field(description=\"The current air pressure in inches of mercury\")\nvisibility: str = Field(description=\"The current visibility in miles\")\nwind_speed: str = Field(description=\"The current wind speed in miles per hour\")\ndew_point: str = Field(description=\"The current dew point in degrees Fahrenheit\")\nuv_index: str = Field(description=\"The current UV index\")\nmoon_phase: str = Field(description=\"The current moon phase\")  \n```\n\nFinally, letâ€™s pass these objects to `scrape_url`:\n\n```python\nurl = \"https://weather.com\"  \ndata = app.scrape_url(\nurl,\nparams={\n\"formats\": [\"screenshot\", \"markdown\", \"extract\"],\n\"actions\": actions,\n\"extract\": {\n\"schema\": WeatherData.model_json_schema(),\n\"prompt\": \"Extract the following weather data from the weather.com page: temperature, temperature high, temperature low, humidity, pressure, visibility, wind speed, dew point, UV index, and moon phase\",\n},\n},\n)  \n```\n\nThe scraping only happens once all actions are performed. Letâ€™s see if it was successful by looking at the `extract` key:\n\n```python\ndata['extract']  \n```\n\n```python\n{'location': 'London, England, United Kingdom',\n'temperature': '33Â°',\n'temperature_high': '39Â°',\n'temperature_low': '33Â°',\n'humidity': '79%',\n'pressure': '29.52in',\n'visibility': '10 mi',\n'wind_speed': '5 mph',\n'dew_point': '28Â°',\n'uv_index': '0 of 11',\n'moon_phase': 'Waning Gibbous'}  \n```\n\nAll details are accounted for! But, for illustration, we need to take a closer look at the response structure when using JS-based actions:\n\n```python\ndata.keys()  \n```\n\n```python\ndict_keys(['markdown', 'screenshot', 'actions', 'metadata', 'extract'])  \n```\n\nThe response has a new actions key:\n\n```python\ndata['actions']  \n```\n\n```python\n{'screenshots': ['https://service.firecrawl.dev/storage/v1/object/public/media/screenshot-16bf71d8-dcb5-47eb-9af4-5fa84195b91d.png'],\n'scrapes': []}  \n```\n\nThe actions array contained a single screenshot-generating action, which is reflected in the output above.\n\nLetâ€™s look at the screenshot:\n\n```python\nfrom IPython.display import Image  \nImage(data['actions']['screenshots'][0])  \n```\n\n![Screenshot of weather.com search interface showing search bar with typed city name, demonstrating automated web scraping process with Firecrawl](https://www.firecrawl.dev/images/blog/scrape-masterclass/notebook_files/notebook_96_0.png)\n\nThe image shows the stage where the scraper just typed the search query.\n\nNow, we have to convert this whole process into a function that works for any given city:\n\n```python\nfrom pydantic import BaseModel, Field\nfrom typing import Optional, Dict, Any  \nclass WeatherData(BaseModel):\nlocation: str = Field(description=\"The name of the city\")\ntemperature: str = Field(\ndescription=\"The current temperature in degrees Fahrenheit\"\n)\ntemperature_high: str = Field(\ndescription=\"The high temperature for the day in degrees Fahrenheit\"\n)\ntemperature_low: str = Field(\ndescription=\"The low temperature for the day in degrees Fahrenheit\"\n)\nhumidity: str = Field(description=\"The current humidity as a percentage\")\npressure: str = Field(description=\"The current air pressure in inches of mercury\")\nvisibility: str = Field(description=\"The current visibility in miles\")\nwind_speed: str = Field(description=\"The current wind speed in miles per hour\")\ndew_point: str = Field(description=\"The current dew point in degrees Fahrenheit\")\nuv_index: str = Field(description=\"The current UV index\")\nmoon_phase: str = Field(description=\"The current moon phase\")  \ndef scrape_weather_data(app: FirecrawlApp, city: str) -> Optional[WeatherData]:\ntry:\n# Define the actions to search for the city\nactions = [\\\n{\"type\": \"wait\", \"milliseconds\": 3000},\\\n{\"type\": \"click\", \"selector\": 'input[id=\"LocationSearch_input\"]'},\\\n{\"type\": \"write\", \"text\": city},\\\n{\"type\": \"wait\", \"milliseconds\": 1000},\\\n{\"type\": \"click\", \"selector\": \"button[data-testid='ctaButton']\"},\\\n{\"type\": \"wait\", \"milliseconds\": 3000},\\\n]  \n# Perform the scraping\ndata = app.scrape_url(\n\"https://weather.com\",\nparams={\n\"formats\": [\"extract\"],\n\"actions\": actions,\n\"extract\": {\n\"schema\": WeatherData.model_json_schema(),\n\"prompt\": \"Extract the following weather data from the weather.com page: temperature, temperature high, temperature low, humidity, pressure, visibility, wind speed, dew point, UV index, and moon phase\",\n},\n},\n)  \n# Return the extracted weather data\nreturn WeatherData(**data[\"extract\"])  \nexcept Exception as e:\nprint(f\"Error scraping weather data for {city}: {str(e)}\")\nreturn None  \n```\n\nThe code is the same but it is wrapped inside a function. Letâ€™s test it on various cities:\n\n```python\ncities = [\"Tashkent\", \"New York\", \"Tokyo\", \"Paris\", \"Istanbul\"]\ndata_full = []  \nfor city in cities:\nweather_data = scrape_weather_data(app, city)\ndata_full.append(weather_data)  \n```\n\nWe can convert the data for all cities into a DataFrame now:\n\n```python\nimport pandas as pd  \n# Convert list of WeatherData objects into dictionaries\ndata_dicts = [city.model_dump() for city in data_full]  \n# Convert list of dictionaries into DataFrame\ndf = pd.DataFrame(data_dicts)  \nprint(df.head())  \n```\n\n| location | temperature | temperature_high | temperature_low | humidity | pressure | visibility | wind_speed | dew_point | uv_index | moon_phase |\n| --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- |\n| Tashkent, Uzbekistan | 48 | 54 | 41 | 81 | 30.30 | 2.5 | 2 | 43 | 0 | Waning Gibbous |\n| New York City, NY | 48Â° | 49Â° | 39Â° | 93% | 29.45 in | 4 mi | 10 mph | 46Â° | 0 of 11 | Waning Gibbous |\n| Tokyo, Tokyo Prefecture, Japan | 47Â° | 61Â° | 48Â° | 95% | 29.94 in | 10 mi | 1 mph | 45Â° | 0 of 11 | Waning Gibbous |\n| Paris, France | 34Â° | 36Â° | 30Â° | 93% | 29.42 in | 2.4 mi | 11 mph | 33Â° | 0 of 11 | Waning Gibbous |\n| Istanbul, TÃ¼rkiye | 47Â° | 67Â° | 44Â° | 79% | 29.98 in | 8 mi | 4 mph | 41Â° | 0 of 11 | Waning Gibbous |\n\nWe have successfully scraped weather data from multiple cities using Firecrawl and organized it into a structured DataFrame. This demonstrates how we can efficiently collect and analyze data generated by dynamic websites for further analysis and monitoring.\n\n## Conclusion\n\nIn this comprehensive guide, weâ€™ve explored Firecrawlâ€™s `/scrape` endpoint and its powerful capabilities for modern web scraping. We covered:\n\n- Basic scraping setup and configuration options\n- Multiple output formats including HTML, markdown, and screenshots\n- Structured data extraction using both natural language prompts and Pydantic schemas\n- Batch operations for processing multiple URLs efficiently\n- Advanced techniques for scraping JavaScript-heavy dynamic websites\n\nThrough practical examples like extracting news articles from the NY Times and weather data from weather.com, weâ€™ve demonstrated how Firecrawl simplifies complex scraping tasks while providing flexible output formats suitable for data engineering and AI/ML pipelines.\n\nThe combination of LLM-powered extraction, structured schemas, and browser automation capabilities makes Firecrawl a versatile tool for gathering high-quality web data at scale, whether youâ€™re building training datasets, monitoring websites, or conducting research.\n\nTo discover more what Firecrawl has to offer, refer to [our guide on the `/crawl` endpoint](https://www.firecrawl.dev/blog/mastering-the-crawl-endpoint-in-firecrawl), which scrapes websites in their entirety with a single command while using the `/scrape` endpoint under the hood.\n\nFor more hands-on uses-cases of Firecrawl, these posts may interest you as well:\n\n- [Using Prompt Caching With Anthropic](https://www.firecrawl.dev/blog/using-prompt-caching-with-anthropic)\n- [Scraping Job Boards With Firecrawl and OpenAI](https://www.firecrawl.dev/blog/scrape-job-boards-firecrawl-openai)\n- [Scraping and Analyzing Airbnb Listings in Python Tutorial](https://www.firecrawl.dev/blog/scrape-analyze-airbnb-data-with-e2b)\n\nArticle updated recently\n\n[ðŸ”¥](https://www.firecrawl.dev/)\n\n## Ready to _Build?_\n\nStart scraping web data for your AI apps today.\n\nNo credit card needed.\n\nGet Started\n\n## About the Author\n\n[![Bex Tuychiev image](https://www.firecrawl.dev/bex.jpg)\\\nBex Tuychiev@bextuychiev](https://x.com/bextuychiev)\n\nBex is a Top 10 AI writer on Medium and a Kaggle Master with over 15k followers. He loves writing detailed guides, tutorials, and notebooks on complex data science and machine learning topics\n\n### More articles by Bex Tuychiev\n\n[Building an Automated Price Tracking Tool\\\n\\\nBuild an automated e-commerce price tracker in Python. Learn web scraping, price monitoring, and automated alerts using Firecrawl, Streamlit, PostgreSQL.](https://www.firecrawl.dev/blog/automated-price-tracking-tutorial-python) [Web Scraping Automation: How to Run Scrapers on a Schedule\\\n\\\nLearn how to automate web scraping in Python using free tools like schedule, asyncio, cron jobs and GitHub Actions. This comprehensive guide covers local and cloud-based scheduling methods to run scrapers reliably in 2025.](https://www.firecrawl.dev/blog/automated-web-scraping-free-2025) [Automated Data Collection - A Comprehensive Guide\\\n\\\nLearn how to build robust automated data collection systems using modern tools and best practices. This guide covers everything from selecting the right tools to implementing scalable collection pipelines.](https://www.firecrawl.dev/blog/automated-data-collection-guide) [BeautifulSoup4 vs. Scrapy - A Comprehensive Comparison for Web Scraping in Python\\\n\\\nLearn the key differences between BeautifulSoup4 and Scrapy for web scraping in Python. Compare their features, performance, and use cases to choose the right tool for your web scraping needs.](https://www.firecrawl.dev/blog/beautifulsoup4-vs-scrapy-comparison) [How to Build an Automated Competitor Price Monitoring System with Python\\\n\\\nLearn how to build an automated competitor price monitoring system in Python that tracks prices across e-commerce sites, provides real-time comparisons, and maintains price history using Firecrawl, Streamlit, and GitHub Actions.](https://www.firecrawl.dev/blog/automated-competitor-price-scraping) [Scraping Company Data and Funding Information in Bulk With Firecrawl and Claude\\\n\\\nLearn how to build a web scraper in Python that gathers company details, funding rounds, and investor information from public sources like Crunchbase using Firecrawl and Claude for automated data collection and analysis.](https://www.firecrawl.dev/blog/crunchbase-scraping-with-firecrawl-claude) [Data Enrichment: A Complete Guide to Enhancing Your Data Quality\\\n\\\nLearn how to enrich your data quality with a comprehensive guide covering data enrichment tools, best practices, and real-world examples. Discover how to leverage modern solutions like Firecrawl to automate data collection, validation, and integration for better business insights.](https://www.firecrawl.dev/blog/complete-guide-to-data-enrichment) [Building an Intelligent Code Documentation RAG Assistant with DeepSeek and Firecrawl\\\n\\\nLearn how to build an intelligent documentation assistant powered by DeepSeek and RAG (Retrieval Augmented Generation) that can answer questions about any documentation website by combining language models with efficient information retrieval.](https://www.firecrawl.dev/blog/deepseek-rag-documentation-assistant)",
      "metadata": {
        "title": "How to Use Firecrawl's Scrape API: Complete Web Scraping Tutorial",
        "url": "https://www.firecrawl.dev/blog/mastering-firecrawl-scrape-endpoint"
      }
    },
    {
      "id": "097f7c07-c8dd-46cc-92f0-74671696b58b",
      "source": "firecrawl/blog/firecrawl-june-2024-updates.md",
      "content": "---\ntitle: Firecrawl June 2024 Updates\nurl: https://www.firecrawl.dev/blog/firecrawl-june-2024-updates\n---  \nIntroducing /extract - Get web data with a prompt [Try now](https://www.firecrawl.dev/extract)  \nJune 30, 2024  \nâ€¢  \n[![Nicolas Camara image](https://www.firecrawl.dev/nick-img.jpeg)Nicolas Camara](https://x.com/nickscamara_)  \n# Firecrawl June 2024 Updates  \n![Firecrawl June 2024 Updates image](https://www.firecrawl.dev/images/blog/dashboard2.png)  \nWe are excited to share our latest updates from June!  \n**TLDR:**  \n- New Integrations live with Gamma, Dify, Praison, and Flowise\n- Firecrawl Dashboard Launched\n- New tutorials and examples out\n- Platform Improvements",
      "metadata": {
        "title": "Firecrawl June 2024 Updates",
        "url": "https://www.firecrawl.dev/blog/firecrawl-june-2024-updates"
      }
    },
    {
      "id": "7bb43a77-bdca-4594-be11-866397c80f8d",
      "source": "firecrawl/blog/firecrawl-june-2024-updates.md",
      "content": "Integrations\n\nWeâ€™ve been busy collaborating with some amazing partners to bring the power of Firecrawl to more platforms and users. Here are all of our current integrations:  \n![Firecrawl integrations](https://www.firecrawl.dev/images/blog/integrations.png)  \n**New Integrations:**  \n- **Gammaâ€™s new Import from URL feature**: With this integration, users can generate entire presentations from any website in under a minute âœ¨\n- **Firecrawl now integrates with Dify.ai**: Our collaboration with this leading open-source LLM development platform is now live on their open-source repo and Dify cloud.\n- **Integration with FlowiseAI**: Flowise users can now easily add clean web data to their drag-and-drop LLM workflows.",
      "metadata": {
        "title": "Firecrawl June 2024 Updates",
        "url": "https://www.firecrawl.dev/blog/firecrawl-june-2024-updates"
      }
    },
    {
      "id": "0f29cdd2-243c-47f2-b58e-feb3fd3c4f36",
      "source": "firecrawl/blog/firecrawl-june-2024-updates.md",
      "content": "New Dashboard Launched\n\nWe are excited to launch our brand new dashboard ðŸ”¥  \n![Firecrawl dashboard](https://www.firecrawl.dev/images/blog/dashboard.png)  \nInside you can do things like:  \n- View and download data from jobs\n- See breakdowns and analytics of usage\n- Easily access the playground for testing  \nTry it out today by logging in and clicking Dashboard!",
      "metadata": {
        "title": "Firecrawl June 2024 Updates",
        "url": "https://www.firecrawl.dev/blog/firecrawl-june-2024-updates"
      }
    },
    {
      "id": "5f033b64-168a-4943-bb1c-fc8cdb499267",
      "source": "firecrawl/blog/firecrawl-june-2024-updates.md",
      "content": "Platform Improvements\n\nWeâ€™re constantly working to enhance Firecrawlâ€™s capabilities and performance.  \n![Firecrawl dashboard](https://www.firecrawl.dev/images/blog/platform.jpg)  \nHere are some notable improvements weâ€™ve made:  \n- **Authenticated Web Scraping**: One of our most requested features is now live! With the new headers option, you can forward authentication cookies to scrape data behind auth walls. No more barriers to the data you need.\n- **Improved Metadata for Scraped Pages**: Weâ€™ve further enriched the metadata returned for each scraped page.\n- **SDK Support for Local Instances**: You can now use Firecrawlâ€™s Python and Node SDKs locally.  \nPlus way more, follow us at [@firecrawl_dev](https://x.com/firecrawl_dev) to see everything we launch!",
      "metadata": {
        "title": "Firecrawl June 2024 Updates",
        "url": "https://www.firecrawl.dev/blog/firecrawl-june-2024-updates"
      }
    },
    {
      "id": "b6fa9337-550d-4dd4-9663-698b6aee511a",
      "source": "firecrawl/blog/firecrawl-june-2024-updates.md",
      "content": "Fresh Tutorials and Examples\n\nWant to see Firecrawl in action? Check out our examples!  \nWeâ€™ve released these new tutorials and examples:  \n- Chat with websites locally powered by Firecrawl and Ollama\n- Tutorial with Weaviate demoing Generative Feedback Loops\n- Cluster common topics from any web page with Firecrawl and E2B  \nThatâ€™s all for this update! Stay tuned for the next one ðŸš€  \nArticle updated recently  \n[ðŸ”¥](https://www.firecrawl.dev/)",
      "metadata": {
        "title": "Firecrawl June 2024 Updates",
        "url": "https://www.firecrawl.dev/blog/firecrawl-june-2024-updates"
      }
    },
    {
      "id": "ec4b3b4a-f136-474e-8b27-5469b364231a",
      "source": "firecrawl/blog/firecrawl-june-2024-updates.md",
      "content": "Ready to _Build?_\n\nStart scraping web data for your AI apps today.  \nNo credit card needed.  \nGet Started",
      "metadata": {
        "title": "Firecrawl June 2024 Updates",
        "url": "https://www.firecrawl.dev/blog/firecrawl-june-2024-updates"
      }
    },
    {
      "id": "376ea389-a19b-4eeb-bfea-9a4736c422bc",
      "source": "firecrawl/blog/firecrawl-june-2024-updates.md",
      "content": "About the Author\n\n[![Nicolas Camara image](https://www.firecrawl.dev/nick-img.jpeg)\\\nNicolas Camara@nickscamara_](https://x.com/nickscamara_)  \nNicolas Camara is the Chief Technology Officer (CTO) at Firecrawl.\nHe previously built and scaled Mendable, one of the pioneering \"chat with your documents\" apps,\nwhich had major Fortune 500 customers like Snapchat, Coinbase, and MongoDB.\nPrior to that, Nicolas built SideGuide, the first code-learning tool inside VS Code,\nand grew a community of 50,000 users. Nicolas studied Computer Science and has over 10 years of experience in building software.",
      "metadata": {
        "title": "Firecrawl June 2024 Updates",
        "url": "https://www.firecrawl.dev/blog/firecrawl-june-2024-updates"
      }
    },
    {
      "id": "13607030-c69b-40ce-ba5d-4cd602cccfb6",
      "source": "firecrawl/blog/firecrawl-june-2024-updates.md",
      "content": "About the Author > More articles by Nicolas Camara\n\n[Using OpenAI's Realtime API and Firecrawl to Talk with Any Website\\\n\\\nBuild a real-time conversational agent that interacts with any website using OpenAI's Realtime API and Firecrawl.](https://www.firecrawl.dev/blog/How-to-Talk-with-Any-Website-Using-OpenAIs-Realtime-API-and-Firecrawl) [Extract website data using LLMs\\\n\\\nLearn how to use Firecrawl and Groq to extract structured data from a web page in a few lines of code.](https://www.firecrawl.dev/blog/data-extraction-using-llms) [Getting Started with Grok-2: Setup and Web Crawler Example\\\n\\\nA detailed guide on setting up Grok-2 and building a web crawler using Firecrawl.](https://www.firecrawl.dev/blog/grok-2-setup-and-web-crawler-example) [Launch Week I / Day 6: LLM Extract (v1)\\\n\\\nExtract structured data from your web pages using the extract format in /scrape.](https://www.firecrawl.dev/blog/launch-week-i-day-6-llm-extract) [Launch Week I / Day 7: Crawl Webhooks (v1)\\\n\\\nNew /crawl webhook support. Send notifications to your apps during a crawl.](https://www.firecrawl.dev/blog/launch-week-i-day-7-webhooks) [OpenAI Swarm Tutorial: Create Marketing Campaigns for Any Website\\\n\\\nA guide to building a multi-agent system using OpenAI Swarm and Firecrawl for AI-driven marketing strategies](https://www.firecrawl.dev/blog/openai-swarm-agent-tutorial) [Build a 'Chat with website' using Groq Llama 3\\\n\\\nLearn how to use Firecrawl, Groq Llama 3, and Langchain to build a 'Chat with your website' bot.](https://www.firecrawl.dev/blog/chat-with-website) [Scrape and Analyze Airbnb Data with Firecrawl and E2B\\\n\\\nLearn how to scrape and analyze Airbnb data using Firecrawl and E2B in a few lines of code.](https://www.firecrawl.dev/blog/scrape-analyze-airbnb-data-with-e2b)",
      "metadata": {
        "title": "Firecrawl June 2024 Updates",
        "url": "https://www.firecrawl.dev/blog/firecrawl-june-2024-updates"
      }
    },
    {
      "id": "2425e1a3-c424-4118-82f0-e921be7e05a6",
      "source": "firecrawl/blog/How-to-Talk-with-Any-Website-Using-OpenAIs-Realtime-API-and-Firecrawl.md",
      "content": "---\ntitle: Using OpenAI's Realtime API and Firecrawl to Talk with Any Website\nurl: https://www.firecrawl.dev/blog/How-to-Talk-with-Any-Website-Using-OpenAIs-Realtime-API-and-Firecrawl\n---  \nIntroducing /extract - Get web data with a prompt [Try now](https://www.firecrawl.dev/extract)  \nOct 11, 2024  \nâ€¢  \n[![Nicolas Camara image](https://www.firecrawl.dev/nick-img.jpeg)Nicolas Camara](https://x.com/nickscamara_)  \n# Using OpenAI's Realtime API and Firecrawl to Talk with Any Website  \nUsing OpenAI's Realtime API and Firecrawl to Talk with Any Website ðŸ”¥ - YouTube  \nFirecrawl  \n472 subscribers  \n[Using OpenAI's Realtime API and Firecrawl to Talk with Any Website ðŸ”¥](https://www.youtube.com/watch?v=3aFH12yjjcA)  \nFirecrawl  \nSearch  \nInfo  \nShopping  \nTap to unmute  \nIf playback doesn't begin shortly, try restarting your device.  \nShare  \nInclude playlist  \nAn error occurred while retrieving sharing information. Please try again later.  \nWatch later  \nShare  \nCopy link  \n0:00  \n/ â€¢Live  \nâ€¢  \n[Watch on YouTube](https://www.youtube.com/watch?v=3aFH12yjjcA \"Watch on YouTube\")  \nInteracting with any website through a conversational agent in real time is now possible thanks to OpenAIâ€™s new Realtime API and Firecrawl. This powerful combination allows developers to build low-latency, multi-modal conversational experiences that can fetch and interact with live web content on the fly.  \nIn this tutorial, weâ€™ll guide you through the process of integrating Firecrawlâ€™s scraping and mapping tools into the OpenAI Realtime API Console Demo. By the end, youâ€™ll have a real-time conversational agent capable of talking with any website.",
      "metadata": {
        "title": "Using OpenAI's Realtime API and Firecrawl to Talk with Any Website",
        "url": "https://www.firecrawl.dev/blog/How-to-Talk-with-Any-Website-Using-OpenAIs-Realtime-API-and-Firecrawl"
      }
    },
    {
      "id": "d7ffc03a-f557-4450-a7f4-051d4f660446",
      "source": "firecrawl/blog/How-to-Talk-with-Any-Website-Using-OpenAIs-Realtime-API-and-Firecrawl.md",
      "content": "Prerequisites\n\nBefore you begin, make sure you have the following:  \n- **Node.js and npm** installed on your machine.\n- An **OpenAI API key** with access to the Realtime API.\n- A **Firecrawl API key**.\n- Basic understanding of **React** and **TypeScript**.",
      "metadata": {
        "title": "Using OpenAI's Realtime API and Firecrawl to Talk with Any Website",
        "url": "https://www.firecrawl.dev/blog/How-to-Talk-with-Any-Website-Using-OpenAIs-Realtime-API-and-Firecrawl"
      }
    },
    {
      "id": "7fea3859-810d-4139-9018-629cdaf2b94a",
      "source": "firecrawl/blog/How-to-Talk-with-Any-Website-Using-OpenAIs-Realtime-API-and-Firecrawl.md",
      "content": "Step 1: Clone the OpenAI Realtime API Console Demo\n\nFirst, clone the repository that contains the OpenAI Realtime API Console Demo integrated with Firecrawl.  \n```bash\ngit clone https://github.com/nickscamara/firecrawl-openai-realtime.git\ncd firecrawl-openai-realtime\n\n```",
      "metadata": {
        "title": "Using OpenAI's Realtime API and Firecrawl to Talk with Any Website",
        "url": "https://www.firecrawl.dev/blog/How-to-Talk-with-Any-Website-Using-OpenAIs-Realtime-API-and-Firecrawl"
      }
    },
    {
      "id": "01ebed6f-4102-4127-94e3-a8b149dd7a27",
      "source": "firecrawl/blog/How-to-Talk-with-Any-Website-Using-OpenAIs-Realtime-API-and-Firecrawl.md",
      "content": "Step 3: Set Up Environment Variables\n\nCreate a `.env` file in the root directory and add your OpenAI and Firecrawl API keys:  \n```env\nOPENAI_API_KEY=your-openai-api-key\nFIRECRAWL_API_KEY=your-firecrawl-api-key\n\n```  \nIf youâ€™re running a local relay server, set the relay server URL:  \n```env\nREACT_APP_LOCAL_RELAY_SERVER_URL=http://localhost:8081\n\n```",
      "metadata": {
        "title": "Using OpenAI's Realtime API and Firecrawl to Talk with Any Website",
        "url": "https://www.firecrawl.dev/blog/How-to-Talk-with-Any-Website-Using-OpenAIs-Realtime-API-and-Firecrawl"
      }
    },
    {
      "id": "e7b6bf62-b266-4d99-8763-883a8e721685",
      "source": "firecrawl/blog/How-to-Talk-with-Any-Website-Using-OpenAIs-Realtime-API-and-Firecrawl.md",
      "content": "Step 4: Integrate Firecrawl Tools into the Realtime API Console Demo\n\nOpen the `ConsolePage.tsx` file located at `src/pages/ConsolePage.tsx`.",
      "metadata": {
        "title": "Using OpenAI's Realtime API and Firecrawl to Talk with Any Website",
        "url": "https://www.firecrawl.dev/blog/How-to-Talk-with-Any-Website-Using-OpenAIs-Realtime-API-and-Firecrawl"
      }
    },
    {
      "id": "18d1b729-b47f-465d-a502-5d34dcedd4b8",
      "source": "firecrawl/blog/How-to-Talk-with-Any-Website-Using-OpenAIs-Realtime-API-and-Firecrawl.md",
      "content": "Step 4: Integrate Firecrawl Tools into the Realtime API Console Demo > Import Firecrawl\n\nAt the top of the file, import the Firecrawl SDK:  \n```typescript\nimport FirecrawlApp from \"@mendable/firecrawl-js\";\n\n```",
      "metadata": {
        "title": "Using OpenAI's Realtime API and Firecrawl to Talk with Any Website",
        "url": "https://www.firecrawl.dev/blog/How-to-Talk-with-Any-Website-Using-OpenAIs-Realtime-API-and-Firecrawl"
      }
    },
    {
      "id": "b18d0dfa-84b0-459c-8d5d-e157e72d70c5",
      "source": "firecrawl/blog/How-to-Talk-with-Any-Website-Using-OpenAIs-Realtime-API-and-Firecrawl.md",
      "content": "Step 4: Integrate Firecrawl Tools into the Realtime API Console Demo > Add the â€˜scrape_dataâ€™ Tool\n\nWithin the `useEffect` hook where tools are added to the client, add the `scrape_data` tool:  \n```typescript\nclient.addTool(\n{\nname: \"scrape_data\",\ndescription: \"Goes to or scrapes data from a given URL using Firecrawl.\",\nparameters: {\ntype: \"object\",\nproperties: {\nurl: {\ntype: \"string\",\ndescription: \"URL to scrape data from\",\n},\n},\nrequired: [\"url\"],\n},\n},\nasync ({ url }: { url: string }) => {\nconst firecrawl = new FirecrawlApp({\napiKey: process.env.FIRECRAWL_API_KEY || \"\",\n});\nconst data = await firecrawl.scrapeUrl(url, {\nformats: [\"markdown\", \"screenshot\"],\n});\nif (!data.success) {\nreturn \"Failed to scrape data from the given URL.\";\n}\nsetScreenshot(data.screenshot || \"\");\nreturn data.markdown;\n},\n);\n\n```  \nThis tool allows the assistant to scrape data from any URL using Firecrawl.",
      "metadata": {
        "title": "Using OpenAI's Realtime API and Firecrawl to Talk with Any Website",
        "url": "https://www.firecrawl.dev/blog/How-to-Talk-with-Any-Website-Using-OpenAIs-Realtime-API-and-Firecrawl"
      }
    },
    {
      "id": "23eef668-1a16-4371-a6d6-6e1c99eca748",
      "source": "firecrawl/blog/How-to-Talk-with-Any-Website-Using-OpenAIs-Realtime-API-and-Firecrawl.md",
      "content": "Step 4: Integrate Firecrawl Tools into the Realtime API Console Demo > Add the â€˜map_websiteâ€™ Tool\n\nNext, add the `map_website` tool to enable searching for pages with specific keywords on a website:  \n```typescript\nclient.addTool(\n{\nname: \"map_website\",\ndescription:\n\"Searches a website for pages containing specific keywords using Firecrawl.\",\nparameters: {\ntype: \"object\",\nproperties: {\nurl: {\ntype: \"string\",\ndescription: \"URL of the website to search\",\n},\nsearch: {\ntype: \"string\",\ndescription: \"Keywords to search for (2-3 max)\",\n},\n},\nrequired: [\"url\", \"search\"],\n},\n},\nasync ({ url, search }: { url: string; search: string }) => {\nconst firecrawl = new FirecrawlApp({\napiKey: process.env.FIRECRAWL_API_KEY || \"\",\n});\nconst mapData = await firecrawl.mapUrl(url, { search });\nif (!mapData.success || !mapData.links?.length) {\nreturn \"No pages found with the specified keywords.\";\n}\nconst topLink = mapData.links[0];\nconst scrapeData = await firecrawl.scrapeUrl(topLink, {\nformats: [\"markdown\", \"screenshot\"],\n});\nif (!scrapeData.success) {\nreturn \"Failed to retrieve data from the found page.\";\n}\nsetScreenshot(scrapeData.screenshot || \"\");\nreturn scrapeData.markdown;\n},\n);\n\n```  \nThis tool allows the assistant to search a website for specific content and retrieve it.",
      "metadata": {
        "title": "Using OpenAI's Realtime API and Firecrawl to Talk with Any Website",
        "url": "https://www.firecrawl.dev/blog/How-to-Talk-with-Any-Website-Using-OpenAIs-Realtime-API-and-Firecrawl"
      }
    },
    {
      "id": "aaa6537b-3706-44c5-a2ca-111791027a0b",
      "source": "firecrawl/blog/How-to-Talk-with-Any-Website-Using-OpenAIs-Realtime-API-and-Firecrawl.md",
      "content": "Step 4: Integrate Firecrawl Tools into the Realtime API Console Demo > Manage Screenshot State\n\nAt the top of your `ConsolePage` component, add state management for the screenshot:  \n```typescript\nconst [screenshot, setScreenshot] = useState<string>(\"\");\n\n```",
      "metadata": {
        "title": "Using OpenAI's Realtime API and Firecrawl to Talk with Any Website",
        "url": "https://www.firecrawl.dev/blog/How-to-Talk-with-Any-Website-Using-OpenAIs-Realtime-API-and-Firecrawl"
      }
    },
    {
      "id": "56337b60-cca2-447a-a56c-d4f2980d94c0",
      "source": "firecrawl/blog/How-to-Talk-with-Any-Website-Using-OpenAIs-Realtime-API-and-Firecrawl.md",
      "content": "Step 4: Integrate Firecrawl Tools into the Realtime API Console Demo > Display the Screenshot in the UI\n\nIn the UI, display the screenshot by adding the following within the appropriate JSX:  \n```jsx\n{\nscreenshot && <img src={screenshot} alt=\"Website Screenshot\" />;\n}\n\n```",
      "metadata": {
        "title": "Using OpenAI's Realtime API and Firecrawl to Talk with Any Website",
        "url": "https://www.firecrawl.dev/blog/How-to-Talk-with-Any-Website-Using-OpenAIs-Realtime-API-and-Firecrawl"
      }
    },
    {
      "id": "973d7fee-dc57-4856-b9c7-2e1516055f1f",
      "source": "firecrawl/blog/How-to-Talk-with-Any-Website-Using-OpenAIs-Realtime-API-and-Firecrawl.md",
      "content": "Testing the Agent\n\nNow, you can test your agent by initiating a conversation. For example, ask:  \n**User**: â€œCan you get the latest blog post from [https://mendable.ai](https://mendable.ai/)?â€  \nThe assistant will use the `scrape_data` tool to fetch content from the specified URL and present it to you.",
      "metadata": {
        "title": "Using OpenAI's Realtime API and Firecrawl to Talk with Any Website",
        "url": "https://www.firecrawl.dev/blog/How-to-Talk-with-Any-Website-Using-OpenAIs-Realtime-API-and-Firecrawl"
      }
    },
    {
      "id": "8a896c15-f78d-41cd-992a-5d6fb5a29dfd",
      "source": "firecrawl/blog/How-to-Talk-with-Any-Website-Using-OpenAIs-Realtime-API-and-Firecrawl.md",
      "content": "Conclusion\n\nBy integrating Firecrawlâ€™s scraping and mapping tools into the OpenAI Realtime API Console Demo, youâ€™ve created a powerful conversational agent capable of interacting with any website in real time. This setup opens up endless possibilities for building advanced AI applications that can access and process live web content on demand.",
      "metadata": {
        "title": "Using OpenAI's Realtime API and Firecrawl to Talk with Any Website",
        "url": "https://www.firecrawl.dev/blog/How-to-Talk-with-Any-Website-Using-OpenAIs-Realtime-API-and-Firecrawl"
      }
    },
    {
      "id": "79174b18-e43a-47b5-b8bb-f5fe459fe826",
      "source": "firecrawl/blog/How-to-Talk-with-Any-Website-Using-OpenAIs-Realtime-API-and-Firecrawl.md",
      "content": "References\n\n- [OpenAI Realtime API Documentation](https://platform.openai.com/docs/guides/realtime)\n- [Firecrawl Node SDK](https://docs.firecrawl.dev/sdks/node)\n- [OpenAI Realtime API Console Demo with Firecrawl](https://github.com/nickscamara/firecrawl-openai-realtime)\n- [Firecrawl Official Website](https://www.firecrawl.dev/)  \nArticle updated recently  \n[ðŸ”¥](https://www.firecrawl.dev/)",
      "metadata": {
        "title": "Using OpenAI's Realtime API and Firecrawl to Talk with Any Website",
        "url": "https://www.firecrawl.dev/blog/How-to-Talk-with-Any-Website-Using-OpenAIs-Realtime-API-and-Firecrawl"
      }
    },
    {
      "id": "d6243639-c3ff-43a7-a197-9f040e9431a1",
      "source": "firecrawl/blog/How-to-Talk-with-Any-Website-Using-OpenAIs-Realtime-API-and-Firecrawl.md",
      "content": "Ready to _Build?_\n\nStart scraping web data for your AI apps today.  \nNo credit card needed.  \nGet Started",
      "metadata": {
        "title": "Using OpenAI's Realtime API and Firecrawl to Talk with Any Website",
        "url": "https://www.firecrawl.dev/blog/How-to-Talk-with-Any-Website-Using-OpenAIs-Realtime-API-and-Firecrawl"
      }
    },
    {
      "id": "061cdf1d-7fc9-4b11-91d2-9a1261bbac7e",
      "source": "firecrawl/blog/How-to-Talk-with-Any-Website-Using-OpenAIs-Realtime-API-and-Firecrawl.md",
      "content": "About the Author\n\n[![Nicolas Camara image](https://www.firecrawl.dev/nick-img.jpeg)\\\nNicolas Camara@nickscamara_](https://x.com/nickscamara_)  \nNicolas Camara is the Chief Technology Officer (CTO) at Firecrawl.\nHe previously built and scaled Mendable, one of the pioneering \"chat with your documents\" apps,\nwhich had major Fortune 500 customers like Snapchat, Coinbase, and MongoDB.\nPrior to that, Nicolas built SideGuide, the first code-learning tool inside VS Code,\nand grew a community of 50,000 users. Nicolas studied Computer Science and has over 10 years of experience in building software.",
      "metadata": {
        "title": "Using OpenAI's Realtime API and Firecrawl to Talk with Any Website",
        "url": "https://www.firecrawl.dev/blog/How-to-Talk-with-Any-Website-Using-OpenAIs-Realtime-API-and-Firecrawl"
      }
    },
    {
      "id": "8c1a9833-d534-4c4d-a83b-3742a01c5c34",
      "source": "firecrawl/blog/How-to-Talk-with-Any-Website-Using-OpenAIs-Realtime-API-and-Firecrawl.md",
      "content": "About the Author > More articles by Nicolas Camara\n\n[Using OpenAI's Realtime API and Firecrawl to Talk with Any Website\\\n\\\nBuild a real-time conversational agent that interacts with any website using OpenAI's Realtime API and Firecrawl.](https://www.firecrawl.dev/blog/How-to-Talk-with-Any-Website-Using-OpenAIs-Realtime-API-and-Firecrawl) [Extract website data using LLMs\\\n\\\nLearn how to use Firecrawl and Groq to extract structured data from a web page in a few lines of code.](https://www.firecrawl.dev/blog/data-extraction-using-llms) [Getting Started with Grok-2: Setup and Web Crawler Example\\\n\\\nA detailed guide on setting up Grok-2 and building a web crawler using Firecrawl.](https://www.firecrawl.dev/blog/grok-2-setup-and-web-crawler-example) [Launch Week I / Day 6: LLM Extract (v1)\\\n\\\nExtract structured data from your web pages using the extract format in /scrape.](https://www.firecrawl.dev/blog/launch-week-i-day-6-llm-extract) [Launch Week I / Day 7: Crawl Webhooks (v1)\\\n\\\nNew /crawl webhook support. Send notifications to your apps during a crawl.](https://www.firecrawl.dev/blog/launch-week-i-day-7-webhooks) [OpenAI Swarm Tutorial: Create Marketing Campaigns for Any Website\\\n\\\nA guide to building a multi-agent system using OpenAI Swarm and Firecrawl for AI-driven marketing strategies](https://www.firecrawl.dev/blog/openai-swarm-agent-tutorial) [Build a 'Chat with website' using Groq Llama 3\\\n\\\nLearn how to use Firecrawl, Groq Llama 3, and Langchain to build a 'Chat with your website' bot.](https://www.firecrawl.dev/blog/chat-with-website) [Scrape and Analyze Airbnb Data with Firecrawl and E2B\\\n\\\nLearn how to scrape and analyze Airbnb data using Firecrawl and E2B in a few lines of code.](https://www.firecrawl.dev/blog/scrape-analyze-airbnb-data-with-e2b)",
      "metadata": {
        "title": "Using OpenAI's Realtime API and Firecrawl to Talk with Any Website",
        "url": "https://www.firecrawl.dev/blog/How-to-Talk-with-Any-Website-Using-OpenAIs-Realtime-API-and-Firecrawl"
      }
    },
    {
      "id": "c64b5d7a-1624-4bbd-bc36-0547573e81c6",
      "source": "firecrawl/blog/getting-started-with-predicted-outputs-openai.md",
      "content": "---\ntitle: Getting Started with OpenAI's Predicted Outputs for Faster LLM Responses\nurl: https://www.firecrawl.dev/blog/getting-started-with-predicted-outputs-openai\n---  \nIntroducing /extract - Get web data with a prompt [Try now](https://www.firecrawl.dev/extract)  \nNov 5, 2024  \nâ€¢  \n[![Eric Ciarla image](https://www.firecrawl.dev/eric-img.jpeg)Eric Ciarla](https://x.com/ericciarla)  \n# Getting Started with OpenAI's Predicted Outputs for Faster LLM Responses  \n![Getting Started with OpenAI's Predicted Outputs for Faster LLM Responses image](https://www.firecrawl.dev/images/blog/openai-predicted-outputs.jpg)  \nLeveraging the full potential of Large Language Models (LLMs) often involves balancing between response accuracy and latency. OpenAIâ€™s new Predicted Outputs feature introduces a way to significantly reduce response times by informing the model about the expected output in advance.  \nIn this article, weâ€™ll explore how to use Predicted Outputs with the GPT-4o and GPT-4o-mini models to make your AI applications super fast ðŸš€. Weâ€™ll also provide a practical example of transforming blog posts into SEO-optimized content, a powerful use case enabled by this feature.",
      "metadata": {
        "title": "Getting Started with OpenAI's Predicted Outputs for Faster LLM Responses",
        "url": "https://www.firecrawl.dev/blog/getting-started-with-predicted-outputs-openai"
      }
    },
    {
      "id": "c430fe04-5ab7-4869-9989-1c83cb8883ed",
      "source": "firecrawl/blog/getting-started-with-predicted-outputs-openai.md",
      "content": "Limitations to Keep in Mind\n\nWhile Predicted Outputs are powerful, there are some limitations:  \n- Supported only with **GPT-4o** and **GPT-4o-mini** models.\n- Certain API parameters are not supported, such as `n` values greater than 1, `logprobs`, `presence_penalty` greater than 0, among others.",
      "metadata": {
        "title": "Getting Started with OpenAI's Predicted Outputs for Faster LLM Responses",
        "url": "https://www.firecrawl.dev/blog/getting-started-with-predicted-outputs-openai"
      }
    },
    {
      "id": "a9c84935-0695-4ee1-bcab-4e9dcc1df3b5",
      "source": "firecrawl/blog/getting-started-with-predicted-outputs-openai.md",
      "content": "How to Use Predicted Outputs\n\nLetâ€™s dive into how you can implement Predicted Outputs in your application. Weâ€™ll walk through an example where we optimize a blog post by adding internal links to relevant pages within the same website.",
      "metadata": {
        "title": "Getting Started with OpenAI's Predicted Outputs for Faster LLM Responses",
        "url": "https://www.firecrawl.dev/blog/getting-started-with-predicted-outputs-openai"
      }
    },
    {
      "id": "a9b5a10c-17ce-4513-bedd-b0a779431be0",
      "source": "firecrawl/blog/getting-started-with-predicted-outputs-openai.md",
      "content": "How to Use Predicted Outputs > Prerequisites\n\nMake sure you have the following installed:  \n```bash\npip install firecrawl-py openai\n\n```",
      "metadata": {
        "title": "Getting Started with OpenAI's Predicted Outputs for Faster LLM Responses",
        "url": "https://www.firecrawl.dev/blog/getting-started-with-predicted-outputs-openai"
      }
    },
    {
      "id": "a3742be1-da93-4cdd-9466-0d84eeccae4b",
      "source": "firecrawl/blog/getting-started-with-predicted-outputs-openai.md",
      "content": "How to Use Predicted Outputs > Step 1: Set Up Your Environment\n\nInitialize the necessary libraries and load your API keys.  \n```python\nimport os\nimport json\nfrom firecrawl import FirecrawlApp\nfrom dotenv import load_dotenv\nfrom openai import OpenAI\n\n# Load environment variables\nload_dotenv()\n\n# Retrieve API keys from environment variables\nfirecrawl_api_key = os.getenv(\"FIRECRAWL_API_KEY\")\nopenai_api_key = os.getenv(\"OPENAI_API_KEY\")\n\n# Initialize the FirecrawlApp and OpenAI client\napp = FirecrawlApp(api_key=firecrawl_api_key)\nclient = OpenAI(api_key=openai_api_key)\n\n```",
      "metadata": {
        "title": "Getting Started with OpenAI's Predicted Outputs for Faster LLM Responses",
        "url": "https://www.firecrawl.dev/blog/getting-started-with-predicted-outputs-openai"
      }
    },
    {
      "id": "6a8ab9bd-4f94-43b9-b56a-21043e108b8e",
      "source": "firecrawl/blog/getting-started-with-predicted-outputs-openai.md",
      "content": "How to Use Predicted Outputs > Step 2: Scrape the Blog Content\n\nWeâ€™ll start by scraping the content of a blog post that we want to optimize.  \n```python\n# Get the blog URL (you can input your own)\nblog_url = \"https://www.firecrawl.dev/blog/how-to-use-openai-o1-reasoning-models-in-applications\"\n\n# Scrape the blog content in markdown format\nblog_scrape_result = app.scrape_url(blog_url, params={'formats': ['markdown']})\nblog_content = blog_scrape_result.get('markdown', '')\n\n```",
      "metadata": {
        "title": "Getting Started with OpenAI's Predicted Outputs for Faster LLM Responses",
        "url": "https://www.firecrawl.dev/blog/getting-started-with-predicted-outputs-openai"
      }
    },
    {
      "id": "1aeff8dc-ff35-41e6-9aaa-15d8b798de0f",
      "source": "firecrawl/blog/getting-started-with-predicted-outputs-openai.md",
      "content": "How to Use Predicted Outputs > Step 3: Map the Website for Internal Links\n\nNext, weâ€™ll get a list of other pages on the website to which we can add internal links.  \n```python\n# Extract the top-level domain\ntop_level_domain = '/'.join(blog_url.split('/')[:3])\n\n# Map the website to get all internal links\nsite_map = app.map_url(top_level_domain)\nsite_links = site_map.get('links', [])\n\n```",
      "metadata": {
        "title": "Getting Started with OpenAI's Predicted Outputs for Faster LLM Responses",
        "url": "https://www.firecrawl.dev/blog/getting-started-with-predicted-outputs-openai"
      }
    },
    {
      "id": "077f4a41-dc46-4a56-982b-89022d8506a8",
      "source": "firecrawl/blog/getting-started-with-predicted-outputs-openai.md",
      "content": "How to Use Predicted Outputs > Step 4: Prepare the Prompt and Prediction\n\nWeâ€™ll create a prompt instructing the model to add internal links to the blog post and provide the original content as a prediction.  \n```python\nprompt = f\"\"\"\nYou are an AI assistant helping to improve a blog post.\n\nHere is the original blog post content:\n\n{blog_content}\n\nHere is a list of other pages on the website:\n\n{json.dumps(site_links, indent=2)}\n\nPlease revise the blog post to include internal links to some of these pages where appropriate. Make sure the internal links are relevant and enhance the content.\n\nOnly return the revised blog post in markdown format.\n\"\"\"\n\n```",
      "metadata": {
        "title": "Getting Started with OpenAI's Predicted Outputs for Faster LLM Responses",
        "url": "https://www.firecrawl.dev/blog/getting-started-with-predicted-outputs-openai"
      }
    },
    {
      "id": "e44bbaf8-6d1e-4404-bf9c-d55385b86ad9",
      "source": "firecrawl/blog/getting-started-with-predicted-outputs-openai.md",
      "content": "How to Use Predicted Outputs > Step 5: Use Predicted Outputs with the OpenAI API\n\nNow, weâ€™ll call the OpenAI API using the `prediction` parameter to provide the existing content.  \n```python\ncompletion = client.chat.completions.create(\nmodel=\"gpt-4o-mini\",\nmessages=[\\\n{\\\n\"role\": \"user\",\\\n\"content\": prompt\\\n}\\\n],\nprediction={\n\"type\": \"content\",\n\"content\": blog_content\n}\n)\nrevised_blog_post = completion.choices[0].message.content\n\n```",
      "metadata": {
        "title": "Getting Started with OpenAI's Predicted Outputs for Faster LLM Responses",
        "url": "https://www.firecrawl.dev/blog/getting-started-with-predicted-outputs-openai"
      }
    },
    {
      "id": "8fd43284-6914-4999-a385-34fddb8a56ef",
      "source": "firecrawl/blog/getting-started-with-predicted-outputs-openai.md",
      "content": "How to Use Predicted Outputs > Step 6: Compare the Original and Revised Content\n\nFinally, weâ€™ll compare the number of links in the original and revised blog posts to see the improvements.  \n```python\nimport re\n\ndef count_links(markdown_content):\nreturn len(re.findall(r'[.*?](.*?)', markdown_content))\n\noriginal_links_count = count_links(blog_content)\nrevised_links_count = count_links(revised_blog_post)\n\nprint(f\"Number of links in the original blog post: {original_links_count}\")\nprint(f\"Number of links in the revised blog post: {revised_links_count}\")\n\n```",
      "metadata": {
        "title": "Getting Started with OpenAI's Predicted Outputs for Faster LLM Responses",
        "url": "https://www.firecrawl.dev/blog/getting-started-with-predicted-outputs-openai"
      }
    },
    {
      "id": "0472d953-55b7-483e-99b4-cb51af456164",
      "source": "firecrawl/blog/getting-started-with-predicted-outputs-openai.md",
      "content": "References\n\n- [Using Predicted Outputs](https://platform.openai.com/docs/guides/latency-optimization#use-predicted-outputs)\n- [Firecrawl Documentation](https://www.firecrawl.dev/docs)  \nArticle updated recently  \n[ðŸ”¥](https://www.firecrawl.dev/)",
      "metadata": {
        "title": "Getting Started with OpenAI's Predicted Outputs for Faster LLM Responses",
        "url": "https://www.firecrawl.dev/blog/getting-started-with-predicted-outputs-openai"
      }
    },
    {
      "id": "2e47e0f4-efef-4321-b8bb-588b31d5bf38",
      "source": "firecrawl/blog/getting-started-with-predicted-outputs-openai.md",
      "content": "Ready to _Build?_\n\nStart scraping web data for your AI apps today.  \nNo credit card needed.  \nGet Started",
      "metadata": {
        "title": "Getting Started with OpenAI's Predicted Outputs for Faster LLM Responses",
        "url": "https://www.firecrawl.dev/blog/getting-started-with-predicted-outputs-openai"
      }
    },
    {
      "id": "a48e9738-408b-4791-8454-a80c7eda51c3",
      "source": "firecrawl/blog/getting-started-with-predicted-outputs-openai.md",
      "content": "About the Author\n\n[![Eric Ciarla image](https://www.firecrawl.dev/eric-img.jpeg)\\\nEric Ciarla@ericciarla](https://x.com/ericciarla)  \nEric Ciarla is the Chief Operating Officer (COO) of Firecrawl and leads marketing. He also worked on Mendable.ai and sold it to companies like Snapchat, Coinbase, and MongoDB.\nPreviously worked at Ford and Fracta as a Data Scientist. Eric also co-founded SideGuide, a tool for learning code within VS Code with 50,000 users.",
      "metadata": {
        "title": "Getting Started with OpenAI's Predicted Outputs for Faster LLM Responses",
        "url": "https://www.firecrawl.dev/blog/getting-started-with-predicted-outputs-openai"
      }
    },
    {
      "id": "2bc5fd64-a8f4-416a-b5a4-2dd1dd47e5a2",
      "source": "firecrawl/blog/getting-started-with-predicted-outputs-openai.md",
      "content": "About the Author > More articles by Eric Ciarla\n\n[How to Create an llms.txt File for Any Website\\\n\\\nLearn how to generate an llms.txt file for any website using the llms.txt Generator and Firecrawl.](https://www.firecrawl.dev/blog/How-to-Create-an-llms-txt-File-for-Any-Website) [Cloudflare Error 1015: How to solve it?\\\n\\\nCloudflare Error 1015 is a rate limiting error that occurs when Cloudflare detects that you are exceeding the request limit set by the website owner.](https://www.firecrawl.dev/blog/cloudflare-error-1015-how-to-solve-it) [Build an agent that checks for website contradictions\\\n\\\nUsing Firecrawl and Claude to scrape your website's data and look for contradictions.](https://www.firecrawl.dev/blog/contradiction-agent) [Why Companies Need a Data Strategy for Generative AI\\\n\\\nLearn why a well-defined data strategy is essential for building robust, production-ready generative AI systems, and discover practical steps for curation, maintenance, and integration.](https://www.firecrawl.dev/blog/why-companies-need-a-data-strategy-for-generative-ai) [Getting Started with OpenAI's Predicted Outputs for Faster LLM Responses\\\n\\\nA guide to leveraging Predicted Outputs to speed up LLM tasks with GPT-4o models.](https://www.firecrawl.dev/blog/getting-started-with-predicted-outputs-openai) [How to easily install requests with pip and python\\\n\\\nA tutorial on installing the requests library in Python using various methods, with usage examples and troubleshooting tips](https://www.firecrawl.dev/blog/how-to-easily-install-requests-with-pip-and-python) [How to quickly install BeautifulSoup with Python\\\n\\\nA guide on installing the BeautifulSoup library in Python using various methods, with usage examples and troubleshooting tips](https://www.firecrawl.dev/blog/how-to-quickly-install-beautifulsoup-with-python) [How to Use OpenAI's o1 Reasoning Models in Your Applications\\\n\\\nLearn how to harness OpenAI's latest o1 series models for complex reasoning tasks in your apps.](https://www.firecrawl.dev/blog/how-to-use-openai-o1-reasoning-models-in-applications)",
      "metadata": {
        "title": "Getting Started with OpenAI's Predicted Outputs for Faster LLM Responses",
        "url": "https://www.firecrawl.dev/blog/getting-started-with-predicted-outputs-openai"
      }
    },
    {
      "id": "2a190e89-bafc-4b02-8004-3c1984cba712",
      "source": "firecrawl/blog/deploy-web-scrapers.md",
      "content": "---\ntitle: How to Deploy Python Web Scrapers\nurl: https://www.firecrawl.dev/blog/deploy-web-scrapers\n---  \nIntroducing /extract - Get web data with a prompt [Try now](https://www.firecrawl.dev/extract)  \nDec 16, 2024  \nâ€¢  \n[![Bex Tuychiev image](https://www.firecrawl.dev/bex.jpg)Bex Tuychiev](https://x.com/bextuychiev)  \n# How to Deploy Python Web Scrapers  \n![How to Deploy Python Web Scrapers image](https://www.firecrawl.dev/images/blog/deploying-web-scrapers/deploy-web-scrapers.jpg)",
      "metadata": {
        "title": "How to Deploy Python Web Scrapers",
        "url": "https://www.firecrawl.dev/blog/deploy-web-scrapers"
      }
    },
    {
      "id": "a7b30c60-8a00-4bb2-a0d9-11350c372970",
      "source": "firecrawl/blog/deploy-web-scrapers.md",
      "content": "Introduction\n\nWeb scraping projects may start on your machine, but unless youâ€™re willing to ship your laptop to random strangers on the Internet, youâ€™ll need cloud services.  \n![Funny meme illustrating why cloud deployment is better than local workflows](https://www.firecrawl.dev/images/blog/deploying-web-scrapers/meme.png)  \nThere are many compelling reasons to move web scrapers to the cloud and make them more reliable. This guide explores several methods for automating and deploying web scrapers in 2025, focusing on free solutions.  \nHere is a general outline of concepts we will cover:  \n- Setting up automated scraping with GitHub Actions\n- Deploying to PaaS platforms (Heroku, PythonAnywhere)\n- Best practices for monitoring, security, and optimization",
      "metadata": {
        "title": "How to Deploy Python Web Scrapers",
        "url": "https://www.firecrawl.dev/blog/deploy-web-scrapers"
      }
    },
    {
      "id": "4ac55dda-a745-48ef-b38b-434a4ed65e60",
      "source": "firecrawl/blog/deploy-web-scrapers.md",
      "content": "Why Move Web Scrapers to the Cloud?\n\nThe number one reason to deploy scrapers to the cloud is reliability. Cloud-based scrapers run 24/7, without cigarette breaks and the best part, without depending on your local machine.  \nCloud-based scrapers also handle large-scale data operations more easily and gracefully, often juggling multiple scraping tasks. And, if you are a bit more aggressive in your request frequencies and get a dreaded IP ban, cloud services can give access to other IP addresses and geographic locations.  \nMoreover, you are not limited by your laptopâ€™s specs because cloud gives you dedicated resources. While these resources may gouge a hole in your pocket for large-scale scraping operations, many platforms offer generous free tiers, as we will explore soon.",
      "metadata": {
        "title": "How to Deploy Python Web Scrapers",
        "url": "https://www.firecrawl.dev/blog/deploy-web-scrapers"
      }
    },
    {
      "id": "9a93e1b6-617a-4fc1-af4a-e1c1372cbeb1",
      "source": "firecrawl/blog/deploy-web-scrapers.md",
      "content": "How to Choose the Right Deployment Method For Your Scrapers\n\nWe are about to list three deployment methods in this article, so you might get a decision fatigue. To prevent that, give this section a cursory read as it helps you choose the right one based on your scale requirements, technical complexity, and budget.",
      "metadata": {
        "title": "How to Deploy Python Web Scrapers",
        "url": "https://www.firecrawl.dev/blog/deploy-web-scrapers"
      }
    },
    {
      "id": "87aaf565-2f7e-482d-a932-6b4aaa3e3d5d",
      "source": "firecrawl/blog/deploy-web-scrapers.md",
      "content": "How to Choose the Right Deployment Method For Your Scrapers > Scale requirements\n\nIn this section, weâ€™ll dive into three deployment tiers that match your scale - from lightweight solutions to heavy-duty scraping powerhouses.",
      "metadata": {
        "title": "How to Deploy Python Web Scrapers",
        "url": "https://www.firecrawl.dev/blog/deploy-web-scrapers"
      }
    },
    {
      "id": "7325e1a5-de37-47eb-b330-14faf0138e0a",
      "source": "firecrawl/blog/deploy-web-scrapers.md",
      "content": "How to Choose the Right Deployment Method For Your Scrapers > Scale requirements > Small scale (1-1000 requests/day)\n\n- **Best Options**:\n- GitHub Actions\n- PythonAnywhere\n- Heroku (Free Tier)\n- **Why**: These platforms offer sufficient resources for basic scraping needs without cost\n- **Limitations**: Daily request caps and runtime restrictions",
      "metadata": {
        "title": "How to Deploy Python Web Scrapers",
        "url": "https://www.firecrawl.dev/blog/deploy-web-scrapers"
      }
    },
    {
      "id": "7d3913db-32a9-4677-8c7f-4246dcf9cf36",
      "source": "firecrawl/blog/deploy-web-scrapers.md",
      "content": "How to Choose the Right Deployment Method For Your Scrapers > Scale requirements > Medium Scale (1000-10000 requests/day)\n\n- **Best Options**:\n- AWS Lambda\n- Google Cloud Functions\n- Docker containers on basic VPS\n- **Why**: Better handling of concurrent requests and flexible scaling\n- **Considerations**: Cost begins to factor in, but still manageable",
      "metadata": {
        "title": "How to Deploy Python Web Scrapers",
        "url": "https://www.firecrawl.dev/blog/deploy-web-scrapers"
      }
    },
    {
      "id": "01832771-ad2d-46d8-a9f8-f82dab6c65ea",
      "source": "firecrawl/blog/deploy-web-scrapers.md",
      "content": "How to Choose the Right Deployment Method For Your Scrapers > Scale requirements > Large Scale (10000+ requests/day)\n\n- **Best Options**:\n- Kubernetes clusters\n- Multi-region serverless deployments\n- Specialized scraping platforms\n- **Why**: Robust infrastructure for high-volume operations\n- **Trade-offs**: Higher complexity and cost vs. reliability",
      "metadata": {
        "title": "How to Deploy Python Web Scrapers",
        "url": "https://www.firecrawl.dev/blog/deploy-web-scrapers"
      }
    },
    {
      "id": "25970e6c-cf04-47d5-9d0a-278e211a54d7",
      "source": "firecrawl/blog/deploy-web-scrapers.md",
      "content": "How to Choose the Right Deployment Method For Your Scrapers > Technical complexity\n\nNow, letâ€™s categorize the methods based on how fast you can get them up and running.",
      "metadata": {
        "title": "How to Deploy Python Web Scrapers",
        "url": "https://www.firecrawl.dev/blog/deploy-web-scrapers"
      }
    },
    {
      "id": "a5454a4c-9dbe-4cc4-aabf-4c3cc3da2871",
      "source": "firecrawl/blog/deploy-web-scrapers.md",
      "content": "How to Choose the Right Deployment Method For Your Scrapers > Technical complexity > Low Complexity Solutions\n\n- **GitHub Actions**\n- Pros: Simple setup, version control integration\n- Cons: Limited customization\n- **PythonAnywhere**\n- Pros: User-friendly interface\n- Cons: Resource constraints",
      "metadata": {
        "title": "How to Deploy Python Web Scrapers",
        "url": "https://www.firecrawl.dev/blog/deploy-web-scrapers"
      }
    },
    {
      "id": "6ca1fce8-0ec3-45f7-be72-b21a2268acbc",
      "source": "firecrawl/blog/deploy-web-scrapers.md",
      "content": "How to Choose the Right Deployment Method For Your Scrapers > Technical complexity > Medium Complexity Solutions\n\n- **Serverless (AWS Lambda/Google Functions)**\n- Pros: Managed infrastructure, auto-scaling\n- Cons: Learning curve for configuration\n- **Docker Containers**\n- Pros: Consistent environments\n- Cons: Container management overhead",
      "metadata": {
        "title": "How to Deploy Python Web Scrapers",
        "url": "https://www.firecrawl.dev/blog/deploy-web-scrapers"
      }
    },
    {
      "id": "fe30bb8d-7b3e-47bf-8d4b-327523803839",
      "source": "firecrawl/blog/deploy-web-scrapers.md",
      "content": "How to Choose the Right Deployment Method For Your Scrapers > Technical complexity > High Complexity Solutions\n\n- **Kubernetes**\n- Pros: Ultimate flexibility and scalability\n- Cons: Significant operational overhead\n- **Custom Infrastructure**\n- Pros: Complete control\n- Cons: Requires DevOps expertise",
      "metadata": {
        "title": "How to Deploy Python Web Scrapers",
        "url": "https://www.firecrawl.dev/blog/deploy-web-scrapers"
      }
    },
    {
      "id": "0323914c-b884-4c1c-8f0c-f1a9f8f7ae4d",
      "source": "firecrawl/blog/deploy-web-scrapers.md",
      "content": "How to Choose the Right Deployment Method For Your Scrapers > Budget Considerations\n\nIn terms of cost, all methods in the article has the following generous free tiers or next-to-nothing cheap starting plans:  \n- GitHub Actions: 2000 minutes/month\n- Heroku: $5 and up\n- AWS Lambda: starting at 0.2$ per 1 million requests\n- Google Cloud Functions: 2 million invocations/month  \nThese limits are based on various cost factors like compute time, data transfer speeds, storage requirements and additional services like databases or monitoring.  \nHere is a little decision matrix to distill all this information:  \n| Factor | Small Project | Medium Project | Large Project |\n| --- | --- | --- | --- |\n| Best Platform | GitHub Actions | AWS Lambda | Kubernetes |\n| Monthly Cost | $0 | $10-50 | $100+ |\n| Setup Time | 1-2 hours | 1-2 days | 1-2 weeks |\n| Maintenance | Minimal | Moderate | Significant |\n| Scalability | Limited | Good | Excellent |  \n* * *  \nStart small with simpler platforms and gather data on your scraping needs by measuring actual usage patterns. From there, you can gradually scale and potentially implement hybrid approaches that balance complexity and benefits.",
      "metadata": {
        "title": "How to Deploy Python Web Scrapers",
        "url": "https://www.firecrawl.dev/blog/deploy-web-scrapers"
      }
    },
    {
      "id": "038f4827-63ad-4bdb-aea4-56c409b63e46",
      "source": "firecrawl/blog/deploy-web-scrapers.md",
      "content": "Prerequisites\n\nThis article assumes familiarity with web scraping fundamentals like HTML parsing, CSS selectors, HTTP requests, and handling dynamic content. You should also be comfortable with Python basics including functions, loops, and working with external libraries. Basic knowledge of command line tools and git version control will be essential for deployment.",
      "metadata": {
        "title": "How to Deploy Python Web Scrapers",
        "url": "https://www.firecrawl.dev/blog/deploy-web-scrapers"
      }
    },
    {
      "id": "af269c6e-1eb4-46a6-87d2-322e3048928e",
      "source": "firecrawl/blog/deploy-web-scrapers.md",
      "content": "Prerequisites > Required accounts\n\nBefore starting with any deployment method, youâ€™ll need to create accounts on these platforms:  \n1. **[GitHub account](https://github.com/)** (Required)\n- Needed for version control and GitHub Actions\n2. **Cloud Platform Account** (Choose at least one)\n- [Heroku account](https://signup.heroku.com/)\n- [PythonAnywhere account](https://www.pythonanywhere.com/)\n3. **[Firecrawl account](https://firecrawl.dev/)** (Optional)\n- Only needed if you decide to use an AI-based scraper (more on Firecrawl soon).  \nNote: Most cloud platforms require a credit card for verification, even when using free tiers. However, they wonâ€™t charge you unless you exceed free tier limits.",
      "metadata": {
        "title": "How to Deploy Python Web Scrapers",
        "url": "https://www.firecrawl.dev/blog/deploy-web-scrapers"
      }
    },
    {
      "id": "64934f5c-62fc-496e-a0ee-a9885057f5e9",
      "source": "firecrawl/blog/deploy-web-scrapers.md",
      "content": "Prerequisites > Building a basic scraper\n\nTo demonstrate deployment concepts effectively, weâ€™ll start by building a basic web scraper using Firecrawl, a modern scraping API that simplifies many common challenges.  \n[Firecrawl](https://docs.firecrawl.dev/) offers several key advantages compared to traditional Python web scraping libraries:  \n- Dead simple to use with only a few dependencies\n- Handles complex scraping challenges automatically (proxies, anti-bot mechanisms, dynamic JS content)\n- Converts web content into clean, LLM-ready markdown format\n- Supports multiple output formats (markdown, structured data, screenshots, HTML)\n- Reliable extraction with retry mechanisms and error handling\n- Supports custom actions (click, scroll, input, wait) before data extraction\n- Geographic location customization for avoiding IP bans\n- Built-in rate limiting and request management  \nAs an example, we will build a simple scraper for [ProductHunt](https://www.producthunt.com/). Specifically, we will scrape the â€œYesterdayâ€™s Top Productsâ€ list from the homepage:  \n![Screenshot of Product Hunt homepage showing Yesterday's Top Products section with product cards displaying titles, descriptions, upvotes and comments](https://www.firecrawl.dev/images/blog/deploying-web-scrapers/ph-homepage.png)  \nThe scraper extracts the following information from each product:  \n![Sample Product Hunt product card showing key data fields including product name, description, upvotes, comments, topics and logo that our Firecrawl scraper will extract](https://www.firecrawl.dev/images/blog/deploying-web-scrapers/ph-sample.png)  \nLetâ€™s get building:  \n```bash\nmkdir product-hunt-scraper\ncd product-hunt-scraper\ntouch scraper.py .env\npython -m venv venv\nsource venv/bin/activate\npip install pydantic firecrawl-py\necho \"FIRECRAWL_API_KEY='your-api-key-here' >> .env\"\n\n```  \nThese commands set up a working directory for the scraper, along with a virtual environment and the main script. The last part also saves your Firecrawl API key, which you can get through their free plan by [signing up for an account](https://www.firecrawl.dev/).  \nLetâ€™s work on the code now:  \n```python\nimport json\n\nfrom firecrawl import FirecrawlApp\nfrom dotenv import load_dotenv\nfrom pydantic import BaseModel, Field\nfrom datetime import datetime\n\nload_dotenv()\n\n```  \nFirst, we import a few packages and the `FirecrawlApp` class - we use it to establish a connection with Firecrawlâ€™s scraping engine. Then, we define a Pydantic class outlining the details we want to scrape from each product:  \n```python\nclass Product(BaseModel):\nname: str = Field(description=\"The name of the product\")\ndescription: str = Field(description=\"A short description of the product\")\nurl: str = Field(description=\"The URL of the product\")\n\ntopics: list[str] = Field(\ndescription=\"A list of topics the product belongs to. Can be found below the product description.\"\n)\n\nn_upvotes: int = Field(description=\"The number of upvotes the product has\")\nn_comments: int = Field(description=\"The number of comments the product has\")\n\nrank: int = Field(\ndescription=\"The rank of the product on Product Hunt's Yesterday's Top Products section.\"\n)\nlogo_url: str = Field(description=\"The URL of the product's logo.\")\n\n```  \nThe field descriptions in this class play a crucial role in guiding the LLM scraping engine. By providing natural language descriptions for each field, we tell the LLM exactly what information to look for and where to find it on the page. For example, when we say â€œA list of topics under the product descriptionâ€, the LLM understands both the content we want (topics) and its location (below the description).  \nThis natural language approach allows Firecrawl to intelligently parse the pageâ€™s HTML structure and extract the right information without requiring explicit CSS selectors or XPaths. The LLM analyzes the semantic meaning of our descriptions and matches them to the appropriate elements on the page.  \nThis approach offers two practical advantages: it reduces initial development time since you donâ€™t need to manually inspect HTML structures, and it provides long-lasting resilience against HTML changes. Since the LLM understands the semantic meaning of the elements rather than relying on specific selectors, it can often continue working even when class names or IDs are updated. This makes it suitable for scenarios where long-term maintenance is a consideration.  \nGetting back to code, we write another Pydantic class for scraping a collection of Products from the â€˜Yesterdayâ€™s Top Productsâ€™ list:  \n```python\nclass YesterdayTopProducts(BaseModel):\nproducts: list[Product] = Field(\ndescription=\"A list of top products from yesterday on Product Hunt.\"\n)\n\n```  \nThe `YesterdayTopProducts` parent class is essential - without it, Firecrawl would only scrape a single product instead of the full list. This happens because Firecrawl strictly adheres to the provided schema structure, ensuring consistent and reliable output on every scraping run.  \nNow, we define a function that scrapes ProductHunt based on the schema we just defined:  \n```python\nBASE_URL = \"https://www.producthunt.com\"\n\ndef get_yesterday_top_products():\napp = FirecrawlApp()\n\ndata = app.scrape_url(\nBASE_URL,\nparams={\n\"formats\": [\"extract\"],\n\"extract\": {\n\"schema\": YesterdayTopProducts.model_json_schema(),\n\n\"prompt\": \"Extract the top products listed under the 'Yesterday's Top Products' section. There will be exactly 5 products.\",\n},\n},\n)\n\nreturn data[\"extract\"][\"products\"]\n\n```  \nThis function initializes a Firecrawl app, which reads your Firecrawl API key stored in an `.env` file and scrapes the URL. Notice the parameters being passed to the `scrape_url()` method:  \n- `formats` specifies how the data should be scraped and extracted. Firecrawl supports other formats like markdown, HTML, screenshots or links.\n- `schema`: The JSON schema produced by the Pydantic class\n- `prompt`: A general prompt guiding the underlying LLM on what to do. Providing a prompt usually improves the performance.  \nIn the end, the function returns the extracted products, which will be a list of dictionaries.  \nThe final step is writing a function to save this data to a JSON file:  \n```python\ndef save_yesterday_top_products():\nproducts = get_yesterday_top_products()\n\ndate_str = datetime.now().strftime(\"%Y_%m_%d\")\nfilename = f\"ph_top_products_{date_str}.json\"\n\nwith open(filename, \"w\") as f:\njson.dump(products, f)\n\nif __name__ == \"__main__\":\nsave_yesterday_top_products()\n\n```  \nThis function runs the previous one and saves the returned data to a JSON file identifiable with the following dayâ€™s date.  \n* * *  \nWeâ€™ve just built a scraper that resiliently extracts data from ProductHunt. To keep things simple, we implemented everything in a single script with straightforward data persistence. In production environments with larger data flows and more complex websites, your project would likely span multiple directories and files.  \nNevertheless, the deployment methods weâ€™ll explore work for nearly any type of project. You can always restructure your own projects to follow this pattern of having a single entry point that coordinates all the intermediate scraping stages.",
      "metadata": {
        "title": "How to Deploy Python Web Scrapers",
        "url": "https://www.firecrawl.dev/blog/deploy-web-scrapers"
      }
    },
    {
      "id": "ef83c3eb-a506-4b77-9317-a1585df4ba9c",
      "source": "firecrawl/blog/deploy-web-scrapers.md",
      "content": "Deploying Web Scrapers With GitHub Actions\n\nGitHub Actions is a powerful CI/CD platform built into GitHub that allows you to automate workflows, including running scrapers on a schedule. It provides a simple way to deploy and run automated tasks without managing infrastructure, making it an ideal choice for web scraping projects.  \nTo get started, initialize Git and commit the work weâ€™ve completed so far in your working directory:  \n```python\ngit init\ntouch .gitignore\necho \".env\" >> .gitignore # Remove the .env file from Git indexing\ngit add .\ngit commit -m \"Initial commit\"\n\n```  \nThen, create an empty GitHub repository, copy its link and set it as the remote for your local repo:  \n```bash\ngit remote add origin your-repo-link\ngit push\n\n```",
      "metadata": {
        "title": "How to Deploy Python Web Scrapers",
        "url": "https://www.firecrawl.dev/blog/deploy-web-scrapers"
      }
    },
    {
      "id": "dc1b4191-7514-4f9f-901b-850040fbe2a2",
      "source": "firecrawl/blog/deploy-web-scrapers.md",
      "content": "Deploying Web Scrapers With GitHub Actions > Creating workflow files\n\nWorkflow files are YAML configuration files that tell GitHub Actions how to automate tasks in your repository. For our web scraping project, these files will define when and how to run our scraper.  \nThese files live in the `.github/workflows` directory of your repository and contain instructions for:  \n- When to trigger the workflow (like on a schedule or when code is pushed)\n- What environment to use (Python version, dependencies to install)\n- The actual commands to run your scraper\n- What to do with the scraped data  \nEach workflow file acts like a recipe that GitHub Actions follows to execute your scraper automatically. This automation is perfect for web scraping since we often want to collect data on a regular schedule without manual intervention.  \nFor our scraper, we need a single workflow file that executes the `scraper.py` file. Letâ€™s set it up:  \n```bash\nmkdir -p .github/workflows\ntouch .github/workflows/ph-scraper.yml\n\n```  \nOpen the newly-created file and paste the following contents:  \n```yml\nname: Product Hunt Scraper\n\non:\nschedule:\n- cron: \"0 1 * * *\" # Runs at 1 AM UTC daily\nworkflow_dispatch: # Allows manual trigger\n\npermissions:\ncontents: write\n\njobs:\nscrape:\nruns-on: ubuntu-latest\n\nsteps:\n- name: Checkout repository\nuses: actions/checkout@v3\nwith:\npersist-credentials: true\n\n- name: Set up Python\nuses: actions/setup-python@v4\nwith:\npython-version: \"3.10\"\n\n- name: Install dependencies\nrun: |\npython -m pip install --upgrade pip\npip install -r requirements.txt\n\n- name: Run scraper\nenv:\nFIRECRAWL_API_KEY: ${{ secrets.FIRECRAWL_API_KEY }}\nrun: python scraper.py\n\n- name: Commit and push if changes\nrun: |\ngit config --local user.email \"github-actions[bot]@users.noreply.github.com\"\ngit config --local user.name \"github-actions[bot]\"\ngit add *.json\ngit diff --quiet && git diff --staged --quiet || git commit -m \"Update ProductHunt data [skip ci]\"\ngit push\n\n```  \nThe YAML file defines a GitHub Actions workflow for automated web scraping:  \nname: Specifies the workflow name that appears in GitHub Actions UI  \non: Defines how workflow triggers:  \n- schedule: Uses cron syntax to run daily at 1 AM UTC\n- workflow_dispatch: Enables manual workflow triggering  \njobs: Contains the workflow jobs:  \n- scrape: Main job that runs on ubuntu-latest\n- steps: Sequential actions to execute:\n1. Checkout repository using `actions/checkout`\n2. Setup Python 3.10 environment\n3. Install project dependencies from `requirements.txt`\n4. Run the scraper with environment variables\n5. Commit and push any changes to the repository  \nThe workflow automatically handles repository interaction, dependency management, and data updates while providing both scheduled and manual execution options. You can read the [official GitHub guide on workflow file syntax](https://docs.github.com/en/actions/writing-workflows/workflow-syntax-for-github-actions#about-yaml-syntax-for-workflows) to learn more.  \nFor this workflow file to sun successfully, we need to take a couple of additional steps.",
      "metadata": {
        "title": "How to Deploy Python Web Scrapers",
        "url": "https://www.firecrawl.dev/blog/deploy-web-scrapers"
      }
    },
    {
      "id": "da010a5b-3ff5-4777-8d22-737da2c724b1",
      "source": "firecrawl/blog/deploy-web-scrapers.md",
      "content": "Deploying Web Scrapers With GitHub Actions > Generating a `requirements.txt` file\n\nOne of the steps in the workflow file is installing the dependencies for our project using a `requirements.txt` file, which is a standard format for listing packages used in your project.  \nFor simple projects, you can create this file manually and adding each package on a new line like:  \n```text\npydantic\nfirecrawl-py\n\n```  \nHowever, if you have a large project with multiple files and dozens of dependencies, you need an automated method. The simplest one I can suggest is using `pipreqs` package:  \n```bash\npip install pipreqs\npipreqs .\n\n```  \n`pipreqs` is a lightweight package that scans all Python scripts in your project and adds them to a new `requirements.txt` file with their used versions.",
      "metadata": {
        "title": "How to Deploy Python Web Scrapers",
        "url": "https://www.firecrawl.dev/blog/deploy-web-scrapers"
      }
    },
    {
      "id": "189ae3f4-6fea-431d-94dc-423fe42d0882",
      "source": "firecrawl/blog/deploy-web-scrapers.md",
      "content": "Deploying Web Scrapers With GitHub Actions > Storing secrets\n\nIf you notice, the workflow file has a step that executes `scraper.py`:  \n```yml\n- name: Run scraper\nenv:\nFIRECRAWL_API_KEY: ${{ secrets.FIRECRAWL_API_KEY }}\nrun: python scraper.py\n\n```  \nThe workflow retrieves environment variables using the `secrets.SECRET_NAME` syntax. Since the `.env` file containing your Firecrawl API key isnâ€™t uploaded to GitHub for security reasons, youâ€™ll need to store the key in your GitHub repository secrets.  \nTo add your API key as a secret:  \n1. Navigate to your GitHub repository\n2. Click on â€œSettingsâ€\n3. Select â€œSecrets and variablesâ€ then â€œActionsâ€\n4. Click â€œNew repository secretâ€\n5. Enter â€œFIRECRAWL_API_KEYâ€ as the name\n6. Paste your API key as the value\n7. Click â€œAdd secretâ€  \nThis allows the workflow to securely access your API key during execution without exposing it in the repository.",
      "metadata": {
        "title": "How to Deploy Python Web Scrapers",
        "url": "https://www.firecrawl.dev/blog/deploy-web-scrapers"
      }
    },
    {
      "id": "69853940-bd52-44af-8a58-4e5b33af7224",
      "source": "firecrawl/blog/deploy-web-scrapers.md",
      "content": "Deploying Web Scrapers With GitHub Actions > Running the workflow\n\nBefore running the workflow, we need to commit all new changes and push them to GitHub:  \n```python\ngit add .\ngit commit -m \"Descriptive commit message\"\n\n```  \nThis makes our workflow visible to GitHub.  \nAt the top of the workflow file, we set a schedule for the workflow file to run at 1 AM using `cron` syntax:  \n```yaml\non:\nschedule:\n- cron: \"0 1 * * *\" # Runs at 1 AM UTC daily\nworkflow_dispatch: # Allows manual trigger\n\n```  \nThe `cron` syntax consists of 5 fields representing minute (0-59), hour (0-23), day of month (1-31), month (1-12), and day of week (0-6, where 0 is Sunday). Each field can contain specific values, ranges (1-5), lists (1,3,5), or asterisks (_) meaning â€œeveryâ€. For example, `0 1 _ * *` means â€œat minute 0 of hour 1 (1 AM UTC) on every day of every monthâ€. Here are some more patterns:  \n- `0 */2 * * *`: Every 2 hours\n- `0 9-17 * * 1-5`: Every hour from 9 AM to 5 PM on weekdays\n- `*/15 * * * *`: Every 15 minutes\n- `0 0 * * 0`: Every Sunday at midnight\n- `0 0 1 * *`: First day of every month at midnight\n- `30 18 * * 1,3,5`: Monday, Wednesday, Friday at 6:30 PM  \nSo, once the workflow file is pushed to GitHub, the scraper is scheduler to run. However, the `workflow_dispatch` parameter in the file allows us to run the scraper manually for debugging.  \n![Screenshot of GitHub Actions workflow interface showing run workflow button and workflow status for web scraper automation](https://www.firecrawl.dev/blog/deploying-web-scrapers/workflow.png)  \nNavigate to the Actions tab of your GitHub repository, click on the workflow name and press â€œRun workflowâ€. In about a minute (if the workflow is successful), you will see the top five products from yesterday on ProductHunt saved as a JSON file to your repository.  \nWhenever you want to interrupt the scraping schedule, click on the three buttons in the top-right corner of the workflow page and disable it.",
      "metadata": {
        "title": "How to Deploy Python Web Scrapers",
        "url": "https://www.firecrawl.dev/blog/deploy-web-scrapers"
      }
    },
    {
      "id": "07e0d117-c995-4a7e-9eac-07cf3d5b4ec0",
      "source": "firecrawl/blog/deploy-web-scrapers.md",
      "content": "Deploying Web Scrapers With Heroku\n\n[Heroku](https://heroku.com/) is a Platform-as-a-Service (PaaS) that makes deploying applications straightforward, even for beginners. While it removed its generous free tier in 2022, its basic $5 â€œdynoâ€ plan still has some free features we can take advantage of for the purposes of this tutorial.",
      "metadata": {
        "title": "How to Deploy Python Web Scrapers",
        "url": "https://www.firecrawl.dev/blog/deploy-web-scrapers"
      }
    },
    {
      "id": "4e27ebdc-4a21-4db4-9c90-adcfa9e95304",
      "source": "firecrawl/blog/deploy-web-scrapers.md",
      "content": "Deploying Web Scrapers With Heroku > Setting up Heroku\n\nFirst, install the Heroku CLI and login to your account:  \n```bash\nbrew install heroku/brew/heroku # macOS\ncurl https://cli-assets.heroku.com/install.sh | sh # Linux\nheroku login # Opens your web browser\n\n```  \nThen, create a new Heroku app and set it as a remote for your repository:  \n```bash\nheroku create ph-scraper-your-name # Make the app name unique\nheroku git:remote -a ph-scraper-your-name\n\n```  \nAfter this step, if you visit [dashboard.heroku.com](https://dashboard.heroku.com/), your app must be visible.",
      "metadata": {
        "title": "How to Deploy Python Web Scrapers",
        "url": "https://www.firecrawl.dev/blog/deploy-web-scrapers"
      }
    },
    {
      "id": "611ee2c6-5e36-4ea7-b7c1-4ca59c9c29e7",
      "source": "firecrawl/blog/deploy-web-scrapers.md",
      "content": "Deploying Web Scrapers With Heroku > Configuring the Application\n\nHeroku requires a few additional files to run your application. First, create a `Procfile` that tells Heroku what command to run:  \n```bash\ntouch Procfile\necho \"worker: python scraper.py\" > Procfile\n\n```  \nNext, create a `runtime.txt` to specify the Python version:  \n```bash\ntouch runtime.txt\necho \"python-3.10.12\" > runtime.txt\n\n```",
      "metadata": {
        "title": "How to Deploy Python Web Scrapers",
        "url": "https://www.firecrawl.dev/blog/deploy-web-scrapers"
      }
    },
    {
      "id": "6db3767e-643b-49e2-9458-4b2d04a657dd",
      "source": "firecrawl/blog/deploy-web-scrapers.md",
      "content": "Deploying Web Scrapers With Heroku > Environment Variables\n\nInstead of using a `.env` file, Heroku requires you to set your environment variables directly using the Heroku CLI:  \n```bash\nheroku config:set FIRECRAWL_API_KEY='your-api-key-here'\n\n```  \nYou can verify the variables are set correctly with:  \n```bash\nheroku config\n\n```",
      "metadata": {
        "title": "How to Deploy Python Web Scrapers",
        "url": "https://www.firecrawl.dev/blog/deploy-web-scrapers"
      }
    },
    {
      "id": "7b839db4-f3f0-49b5-9592-7d9162484139",
      "source": "firecrawl/blog/deploy-web-scrapers.md",
      "content": "Deploying Web Scrapers With Heroku > Scheduling Scraper Runs\n\nHeroku uses an add-on called [â€œSchedulerâ€](https://elements.heroku.com/addons/scheduler) for running periodic tasks. Install it with:  \n```bash\nheroku addons:create scheduler:standard\n\n```  \nThen open the scheduler dashboard:  \n```bash\nheroku addons:open scheduler\n\n```  \nIn the web interface, add a new job with the command `python scraper.py` and set your desired frequency (daily, hourly, or every 10 minutes).  \n![Screenshot of Heroku Scheduler dashboard showing job configuration interface with frequency dropdown and command input field for scheduling automated web scraper tasks](https://www.firecrawl.dev/images/blog/deploying-web-scrapers/heroku-scheduler.png)",
      "metadata": {
        "title": "How to Deploy Python Web Scrapers",
        "url": "https://www.firecrawl.dev/blog/deploy-web-scrapers"
      }
    },
    {
      "id": "eab35338-1612-420d-8df9-28b6fe476edc",
      "source": "firecrawl/blog/deploy-web-scrapers.md",
      "content": "Deploying Web Scrapers With Heroku > Deployment and Monitoring\n\nNow, to launch everything, you need to deploy your application by committing and pushing the local changes to Heroku:  \n```bash\ngit add .\ngit commit -m \"Add Heroku-related files\"\ngit push heroku main\n\n```  \nYou can periodically monitor the health of your application with the following command:  \n```bash\nheroku logs --tail\n\n```",
      "metadata": {
        "title": "How to Deploy Python Web Scrapers",
        "url": "https://www.firecrawl.dev/blog/deploy-web-scrapers"
      }
    },
    {
      "id": "2f11fa03-4078-4d6c-b46e-b17307e52809",
      "source": "firecrawl/blog/deploy-web-scrapers.md",
      "content": "Deploying Web Scrapers With Heroku > Platform Limitations\n\nThe basic $5 dyno has some important limitations to consider:  \n- Sleeps after 30 minutes of inactivity\n- Limited to 512MB RAM\n- Shares CPU with other applications\n- Maximum of 23 hours active time per day  \nFor most small to medium scraping projects, these limitations arenâ€™t problematic. However, if you need more resources, you can upgrade to Standard (25/month)orPerformance(25/month) or Performance (25/month)orPerformance(250/month) dynos.",
      "metadata": {
        "title": "How to Deploy Python Web Scrapers",
        "url": "https://www.firecrawl.dev/blog/deploy-web-scrapers"
      }
    },
    {
      "id": "5bae8e1d-22ce-4bee-a342-7468c24ab125",
      "source": "firecrawl/blog/deploy-web-scrapers.md",
      "content": "Deploying Web Scrapers With Heroku > Data Persistence\n\nSince Herokuâ€™s filesystem is temporary, youâ€™ll need to modify the scraper to store data externally. Hereâ€™s a quick example using AWS S3:  \n```python\nimport boto3 # pip install boto3\nfrom datetime import datetime\n\ndef save_yesterday_top_products():\nproducts = get_yesterday_top_products()\n\n# Initialize S3 client\ns3 = boto3.client('s3')\n\n# Create filename with date\ndate_str = datetime.now().strftime(\"%Y_%m_%d\")\nfilename = f\"ph_top_products_{date_str}.json\"\n\n# Upload to S3\ns3.put_object(\nBucket='your-bucket-name',\nKey=filename,\nBody=json.dumps(products)\n)\n\n```  \nFor this to work, you must already have an AWS account and an existing S3 bucket. Also, you must set your AWS credentials as Heroku secrets through the Heroku CLI:  \n```bash\nheroku config:set AWS_ACCESS_KEY_ID='your-key'\nheroku config:set AWS_SECRET_ACCESS_KEY='your-secret'\n\n```  \nOnce you do, add `boto3` to the list of dependencies in your `requirements.txt` file:  \n```bash\necho \"boto3\" >> requirements.txt\n\n```  \nFinally, commit and push the changes:  \n```bash\ngit add .\ngit commit -m \"Switch data persistence to S3\"\ngit push heroku main\ngit push origin main\n\n```  \nYou can confirm that the app is functioning properly by setting the schedule frequency to 10 minutes and checking your S3 bucket for the JSON file containing the top five products from ProductHunt.",
      "metadata": {
        "title": "How to Deploy Python Web Scrapers",
        "url": "https://www.firecrawl.dev/blog/deploy-web-scrapers"
      }
    },
    {
      "id": "2debba27-60f7-4298-b3e6-417bfe643d5c",
      "source": "firecrawl/blog/deploy-web-scrapers.md",
      "content": "Deploying Web Scrapers With Heroku > Stopping Heroku Apps\n\nTo stop your app, you can use a few different methods:  \n- Pause the dyno:  \n```bash\nheroku ps:scale worker=0\n\n```  \nThis stops the worker dyno without deleting the app. To resume later:  \n```bash\nheroku ps:scale worker=1\n\n```  \n- Disable the scheduler:  \n```bash\nheroku addons:destroy scheduler\n\n```  \nOr visit the Heroku dashboard and remove the scheduler add-on manually.  \n- Delete the entire app:  \n```bash\nheroku apps:destroy --app your-app-name --confirm your-app-name\n\n```  \nâš ï¸ Warning: This permanently deletes your app and all its data.  \n- Maintenance mode  \n```bash\nheroku maintenance:on\n\n```  \nThis puts the app in maintenance mode. To disable:  \n```bash\nheroku maintenance:off\n\n```  \n* * *  \nTo learn more about Heroku and how to run Python applications on its servers, please refer to [their Python support documentation](https://devcenter.heroku.com/categories/python-support).",
      "metadata": {
        "title": "How to Deploy Python Web Scrapers",
        "url": "https://www.firecrawl.dev/blog/deploy-web-scrapers"
      }
    },
    {
      "id": "31fb14bc-926e-443d-b3bb-23f9c3cb1403",
      "source": "firecrawl/blog/deploy-web-scrapers.md",
      "content": "Deploying Web Scrapers With PythonAnywhere\n\n[PythonAnywhere](https://www.pythonanywhere.com/) is a cloud-based Python development environment that offers an excellent platform for hosting web scrapers. It provides a free tier that includes:  \n- Daily scheduled tasks\n- Web-based console access\n- 512MB storage\n- Basic CPU and memory allocation",
      "metadata": {
        "title": "How to Deploy Python Web Scrapers",
        "url": "https://www.firecrawl.dev/blog/deploy-web-scrapers"
      }
    },
    {
      "id": "2a8e10d7-b84a-46e3-99db-d2bd56581c69",
      "source": "firecrawl/blog/deploy-web-scrapers.md",
      "content": "Deploying Web Scrapers With PythonAnywhere > Setting Up PythonAnywhere\n\nFirst, create a free account at [pythonanywhere.com](https://www.pythonanywhere.com/). Once logged in, follow these steps:  \nOpen a Bash console from your PythonAnywhere dashboard and execute these commands to clone the GitHub repository weâ€™ve been building:  \n```bash\n\n$ git clone https://github.com/your-username/your-repo.git\n$ cd your-repo\n$ python3 -m venv venv\n$ source venv/bin/activate\n$ pip install -r requirements.txt\n\n# Recreate your .env file\n$ touch .env\n$ echo \"FIRECRAWL_API_KEY='your-api-key-here'\" >> .env\n\n```",
      "metadata": {
        "title": "How to Deploy Python Web Scrapers",
        "url": "https://www.firecrawl.dev/blog/deploy-web-scrapers"
      }
    },
    {
      "id": "f1d06ec2-0949-4135-9da9-36a65f9daaa4",
      "source": "firecrawl/blog/deploy-web-scrapers.md",
      "content": "Deploying Web Scrapers With PythonAnywhere > Scheduling the Scraper\n\nPA free tier includes a scheduler with a daily frequency. To enable it, follow these steps  \n1. Go to the â€œTasksâ€ tab in your PythonAnywhere dashboard accessible via [https://www.pythonanywhere.com/user/your-username](https://www.pythonanywhere.com/user/your-username)\n2. Add a new scheduled task.\n3. Set the timing using the provided interface.\n4. Enter the command to run your scraper:  \n```bash\ncd /home/your-username/your-repo && source venv/bin/activate && python scraper.py\n\n```  \nThe command changes the working directory to the project location, activates the virtual environment and executes the scraper.  \n![Screenshot of PythonAnywhere task scheduler interface showing scheduling options and command input field for automated web scraping tasks](https://www.firecrawl.dev/images/blog/deploying-web-scrapers/pa-scheduler.png)",
      "metadata": {
        "title": "How to Deploy Python Web Scrapers",
        "url": "https://www.firecrawl.dev/blog/deploy-web-scrapers"
      }
    },
    {
      "id": "228decfb-820e-4883-8393-70db52b15966",
      "source": "firecrawl/blog/deploy-web-scrapers.md",
      "content": "Deploying Web Scrapers With PythonAnywhere > Data Storage Options\n\nPythonAnywhereâ€™s filesystem is persistent, unlike Heroku, so you can store JSON files directly. However, for better scalability, consider using cloud storage:  \n```python\ndef save_yesterday_top_products():\n\"\"\"\nChange back to JSON-based storage.\n\"\"\"\nproducts = get_yesterday_top_products()\n\n# Local storage (works on PythonAnywhere)\ndate_str = datetime.now().strftime(\"%Y_%m_%d\")\nfilename = f\"data/ph_top_products_{date_str}.json\"\n\n# Create data directory if it doesn't exist\nos.makedirs(\"data\", exist_ok=True)\n\nwith open(filename, \"w\") as f:\njson.dump(products, f)\n\n```",
      "metadata": {
        "title": "How to Deploy Python Web Scrapers",
        "url": "https://www.firecrawl.dev/blog/deploy-web-scrapers"
      }
    },
    {
      "id": "93c2c75e-c6af-449f-bde8-82e1d277f4b3",
      "source": "firecrawl/blog/deploy-web-scrapers.md",
      "content": "Deploying Web Scrapers With PythonAnywhere > Platform benefits & limitations\n\nPythonAnywhere offers several advantages for web scraping:  \n- **Always-on environment**: Unlike Herokuâ€™s free tier, PythonAnywhere doesnâ€™t sleep\n- **Persistent storage**: Files remain stored between runs\n- **Simple interface**: User-friendly web console and file editor\n- **Built-in scheduler**: No need for additional add-ons\n- **Free SSL**: HTTPS requests work out of the box\n- **Multiple Python Versions**: Support for different Python versions  \nThe free tier has some restrictions:  \n- Limited to 1 daily task\n- CPU/RAM throttling\n- 512MB storage limit",
      "metadata": {
        "title": "How to Deploy Python Web Scrapers",
        "url": "https://www.firecrawl.dev/blog/deploy-web-scrapers"
      }
    },
    {
      "id": "8f170171-fa44-4b19-9624-6abd1fdb6718",
      "source": "firecrawl/blog/deploy-web-scrapers.md",
      "content": "Deploying Web Scrapers With PythonAnywhere > Stopping or Modifying Tasks\n\nTo manage your scraper:  \n1. **Pause**: Disable the scheduled task in the Tasks tab\n2. **Modify schedule**: Edit timing in the Tasks interface\n3. **Delete**: Remove the task completely\n4. **Update code**: Pull latest changes from git repository from any PythonAnywhere bash console:  \n```bash\ncd your-repo\ngit pull origin main\n\n```",
      "metadata": {
        "title": "How to Deploy Python Web Scrapers",
        "url": "https://www.firecrawl.dev/blog/deploy-web-scrapers"
      }
    },
    {
      "id": "8e7994f3-8889-494b-98d7-96a26d1703a5",
      "source": "firecrawl/blog/deploy-web-scrapers.md",
      "content": "Best Practices and Optimization\n\nOur scraper and deployment methods are far from perfect. In this section, we will cover some best practices and tips to optimize its performance.",
      "metadata": {
        "title": "How to Deploy Python Web Scrapers",
        "url": "https://www.firecrawl.dev/blog/deploy-web-scrapers"
      }
    },
    {
      "id": "1c335608-6366-4c44-97ba-f906d11ec144",
      "source": "firecrawl/blog/deploy-web-scrapers.md",
      "content": "Best Practices and Optimization > Error Handling & Monitoring\n\nProper error handling and monitoring are crucial for maintaining a reliable web scraper. Below, we will implement a few mechanisms.",
      "metadata": {
        "title": "How to Deploy Python Web Scrapers",
        "url": "https://www.firecrawl.dev/blog/deploy-web-scrapers"
      }
    },
    {
      "id": "2ad7cb3e-61c0-4770-9bb5-70780126d969",
      "source": "firecrawl/blog/deploy-web-scrapers.md",
      "content": "Best Practices and Optimization > Error Handling & Monitoring > Implement a robust retry mechanism\n\n```python\nfrom tenacity import retry, stop_after_attempt, wait_exponential\n\n@retry(\nstop=stop_after_attempt(3),\nwait=wait_exponential(multiplier=1, min=4, max=10),\nreraise=True\n)\ndef get_yesterday_top_products():\ntry:\napp = FirecrawlApp()\ndata = app.scrape_url(\nBASE_URL,\nparams={\n\"formats\": [\"extract\"],\n\"extract\": {\n\"schema\": YesterdayTopProducts.model_json_schema(),\n\"prompt\": \"Extract the top products listed under the 'Yesterday's Top Products' section.\"\n},\n},\n)\nreturn data[\"extract\"][\"products\"]\nexcept Exception as e:\nlogger.error(f\"Scraping failed: {str(e)}\")\nraise\n\n```  \nAbove, we are implementing a retry mechanism using the tenacity library. It will retry the scraping operation up to 3 times with exponential backoff between attempts. The wait time starts at 4 seconds and increases exponentially up to 10 seconds between retries. If all retries fail, it will raise the last exception. Any errors are logged before being re-raised to trigger the retry mechanism.",
      "metadata": {
        "title": "How to Deploy Python Web Scrapers",
        "url": "https://www.firecrawl.dev/blog/deploy-web-scrapers"
      }
    },
    {
      "id": "e473aa47-cfd5-4ce2-950b-efd5141e831c",
      "source": "firecrawl/blog/deploy-web-scrapers.md",
      "content": "Best Practices and Optimization > Error Handling & Monitoring > Implement comprehensive logging\n\n```python\nimport logging\nfrom datetime import datetime\n\ndef setup_logging():\n\"\"\"Configure logging with both file and console handlers.\"\"\"\nlogger = logging.getLogger(__name__)\nlogger.setLevel(logging.INFO)\n\n# Create formatters\ndetailed_formatter = logging.Formatter(\n'%(asctime)s - %(name)s - %(levelname)s - %(message)s'\n)\nsimple_formatter = logging.Formatter('%(levelname)s: %(message)s')\n\n# File handler\nfile_handler = logging.FileHandler(\nf'logs/scraper_{datetime.now().strftime(\"%Y%m%d\")}.log'\n)\nfile_handler.setFormatter(detailed_formatter)\n\n# Console handler\nconsole_handler = logging.StreamHandler()\nconsole_handler.setFormatter(simple_formatter)\n\n# Add handlers\nlogger.addHandler(file_handler)\nlogger.addHandler(console_handler)\n\nreturn logger\n\nlogger = setup_logging()\n\n```  \nThe logging setup above configures comprehensive logging for our web scraper, which is essential for monitoring, debugging and maintaining the scraper in production. It creates two logging handlers - one that writes detailed logs to dated files (including timestamps and log levels), and another that outputs simplified logs to the console. This dual logging approach helps us track scraper execution both in real-time via console output and historically through log files. Having proper logging is crucial for diagnosing issues, monitoring performance, and ensuring the reliability of our web scraping system.",
      "metadata": {
        "title": "How to Deploy Python Web Scrapers",
        "url": "https://www.firecrawl.dev/blog/deploy-web-scrapers"
      }
    },
    {
      "id": "37eacd2c-ee1a-4e28-b6b7-1e1655c5558c",
      "source": "firecrawl/blog/deploy-web-scrapers.md",
      "content": "Best Practices and Optimization > Error Handling & Monitoring > Set up monitoring alerts\n\n```python\nimport requests\n\ndef send_alert(message, webhook_url):\n\"\"\"Send alerts to Slack/Discord/etc.\"\"\"\npayload = {\"text\": message}\ntry:\nrequests.post(webhook_url, json=payload)\nexcept Exception as e:\nlogger.error(f\"Failed to send alert: {str(e)}\")\n\ndef monitor_scraping_health(products):\n\"\"\"Monitor scraping health and send alerts if needed.\"\"\"\nif not products:\nsend_alert(\n\"âš ï¸ Warning: No products scraped from ProductHunt\",\nos.getenv(\"WEBHOOK_URL\")\n)\nreturn False\n\nif len(products) < 5:\nsend_alert(\nf\"âš ï¸ Warning: Only {len(products)} products scraped (expected 5)\",\nos.getenv(\"WEBHOOK_URL\")\n)\nreturn False\n\nreturn True\n\n```  \nThe monitoring functions above help ensure our scraper is working properly. The `send_alert()` function sends notifications to messaging platforms like Slack or Discord when issues occur, requiring a webhook URL configured in environment variables. The `monitor_scraping_health()` function checks if weâ€™re getting the expected amount of scraped data and triggers alerts if not. Learn more about setting up webhooks in [Discord](https://discord.com/developers/docs/resources/webhook) and [Slack](https://api.slack.com/messaging/webhooks).",
      "metadata": {
        "title": "How to Deploy Python Web Scrapers",
        "url": "https://www.firecrawl.dev/blog/deploy-web-scrapers"
      }
    },
    {
      "id": "bd053763-9a02-4cb2-9bad-25f3c1ef8fc7",
      "source": "firecrawl/blog/deploy-web-scrapers.md",
      "content": "Best Practices and Optimization > Data Management\n\nProper data management is crucial for a production web scraper. This includes validating the scraped data to ensure quality and consistency, as well as implementing efficient storage mechanisms to handle large volumes of data. Letâ€™s look at the key components.",
      "metadata": {
        "title": "How to Deploy Python Web Scrapers",
        "url": "https://www.firecrawl.dev/blog/deploy-web-scrapers"
      }
    },
    {
      "id": "45bc7aad-e7b5-45c4-9b23-f7659338b775",
      "source": "firecrawl/blog/deploy-web-scrapers.md",
      "content": "Best Practices and Optimization > Data Management > Implement data validation\n\n```python\nfrom typing import Optional\nfrom datetime import datetime\n\nclass ProductValidator:\n@staticmethod\ndef validate_product(product: dict) -> Optional[str]:\n\"\"\"Validate product data and return error message if invalid.\"\"\"\nrequired_fields = ['name', 'description', 'url', 'topics']\n\nfor field in required_fields:\nif not product.get(field):\nreturn f\"Missing required field: {field}\"\n\nif not isinstance(product.get('n_upvotes'), int):\nreturn \"Invalid upvote count\"\n\nif not product.get('url').startswith('http'):\nreturn \"Invalid URL format\"\n\nreturn None\n\ndef validate_products(products: list) -> list:\n\"\"\"Validate and filter products.\"\"\"\nvalid_products = []\n\nfor product in products:\nerror = ProductValidator.validate_product(product)\nif error:\nlogger.warning(f\"Invalid product data: {error}\")\ncontinue\nvalid_products.append(product)\n\nreturn valid_products\n\n```  \nA class like `ProductValidator` is important for ensuring data quality and consistency in our web scraping pipeline. It validates product data against required fields and format specifications before storage. This validation step helps prevent corrupted or incomplete data from entering our system, making downstream processing more reliable. The class provides static methods to validate individual products and entire product lists, checking for required fields like name and description, proper URL formatting, and valid upvote counts.",
      "metadata": {
        "title": "How to Deploy Python Web Scrapers",
        "url": "https://www.firecrawl.dev/blog/deploy-web-scrapers"
      }
    },
    {
      "id": "4d7d4808-342f-400b-a699-e75a8164674b",
      "source": "firecrawl/blog/deploy-web-scrapers.md",
      "content": "Best Practices and Optimization > Data Management > Implement efficient storage\n\n```python\nimport json\nimport gzip\nfrom pathlib import Path\n\nclass DataManager:\ndef __init__(self, base_dir: str = \"data\"):\nself.base_dir = Path(base_dir)\nself.base_dir.mkdir(exist_ok=True)\n\ndef save_products(self, products: list, compress: bool = True):\n\"\"\"Save products with optional compression.\"\"\"\ndate_str = datetime.now().strftime(\"%Y_%m_%d\")\n\nif compress:\nfilename = self.base_dir / f\"ph_products_{date_str}.json.gz\"\nwith gzip.open(filename, 'wt', encoding='utf-8') as f:\njson.dump(products, f)\nelse:\nfilename = self.base_dir / f\"ph_products_{date_str}.json\"\nwith open(filename, 'w', encoding='utf-8') as f:\njson.dump(products, f)\n\ndef load_products(self, date_str: str) -> list:\n\"\"\"Load products for a specific date.\"\"\"\ngz_file = self.base_dir / f\"ph_products_{date_str}.json.gz\"\njson_file = self.base_dir / f\"ph_products_{date_str}.json\"\n\nif gz_file.exists():\nwith gzip.open(gz_file, 'rt', encoding='utf-8') as f:\nreturn json.load(f)\nelif json_file.exists():\nwith open(json_file, 'r', encoding='utf-8') as f:\nreturn json.load(f)\nreturn []\n\n```  \nThe `DataManager` class extends our plain storage function from the previous sections. It provides a robust and organized way to handle data persistence for our web scraper. The class implements both compressed and uncompressed storage options using `gzip`, which helps optimize disk space usage while maintaining data accessibility. By organizing data by date and providing consistent file naming conventions, it enables easy tracking and retrieval of historical product data.",
      "metadata": {
        "title": "How to Deploy Python Web Scrapers",
        "url": "https://www.firecrawl.dev/blog/deploy-web-scrapers"
      }
    },
    {
      "id": "5c4201ac-2621-467f-b94f-755b0360a81a",
      "source": "firecrawl/blog/deploy-web-scrapers.md",
      "content": "Conclusion\n\nAnd thatâ€™s a wrap! Weâ€™ve covered several ways to deploy web scrapers in 2025, from simple GitHub Actions to more complex setups with Heroku and PythonAnywhere. Each method has its own sweet spot:  \n- GitHub Actions: Great for simple scrapers that run daily/weekly\n- Heroku: Perfect for more frequent scraping with its flexible scheduler\n- PythonAnywhere: Solid choice for beginners with its user-friendly interface  \nRemember, start small and scale up as needed. No need to jump straight into complex setups if GitHub Actions does the job. The best deployment method is the one that matches your specific needs and technical comfort level. ðŸ•·ï¸  \nHere are some related resources that might interest you:  \n- [GitHub Actions documentation](https://docs.github.com/en/actions)\n- [Firecrawl documentation](https://docs.firecrawl.dev/)\n- [Comprehensive guide on Firecrawlâ€™s `scrape_url` method](https://www.firecrawl.dev/blog/mastering-firecrawl-scrape-endpoint)\n- [How to generate sitemaps in Python using Firecrawl](https://www.firecrawl.dev/blog/how-to-generate-sitemaps-using-firecrawl-map-endpoint)  \nThank you for reading!  \nArticle updated recently  \n[ðŸ”¥](https://www.firecrawl.dev/)",
      "metadata": {
        "title": "How to Deploy Python Web Scrapers",
        "url": "https://www.firecrawl.dev/blog/deploy-web-scrapers"
      }
    },
    {
      "id": "ea7f4ea9-f0b9-4cfb-be79-7c378d54e39a",
      "source": "firecrawl/blog/deploy-web-scrapers.md",
      "content": "Ready to _Build?_\n\nStart scraping web data for your AI apps today.  \nNo credit card needed.  \nGet Started",
      "metadata": {
        "title": "How to Deploy Python Web Scrapers",
        "url": "https://www.firecrawl.dev/blog/deploy-web-scrapers"
      }
    },
    {
      "id": "a8e06d07-71e8-4fa0-bd41-be5de15de6f9",
      "source": "firecrawl/blog/deploy-web-scrapers.md",
      "content": "About the Author\n\n[![Bex Tuychiev image](https://www.firecrawl.dev/bex.jpg)\\\nBex Tuychiev@bextuychiev](https://x.com/bextuychiev)  \nBex is a Top 10 AI writer on Medium and a Kaggle Master with over 15k followers. He loves writing detailed guides, tutorials, and notebooks on complex data science and machine learning topics",
      "metadata": {
        "title": "How to Deploy Python Web Scrapers",
        "url": "https://www.firecrawl.dev/blog/deploy-web-scrapers"
      }
    },
    {
      "id": "22d2ab8e-63b5-4378-a26f-0e4fe144fd60",
      "source": "firecrawl/blog/deploy-web-scrapers.md",
      "content": "About the Author > More articles by Bex Tuychiev\n\n[Building an Automated Price Tracking Tool\\\n\\\nBuild an automated e-commerce price tracker in Python. Learn web scraping, price monitoring, and automated alerts using Firecrawl, Streamlit, PostgreSQL.](https://www.firecrawl.dev/blog/automated-price-tracking-tutorial-python) [Web Scraping Automation: How to Run Scrapers on a Schedule\\\n\\\nLearn how to automate web scraping in Python using free tools like schedule, asyncio, cron jobs and GitHub Actions. This comprehensive guide covers local and cloud-based scheduling methods to run scrapers reliably in 2025.](https://www.firecrawl.dev/blog/automated-web-scraping-free-2025) [Automated Data Collection - A Comprehensive Guide\\\n\\\nLearn how to build robust automated data collection systems using modern tools and best practices. This guide covers everything from selecting the right tools to implementing scalable collection pipelines.](https://www.firecrawl.dev/blog/automated-data-collection-guide) [BeautifulSoup4 vs. Scrapy - A Comprehensive Comparison for Web Scraping in Python\\\n\\\nLearn the key differences between BeautifulSoup4 and Scrapy for web scraping in Python. Compare their features, performance, and use cases to choose the right tool for your web scraping needs.](https://www.firecrawl.dev/blog/beautifulsoup4-vs-scrapy-comparison) [How to Build an Automated Competitor Price Monitoring System with Python\\\n\\\nLearn how to build an automated competitor price monitoring system in Python that tracks prices across e-commerce sites, provides real-time comparisons, and maintains price history using Firecrawl, Streamlit, and GitHub Actions.](https://www.firecrawl.dev/blog/automated-competitor-price-scraping) [Scraping Company Data and Funding Information in Bulk With Firecrawl and Claude\\\n\\\nLearn how to build a web scraper in Python that gathers company details, funding rounds, and investor information from public sources like Crunchbase using Firecrawl and Claude for automated data collection and analysis.](https://www.firecrawl.dev/blog/crunchbase-scraping-with-firecrawl-claude) [Data Enrichment: A Complete Guide to Enhancing Your Data Quality\\\n\\\nLearn how to enrich your data quality with a comprehensive guide covering data enrichment tools, best practices, and real-world examples. Discover how to leverage modern solutions like Firecrawl to automate data collection, validation, and integration for better business insights.](https://www.firecrawl.dev/blog/complete-guide-to-data-enrichment) [Building an Intelligent Code Documentation RAG Assistant with DeepSeek and Firecrawl\\\n\\\nLearn how to build an intelligent documentation assistant powered by DeepSeek and RAG (Retrieval Augmented Generation) that can answer questions about any documentation website by combining language models with efficient information retrieval.](https://www.firecrawl.dev/blog/deepseek-rag-documentation-assistant)",
      "metadata": {
        "title": "How to Deploy Python Web Scrapers",
        "url": "https://www.firecrawl.dev/blog/deploy-web-scrapers"
      }
    },
    {
      "id": "c6e9fb73-6b3b-40d1-bfa3-cc79253f7dc4",
      "source": "firecrawl/blog/scrape-analyze-airbnb-data-with-e2b.md",
      "content": "---\ntitle: Scrape and Analyze Airbnb Data with Firecrawl and E2B\nurl: https://www.firecrawl.dev/blog/scrape-analyze-airbnb-data-with-e2b\n---  \nIntroducing /extract - Get web data with a prompt [Try now](https://www.firecrawl.dev/extract)  \nMay 23, 2024  \nâ€¢  \n[![Nicolas Camara image](https://www.firecrawl.dev/nick-img.jpeg)Nicolas Camara](https://x.com/nickscamara_)  \n# Scrape and Analyze Airbnb Data with Firecrawl and E2B  \n![Scrape and Analyze Airbnb Data with Firecrawl and E2B image](https://www.firecrawl.dev/images/blog/firecrawl-e2b-airbnb.png)  \nThis cookbook demonstrates how to scrape Airbnb data and analyze it using [Firecrawl](https://www.firecrawl.dev/) and the [Code Interpreter SDK](https://github.com/e2b-dev/code-interpreter) from E2B.  \nFeel free to clone the [Github Repository](https://github.com/e2b-dev/e2b-cookbook) or follow along with the steps below.",
      "metadata": {
        "title": "Scrape and Analyze Airbnb Data with Firecrawl and E2B",
        "url": "https://www.firecrawl.dev/blog/scrape-analyze-airbnb-data-with-e2b"
      }
    },
    {
      "id": "9ca443f3-cd8b-4c4c-857d-41205799d1bd",
      "source": "firecrawl/blog/scrape-analyze-airbnb-data-with-e2b.md",
      "content": "Prerequisites\n\n- Node.js installed on your machine\n- Get [E2B API key](https://e2b.dev/docs/getting-started/api-key)\n- Get [Firecrawl API key](https://firecrawl.dev/)\n- Get [Anthropic API key](https://anthropic.com/)",
      "metadata": {
        "title": "Scrape and Analyze Airbnb Data with Firecrawl and E2B",
        "url": "https://www.firecrawl.dev/blog/scrape-analyze-airbnb-data-with-e2b"
      }
    },
    {
      "id": "acc063cb-58e1-4a06-93fb-b51894f72a81",
      "source": "firecrawl/blog/scrape-analyze-airbnb-data-with-e2b.md",
      "content": "Setup\n\nStart by creating a new directory and initializing a new Node.js typescript project:  \n```bash\nmkdir airbnb-analysis\ncd airbnb-analysis\nnpm init -y\n\n```  \nNext, install the required dependencies:  \n```bash\nnpm install @anthropic-ai/sdk @e2b/code-interpreter @mendable/firecrawl-js\n\n```  \nAnd dev dependencies:  \n```bash\nnpm install --save-dev @types/node prettier tsx typescript dotenv zod\n\n```",
      "metadata": {
        "title": "Scrape and Analyze Airbnb Data with Firecrawl and E2B",
        "url": "https://www.firecrawl.dev/blog/scrape-analyze-airbnb-data-with-e2b"
      }
    },
    {
      "id": "a47c3d86-bad5-4a33-a38c-458778d89cce",
      "source": "firecrawl/blog/scrape-analyze-airbnb-data-with-e2b.md",
      "content": "Create a `.env` file\n\nCreate a `.env` file in the root of your project and add the following environment variables:  \n```bash\n# TODO: Get your E2B API key from https://e2b.dev/docs\nE2B_API_KEY=\"\"\n\n# TODO: Get your Firecrawl API key from https://firecrawl.dev\nFIRECRAWL_API_KEY=\"\"\n\n# TODO: Get your Anthropic API key from https://anthropic.com\nANTHROPIC_API_KEY=\"\"\n\n```",
      "metadata": {
        "title": "Scrape and Analyze Airbnb Data with Firecrawl and E2B",
        "url": "https://www.firecrawl.dev/blog/scrape-analyze-airbnb-data-with-e2b"
      }
    },
    {
      "id": "f47c9dd5-94a6-4fff-a364-faa00e109e99",
      "source": "firecrawl/blog/scrape-analyze-airbnb-data-with-e2b.md",
      "content": "Scrape Airbnb data with Firecrawl\n\nCreate a new file `scraping.ts`.",
      "metadata": {
        "title": "Scrape and Analyze Airbnb Data with Firecrawl and E2B",
        "url": "https://www.firecrawl.dev/blog/scrape-analyze-airbnb-data-with-e2b"
      }
    },
    {
      "id": "d10e14b1-0612-48ce-b03f-1bf29d5b31e3",
      "source": "firecrawl/blog/scrape-analyze-airbnb-data-with-e2b.md",
      "content": "Scrape Airbnb data with Firecrawl > Creating the scraping function\n\n```typescript\nimport * as fs from \"fs\";\nimport FirecrawlApp from \"@mendable/firecrawl-js\";\nimport \"dotenv/config\";\nimport { config } from \"dotenv\";\nimport { z } from \"zod\";\n\n```  \n2. Letâ€™s define our `scrapeAirbnb` function which uses Firecrawl to scrape Airbnb listings. We will use Firecrawlâ€™s LLM Extract to try to get the pagination links and then scrape each page in parallel to get the listings. We will save to a JSON file so we can analyze it later and not have to re-scrape.  \n```typescript\nexport async function scrapeAirbnb() {\ntry {\n// Initialize the FirecrawlApp with your API key\nconst app = new FirecrawlApp({\napiKey: process.env.FIRECRAWL_API_KEY,\n});\n\n// Define the URL to crawl\nconst listingsUrl =\n\"https://www.airbnb.com/s/San-Francisco--CA--United-States/homes\";\n\nconst baseUrl = \"https://www.airbnb.com\";\n// Define schema to extract pagination links\nconst paginationSchema = z.object({\npage_links: z\n.array(\nz.object({\nlink: z.string(),\n}),\n)\n.describe(\"Pagination links in the bottom of the page.\"),\n});\n\nconst params2 = {\npageOptions: {\nonlyMainContent: false,\n},\nextractorOptions: {\nextractionSchema: paginationSchema,\n},\ntimeout: 50000, // if needed, sometimes airbnb stalls...\n};\n\n// Start crawling to get pagination links\nconst linksData = await app.scrapeUrl(listingsUrl, params2);\nconsole.log(linksData.data[\"llm_extraction\"]);\n\nlet paginationLinks = linksData.data[\"llm_extraction\"].page_links.map(\n(link) => baseUrl + link.link,\n);\n\n// Just in case is not able to get the pagination links\nif (paginationLinks.length === 0) {\npaginationLinks = [listingsUrl];\n}\n\n// Define schema to extract listings\nconst schema = z.object({\nlistings: z\n.array(\nz.object({\ntitle: z.string(),\nprice_per_night: z.number(),\nlocation: z.string(),\nrating: z.number().optional(),\nreviews: z.number().optional(),\n}),\n)\n.describe(\"Airbnb listings in San Francisco\"),\n});\n\nconst params = {\npageOptions: {\nonlyMainContent: false,\n},\nextractorOptions: {\nextractionSchema: schema,\n},\n};\n\n// Function to scrape a single URL\nconst scrapeListings = async (url) => {\nconst result = await app.scrapeUrl(url, params);\nreturn result.data[\"llm_extraction\"].listings;\n};\n\n// Scrape all pagination links in parallel\nconst listingsPromises = paginationLinks.map((link) =>\nscrapeListings(link),\n);\nconst listingsResults = await Promise.all(listingsPromises);\n\n// Flatten the results\nconst allListings = listingsResults.flat();\n\n// Save the listings to a file\nfs.writeFileSync(\n\"airbnb_listings.json\",\nJSON.stringify(allListings, null, 2),\n);\n// Read the listings from the file\nconst listingsData = fs.readFileSync(\"airbnb_listings.json\", \"utf8\");\nreturn listingsData;\n} catch (error) {\nconsole.error(\"An error occurred:\", error.message);\n}\n}\n\n```",
      "metadata": {
        "title": "Scrape and Analyze Airbnb Data with Firecrawl and E2B",
        "url": "https://www.firecrawl.dev/blog/scrape-analyze-airbnb-data-with-e2b"
      }
    },
    {
      "id": "57386256-655a-4ba6-83fc-5d8722492590",
      "source": "firecrawl/blog/scrape-analyze-airbnb-data-with-e2b.md",
      "content": "Scrape Airbnb data with Firecrawl > Creating the code interpreter\n\nLetâ€™s now prepare our code interepreter to analyze the data. Create a new file `codeInterpreter.ts`.  \nThis is where we will use the E2B Code Interpreter SDK to safely run the code that the LLM will generate and get its output.  \n```typescript\nimport { CodeInterpreter } from \"@e2b/code-interpreter\";\n\nexport async function codeInterpret(\ncodeInterpreter: CodeInterpreter,\ncode: string,\n) {\nconsole.log(\n`n${\"=\".repeat(50)}n> Running following AI-generated code:\nn${code}n${\"=\".repeat(50)}`,\n);\n\nconst exec = await codeInterpreter.notebook.execCell(code, {\n// You can stream logs from the code interpreter\n// onStderr: (stderr: string) => console.log(\"n[Code Interpreter stdout]\", stderr),\n// onStdout: (stdout: string) => console.log(\"n[Code Interpreter stderr]\", stdout),\n//\n// You can also stream additional results like charts, images, etc.\n// onResult: ...\n});\n\nif (exec.error) {\nconsole.log(\"[Code Interpreter error]\", exec.error); // Runtime error\nreturn undefined;\n}\n\nreturn exec;\n}\n\n```",
      "metadata": {
        "title": "Scrape and Analyze Airbnb Data with Firecrawl and E2B",
        "url": "https://www.firecrawl.dev/blog/scrape-analyze-airbnb-data-with-e2b"
      }
    },
    {
      "id": "67b85bb3-ea0d-46b9-9a89-c189af2cdded",
      "source": "firecrawl/blog/scrape-analyze-airbnb-data-with-e2b.md",
      "content": "Scrape Airbnb data with Firecrawl > Preparing the model prompt and tool execution\n\nCreate a file called `model.ts` that will contain the prompts, model names and the tools for execution.  \n```typescript\nimport { Tool } from \"@anthropic-ai/sdk/src/resources/beta/tools\";\n\nexport const MODEL_NAME = \"claude-3-opus-20240229\";\n\nexport const SYSTEM_PROMPT = `\n## your job & context\nyou are a python data scientist. you are given tasks to complete and you run python code to solve them.\n- the python code runs in jupyter notebook.\n- every time you call `execute_python` tool, the python code is executed in a separate cell. it's okay to multiple calls to `execute_python`.\n- display visualizations using matplotlib or any other visualization library directly in the notebook. don't worry about saving the visualizations to a file.\n- you have access to the internet and can make api requests.\n- you also have access to the filesystem and can read/write files.\n- you can install any pip package (if it exists) if you need to but the usual packages for data analysis are already preinstalled.\n- you can run any python code you want, everything is running in a secure sandbox environment.\n`;\n\nexport const tools: Tool[] = [\\\n{\\\nname: \"execute_python\",\\\ndescription:\\\n\"Execute python code in a Jupyter notebook cell and returns any result, stdout, stderr, display_data, and error.\",\\\ninput_schema: {\\\ntype: \"object\",\\\nproperties: {\\\ncode: {\\\ntype: \"string\",\\\ndescription: \"The python code to execute in a single cell.\",\\\n},\\\n},\\\nrequired: [\"code\"],\\\n},\\\n},\\\n];\n\n```",
      "metadata": {
        "title": "Scrape and Analyze Airbnb Data with Firecrawl and E2B",
        "url": "https://www.firecrawl.dev/blog/scrape-analyze-airbnb-data-with-e2b"
      }
    },
    {
      "id": "63e358b0-f11b-4b6b-bdf2-5d52d0f3e6a6",
      "source": "firecrawl/blog/scrape-analyze-airbnb-data-with-e2b.md",
      "content": "Scrape Airbnb data with Firecrawl > Putting it all together\n\nCreate a file `index.ts` to run the scraping and analysis. Here we will load the scraped data, send it to the LLM model, and then interpret the code generated by the model.  \n```typescript\nimport * as fs from \"fs\";\n\nimport \"dotenv/config\";\nimport { CodeInterpreter, Execution } from \"@e2b/code-interpreter\";\nimport Anthropic from \"@anthropic-ai/sdk\";\nimport { Buffer } from \"buffer\";\n\nimport { MODEL_NAME, SYSTEM_PROMPT, tools } from \"./model\";\n\nimport { codeInterpret } from \"./codeInterpreter\";\nimport { scrapeAirbnb } from \"./scraping\";\n\nconst anthropic = new Anthropic();\n/**\n* Chat with Claude to analyze the Airbnb data\n*/\nasync function chat(\ncodeInterpreter: CodeInterpreter,\nuserMessage: string,\n): Promise<Execution | undefined> {\nconsole.log(\"Waiting for Claude...\");\n\nconst msg = await anthropic.beta.tools.messages.create({\nmodel: MODEL_NAME,\nsystem: SYSTEM_PROMPT,\nmax_tokens: 4096,\nmessages: [{ role: \"user\", content: userMessage }],\ntools,\n});\n\nconsole.log(\n`n${\"=\".repeat(50)}nModel response:\n${msg.content}n${\"=\".repeat(50)}`,\n);\nconsole.log(msg);\n\nif (msg.stop_reason === \"tool_use\") {\nconst toolBlock = msg.content.find((block) => block.type === \"tool_use\");\nconst toolName = toolBlock?.name ?? \"\";\nconst toolInput = toolBlock?.input ?? \"\";\n\nconsole.log(\n`n${\"=\".repeat(50)}nUsing tool:\n${toolName}n${\"=\".repeat(50)}`,\n);\n\nif (toolName === \"execute_python\") {\nconst code = toolInput.code;\nreturn codeInterpret(codeInterpreter, code);\n}\nreturn undefined;\n}\n}\n/**\n* Main function to run the scraping and analysis\n*/\nasync function run() {\n// Load the Airbnb prices data from the JSON file\nlet data;\nconst readDataFromFile = () => {\ntry {\nreturn fs.readFileSync(\"airbnb_listings.json\", \"utf8\");\n} catch (err) {\nif (err.code === \"ENOENT\") {\nconsole.log(\"File not found, scraping data...\");\nreturn null;\n} else {\nthrow err;\n}\n}\n};\n\nconst fetchData = async () => {\ndata = readDataFromFile();\nif (!data || data.trim() === \"[]\") {\nconsole.log(\"File is empty or contains an empty list, scraping data...\");\ndata = await scrapeAirbnb();\n}\n};\n\nawait fetchData();\n\n// Parse the JSON data\nconst prices = JSON.parse(data);\n\n// Convert prices array to a string representation of a Python list\nconst pricesList = JSON.stringify(prices);\n\nconst userMessage = `\nLoad the Airbnb prices data from the airbnb listing below and visualize\nthe distribution of prices with a histogram. Listing data: ${pricesList}\n`;\n\nconst codeInterpreter = await CodeInterpreter.create();\nconst codeOutput = await chat(codeInterpreter, userMessage);\nif (!codeOutput) {\nconsole.log(\"No code output\");\nreturn;\n}\n\nconst logs = codeOutput.logs;\nconsole.log(logs);\n\nif (codeOutput.results.length == 0) {\nconsole.log(\"No results\");\nreturn;\n}\n\nconst firstResult = codeOutput.results[0];\nconsole.log(firstResult.text);\n\nif (firstResult.png) {\nconst pngData = Buffer.from(firstResult.png, \"base64\");\nconst filename = \"airbnb_prices_chart.png\";\nfs.writeFileSync(filename, pngData);\nconsole.log(`âœ… Saved chart to ${filename}`);\n}\n\nawait codeInterpreter.close();\n}\n\nrun();\n\n```",
      "metadata": {
        "title": "Scrape and Analyze Airbnb Data with Firecrawl and E2B",
        "url": "https://www.firecrawl.dev/blog/scrape-analyze-airbnb-data-with-e2b"
      }
    },
    {
      "id": "a4ff48ce-44e1-4fd2-851e-80d29e3875f5",
      "source": "firecrawl/blog/scrape-analyze-airbnb-data-with-e2b.md",
      "content": "Scrape Airbnb data with Firecrawl > Running the code\n\nRun the code with:  \n```bash\nnpm run start\n\n```",
      "metadata": {
        "title": "Scrape and Analyze Airbnb Data with Firecrawl and E2B",
        "url": "https://www.firecrawl.dev/blog/scrape-analyze-airbnb-data-with-e2b"
      }
    },
    {
      "id": "97a7b814-18bd-4636-b307-17a0c097bb29",
      "source": "firecrawl/blog/scrape-analyze-airbnb-data-with-e2b.md",
      "content": "Scrape Airbnb data with Firecrawl > Results\n\nAt the end you should get a histogram of the Airbnb prices in San Francisco saved as `airbnb_prices_chart.png`.  \n![Airbnb Prices Chart](https://www.firecrawl.dev/images/blog/airbnb_prices_chart.png)  \nThatâ€™s it! You have successfully scraped Airbnb data and analyzed it using Firecrawl and E2Bâ€™s Code Interpreter SDK. Feel free to experiment with different models and prompts to get more insights from the data.  \nArticle updated recently  \n[ðŸ”¥](https://www.firecrawl.dev/)",
      "metadata": {
        "title": "Scrape and Analyze Airbnb Data with Firecrawl and E2B",
        "url": "https://www.firecrawl.dev/blog/scrape-analyze-airbnb-data-with-e2b"
      }
    },
    {
      "id": "250ab809-5d1a-4dbf-9a62-e45b58b5ad34",
      "source": "firecrawl/blog/scrape-analyze-airbnb-data-with-e2b.md",
      "content": "Ready to _Build?_\n\nStart scraping web data for your AI apps today.  \nNo credit card needed.  \nGet Started",
      "metadata": {
        "title": "Scrape and Analyze Airbnb Data with Firecrawl and E2B",
        "url": "https://www.firecrawl.dev/blog/scrape-analyze-airbnb-data-with-e2b"
      }
    },
    {
      "id": "689ff6d3-6e4d-4086-a4e3-77bd179c3bb0",
      "source": "firecrawl/blog/scrape-analyze-airbnb-data-with-e2b.md",
      "content": "About the Author\n\n[![Nicolas Camara image](https://www.firecrawl.dev/nick-img.jpeg)\\\nNicolas Camara@nickscamara_](https://x.com/nickscamara_)  \nNicolas Camara is the Chief Technology Officer (CTO) at Firecrawl.\nHe previously built and scaled Mendable, one of the pioneering \"chat with your documents\" apps,\nwhich had major Fortune 500 customers like Snapchat, Coinbase, and MongoDB.\nPrior to that, Nicolas built SideGuide, the first code-learning tool inside VS Code,\nand grew a community of 50,000 users. Nicolas studied Computer Science and has over 10 years of experience in building software.",
      "metadata": {
        "title": "Scrape and Analyze Airbnb Data with Firecrawl and E2B",
        "url": "https://www.firecrawl.dev/blog/scrape-analyze-airbnb-data-with-e2b"
      }
    },
    {
      "id": "061dabb1-999a-49d0-aa99-11ba7b200d28",
      "source": "firecrawl/blog/scrape-analyze-airbnb-data-with-e2b.md",
      "content": "About the Author > More articles by Nicolas Camara\n\n[Using OpenAI's Realtime API and Firecrawl to Talk with Any Website\\\n\\\nBuild a real-time conversational agent that interacts with any website using OpenAI's Realtime API and Firecrawl.](https://www.firecrawl.dev/blog/How-to-Talk-with-Any-Website-Using-OpenAIs-Realtime-API-and-Firecrawl) [Extract website data using LLMs\\\n\\\nLearn how to use Firecrawl and Groq to extract structured data from a web page in a few lines of code.](https://www.firecrawl.dev/blog/data-extraction-using-llms) [Getting Started with Grok-2: Setup and Web Crawler Example\\\n\\\nA detailed guide on setting up Grok-2 and building a web crawler using Firecrawl.](https://www.firecrawl.dev/blog/grok-2-setup-and-web-crawler-example) [Launch Week I / Day 6: LLM Extract (v1)\\\n\\\nExtract structured data from your web pages using the extract format in /scrape.](https://www.firecrawl.dev/blog/launch-week-i-day-6-llm-extract) [Launch Week I / Day 7: Crawl Webhooks (v1)\\\n\\\nNew /crawl webhook support. Send notifications to your apps during a crawl.](https://www.firecrawl.dev/blog/launch-week-i-day-7-webhooks) [OpenAI Swarm Tutorial: Create Marketing Campaigns for Any Website\\\n\\\nA guide to building a multi-agent system using OpenAI Swarm and Firecrawl for AI-driven marketing strategies](https://www.firecrawl.dev/blog/openai-swarm-agent-tutorial) [Build a 'Chat with website' using Groq Llama 3\\\n\\\nLearn how to use Firecrawl, Groq Llama 3, and Langchain to build a 'Chat with your website' bot.](https://www.firecrawl.dev/blog/chat-with-website) [Scrape and Analyze Airbnb Data with Firecrawl and E2B\\\n\\\nLearn how to scrape and analyze Airbnb data using Firecrawl and E2B in a few lines of code.](https://www.firecrawl.dev/blog/scrape-analyze-airbnb-data-with-e2b)",
      "metadata": {
        "title": "Scrape and Analyze Airbnb Data with Firecrawl and E2B",
        "url": "https://www.firecrawl.dev/blog/scrape-analyze-airbnb-data-with-e2b"
      }
    },
    {
      "id": "4a49d142-2fd4-4a51-8af0-f736bd213902",
      "source": "firecrawl/blog/an-adventure-in-scaling.md",
      "content": "---\ntitle: Handling 300k requests per day: an adventure in scaling\nurl: https://www.firecrawl.dev/blog/an-adventure-in-scaling\n---  \nIntroducing /extract - Get web data with a prompt [Try now](https://www.firecrawl.dev/extract)  \nSep 13, 2024  \nâ€¢  \n[![GergÅ‘ MÃ³ricz (mogery) image](https://www.firecrawl.dev/mogery.jpg)GergÅ‘ MÃ³ricz (mogery)](https://x.com/mo_geryy)  \n# Handling 300k requests per day: an adventure in scaling  \n![Handling 300k requests per day: an adventure in scaling image](https://www.firecrawl.dev/images/blog/an-adventure-in-scaling.jpg)  \nWhen I joined the Firecrawl team in early July, we spent most of our time working on new features and minor bugfixes. Life was good â€” we could focus mostly on shipping shiny new stuff without worrying as much about architecture and server load. However, as we grew over time, we started experiencing the â€œhug of deathâ€ a lot more. People loved our product so much that our architecture couldnâ€™t take it anymore, and every day there was a brand new fire to put out. We knew that this was unsustainable, and ultimately it damages our DX more than any new feature we could put out could make up for. We knew we had to change things, stat.",
      "metadata": {
        "title": "Handling 300k requests per day: an adventure in scaling",
        "url": "https://www.firecrawl.dev/blog/an-adventure-in-scaling"
      }
    },
    {
      "id": "8778dbbe-eb09-44e7-a5de-20945b87b69e",
      "source": "firecrawl/blog/an-adventure-in-scaling.md",
      "content": "Our architecture, before the storm\n\nWe host our API service on [Fly.io](https://fly.io/), which allows us to easily deploy our code in a Docker container. It also manages load balancing, log collection, zero-downtime deployment strategies, VPC management, and a whole load of other stuff for us, which is very useful.  \nOur main API service has two kinds of â€œprocessesâ€, as Fly calls it: `app` and `worker`.  \n`app` processes use Express to serve the main API, perform scrape requests (which take a relatively short time), and delegate crawls to `worker` processes using the [Bull](https://github.com/OptimalBits/bull) job queue.  \n`worker` processes register themselves as workers on the job queue, and perform crawls (which take a relatively long time).  \nBoth processes use Supabase to handle authentication and store data in Postgres. Bull also runs on top of Redis, which we deployed on [Railway](https://railway.app/), since itâ€™s super easy to use.",
      "metadata": {
        "title": "Handling 300k requests per day: an adventure in scaling",
        "url": "https://www.firecrawl.dev/blog/an-adventure-in-scaling"
      }
    },
    {
      "id": "4f86552e-bce1-44f1-850b-65684f8a5c49",
      "source": "firecrawl/blog/an-adventure-in-scaling.md",
      "content": "Locks are hard\n\nAs more and more people started using us, more and more people started finding bugs. We started getting odd issues with crawls sometimes being stuck for hours without any progress. I charted the timing of these crawls, and I saw that it was happening every time we redeployed.  \nDue to some miscellaneous memory leak issues, we were redeploying our entire service every 2 hours via GitHub Actions, in order to essentially restart all our machines. This killed all our workers, which had acquired locks for these crawl jobs. I was not too familiar with the codebase at this point, and I thought that these locks got hard-stuck on the dead workers, so I to add some code to release all of the current workerâ€™s locks on termination.  \nThis ended up being really complicated, due to multiple factors:  \n1. Other libraries we used also had cleanup code on `SIGTERM`. When you listen to `SIGTERM`, your app doesnâ€™t actually quit until the handler calls `process.exit()`. So, the other libraryâ€™s handler called `process.exit()` when its handler finished, which caused a race condition with our cleanup handler. (This was absolute hell to debug.)\n2. Fly.io sometimes didnâ€™t respect our configuration, and hard- `SIGKILL` ed our application before the 30 second timeout we specified our config. This cut our cleanup code short.\n3. There was no easy way to remove a lock via the Bull API. The only legitimate way it could be done was to:\n1. Get all in-progress jobs of this worker\n2. Set their status to failed\n3. Delete them from the queue\n4. Re-insert them to the queue\n4. While the cleanup code was running, there was no easy way to disable the current worker, so sometimes jobs the cleanup code re-inserted were immediately picked up by the same worker that was about to be shut down.\n5. Due to our rollover deployment strategy, during a deployment, the re-inserted jobs were picked up by workers that have not been updated yet. This caused all the jobs to be piled up on the last worker to be updated, which caused the cleanup code to run longer than Flyâ€™s maximum process shutdown timeout.  \nWhile I was going down a rabbithole that was spiraling out of control, Thomas (another Firecrawl engineer who mainly works on [Fire-Engine](https://www.firecrawl.dev/blog/introducing-fire-engine-for-firecrawl), which used a similar architecture) discovered that our queue lock options were grossly misconfigured:  \n```typescript\nwebScraperQueue = new Queue(\"web-scraper\", process.env.REDIS_URL, {\nsettings: {\nlockDuration: 2 * 60 * 60 * 1000, // 2 hours in milliseconds\nlockRenewTime: 30 * 60 * 1000, // 30 minutes in milliseconds\n},\n});\n\n```  \nThis was originally written with the understanding that `lockDuration` would be the maximum amount of time a job could take â€” which is not true. When a worker stops renewing the lock every `lockRenewTime` milliseconds, `lockDuration` specifies the amount of time to wait before declaring the job as `stalled` and giving it to another worker. This was causing the crawls to be locked up for 2 hours, similar to what our customers were reporting.  \nAfter I got rid of all my super-complex cleanup code, the fix ended up being this:  \n```typescript\nwebScraperQueue = new Queue(\"web-scraper\", process.env.REDIS_URL, {\nsettings: {\nlockDuration: 2 * 60 * 1000, // 1 minute in milliseconds\nlockRenewTime: 15 * 1000, // 15 seconds in milliseconds\n},\n});\n\n```  \nThank you Thomas for spotting that one and keeping me from going off the deep end!",
      "metadata": {
        "title": "Handling 300k requests per day: an adventure in scaling",
        "url": "https://www.firecrawl.dev/blog/an-adventure-in-scaling"
      }
    },
    {
      "id": "b975125e-0c53-4641-9608-874c2e4106f1",
      "source": "firecrawl/blog/an-adventure-in-scaling.md",
      "content": "Scaling scrape requests, the easy way\n\nAs you might have noticed in the architecture description, we were running scrape requests on the `app` process, the same one that serves our API. We were just starting a scrape in the `/v0/scrape` endpoint handler, and returning the results. This is simple to build, but it isnâ€™t sustainable.  \nWe had no idea how many scrape requests we were running and when, there was no way to retry failed scrape requests, we had no data source to scale the `app` process on (other than are we down or not), and we had to scrape Express along with it. We needed to move scraping to our `worker` process.  \nWe ended up choosing to just add scrape jobs to the same queue as crawling jobs. This way the `app` submitted the job, the `worker` completed it, and the `app` waited for it to be done and returned the data. [We read the old advice about â€œnever wait for jobs to finishâ€](https://blog.taskforce.sh/do-not-wait-for-your-jobs-to-complete/), but we decided to cautiously ignore it, since it would have ruined the amazing simplicity that the scrape endpoint has.  \nThis ended up [being surprisingly simple](https://github.com/mendableai/firecrawl/commit/6798695ee4daf1ce1b289db494d260d718b6752b#diff-6753e371514e1d188e797436080479e7c781d96183601ab8fa203e4df6ca0400), only slightly affected by Bullâ€™s odd API. We had to add a global event handler to check if the job had completed, since it lacked the [`Job.waitUntilFinished`](https://api.docs.bullmq.io/classes/v5.Job.html#waitUntilFinished) function that its successor [BullMQ](https://github.com/taskforcesh/bullmq) already had.  \nWe saw a huge drop in weird behaviour on our `app` machines, and we were able to scale them down in exchange for more `worker` machines, making us way faster.",
      "metadata": {
        "title": "Handling 300k requests per day: an adventure in scaling",
        "url": "https://www.firecrawl.dev/blog/an-adventure-in-scaling"
      }
    },
    {
      "id": "ac3165db-57be-4261-8026-65bc2d557c01",
      "source": "firecrawl/blog/an-adventure-in-scaling.md",
      "content": "Smaller is better\n\nThe redeploy crawl fiasco made us worried about handling big crawls. We could essentially 2x the time a big crawl ran if it was caught in the middle of a redeploy, which is sub-optimal. Some of our workers were also crashing with an OOM error when working on large crawls. We instead decided to break crawls down to individual scrape jobs that chain together and spawn new jobs when they find new URLs.  \nWe decided to make every job in the queue have a scrape type. Scrape jobs that are associated with crawls have an extra bit of metadata tying them to the crawlId. This crawlId refers to some redis keys that coordinate the crawling process.  \nThe crawl itself has some basic data including the origin URL, the team associated with the request, the robots.txt file, and others:  \n```typescript\nexport type StoredCrawl = {\noriginUrl: string;\ncrawlerOptions: any;\npageOptions: any;\nteam_id: string;\nplan: string;\nrobots?: string;\ncancelled?: boolean;\ncreatedAt: number;\n};\n\nexport async function saveCrawl(id: string, crawl: StoredCrawl) {\nawait redisConnection.set(\"crawl:\" + id, JSON.stringify(crawl));\nawait redisConnection.expire(\"crawl:\" + id, 24 * 60 * 60, \"NX\");\n}\n\nexport async function getCrawl(id: string): Promise<StoredCrawl | null> {\nconst x = await redisConnection.get(\"crawl:\" + id);\n\nif (x === null) {\nreturn null;\n}\n\nreturn JSON.parse(x);\n}\n\n```  \nWe also make heavy use of Redis sets to determine which URLs have been already visited when discovering new pages. The Redis `SADD` command adds a new element to a set. Since sets can only store unique values, it returns 1 or 0 based on whether the element was added or not. (The element does not get added if it was already in the set before.) We use this as a lock mechanism, to make sure two workers donâ€™t discover the same URL at the same time and add two jobs for them.  \n```typescript\nasync function lockURL(id: string, url: string): Promise<boolean> {\n// [...]\nconst res =\n(await redisConnection.sadd(\"crawl:\" + id + \":visited\", url)) !== 0;\n// [...]\nreturn res;\n}\n\nasync function onURLDiscovered(crawl: string, url: string) {\nif (await lockURL(crawl, url)) {\n// we are the first ones to discover this URL\nawait addScrapeJob(/* ... */); // add new job for this URL\n}\n}\n\n```  \nYou can take a look at the whole Redis logic around orchestrating crawls [here](https://github.com/mendableai/firecrawl/blob/main/apps/api/src/lib/crawl-redis.ts).  \nWith this change, we saw a huge performance improvement on crawls. This change also allowed us to perform multiple scrape requests of one crawl at the same time, while the old crawler had no scrape concurrency. We were able to stretch a crawl over all of our machines, maximizing the worth we get for each machine we pay for.",
      "metadata": {
        "title": "Handling 300k requests per day: an adventure in scaling",
        "url": "https://www.firecrawl.dev/blog/an-adventure-in-scaling"
      }
    },
    {
      "id": "9f5ffdf9-26b4-4933-8793-55e2723372a0",
      "source": "firecrawl/blog/an-adventure-in-scaling.md",
      "content": "Goodbye Bull, hello BullMQ\n\nEvery time we encountered Bull, we were slapped in the face by how much better BullMQ was. It had a better API, new features, and the most important thing of all: active maintenance. We decided to make the endeavour to switch over to it, first on Fire-Engine, and then on Firecrawl.  \nWith this change, we were able to drop the horrible code for [waiting for a job to complete](https://github.com/mendableai/firecrawl/blob/6798695ee4daf1ce1b289db494d260d718b6752b/apps/api/src/controllers/scrape.ts#L59-L89), and replace it all with `job.waitUntilFinished()`. We were also able to customize our workers to add Sentry instrumentation (more on that later), and to take on jobs based on CPU and RAM usage, instead of a useless max concurrency constant that we had to use with Bull.  \nBullMQ still has its API quirks (e.g. donâ€™t you dare call `Job.moveToCompleted` / `Job.moveToFailed` with the 3rd argument not set to `false`, otherwise you will check out and lock a job that will be returned to you that youâ€™re probably dropping)",
      "metadata": {
        "title": "Handling 300k requests per day: an adventure in scaling",
        "url": "https://www.firecrawl.dev/blog/an-adventure-in-scaling"
      }
    },
    {
      "id": "af0cbd3b-762e-4ed7-8ca7-a6ae3218c85c",
      "source": "firecrawl/blog/an-adventure-in-scaling.md",
      "content": "Our egress fee horror story\n\nOur changes made us super scalable, but they also meant that a lot more traffic was going through Redis. We ended up racking up a 15000$ bill on Railway in August, mostly on Redis egress fees only. This wasnâ€™t sustainable, and we needed to switch quickly.  \nAfter being disappointed with Upstash, and having issues with Dragonfly, we found a way to deploy Redis to Fly.io natively. [We put our own spin on the config](https://github.com/mendableai/firecrawl/blob/f7c4cee404e17b3ed201e005185a5041009d0e6f/apps/redis/fly.toml), and deployed it to our account. However, we were not able to reach the instance from the public IP using `redis-cli` (netcat worked though?!?!), which caused some confusion.  \nWe decided to go another way and use Flyâ€™s [Private Networking](https://fly.io/docs/networking/private-networking/), which provides a direct connection to a Fly app/machine without any load balancer being in front. We crafted a connection string, SSHâ€™d into one of our worker machines, installed `redis-cli`, tried to connect, andâ€¦ it worked! We had a reachable, stable Redis instance in front of us.  \nSo, we went to change the environment variable to the fancy new Fly.io Redis, we deployed the application, andâ€¦ we crashed. After a quick revert, we noticed that [IORedis](https://github.com/redis/ioredis) wasnâ€™t able to connect to the Redis instance, but `redis-cli` stilled worked fine. Soâ€¦ what gives?  \nTurns out, `ioredis` only performs a lookup for an IPv4 address, unless you specify `?family=6`, in which case it only performs a lookup for an IPv6 address. This is not documented anywhere, except in a couple of GitHub issues which are hard to search for. I have been coding for almost 11 years now, and this is the worst configuration quirk I have ever seen. (And I use Nix daily!) In 2024, it would be saner to look for IPv6 by default instead of IPv4. Why not look for both? This is incomprehensible to me.  \nAnyways, after appending `?family=6` to the string, everything worked, except, sometimes notâ€¦",
      "metadata": {
        "title": "Handling 300k requests per day: an adventure in scaling",
        "url": "https://www.firecrawl.dev/blog/an-adventure-in-scaling"
      }
    },
    {
      "id": "93c41dfa-a00d-4588-9482-015dfab688e3",
      "source": "firecrawl/blog/an-adventure-in-scaling.md",
      "content": "Awaiting forever\n\nWe started having huge waves of scrape timeouts. After a bit of investigation, the `Job.waitUntilFinished()` Promise never returned, but after looking at our BullMQ dashboard, we saw that jobs were actually being completed.  \nBullMQ uses Redis streams for all of its event firing/handling code, including `waitUntilFinished`, which waits until the jobâ€™s `finished` event fires. BullMQ enforces a maximum length for the event stream, in order to purge old events that have presumably already been handled, and it defaults to about 10000 maximum events. Under heavy load, our queue was firing so many events, that BullMQ was trimming events before they could be processed. This caused everything that depends on queue events to fail.  \nThis maximum events parameter is configurable, however, it seems like a parameter that weâ€™d have to babysit, and itâ€™s way too cryptic and too easy to forget about. Instead, we opted to rewrite the small amount of code that uses queue events to do polling instead, which is not affected by pub/sub issues like this.  \nInexplicably, this never happened on the old Railway Redis instance, but it happened on every alternative we tried (including Upstash and Dragonfly). Weâ€™re still not sure why we didnâ€™t run into this issue earlier, and BullMQ queue events still work happily on the Fire-Engine side under Dragonfly.",
      "metadata": {
        "title": "Handling 300k requests per day: an adventure in scaling",
        "url": "https://www.firecrawl.dev/blog/an-adventure-in-scaling"
      }
    },
    {
      "id": "eb248a43-a593-4a14-bf36-e7aef3f426b8",
      "source": "firecrawl/blog/an-adventure-in-scaling.md",
      "content": "Adding monitoring\n\nWe were growing tired of going through console logs to diagnose things. We were also worried about how many issues we could potentially be missing. So, we decided to integrate [Sentry](https://sentry.io/) for error and performance monitoring, because I had some great experiences with it in the past.  \nThe moment we added it, we found about 10 high-impact bugs that we had no idea about. I fixed them the day after. We also had an insight into what our services were actively doing â€” I was able to add custom instrumentation to BullMQ, and pass trace IDs over to Fire-Engine, so now we can view the entire process a scrape or crawl goes through until it finishes, all organized in one place.  \n![Sentry Trace view](https://www.firecrawl.dev/images/blog/scaling-sentry.png)  \n(The creation of this image for this post lead me to [decrease the time Firecrawl spends after Fire-Engine is already finished](https://github.com/mendableai/firecrawl/commit/000a316cc362b935976ac47b73ec02923f4175c5). Thanks, Sentry!)  \nSentry has been immensely useful in finding errors, debugging incidents, and improving performance. There is no longer a chance that we have an issue invisibly choking us. With Sentry we see everything that could be going wrong (super exciting to see AIOps tools like [Keep](https://www.keephq.dev/) popping up).",
      "metadata": {
        "title": "Handling 300k requests per day: an adventure in scaling",
        "url": "https://www.firecrawl.dev/blog/an-adventure-in-scaling"
      }
    },
    {
      "id": "60033721-a35b-4fd1-b455-d46852e6e2ad",
      "source": "firecrawl/blog/an-adventure-in-scaling.md",
      "content": "The future\n\nWe are currently stable. I was on-call last weekend and I forgot about it. The phone never rang. It felt very weird after putting out fires for so long, but our investment absolutely paid off. It allowed us to do [our launch week](https://www.firecrawl.dev/blog/firecrawl-launch-week-1-recap), which would not have been possible if we were in panic mode 24/7. It has also allowed our customers to build with confidence, as the increased reliabilty adds another layer of greatness to Firecrawl.  \nHowever, there are still things weâ€™re unhappy with. Fly, while very useful early-stage, doesnâ€™t let us smoothly autoscale. We are currently setting up Kubernetes to give us more control over our scaling.  \nI love making Firecrawl better, be it with features or with added reliability. Weâ€™re in a good place right now, but Iâ€™m sure there will be a lot more adventures with scaling in the future. I hope this post has been useful, since surprisingly few people talk about all this stuff. (We sure had trouble finding resources when we were trying to fix things.) I will likely be back with a part 2 when thereâ€™s more exciting things to talk about.  \nArticle updated recently  \n[ðŸ”¥](https://www.firecrawl.dev/)",
      "metadata": {
        "title": "Handling 300k requests per day: an adventure in scaling",
        "url": "https://www.firecrawl.dev/blog/an-adventure-in-scaling"
      }
    },
    {
      "id": "ff9b37e4-7de4-4201-acbd-b79addc641a7",
      "source": "firecrawl/blog/an-adventure-in-scaling.md",
      "content": "Ready to _Build?_\n\nStart scraping web data for your AI apps today.  \nNo credit card needed.  \nGet Started",
      "metadata": {
        "title": "Handling 300k requests per day: an adventure in scaling",
        "url": "https://www.firecrawl.dev/blog/an-adventure-in-scaling"
      }
    },
    {
      "id": "2c640e8b-ae70-4d04-821d-f0e98f7fb019",
      "source": "firecrawl/blog/an-adventure-in-scaling.md",
      "content": "About the Author\n\n[![GergÅ‘ MÃ³ricz (mogery) image](https://www.firecrawl.dev/mogery.jpg)\\\nGergÅ‘ MÃ³ricz (mogery)@mo_geryy](https://x.com/mo_geryy)  \nGergÅ‘ MÃ³ricz is a Software Engineer at Firecrawl. He works on scaling, monitoring, designing new APIs and features, putting out fires, customer support, and everything else there is to do at a tech startup.\nPreviously coded and scaled a hospitality tech startup, and contributed to Mendable on GitHub.",
      "metadata": {
        "title": "Handling 300k requests per day: an adventure in scaling",
        "url": "https://www.firecrawl.dev/blog/an-adventure-in-scaling"
      }
    },
    {
      "id": "ce90ca3a-8be7-4cc9-a314-62a64077d547",
      "source": "firecrawl/blog/an-adventure-in-scaling.md",
      "content": "About the Author > More articles by GergÅ‘ MÃ³ricz (mogery)\n\n[Handling 300k requests per day: an adventure in scaling\\\n\\\nPutting out fires was taking up all our time, and we had to scale fast. This is how we did it.](https://www.firecrawl.dev/blog/an-adventure-in-scaling)",
      "metadata": {
        "title": "Handling 300k requests per day: an adventure in scaling",
        "url": "https://www.firecrawl.dev/blog/an-adventure-in-scaling"
      }
    },
    {
      "id": "d64de36b-0641-4a68-8954-f203aadc2392",
      "source": "firecrawl/blog/launch-week-ii-day-1-introducing-batch-scrape-endpoint.md",
      "content": "---\ntitle: Launch Week II - Day 1: Introducing the Batch Scrape Endpoint\nurl: https://www.firecrawl.dev/blog/launch-week-ii-day-1-introducing-batch-scrape-endpoint\n---  \nIntroducing /extract - Get web data with a prompt [Try now](https://www.firecrawl.dev/extract)  \nOctober 28, 2024  \nâ€¢  \n[![Eric Ciarla image](https://www.firecrawl.dev/eric-img.jpeg)Eric Ciarla](https://x.com/ericciarla)  \n# Launch Week II - Day 1: Introducing the Batch Scrape Endpoint  \n![Launch Week II - Day 1: Introducing the Batch Scrape Endpoint image](https://www.firecrawl.dev/images/blog/firecrawl-batch-scrape.jpg)  \nWelcome to Day 1 of Firecrawlâ€™s second Launch Week! Weâ€™re kicking things off with the introduction of our latest feature: the **Batch Scrape Endpoint**.  \n**Say Hello to the Batch Scrape Endpoint**  \nThe Batch Scrape endpoint is designed to help you scrape multiple URLs at once, streamlining your web scraping tasks and saving you valuable time. Whether youâ€™re dealing with a small list of pages or hundreds of URLs, this new endpoint makes bulk data retrieval more efficient than ever.  \n**How It Works**  \nSimilar to our existing `/crawl` endpoint, the Batch Scrape endpoint allows you to submit a job that processes multiple URLs in one go. You can choose between synchronous and asynchronous methods:  \n- **Synchronous Method**: Waits for the batch scrape job to complete and returns the results immediately.\n- **Asynchronous Method**: Returns a job ID right away, allowing you to check the job status and retrieve results when itâ€™s convenient for you.  \n**Getting Started with Batch Scrape**  \nUsing the Batch Scrape endpoint is straightforward. Hereâ€™s how you can get started with a simple cURL command:  \n```bash\ncurl -X POST https://api.firecrawl.dev/v1/batch/scrape \\\n-H 'Content-Type: application/json' \\\n-H 'Authorization: Bearer YOUR_API_KEY' \\\n-d '{\n\"urls\": [\"https://docs.firecrawl.dev\", \"https://docs.firecrawl.dev/sdks/overview\"],\n\"formats\": [\"markdown\", \"html\"]\n}'\n\n```  \n**Understanding the Response**  \nIf youâ€™re using the synchronous method, youâ€™ll receive the results directly:  \n```json\n{\n\"status\": \"completed\",\n\"total\": 2,\n\"completed\": 2,\n\"creditsUsed\": 2,\n\"expiresAt\": \"2024-10-21T00:00:00.000Z\",\n\"data\": [\\\n{\\\n\"markdown\": \"...\",\\\n\"html\": \"...\",\\\n\"metadata\": {\\\n\"title\": \"Firecrawl Documentation\",\\\n\"language\": \"en\",\\\n\"sourceURL\": \"https://docs.firecrawl.dev\",\\\n\"description\": \"Official documentation for Firecrawl.\",\\\n\"statusCode\": 200\\\n}\\\n},\\\n{\\\n\"markdown\": \"...\",\\\n\"html\": \"...\",\\\n\"metadata\": {\\\n\"title\": \"Firecrawl SDK Overview\",\\\n\"language\": \"en\",\\\n\"sourceURL\": \"https://docs.firecrawl.dev/sdks/overview\",\\\n\"description\": \"Overview of Firecrawl SDKs.\",\\\n\"statusCode\": 200\\\n}\\\n}\\\n]\n}\n\n```  \nIf you opt for the asynchronous method, youâ€™ll get a job ID to check the status later:  \n```json\n{\n\"success\": true,\n\"id\": \"abc-123-def-456\",\n\"url\": \"https://api.firecrawl.dev/v1/batch/scrape/abc-123-def-456\"\n}\n\n```  \nTo check the job status and retrieve results, use the job ID:  \n```bash\ncurl -X GET https://api.firecrawl.dev/v1/batch/scrape/abc-123-def-456 \\\n-H 'Authorization: Bearer YOUR_API_KEY'\n\n```  \n**Why Use Batch Scrape?**  \n- **Efficiency**: Process multiple URLs in a single request, reducing network overhead.\n- **Flexibility**: Choose between synchronous and asynchronous methods based on your applicationâ€™s needs.\n- **Customization**: Specify output formats like Markdown or HTML to suit your data processing workflows.  \n**Whatâ€™s Next?**  \nWeâ€™re just getting started with Launch Week II! The Batch Scrape endpoint is the first of several new features weâ€™re unveiling this week to enhance your web scraping capabilities.  \nWeâ€™d love to hear how you plan to use the Batch Scrape endpoint in your projects. Your feedback helps us improve and tailor our services to better meet your needs.  \nHappy scraping, and stay tuned for Day 2 of [Launch Week II](https://www.firecrawl.dev/launch-week) tomorrow!  \nArticle updated recently  \n[ðŸ”¥](https://www.firecrawl.dev/)",
      "metadata": {
        "title": "Launch Week II - Day 1: Introducing the Batch Scrape Endpoint",
        "url": "https://www.firecrawl.dev/blog/launch-week-ii-day-1-introducing-batch-scrape-endpoint"
      }
    },
    {
      "id": "c43a694a-e89c-4941-8b89-e4c21e6929bf",
      "source": "firecrawl/blog/launch-week-ii-day-1-introducing-batch-scrape-endpoint.md",
      "content": "Ready to _Build?_\n\nStart scraping web data for your AI apps today.  \nNo credit card needed.  \nGet Started",
      "metadata": {
        "title": "Launch Week II - Day 1: Introducing the Batch Scrape Endpoint",
        "url": "https://www.firecrawl.dev/blog/launch-week-ii-day-1-introducing-batch-scrape-endpoint"
      }
    },
    {
      "id": "a031a882-9300-4c25-90aa-fb5590fc013d",
      "source": "firecrawl/blog/launch-week-ii-day-1-introducing-batch-scrape-endpoint.md",
      "content": "About the Author\n\n[![Eric Ciarla image](https://www.firecrawl.dev/eric-img.jpeg)\\\nEric Ciarla@ericciarla](https://x.com/ericciarla)  \nEric Ciarla is the Chief Operating Officer (COO) of Firecrawl and leads marketing. He also worked on Mendable.ai and sold it to companies like Snapchat, Coinbase, and MongoDB.\nPreviously worked at Ford and Fracta as a Data Scientist. Eric also co-founded SideGuide, a tool for learning code within VS Code with 50,000 users.",
      "metadata": {
        "title": "Launch Week II - Day 1: Introducing the Batch Scrape Endpoint",
        "url": "https://www.firecrawl.dev/blog/launch-week-ii-day-1-introducing-batch-scrape-endpoint"
      }
    },
    {
      "id": "47def16e-9ce0-4961-a1ee-7d8343cd4682",
      "source": "firecrawl/blog/launch-week-ii-day-1-introducing-batch-scrape-endpoint.md",
      "content": "About the Author > More articles by Eric Ciarla\n\n[How to Create an llms.txt File for Any Website\\\n\\\nLearn how to generate an llms.txt file for any website using the llms.txt Generator and Firecrawl.](https://www.firecrawl.dev/blog/How-to-Create-an-llms-txt-File-for-Any-Website) [Cloudflare Error 1015: How to solve it?\\\n\\\nCloudflare Error 1015 is a rate limiting error that occurs when Cloudflare detects that you are exceeding the request limit set by the website owner.](https://www.firecrawl.dev/blog/cloudflare-error-1015-how-to-solve-it) [Build an agent that checks for website contradictions\\\n\\\nUsing Firecrawl and Claude to scrape your website's data and look for contradictions.](https://www.firecrawl.dev/blog/contradiction-agent) [Why Companies Need a Data Strategy for Generative AI\\\n\\\nLearn why a well-defined data strategy is essential for building robust, production-ready generative AI systems, and discover practical steps for curation, maintenance, and integration.](https://www.firecrawl.dev/blog/why-companies-need-a-data-strategy-for-generative-ai) [Getting Started with OpenAI's Predicted Outputs for Faster LLM Responses\\\n\\\nA guide to leveraging Predicted Outputs to speed up LLM tasks with GPT-4o models.](https://www.firecrawl.dev/blog/getting-started-with-predicted-outputs-openai) [How to easily install requests with pip and python\\\n\\\nA tutorial on installing the requests library in Python using various methods, with usage examples and troubleshooting tips](https://www.firecrawl.dev/blog/how-to-easily-install-requests-with-pip-and-python) [How to quickly install BeautifulSoup with Python\\\n\\\nA guide on installing the BeautifulSoup library in Python using various methods, with usage examples and troubleshooting tips](https://www.firecrawl.dev/blog/how-to-quickly-install-beautifulsoup-with-python) [How to Use OpenAI's o1 Reasoning Models in Your Applications\\\n\\\nLearn how to harness OpenAI's latest o1 series models for complex reasoning tasks in your apps.](https://www.firecrawl.dev/blog/how-to-use-openai-o1-reasoning-models-in-applications)",
      "metadata": {
        "title": "Launch Week II - Day 1: Introducing the Batch Scrape Endpoint",
        "url": "https://www.firecrawl.dev/blog/launch-week-ii-day-1-introducing-batch-scrape-endpoint"
      }
    },
    {
      "id": "42000bcc-401a-4ba8-b4f4-ad16ce5b3aab",
      "source": "firecrawl/blog/complete-guide-to-data-enrichment.md",
      "content": "---\ntitle: Data Enrichment: A Complete Guide to Enhancing Your Data Quality\nurl: https://www.firecrawl.dev/blog/complete-guide-to-data-enrichment\n---  \nIntroducing /extract - Get web data with a prompt [Try now](https://www.firecrawl.dev/extract)  \nDec 14, 2024  \nâ€¢  \n[![Bex Tuychiev image](https://www.firecrawl.dev/bex.jpg)Bex Tuychiev](https://x.com/bextuychiev)  \n# Data Enrichment: A Complete Guide to Enhancing Your Data Quality  \n![Data Enrichment: A Complete Guide to Enhancing Your Data Quality image](https://www.firecrawl.dev/images/blog/data_enrichment_guide/complete-data-enrichment-guide.jpg)",
      "metadata": {
        "title": "Data Enrichment: A Complete Guide to Enhancing Your Data Quality",
        "url": "https://www.firecrawl.dev/blog/complete-guide-to-data-enrichment"
      }
    },
    {
      "id": "1a26966c-fe30-424d-b1bb-9783d84574dd",
      "source": "firecrawl/blog/complete-guide-to-data-enrichment.md",
      "content": "Introduction\n\nIn todayâ€™s data-driven business landscape, organizations face a critical challenge: transforming raw data into actionable insights. Data enrichment has emerged as a fundamental process that enables businesses to enhance their existing datasets with additional, valuable information from multiple sources.  \nThis comprehensive guide explores data enrichment â€” what it is, why it matters, and how organizations can implement it effectively. Whether youâ€™re looking to improve customer profiles, enhance marketing campaigns, or strengthen decision-making processes, understanding data enrichment is crucial for maintaining a competitive edge in the near future.  \nAs data sources continue to multiply and diversify, businesses need robust tools and strategies to collect, integrate, and enhance their data efficiently. Modern solutions, including AI-powered platforms like [Firecrawl](https://firecrawl.dev/), are transforming how organizations approach data enrichment by automating collection processes and ensuring data quality at scale.  \nThroughout this guide, weâ€™ll explore:  \n- Essential concepts and methodologies in data enrichment\n- Tools and technologies that streamline the enrichment process\n- Best practices for implementing data enrichment strategies\n- Real-world applications and success stories\n- Common challenges and their solutions  \nWhether youâ€™re new to data enrichment or looking to optimize your existing processes, this guide provides the insights and practical knowledge needed to enhance your data quality and drive better business outcomes.",
      "metadata": {
        "title": "Data Enrichment: A Complete Guide to Enhancing Your Data Quality",
        "url": "https://www.firecrawl.dev/blog/complete-guide-to-data-enrichment"
      }
    },
    {
      "id": "ec84857c-e18f-4197-b8a3-e36f2f81f22a",
      "source": "firecrawl/blog/complete-guide-to-data-enrichment.md",
      "content": "What is Data Enrichment? > Understanding data enrichment tools\n\nModern data enrichment tools automate the complex process of collecting and integrating data from diverse sources. These tools typically offer:  \n- Automated data collection from websites, APIs, and databases\n- Data validation and cleaning capabilities\n- Schema mapping and transformation features\n- Integration with existing business systems\n- Quality assurance and monitoring  \nFor example, web scraping engines like Firecrawl enable organizations to automatically structured web data through AI-powered extraction, eliminating the need for manual data gathering while ensuring consistency and accuracy. We will see a complete Firecrawl data enrichment workflow later.",
      "metadata": {
        "title": "Data Enrichment: A Complete Guide to Enhancing Your Data Quality",
        "url": "https://www.firecrawl.dev/blog/complete-guide-to-data-enrichment"
      }
    },
    {
      "id": "2c059baf-f3fc-4bed-bfd1-de7e47ce294f",
      "source": "firecrawl/blog/complete-guide-to-data-enrichment.md",
      "content": "What is Data Enrichment? > Types of data enrichment services\n\nData enrichment services generally fall into six main categories:  \n![A comprehensive diagram showing the 6 main categories of data enrichment: Demographic, Behavioral, Company, Psychographic, Geographic, and Contact Data, arranged in a visually appealing infographic format](https://www.firecrawl.dev/images/blog/data_enrichment_guide/enrichment_categories.png)  \n1. **Demographic**\n- Adding personal information like age, income, education\n- Enhancing geographic data with location-specific insights\n- Including professional and career-related information\n2. **Behavioral**\n- Incorporating customer interaction history\n- Adding purchase patterns and preferences\n- Including digital footprint data like website visits and engagement\n3. **Company**\n- Adding firmographic data (company size, revenue, industry)\n- Including technological stack information\n- Incorporating business relationships and hierarchies\n4. **Psychographic**\n- Adding personality traits and characteristics\n- Including personal values and attitudes\n- Incorporating lifestyle choices and interests\n- Adding motivation and decision-making patterns\n5. **Geographic**\n- Location intelligence and spatial analysis\n- Regional market data and demographics\n- Geographic business opportunities\n- Local economic indicators\n- Competitive landscape by region\n6. **Contact Data**\n- Email addresses and phone numbers\n- Social media profiles\n- Professional contact details\n- Business communication preferences\n- Contact verification and validation  \nThese different types of data enrichment services can be combined and integrated to create comprehensive data profiles. For example, combining demographic and behavioral data provides deeper customer insights, while merging company and geographic data enables better B2B market analysis. The key is selecting the right combination of enrichment types based on your specific business needs and objectives.  \nAs we move into examining B2B data enrichment specifically, weâ€™ll see how these various enrichment types come together to support business decision-making and growth strategies.",
      "metadata": {
        "title": "Data Enrichment: A Complete Guide to Enhancing Your Data Quality",
        "url": "https://www.firecrawl.dev/blog/complete-guide-to-data-enrichment"
      }
    },
    {
      "id": "2cabaea0-d19e-4c2c-b703-aa2ee99869ac",
      "source": "firecrawl/blog/complete-guide-to-data-enrichment.md",
      "content": "Data Enrichment Tools and Services\n\nData enrichment tools come in various forms, each serving specific use cases and industries. Here are some of the most effective solutions:  \n1. Web scraping and data collection tools  \n- [Firecrawl](https://firecrawl.dev/): AI-powered web scraping for structured data extraction ($0-100/mo)\n- [Bright Data](https://brightdata.com/): Enterprise-grade data collection infrastructure with proxy networks ($500+/mo)\n- [Scrapy](https://scrapy.org/): Open-source Python framework for custom scraping solutions (Free). Key features: Custom extraction rules, proxy rotation, scheduling  \n2. Business intelligence platforms  \n- [ZoomInfo](https://www.zoominfo.com/): Company and contact information enrichment ($15k+/year). Integrates with: [Salesforce](https://salesforce.com/), [HubSpot](https://hubspot.com/), Marketo\n- [Clearbit](https://clearbit.com/): Real-time company and person data API ($99-999/mo). Unique feature: Logo and domain matching\n- [InsideView](https://www.demandbase.com/): Market intelligence and company insights ($500+/mo). Specializes in: Sales intelligence and prospecting  \n3. Customer data platforms (CDPs)  \n- [Segment](https://segment.com/): Customer data collection and integration ($120+/mo). Key differentiator: 300+ pre-built integrations\n- [Tealium](https://www.tealium.com/): Real-time customer data orchestration (Custom pricing). Unique feature: Machine learning predictions\n- [mParticle](https://www.mparticle.com/): Customer data infrastructure ($500+/mo). Best for: Mobile app analytics  \n4. Data validation and cleansing  \n- [Melissa Data](https://www.melissa.com/): Address verification and standardization ($500+/mo)\n- [DemandTools](https://www.validity.com/demandtools/): Salesforce data cleansing ($999+/year)\n- [Informatica Data Quality](https://www.informatica.com/): Enterprise data quality management (Custom pricing)  \n> Disclaimer: The prices of mentioned tools may vary based on your location and date of visit.  \nThese tools provide a foundation for data enrichment, but selecting the right service requires careful evaluation of several key factors.",
      "metadata": {
        "title": "Data Enrichment: A Complete Guide to Enhancing Your Data Quality",
        "url": "https://www.firecrawl.dev/blog/complete-guide-to-data-enrichment"
      }
    },
    {
      "id": "f9c0dc40-15c8-4d9b-ac68-6376ac6dbf1c",
      "source": "firecrawl/blog/complete-guide-to-data-enrichment.md",
      "content": "Data Enrichment Tools and Services > Choosing the right data enrichment solution\n\nWhen selecting a data enrichment service, organizations must evaluate several critical factors. Data quality metrics serve as the foundation, with accuracy rates ideally exceeding 95% and response times under 200ms. Services should maintain 99.9% uptime SLAs and provide comprehensive data validation methods.  \nIntegration capabilities determine implementation success. Services should offer well-documented REST APIs, support JSON/CSV formats, and connect with major CRM platforms. The ability to handle both real-time and batch processing (up to 100k records/hour) provides essential flexibility.  \nCost evaluation requires examining API pricing tiers, volume discounts, and hidden fees. Consider vendor reputation, support quality (24/7 availability), and compliance certifications (GDPR, CCPA, SOC 2). Regular security audits and data handling practices should meet industry standards.",
      "metadata": {
        "title": "Data Enrichment: A Complete Guide to Enhancing Your Data Quality",
        "url": "https://www.firecrawl.dev/blog/complete-guide-to-data-enrichment"
      }
    },
    {
      "id": "c87c90c3-fba4-495a-9c51-b26c6a997bb1",
      "source": "firecrawl/blog/complete-guide-to-data-enrichment.md",
      "content": "Data Enrichment Tools and Services > Custom data enrichment solutions\n\nCustom data enrichment solutions provide organizations with tailored approaches to data enhancement. Web scraping platforms like Firecrawl and Selenium enable automated extraction of public data, while custom APIs facilitate direct integration with specialized data sources. Python frameworks such as Selenium, BeautifulSoup and Pandas streamline data processing and transformation workflows. Common pitfalls include rate limiting issues, unstable selectors, and data quality inconsistencies.  \nThese solutions often incorporate modular architectures for flexibility. Organizations can combine multiple data sources, implement custom validation rules, and design specific enrichment pipelines. Advanced features include proxy rotation for reliable scraping, rate limiting for API compliance, and parallel processing for improved performance. Key challenges include maintaining data consistency across sources and handling API deprecation gracefully.  \nDevelopment of custom solutions requires careful consideration of scalability and maintenance. Teams should implement robust error handling, comprehensive logging, and automated testing. Documentation and version control ensure long-term sustainability, while modular design enables iterative improvements and feature additions. Consider trade-offs between custom development costs versus off-the-shelf solutions - custom solutions typically require 2-3x more initial investment but offer greater control and flexibility.",
      "metadata": {
        "title": "Data Enrichment: A Complete Guide to Enhancing Your Data Quality",
        "url": "https://www.firecrawl.dev/blog/complete-guide-to-data-enrichment"
      }
    },
    {
      "id": "7b319980-dad3-4644-af48-38e5f617fde5",
      "source": "firecrawl/blog/complete-guide-to-data-enrichment.md",
      "content": "Data Enrichment Tools and Services > Firecrawl as a custom data enrichment solution\n\nFirecrawl offers powerful AI-powered scraping capabilities that extend well beyond basic data extraction. Here are some key features that make it an effective data enrichment solution:  \n1. **Natural language data extraction**  \nRather than relying on brittle HTML selectors or XPath expressions, Firecrawl allows you to describe the data you want to extract in plain English. This approach works across different website layouts and remains stable even when sites update their structure. For example, you can request â€œfind all pricing tables that show enterprise plan featuresâ€ or â€œextract author biographies from the bottom of articlesâ€ without specifying exact HTML locations. These capabilities are available through the `scrape_url` method, which is covered in detail in [our guide on using the scrape endpoint](https://www.firecrawl.dev/blog/mastering-firecrawl-scrape-endpoint).  \n2. **Recursive website crawling**  \nBeyond scraping individual URLs, Firecrawl can automatically discover and process entire website sections. The crawler understands site structure semantically, following relevant links while avoiding navigation menus, footers, and other non-content areas. This is especially valuable when enriching data from documentation sites, knowledge bases, or product catalogs. Learn more about the crawling process in [our crawl endpoint guide](https://www.firecrawl.dev/blog/mastering-the-crawl-endpoint-in-firecrawl).  \n3. **Multiple output formats**  \nThe same content can be extracted in different formats to suit your needs:  \n- Structured data for database storage (as shown in our Amazon example)\n- Markdown for documentation\n- Plain text for natural language processing\n- HTML for web archives\n- Screenshots for visual records  \nThis flexibility eliminates the need for additional conversion steps in your data pipeline.  \n4. **Intelligent content processing**  \nThe AI-powered approach helps solve common web scraping challenges:  \n- Automatically detecting and extracting data from tables, lists, and other structured elements\n- Understanding content relationships and hierarchies\n- Handling dynamic JavaScript-rendered content\n- Maintaining context across multiple pages\n- Filtering out irrelevant content like ads and popups  \n5. **Integration capabilities**  \nThe extracted data can feed directly into modern data and AI workflows:  \n- Vector databases for semantic search\n- Large language models for analysis\n- Business intelligence tools for reporting\n- Machine learning pipelines for training data\n- Monitoring systems for change detection  \nThe key advantage across all these use cases is reduced maintenance overhead. Traditional scrapers require constant updates as websites change, while semantic extraction remains stable. This makes it particularly suitable for long-running data enrichment projects where reliability and consistency are crucial.  \nFor a complete example of data enrichment process in Firecrawl, you can read a later section on [Custom Data Enrichment in Python: Amazon Data Case Study](https://www.firecrawl.dev/blog/complete-guide-to-data-enrichment#custom-data-enrichment-in-python-amazon-data-case-study).",
      "metadata": {
        "title": "Data Enrichment: A Complete Guide to Enhancing Your Data Quality",
        "url": "https://www.firecrawl.dev/blog/complete-guide-to-data-enrichment"
      }
    },
    {
      "id": "c3c9ba2c-3fae-428b-b44d-2685a0f1c95a",
      "source": "firecrawl/blog/complete-guide-to-data-enrichment.md",
      "content": "Data Enrichment Tools and Services > Product data enrichment solutions\n\nProduct data enrichment solutions transform basic product information into comprehensive, market-ready datasets. These tools excel at automated attribute extraction, leveraging advanced technologies to identify and categorize product features like dimensions, materials, and industry certifications. Image recognition capabilities enhance product listings with accurate visual data, while competitive pricing analysis ensures market alignment with 95%+ accuracy rates.  \nModern product enrichment platforms support bulk processing across multiple languages at speeds of 50,000+ products per hour, making them ideal for global operations. They often incorporate industry-specific taxonomies to maintain standardization and enable rich media enhancement for improved product presentation. Key integration capabilities include seamless connections with [Shopify](https://shopify.com/), [WooCommerce](https://woocommerce.com/), and other major e-commerce platforms, along with built-in data validation methods for ensuring attribute accuracy and completeness.",
      "metadata": {
        "title": "Data Enrichment: A Complete Guide to Enhancing Your Data Quality",
        "url": "https://www.firecrawl.dev/blog/complete-guide-to-data-enrichment"
      }
    },
    {
      "id": "23a1b48d-f774-40de-ab92-62dc35d83004",
      "source": "firecrawl/blog/complete-guide-to-data-enrichment.md",
      "content": "Data Enrichment Tools and Services > Customer data enrichment solutions\n\nCustomer data enrichment platforms serve as central hubs for creating comprehensive customer profiles, achieving match rates of 85-95% for B2B data and 70-85% for B2C data. At their core, these platforms excel at identity resolution, connecting disparate data points to form unified customer views. They incorporate behavioral analytics to understand customer actions and automatically append demographic information to existing profiles, with leading platforms like Clearbit and ZoomInfo offering 98%+ accuracy rates.  \nIntegration features form the backbone of these platforms, with real-time API access enabling immediate data updates at speeds of 10-20ms per record. Webhook support facilitates automated workflows, while custom field mapping ensures compatibility with existing systems. Sophisticated data synchronization maintains consistency across all connected platforms, with enterprise solutions like [FullContact](https://fullcontact.com/) processing up to 1M records per day.  \nSecurity and compliance remain paramount in customer data enrichment. Modern platforms incorporate robust GDPR compliance measures and granular data privacy controls, typically costing $0.05-0.15 per enriched record at scale. They maintain detailed audit trails and provide comprehensive consent management systems to protect both organizations and their customers.  \nThe key to successful data enrichment lies in selecting tools that align with your specific use cases while maintaining data quality and compliance standards. Best practices include refreshing enriched data every 3-6 months and implementing data quality monitoring. Organizations should start with a pilot program using one or two tools before scaling to a full implementation, with typical ROI ranging from 3-5x for marketing use cases and 5-8x for sales applications.",
      "metadata": {
        "title": "Data Enrichment: A Complete Guide to Enhancing Your Data Quality",
        "url": "https://www.firecrawl.dev/blog/complete-guide-to-data-enrichment"
      }
    },
    {
      "id": "92e642f7-2c68-456a-bfed-98dd115e3b5b",
      "source": "firecrawl/blog/complete-guide-to-data-enrichment.md",
      "content": "B2B Data Enrichment: A Complete Guide\n\nB2B data enrichment has become increasingly critical for organizations seeking to enhance their business intelligence and decision-making capabilities. Letâ€™s explore the key tools and implementation approaches.",
      "metadata": {
        "title": "Data Enrichment: A Complete Guide to Enhancing Your Data Quality",
        "url": "https://www.firecrawl.dev/blog/complete-guide-to-data-enrichment"
      }
    },
    {
      "id": "a150586c-57d2-46cc-abba-163e89ab39e0",
      "source": "firecrawl/blog/complete-guide-to-data-enrichment.md",
      "content": "B2B Data Enrichment: A Complete Guide > B2B data enrichment tools\n\nEnterprise solutions form the backbone of large-scale B2B data operations, offering comprehensive coverage and high accuracy. These platforms excel at providing deep company insights and market intelligence:  \n1. Enterprise solutions  \n- [ZoomInfo](https://zoominfo.com/): Industry leader with 95%+ accuracy for company data\n- [D&B Hoovers](https://dnb.com/): Comprehensive business intelligence with global coverage\n- [InsideView](https://insideview.com/): Real-time company insights and market intelligence  \nAPI-first platforms enable seamless integration into existing workflows, providing real-time data enrichment capabilities for automated systems:  \n2. API-First platforms  \n- [Clearbit enrichment API](https://clearbit.com/): Real-time company data lookup\n- [FullContact API](https://docs.fullcontact.com/): Professional contact verification\n- [Hunter.io](https://hunter.io/): Email verification and discovery  \nData validation tools ensure data quality and compliance, critical for maintaining accurate business records:  \n3. Data Validation Tools  \n- [Melissa Data](https://melissa.com/): Address and contact verification\n- [Neverbounce](https://neverbounce.com/): Email validation services\n- [DueDil](https://duedill.com/): Company verification and compliance  \nTypical pricing models range from:  \n- Pay-per-lookup: $0.05-0.25 per record\n- Monthly subscriptions: $500-5000/month\n- Enterprise contracts: $50,000+ annually",
      "metadata": {
        "title": "Data Enrichment: A Complete Guide to Enhancing Your Data Quality",
        "url": "https://www.firecrawl.dev/blog/complete-guide-to-data-enrichment"
      }
    },
    {
      "id": "a4987154-fba8-4b63-8959-de4f5bfab7dc",
      "source": "firecrawl/blog/complete-guide-to-data-enrichment.md",
      "content": "B2B Data Enrichment: A Complete Guide > Best practices for B2B data\n\n1. Data quality management  \nRegular validation and monitoring are essential for maintaining high-quality B2B data. Implement email format validation using standard regex patterns and schedule bi-monthly data refreshes. Track match rates with a target of >85% accuracy through systematic sampling.  \n2. Integration strategy  \nBuild a robust integration framework using standardized JSON for API communications. Critical components include:  \n- Real-time alert systems for failed enrichments\n- Retry logic with exponential backoff\n- Comprehensive request logging\n- Cost monitoring and caching mechanisms  \n3. Compliance and security  \nEnsure compliance with data protection regulations through proper documentation and security measures. Implement TLS 1.3 for data transfer and AES-256 for storage, while maintaining regular security audits and access reviews.  \n4. Data Scalability  \nDesign systems to handle large-scale data processing efficiently. Focus on data partitioning for datasets exceeding 1M records and implement auto-scaling capabilities to manage processing spikes. Monitor key performance metrics to maintain system health.  \n5. ROI Measurement  \nTrack the business impact of your enrichment efforts by measuring:  \n- Cost vs. revenue impact\n- Conversion rate improvements\n- Lead qualification efficiency\n- Sales cycle optimization  \nKey Performance Indicators:  \n- Match rate: >85%\n- Data accuracy: >95%\n- Enrichment coverage: >90%\n- Time to value: <48 hours  \nLetâ€™s put these best practices into action by exploring a real-world example of data enrichment.",
      "metadata": {
        "title": "Data Enrichment: A Complete Guide to Enhancing Your Data Quality",
        "url": "https://www.firecrawl.dev/blog/complete-guide-to-data-enrichment"
      }
    },
    {
      "id": "90a36443-e327-4f14-ae20-271023ec9b6c",
      "source": "firecrawl/blog/complete-guide-to-data-enrichment.md",
      "content": "Custom Data Enrichment in Python: Amazon Data Case Study\n\nLetâ€™s explore a practical example of data enrichment using Python with [Amazon listings data from 2020](https://www.kaggle.com/datasets/promptcloud/amazon-product-dataset-2020). This Kaggle-hosted dataset provides an excellent opportunity to demonstrate how to enrich outdated product information.",
      "metadata": {
        "title": "Data Enrichment: A Complete Guide to Enhancing Your Data Quality",
        "url": "https://www.firecrawl.dev/blog/complete-guide-to-data-enrichment"
      }
    },
    {
      "id": "3944bd42-2fc9-4757-8449-d8e59c0a468f",
      "source": "firecrawl/blog/complete-guide-to-data-enrichment.md",
      "content": "Custom Data Enrichment in Python: Amazon Data Case Study > Exploring the Amazon listings dataset\n\nFirst, we should download the dataset stored as a ZIP file on Kaggle servers. These commands pull that zip file and extract the CSV file inside in your current working directory:  \n```bash\n# Download the zip file\ncurl -L -o amazon-product-dataset-2020.zip\\\nhttps://www.kaggle.com/api/v1/datasets/download/promptcloud/amazon-product-dataset-2020\nunzip amazon-product-dataset-2020.zip\n\n# Move the desired file and rename it\nmv home/sdf/marketing_sample_for_amazon_com-ecommerce__20200101_20200131__10k_data.csv amazon_listings.csv\n\n# Delete unnecessary files\nrm -rf home amazon-product-dataset-2020.zip\n\n```  \nNext, letâ€™s install a few dependencies we are going to use:  \n```bash\npip install pandas firecrawl-py pydantic python-dotenv\n\n```  \nNow, we can load the dataset with Pandas to explore it:  \n```python\nimport pandas as pd\n\ndf = pd.read_csv(\"amazon_listings.csv\")\n\ndf.shape\n\n```  \n```out\n(10002, 28)\n\n```  \nThere are 10k products with 28 attributes:  \n```python\ndf.columns\n\n```  \n```out\nIndex(\n[\\\n'Uniq Id', 'Product Name', 'Brand Name', 'Asin', 'Category',\\\n'Upc Ean Code', 'List Price', 'Selling Price', 'Quantity',\\\n'Model Number', 'About Product', 'Product Specification',\\\n'Technical Details', 'Shipping Weight', 'Product Dimensions', 'Image',\\\n'Variants', 'Sku', 'Product Url', 'Stock', 'Product Details',\\\n'Dimensions', 'Color', 'Ingredients', 'Direction To Use',\\\n'Is Amazon Seller', 'Size Quantity Variant', 'Product Description'\\\n], dtype='object'\n)\n\n```  \nFor ecommerce listing datasets, missing values are the main problem. Letâ€™s see if thatâ€™s true of this one:  \n```python\nnull_percentages = df.isnull().sum() / df.shape[0]\n\nnull_percentages.sort_values(ascending=False)\n\n```  \n```out\nProduct Description 1.000000\nSku 1.000000\nBrand Name 1.000000\nAsin 1.000000\nSize Quantity Variant 1.000000\nList Price 1.000000\nDirection To Use 1.000000\nQuantity 1.000000\nIngredients 1.000000\nColor 1.000000\nDimensions 1.000000\nProduct Details 1.000000\nStock 1.000000\nUpc Ean Code 0.996601\nProduct Dimensions 0.952110\nVariants 0.752250\nModel Number 0.177165\nProduct Specification 0.163167\nShipping Weight 0.113777\nCategory 0.082983\nTechnical Details 0.078984\nAbout Product 0.027295\nSelling Price 0.010698\nImage 0.000000\nProduct Name 0.000000\nProduct Url 0.000000\nIs Amazon Seller 0.000000\nUniq Id 0.000000\ndtype: float64\n\n```  \nBased on the output, we can see that almost 15 attributes are missing for all products while some have partially incomplete data. Only five attributes are fully present:  \n- Unique ID\n- Is Amazon Seller\n- Product URL\n- Product Name\n- Image  \nEven when details are present, we canâ€™t count on them since they were recorded in 2020 and have probably changed. So, our goal is to enrich this data with updated information as well as filling in the missing columns as best as possible.",
      "metadata": {
        "title": "Data Enrichment: A Complete Guide to Enhancing Your Data Quality",
        "url": "https://www.firecrawl.dev/blog/complete-guide-to-data-enrichment"
      }
    },
    {
      "id": "e1cf4561-be03-4989-b477-187fcc168f5d",
      "source": "firecrawl/blog/complete-guide-to-data-enrichment.md",
      "content": "Custom Data Enrichment in Python: Amazon Data Case Study > Enriching Amazon listings data with Firecrawl\n\nNow that weâ€™ve explored the dataset and identified missing information, letâ€™s use Firecrawl to enrich our Amazon product data. Weâ€™ll create a schema that defines what information we want to extract from each product URL.  \nFirst, letâ€™s import the required libraries and initialize Firecrawl:  \n```python\nfrom firecrawl import FirecrawlApp\nfrom dotenv import load_dotenv\nfrom pydantic import BaseModel, Field\n\nload_dotenv()\napp = FirecrawlApp()\n\n```  \nNext, weâ€™ll define a Pydantic model that describes the product information we want to extract. This schema helps Firecrawlâ€™s AI understand what data to look for on each product page:  \n```python\nclass Product(BaseModel):\nname: str = Field(description=\"The name of the product\")\nimage: str = Field(description=\"The URL of the product image\")\nbrand: str = Field(description=\"The seller or brand of the product\")\ncategory: str = Field(description=\"The category of the product\")\nprice: float = Field(description=\"The current price of the product\")\nrating: float = Field(description=\"The rating of the product\")\nreviews: int = Field(description=\"The number of reviews of the product\")\ndescription: str = Field(description=\"The description of the product written below its price.\")\ndimensions: str = Field(description=\"The dimensions of the product written below its technical details.\")\nweight: str = Field(description=\"The weight of the product written below its technical details.\")\nin_stock: bool = Field(description=\"Whether the product is in stock\")\n\n```  \nFor demonstration purposes, letâ€™s take the last 100 product URLs from our dataset and enrich them:  \n```python\n# Get last 100 URLs\nurls = df['Product Url'].tolist()[-100:]\n\n# Start batch scraping job\nbatch_scrape_data = app.batch_scrape_urls(urls, params={\n\"formats\": [\"extract\"],\n\"extract\": {\"schema\": Product.model_json_schema()}\n})\n\n```  \nHere are the results of the batch-scraping job:  \n```python\n# Separate successful and failed scrapes\nfailed_items = [\\\nitem for item in batch_scrape_data[\"data\"] if item[\"metadata\"][\"statusCode\"] != 200\\\n]\nsuccess_items = [\\\nitem for item in batch_scrape_data[\"data\"] if item[\"metadata\"][\"statusCode\"] == 200\\\n]\n\nprint(f\"Successfully scraped: {len(success_items)} products\")\nprint(f\"Failed to scrape: {len(failed_items)} products\")\n\n```  \n```out\nSuccessfully scraped: 84 products\nFailed to scrape: 16 products\n\n```  \nLetâ€™s examine a successful enrichment result:  \n```python\n{\n'extract': {\n'name': 'Leffler Home Kids Chair, Red',\n'brand': 'Leffler Home',\n'image': 'https://m.media-amazon.com/images/I/71IcFQJ8n4L._AC_SX355_.jpg',\n'price': 0,\n'rating': 0,\n'weight': '26 Pounds',\n'reviews': 0,\n'category': \"Kids' Furniture\",\n'in_stock': False,\n'dimensions': '20\"D x 24\"W x 21\"H',\n'description': 'Every child deserves their own all mine chair...'\n},\n'metadata': {\n'url': 'https://www.amazon.com/Leffler-Home-14000-21-63-01-Kids-Chair/dp/B0784BXPC2',\n'statusCode': 200,\n# ... other metadata\n}\n}\n\n```  \nA few observations about the enrichment results:  \n1. **Success Rate**: Out of 100 URLs, we successfully enriched 84 products. The 16 failures were mostly due to products no longer being available on Amazon (404 errors).  \n2. **Data Quality**: For successful scrapes, we obtained all desired details, including:  \n- Complete product dimensions and weights\n- Updated category information\n- Current availability status\n- High-resolution image URLs\n- Detailed product descriptions  \n3. **Missing Values**: Some numeric fields (price, rating, reviews) returned 0 for unavailable products, which we should handle in our data cleaning step.  \nFor large-scale enrichment, Firecrawl also offers an asynchronous API that can handle thousands of URLs:  \n```python\n# Start async batch job\nbatch_job = app.async_batch_scrape_urls(\nurls,\nparams={\"formats\": [\"extract\"], \"extract\": {\"schema\": Product.model_json_schema()}},\n)\n\n# Check job status until it finishes\njob_status = app.check_batch_scrape_status(batch_job[\"id\"])\n\n```  \nThis approach is particularly useful when enriching the entire dataset of 10,000+ products, as it allows you to monitor progress and handle results in chunks rather than waiting for all URLs to complete.  \nIn the next section, weâ€™ll explore how to merge this enriched data back into our original dataset and handle any discrepancies between old and new information.",
      "metadata": {
        "title": "Data Enrichment: A Complete Guide to Enhancing Your Data Quality",
        "url": "https://www.firecrawl.dev/blog/complete-guide-to-data-enrichment"
      }
    },
    {
      "id": "09ed6142-ac69-4295-b1f2-cfa4e43c4bb4",
      "source": "firecrawl/blog/complete-guide-to-data-enrichment.md",
      "content": "Conclusion\n\nData enrichment has become essential for organizations seeking to maintain competitive advantage through high-quality, comprehensive datasets. Through this guide, weâ€™ve explored how modern tools and techniques can streamline the enrichment process.",
      "metadata": {
        "title": "Data Enrichment: A Complete Guide to Enhancing Your Data Quality",
        "url": "https://www.firecrawl.dev/blog/complete-guide-to-data-enrichment"
      }
    },
    {
      "id": "dc976e6c-ae6b-4513-b1a0-de04706f3725",
      "source": "firecrawl/blog/complete-guide-to-data-enrichment.md",
      "content": "Conclusion > Additional Resources\n\n- [Firecrawl Documentation](https://docs.firecrawl.dev/) - Official documentation for implementing data enrichment with Firecrawl\n- [Web Scraping Ethics Guide](https://towardsdatascience.com/ethics-in-web-scraping-b96b18136f01?gi=3bb0cd74a4e3) - Best practices for ethical web scraping\n- [Clearbit API Documentation](https://clearbit.com/docs) - Detailed guides for company and person data enrichment\n- [FullContact Developer Portal](https://docs.fullcontact.com/) - Resources for identity resolution and customer data enrichment\n- [ZoomInfo API Reference](https://api-docs.zoominfo.com/) - Documentation for B2B data enrichment\n- [Modern Data Stack Blog](https://www.moderndatastack.xyz/) - Latest trends and best practices in data engineering\n- [dbt Developer Blog](https://docs.getdbt.com/blog) - Insights on data transformation and modeling  \nBy following these guidelines and leveraging modern tools, organizations can build robust data enrichment pipelines that provide lasting value for their data-driven initiatives.  \nArticle updated recently  \n[ðŸ”¥](https://www.firecrawl.dev/)",
      "metadata": {
        "title": "Data Enrichment: A Complete Guide to Enhancing Your Data Quality",
        "url": "https://www.firecrawl.dev/blog/complete-guide-to-data-enrichment"
      }
    },
    {
      "id": "e4c8b32a-3cdc-452a-b6d2-50f1e1f1a1f4",
      "source": "firecrawl/blog/complete-guide-to-data-enrichment.md",
      "content": "Ready to _Build?_\n\nStart scraping web data for your AI apps today.  \nNo credit card needed.  \nGet Started",
      "metadata": {
        "title": "Data Enrichment: A Complete Guide to Enhancing Your Data Quality",
        "url": "https://www.firecrawl.dev/blog/complete-guide-to-data-enrichment"
      }
    },
    {
      "id": "603de11b-ddd5-4014-863b-08cafd5912f7",
      "source": "firecrawl/blog/complete-guide-to-data-enrichment.md",
      "content": "About the Author\n\n[![Bex Tuychiev image](https://www.firecrawl.dev/bex.jpg)\\\nBex Tuychiev@bextuychiev](https://x.com/bextuychiev)  \nBex is a Top 10 AI writer on Medium and a Kaggle Master with over 15k followers. He loves writing detailed guides, tutorials, and notebooks on complex data science and machine learning topics",
      "metadata": {
        "title": "Data Enrichment: A Complete Guide to Enhancing Your Data Quality",
        "url": "https://www.firecrawl.dev/blog/complete-guide-to-data-enrichment"
      }
    },
    {
      "id": "effb9ef3-aecc-4420-a7d2-e16041522c78",
      "source": "firecrawl/blog/complete-guide-to-data-enrichment.md",
      "content": "About the Author > More articles by Bex Tuychiev\n\n[Building an Automated Price Tracking Tool\\\n\\\nBuild an automated e-commerce price tracker in Python. Learn web scraping, price monitoring, and automated alerts using Firecrawl, Streamlit, PostgreSQL.](https://www.firecrawl.dev/blog/automated-price-tracking-tutorial-python) [Web Scraping Automation: How to Run Scrapers on a Schedule\\\n\\\nLearn how to automate web scraping in Python using free tools like schedule, asyncio, cron jobs and GitHub Actions. This comprehensive guide covers local and cloud-based scheduling methods to run scrapers reliably in 2025.](https://www.firecrawl.dev/blog/automated-web-scraping-free-2025) [Automated Data Collection - A Comprehensive Guide\\\n\\\nLearn how to build robust automated data collection systems using modern tools and best practices. This guide covers everything from selecting the right tools to implementing scalable collection pipelines.](https://www.firecrawl.dev/blog/automated-data-collection-guide) [BeautifulSoup4 vs. Scrapy - A Comprehensive Comparison for Web Scraping in Python\\\n\\\nLearn the key differences between BeautifulSoup4 and Scrapy for web scraping in Python. Compare their features, performance, and use cases to choose the right tool for your web scraping needs.](https://www.firecrawl.dev/blog/beautifulsoup4-vs-scrapy-comparison) [How to Build an Automated Competitor Price Monitoring System with Python\\\n\\\nLearn how to build an automated competitor price monitoring system in Python that tracks prices across e-commerce sites, provides real-time comparisons, and maintains price history using Firecrawl, Streamlit, and GitHub Actions.](https://www.firecrawl.dev/blog/automated-competitor-price-scraping) [Scraping Company Data and Funding Information in Bulk With Firecrawl and Claude\\\n\\\nLearn how to build a web scraper in Python that gathers company details, funding rounds, and investor information from public sources like Crunchbase using Firecrawl and Claude for automated data collection and analysis.](https://www.firecrawl.dev/blog/crunchbase-scraping-with-firecrawl-claude) [Data Enrichment: A Complete Guide to Enhancing Your Data Quality\\\n\\\nLearn how to enrich your data quality with a comprehensive guide covering data enrichment tools, best practices, and real-world examples. Discover how to leverage modern solutions like Firecrawl to automate data collection, validation, and integration for better business insights.](https://www.firecrawl.dev/blog/complete-guide-to-data-enrichment) [Building an Intelligent Code Documentation RAG Assistant with DeepSeek and Firecrawl\\\n\\\nLearn how to build an intelligent documentation assistant powered by DeepSeek and RAG (Retrieval Augmented Generation) that can answer questions about any documentation website by combining language models with efficient information retrieval.](https://www.firecrawl.dev/blog/deepseek-rag-documentation-assistant)",
      "metadata": {
        "title": "Data Enrichment: A Complete Guide to Enhancing Your Data Quality",
        "url": "https://www.firecrawl.dev/blog/complete-guide-to-data-enrichment"
      }
    },
    {
      "id": "03b56918-c397-4c19-9e89-ee2560d34009",
      "source": "firecrawl/blog/why-companies-need-a-data-strategy-for-generative-ai.md",
      "content": "---\ntitle: Why Companies Need a Data Strategy for Generative AI\nurl: https://www.firecrawl.dev/blog/why-companies-need-a-data-strategy-for-generative-ai\n---  \nIntroducing /extract - Get web data with a prompt [Try now](https://www.firecrawl.dev/extract)  \nDec 15, 2024  \nâ€¢  \n[![Eric Ciarla image](https://www.firecrawl.dev/eric-img.jpeg)Eric Ciarla](https://x.com/ericciarla)  \n# Why Companies Need a Data Strategy for Generative AI  \n![Why Companies Need a Data Strategy for Generative AI image](https://www.firecrawl.dev/images/blog/data-strategy.jpg)",
      "metadata": {
        "title": "Why Companies Need a Data Strategy for Generative AI",
        "url": "https://www.firecrawl.dev/blog/why-companies-need-a-data-strategy-for-generative-ai"
      }
    },
    {
      "id": "44e085cf-3f2d-4474-af13-b38f806468ee",
      "source": "firecrawl/blog/why-companies-need-a-data-strategy-for-generative-ai.md",
      "content": "Companies Need a Data Strategy for Generative AI > Problems that come up with building Generative AI Apps\n\nAs mentioned before, it is pretty easy for someone to get a basic retrieval augmented generation (RAG) search system working with a subset of company data. But as you scale with more data and users, this approach breaks and yields mediocre results at best. Hereâ€™s why:  \n- Context crowding: Correct context for a given query gets crowded out by bad context. Take the Snap AR docs for example, they have 4 different products on their developer documentation website and they all have getting started pages. If a user asks a vague query like â€œhow do I get startedâ€ to a basic RAG chatbot, the answer is going to most likely be an incorrect amalgamation of the 4 getting started guides.\n- Outdated data: Information and processes constantly iterate, and documentation is not always maintained. One of our first customers, Spectrocloud, was benchmarking our chatbot before going into production and they found that one answer in particular was not correct. At first we thought that the model (GPT-3 at the time) was hallucinating, but after manually searching the docs we found the outdated source information on an obscure part of the documentation.\n- Data cleanliness: If data isnâ€™t clean, performance worsens and costs soar. We powered the chatbot on the documentation for Langchain, and data cleanliness and specifically prompt injection was a huge issue. Many of the Langchain pages had prompt examples embedded in them, which confused the model at inference time. Early on with Langchain we also noticed that a lot of unnecessary extra information was in our index like navigation menus on every page.\n- Data access: Accessing a variety of data sources is often critical for companies, but it introduces a host of challenges. For example, at Firecrawl, weâ€™ve seen that many large companies simply want to access web data from their own websites, but even this can involve complex permissioning, authentication, and data-fetching hurdles.",
      "metadata": {
        "title": "Why Companies Need a Data Strategy for Generative AI",
        "url": "https://www.firecrawl.dev/blog/why-companies-need-a-data-strategy-for-generative-ai"
      }
    },
    {
      "id": "8a351e49-1355-42a6-94e9-6dc8d195f96c",
      "source": "firecrawl/blog/why-companies-need-a-data-strategy-for-generative-ai.md",
      "content": "Companies Need a Data Strategy for Generative AI > Forming a data strategy to solve these problems\n\nTo mitigate these issues, companies building these apps should have a data strategy with the goal of curating and maintaining quality data. Based on the aforementioned problems, here are some practical suggestions to guide your strategy.  \n- Metadata Management: Good metadata is your first defense against context crowding. Every piece of content should be tagged with essential details like product association, who created it, and who can access it. This enables advanced filtering and more accurate responses.\n- Data Maintenance: To keep data fresh and reliable, the teams that create content should be responsible for regular reviews and updates. When underlying information changes, the corresponding documentation needs to change with it.\n- Data Sanitation: Raw data rarely arrives in ideal form. Before ingestion, strip away unnecessary formatting and information while preserving the essential details. While each content source requires different handling, tools like Unstructured can help standardize this process.\n- Data Access & Integration: Build the infrastructure to access your data sources seamlessly. Youâ€™ll need continuous data flow from knowledge bases, ticketing systems, websites, and more. Tools like Firecrawl can help build these pipelines and ensure high-quality data ingestion.",
      "metadata": {
        "title": "Why Companies Need a Data Strategy for Generative AI",
        "url": "https://www.firecrawl.dev/blog/why-companies-need-a-data-strategy-for-generative-ai"
      }
    },
    {
      "id": "f6142661-0d60-4e8a-acaa-760bb6c20900",
      "source": "firecrawl/blog/why-companies-need-a-data-strategy-for-generative-ai.md",
      "content": "Companies Need a Data Strategy for Generative AI > Conclusion\n\nThe industry is still in the early stages of solving these complex issues, thereâ€™s also significant opportunity for innovative companies to emerge and tackle various aspects of this problem. Startups like Glean, Unstructured, and our own Firecrawl have made some incredible progress, but no one has solved it all. No matter what tools emerge to make the process easier, having a robust data strategy is foundational to building production ready Generative AI Apps.\nThank you to Alex Meckes, SK Ramakuru, and Brian Leonard for their valuable insights and feedback that helped shape this post!  \nArticle updated recently  \n[ðŸ”¥](https://www.firecrawl.dev/)",
      "metadata": {
        "title": "Why Companies Need a Data Strategy for Generative AI",
        "url": "https://www.firecrawl.dev/blog/why-companies-need-a-data-strategy-for-generative-ai"
      }
    },
    {
      "id": "ebf46ebd-dccc-4323-9ae0-3a5ee806da76",
      "source": "firecrawl/blog/why-companies-need-a-data-strategy-for-generative-ai.md",
      "content": "Ready to _Build?_\n\nStart scraping web data for your AI apps today.  \nNo credit card needed.  \nGet Started",
      "metadata": {
        "title": "Why Companies Need a Data Strategy for Generative AI",
        "url": "https://www.firecrawl.dev/blog/why-companies-need-a-data-strategy-for-generative-ai"
      }
    },
    {
      "id": "8452f794-88f0-4107-ae13-ae9cf04366e6",
      "source": "firecrawl/blog/why-companies-need-a-data-strategy-for-generative-ai.md",
      "content": "About the Author\n\n[![Eric Ciarla image](https://www.firecrawl.dev/eric-img.jpeg)\\\nEric Ciarla@ericciarla](https://x.com/ericciarla)  \nEric Ciarla is the Chief Operating Officer (COO) of Firecrawl and leads marketing. He also worked on Mendable.ai and sold it to companies like Snapchat, Coinbase, and MongoDB.\nPreviously worked at Ford and Fracta as a Data Scientist. Eric also co-founded SideGuide, a tool for learning code within VS Code with 50,000 users.",
      "metadata": {
        "title": "Why Companies Need a Data Strategy for Generative AI",
        "url": "https://www.firecrawl.dev/blog/why-companies-need-a-data-strategy-for-generative-ai"
      }
    },
    {
      "id": "e2229ab5-5895-456b-931d-c6c052e19f6e",
      "source": "firecrawl/blog/why-companies-need-a-data-strategy-for-generative-ai.md",
      "content": "About the Author > More articles by Eric Ciarla\n\n[How to Create an llms.txt File for Any Website\\\n\\\nLearn how to generate an llms.txt file for any website using the llms.txt Generator and Firecrawl.](https://www.firecrawl.dev/blog/How-to-Create-an-llms-txt-File-for-Any-Website) [Cloudflare Error 1015: How to solve it?\\\n\\\nCloudflare Error 1015 is a rate limiting error that occurs when Cloudflare detects that you are exceeding the request limit set by the website owner.](https://www.firecrawl.dev/blog/cloudflare-error-1015-how-to-solve-it) [Build an agent that checks for website contradictions\\\n\\\nUsing Firecrawl and Claude to scrape your website's data and look for contradictions.](https://www.firecrawl.dev/blog/contradiction-agent) [Why Companies Need a Data Strategy for Generative AI\\\n\\\nLearn why a well-defined data strategy is essential for building robust, production-ready generative AI systems, and discover practical steps for curation, maintenance, and integration.](https://www.firecrawl.dev/blog/why-companies-need-a-data-strategy-for-generative-ai) [Getting Started with OpenAI's Predicted Outputs for Faster LLM Responses\\\n\\\nA guide to leveraging Predicted Outputs to speed up LLM tasks with GPT-4o models.](https://www.firecrawl.dev/blog/getting-started-with-predicted-outputs-openai) [How to easily install requests with pip and python\\\n\\\nA tutorial on installing the requests library in Python using various methods, with usage examples and troubleshooting tips](https://www.firecrawl.dev/blog/how-to-easily-install-requests-with-pip-and-python) [How to quickly install BeautifulSoup with Python\\\n\\\nA guide on installing the BeautifulSoup library in Python using various methods, with usage examples and troubleshooting tips](https://www.firecrawl.dev/blog/how-to-quickly-install-beautifulsoup-with-python) [How to Use OpenAI's o1 Reasoning Models in Your Applications\\\n\\\nLearn how to harness OpenAI's latest o1 series models for complex reasoning tasks in your apps.](https://www.firecrawl.dev/blog/how-to-use-openai-o1-reasoning-models-in-applications)",
      "metadata": {
        "title": "Why Companies Need a Data Strategy for Generative AI",
        "url": "https://www.firecrawl.dev/blog/why-companies-need-a-data-strategy-for-generative-ai"
      }
    },
    {
      "id": "d811cd51-2121-44a8-93dc-6638dbe1f36f",
      "source": "firecrawl/blog/how-to-generate-sitemaps-using-firecrawl-map-endpoint.md",
      "content": "---\ntitle: How to Generate Sitemaps Using Firecrawl's /map Endpoint: A Complete Guide\nurl: https://www.firecrawl.dev/blog/how-to-generate-sitemaps-using-firecrawl-map-endpoint\n---  \nIntroducing /extract - Get web data with a prompt [Try now](https://www.firecrawl.dev/extract)  \nNov 29, 2024  \nâ€¢  \n[![Bex Tuychiev image](https://www.firecrawl.dev/bex.jpg)Bex Tuychiev](https://x.com/bextuychiev)  \n# How to Generate Sitemaps Using Firecrawl's /map Endpoint: A Complete Guide  \n![How to Generate Sitemaps Using Firecrawl's /map Endpoint: A Complete Guide image](https://www.firecrawl.dev/images/blog/generating-sitemaps/how-to-generate-sitemap-using-firecrawl-map-endpoint.jpg)",
      "metadata": {
        "title": "How to Generate Sitemaps Using Firecrawl's /map Endpoint: A Complete Guide",
        "url": "https://www.firecrawl.dev/blog/how-to-generate-sitemaps-using-firecrawl-map-endpoint"
      }
    },
    {
      "id": "8bbdb25e-21c4-4ceb-8f49-dd374413ca1d",
      "source": "firecrawl/blog/how-to-generate-sitemaps-using-firecrawl-map-endpoint.md",
      "content": "Introduction\n\nIn this guide, weâ€™ll explore Firecrawlâ€™s `/map` endpoint - a powerful tool for automated website mapping and URL discovery. Weâ€™ll cover what it does, why it matters, and how to use it effectively in your web development workflow.  \nWebsite mapping has become increasingly critical in modern web development. As sites grow more complex with dynamic content and single-page applications, having a clear understanding of a siteâ€™s structure and URL hierarchy is essential for navigation and URL discovery.  \nThe /map endpoint helps solve common challenges like keeping track of site structure, identifying broken links, and ensuring search engines can properly crawl and index your content. Letâ€™s dive into how it works.",
      "metadata": {
        "title": "How to Generate Sitemaps Using Firecrawl's /map Endpoint: A Complete Guide",
        "url": "https://www.firecrawl.dev/blog/how-to-generate-sitemaps-using-firecrawl-map-endpoint"
      }
    },
    {
      "id": "c55421e7-4b00-40a8-b85d-3eaf7877b725",
      "source": "firecrawl/blog/how-to-generate-sitemaps-using-firecrawl-map-endpoint.md",
      "content": "Table of Contents\n\n- [Introduction](https://www.firecrawl.dev/blog/how-to-generate-sitemaps-using-firecrawl-map-endpoint#introduction)\n- [Understanding Firecrawlâ€™s `/map` Endpoint: Features and Benefits](https://www.firecrawl.dev/blog/how-to-generate-sitemaps-using-firecrawl-map-endpoint#understanding-firecrawls-map-endpoint-features-and-benefits)\n- [What is Site Mapping and Why is it Essential for Modern Websites?](https://www.firecrawl.dev/blog/how-to-generate-sitemaps-using-firecrawl-map-endpoint#what-is-site-mapping-and-why-is-it-essential-for-modern-websites)\n- [Guide to Sitemap Types: Visual vs XML Sitemaps](https://www.firecrawl.dev/blog/how-to-generate-sitemaps-using-firecrawl-map-endpoint#guide-to-sitemap-types-visual-vs-xml-sitemaps)\n- [How Firecrawlâ€™s `/map` Endpoint Solves These Challenges](https://www.firecrawl.dev/blog/how-to-generate-sitemaps-using-firecrawl-map-endpoint#how-firecrawls-map-endpoint-solves-these-challenges)\n- [Limitations of `/map` in the alpha stage](https://www.firecrawl.dev/blog/how-to-generate-sitemaps-using-firecrawl-map-endpoint#limitations-of-map-in-the-alpha-stage)\n- [Step-by-Step Guide to Using the /map Endpoint](https://www.firecrawl.dev/blog/how-to-generate-sitemaps-using-firecrawl-map-endpoint#step-by-step-guide-to-using-the-map-endpoint)\n- [Further Configuration Options for Website Mapping](https://www.firecrawl.dev/blog/how-to-generate-sitemaps-using-firecrawl-map-endpoint#further-configuration-options-for-website-mapping)\n- [Optimizing URL discovery with `search` parameter](https://www.firecrawl.dev/blog/how-to-generate-sitemaps-using-firecrawl-map-endpoint#optimizing-url-discovery-with-search-parameter)\n- [Essential `/map` parameters for customized site mappning](https://www.firecrawl.dev/blog/how-to-generate-sitemaps-using-firecrawl-map-endpoint#essential-map-parameters-for-customized-site-mappning)\n- [Comparing `/crawl` and `/map`: When to Use Each Endpoint](https://www.firecrawl.dev/blog/how-to-generate-sitemaps-using-firecrawl-map-endpoint#comparing-crawl-and-map-when-to-use-each-endpoint)\n- [Step-by-Step Guide: Creating XML Sitemaps with `/map`](https://www.firecrawl.dev/blog/how-to-generate-sitemaps-using-firecrawl-map-endpoint#step-by-step-guide-creating-xml-sitemaps-with-map)\n- [Advanced Visualization: Building Interactive Visual Sitemaps with `/map`](https://www.firecrawl.dev/blog/how-to-generate-sitemaps-using-firecrawl-map-endpoint#advanced-visualization-building-interactive-visual-sitemaps-with-map)\n- [Conclusion](https://www.firecrawl.dev/blog/how-to-generate-sitemaps-using-firecrawl-map-endpoint#conclusion)",
      "metadata": {
        "title": "How to Generate Sitemaps Using Firecrawl's /map Endpoint: A Complete Guide",
        "url": "https://www.firecrawl.dev/blog/how-to-generate-sitemaps-using-firecrawl-map-endpoint"
      }
    },
    {
      "id": "9bd6905e-0a27-4f98-8614-dae6001a95a1",
      "source": "firecrawl/blog/how-to-generate-sitemaps-using-firecrawl-map-endpoint.md",
      "content": "Understanding Firecrawlâ€™s `/map` Endpoint: Features and Benefits\n\nTo understand what the `/map` endpoint does, letâ€™s briefly cover whatâ€™s **site mapping** and why it is important.",
      "metadata": {
        "title": "How to Generate Sitemaps Using Firecrawl's /map Endpoint: A Complete Guide",
        "url": "https://www.firecrawl.dev/blog/how-to-generate-sitemaps-using-firecrawl-map-endpoint"
      }
    },
    {
      "id": "fad7cf9b-6512-45dc-84a4-3c6179724e4a",
      "source": "firecrawl/blog/how-to-generate-sitemaps-using-firecrawl-map-endpoint.md",
      "content": "Understanding Firecrawlâ€™s `/map` Endpoint: Features and Benefits > What is Site Mapping and Why is it Essential for Modern Websites?\n\nPut simply, a sitemap is a list or a diagram that communicates the structure of web pages in a website. It is useful for a number of reasons.  \nFirst, it helps developers, and site owners understand and maintain their websiteâ€™s structure. Having a clear overview of how pages are connected makes it easier to manage content, identify navigation issues, and ensure a logical flow for users.  \nSecond, sitemaps are crucial for SEO. Search engines use sitemaps to discover and index pages more efficiently. A well-structured sitemap helps ensure all your important content gets crawled and indexed properly.  \nThird, sitemaps can help identify potential issues like broken links, orphaned pages (pages with no incoming links), or circular references. This makes troubleshooting and maintenance much more manageable.  \nFinally, sitemaps are valuable for an AI agents to discover and navigate around websites, finding information faster than clicking through traditional navigation.",
      "metadata": {
        "title": "How to Generate Sitemaps Using Firecrawl's /map Endpoint: A Complete Guide",
        "url": "https://www.firecrawl.dev/blog/how-to-generate-sitemaps-using-firecrawl-map-endpoint"
      }
    },
    {
      "id": "a0a499a5-d770-4a65-9107-501d4a5e058b",
      "source": "firecrawl/blog/how-to-generate-sitemaps-using-firecrawl-map-endpoint.md",
      "content": "Understanding Firecrawlâ€™s `/map` Endpoint: Features and Benefits > Guide to Sitemap Types: Visual vs XML Sitemaps\n\nThere are two main types of sitemaps: visual and XML.  \nVisual sitemaps are diagrams or flowcharts that show how websites are structured at-a-glance. They typically use boxes, lines, and other visual elements to represent pages and their relationships. These visual representations make it easy for stakeholders, designers, and developers to quickly understand site hierarchy, navigation paths, and content organization. Theyâ€™re particularly useful during the planning and design phases of web development, as well as for communicating site structure to non-technical team members.  \n![Visual sitemap example showing hierarchical website structure with connected pages and sections for better site organization and planning](https://cdn.prod.website-files.com/649ae86ac1a4be707b656519/65a84b0bfe7e308f557dce75_Generate%20a%20sitemap%20online.webp)  \nSource: [Flowapp](https://www.flowmapp.com/features/generate-sitemap-online)  \nXML sitemaps are shown to the public much less frequently because they contain structured XML code that can look intimidating to non-technical users. But an XML sitemap is just an organized file containing all the URLs of a website that is readable to search engines. It includes important metadata about each URL like when it was last modified, how often it changes, and its relative importance. Search engines like Google use this information to crawl websites more intelligently and ensure all important pages are indexed. While XML sitemaps arenâ€™t meant for human consumption, they play a vital role in SEO and are often required for large websites to achieve optimal search engine visibility.  \n![Example of an XML sitemap showing structured URL data with lastmod, changefreq and priority tags for search engine optimization](https://www.firecrawl.dev/images/blog/generating-sitemaps/notebook_files/image.png)  \nSource: [DataCamp](https://www.datacamp.com/sitemap/es/tutorial/category.xml)",
      "metadata": {
        "title": "How to Generate Sitemaps Using Firecrawl's /map Endpoint: A Complete Guide",
        "url": "https://www.firecrawl.dev/blog/how-to-generate-sitemaps-using-firecrawl-map-endpoint"
      }
    },
    {
      "id": "257ad7e1-876a-46e1-a82e-0a8a48704cb5",
      "source": "firecrawl/blog/how-to-generate-sitemaps-using-firecrawl-map-endpoint.md",
      "content": "Understanding Firecrawlâ€™s `/map` Endpoint: Features and Benefits > How Firecrawlâ€™s `/map` Endpoint Solves These Challenges\n\nWhen you are building a website from scratch, you usually need a visual sitemap and can develop the XML one over the time as you add more pages. However, if you neglected these steps early on and suddenly find yourself with a massive website, possibly with thousands of URLs, creating either type of sitemap manually becomes an overwhelming task. This is where automated solutions like the `/map` endpoint can become invaluable.  \nThe real challenge of mapping existing sites is finding all the URLs that exist on your website. Without automated tools, youâ€™d need to manually click through every link, record every URL, and track which pages link to which others. Traditional web scraping solutions using Python libraries like `beautifulsoup`, `scrapy` or `lxml` can automate this process but they can quickly become useless when dealing with modern web applications that heavily rely on JavaScript for rendering content, use complex authentication systems, or implement rate limiting and bot detection.  \nThese traditional approaches are not only time-consuming but also error-prone, as itâ€™s easy to miss URLs in JavaScript-rendered content, dynamically generated pages, or deeply nested navigation menus.  \nThe `/map` endpoint solves these challenges and provides the fastest and easiest solution to go from a single URL to a map of the entire website. The /map endpoint is particularly useful in scenarios where:  \n- You want to give end-users control over which links to scrape by presenting them with options\n- Rapid discovery of all available links on a website is crucial\n- You need to focus on topic-specific content, you can use the search parameter to find relevant pages\n- You only want to extract data from particular sections of a website rather than crawling everything",
      "metadata": {
        "title": "How to Generate Sitemaps Using Firecrawl's /map Endpoint: A Complete Guide",
        "url": "https://www.firecrawl.dev/blog/how-to-generate-sitemaps-using-firecrawl-map-endpoint"
      }
    },
    {
      "id": "6fb60bf4-2bd4-4385-b1bc-eb768f7e5bfa",
      "source": "firecrawl/blog/how-to-generate-sitemaps-using-firecrawl-map-endpoint.md",
      "content": "Understanding Firecrawlâ€™s `/map` Endpoint: Features and Benefits > Limitations of `/map` in the alpha stage\n\nWhile the /map endpoint is still in alpha stage, it has some limitations. The endpoint prioritizes speed so it may not capture all website links. The vision for this endpoint is to maintain its blazing-fast speed and still capture every single link in a given website. Feedback and suggestions are welcome.",
      "metadata": {
        "title": "How to Generate Sitemaps Using Firecrawl's /map Endpoint: A Complete Guide",
        "url": "https://www.firecrawl.dev/blog/how-to-generate-sitemaps-using-firecrawl-map-endpoint"
      }
    },
    {
      "id": "579bb5a5-b509-41d4-8435-70fee424b6fd",
      "source": "firecrawl/blog/how-to-generate-sitemaps-using-firecrawl-map-endpoint.md",
      "content": "Step-by-Step Guide to Using the /map Endpoint\n\nFirecrawl is a scraping engine exposed as a REST API, which means you can use it from the command-line using cURL or by using one of its SDKs in Python, Node, Go or Rust. In this tutorial, we will use its Python SDK, so please install it in your environment:  \n```bash\npip install firecrawl-py\n\n```  \nThe next step is obtaining a Firecrawl API key by signing up at [firecrawl.dev](https://www.firecrawl.dev/) and choosing a plan (the free plan is fine for this tutorial).  \nOnce you have your API key, you should save it in a .env file, which provides a secure way to store sensitive credentials without exposing them in your code:  \n```bash\ntouch .env\necho \"FIRECRAWL_API_KEY='YOUR-API-KEY'\" >> .env\n\n```  \nThen, you should install python-dotenv to automatically load the variables in `.env` files in Python scripts and notebooks:  \n```bash\npip install python-dotenv\n\n```  \nThen, using the /map endpoint is as easy as the following code:  \n```python\nfrom firecrawl import FirecrawlApp\nfrom dotenv import load_dotenv; load_dotenv()\n\napp = FirecrawlApp()\n\nresponse = app.map_url(url=\"https://firecrawl.dev\")\n\n```  \nIn this code snippet, weâ€™re using the Firecrawl Python SDK to map a URL. Letâ€™s break down whatâ€™s happening:  \nFirst, we import two key components:  \n- FirecrawlApp from the firecrawl package, which provides the main interface to interact with Firecrawlâ€™s API\n- `load_dotenv` from `dotenv` to load our environment variables containing the API key  \nAfter importing, we initialize a FirecrawlApp instance, which automatically picks up our API key from the environment variables.  \nFinally, we make a request to map the URL `https://firecrawl.dev` using the `map_url()` method. This crawls the website and returns information about its structure and pages, taking about two seconds on my machine (the speed may vary based on internet speeds).  \nLetâ€™s look at the `response` dictionary:  \n```python\nresponse.keys()\n\n```  \n```python\ndict_keys(['success', 'links'])\n\n```  \nIt only has two keys: â€˜successâ€™ and â€˜linksâ€™. The â€˜successâ€™ key indicates whether the request was successful, and the â€˜linksâ€™ key contains the URLs found on the website:  \n```python\nlen(response['links'])\n\n```  \n```python\n98\n\n```",
      "metadata": {
        "title": "How to Generate Sitemaps Using Firecrawl's /map Endpoint: A Complete Guide",
        "url": "https://www.firecrawl.dev/blog/how-to-generate-sitemaps-using-firecrawl-map-endpoint"
      }
    },
    {
      "id": "742c6095-5861-42bc-ac14-c2e3cb3dd0e4",
      "source": "firecrawl/blog/how-to-generate-sitemaps-using-firecrawl-map-endpoint.md",
      "content": "Further Configuration Options for Website Mapping > Optimizing URL discovery with `search` parameter\n\nThe most notable feature of the endpoint is its `search` parameter. This parameter allows you to filter the URLs returned by the crawler based on specific patterns or criteria. For example, you can use it to only retrieve URLs containing certain keywords or matching specific paths. This makes it incredibly useful for focused crawling tasks where youâ€™re only interested in a subset of pages for massive websites.  \nLetâ€™s use this feature on the Stripe documentation and only search for pages related to taxes:  \n```python\nurl = \"https://docs.stripe.com\"\n\nresponse = app.map_url(url=url, params={\"search\": \"tax\"})\n\n```  \nThe response structure will be the same:  \n```python\nresponse[\"links\"][:10]\n\n```  \n```python\n['https://docs.stripe.com/tax',\\\n'https://docs.stripe.com/tax/how-tax-works',\\\n'https://docs.stripe.com/tax/reports',\\\n'https://docs.stripe.com/tax/calculating',\\\n'https://docs.stripe.com/api/tax_rates',\\\n'https://docs.stripe.com/tax/tax-codes',\\\n'https://docs.stripe.com/tax/zero-tax',\\\n'https://docs.stripe.com/tax/products-prices-tax-codes-tax-behavior',\\\n'https://docs.stripe.com/payments/checkout/taxes',\\\n'https://docs.stripe.com/billing/taxes/tax-rates']\n\n```  \nLetâ€™s count up the found links:  \n```python\nlen(response[\"links\"])\n\n```  \n```out\n2677\n\n```  \nMore than 2600 in only three seconds!",
      "metadata": {
        "title": "How to Generate Sitemaps Using Firecrawl's /map Endpoint: A Complete Guide",
        "url": "https://www.firecrawl.dev/blog/how-to-generate-sitemaps-using-firecrawl-map-endpoint"
      }
    },
    {
      "id": "21f9a3c9-406f-460e-84ad-1401155b32da",
      "source": "firecrawl/blog/how-to-generate-sitemaps-using-firecrawl-map-endpoint.md",
      "content": "Further Configuration Options for Website Mapping > Essential `/map` parameters for customized site mappning\n\nThere are some additional parameters /map accepts to control its behavior:  \n**`ignoreSitemap`**  \n- Type: boolean\n- Default: true\n- Description: When set to true, the crawler will not attempt to parse or use the websiteâ€™s `sitemap.xml` file during crawling. This can be useful when you want to discover pages through navigation links only.  \n**`sitemapOnly`**  \n- Type: boolean\n- Default: false\n- Description: When enabled, the crawler will exclusively return URLs found in the websiteâ€™s sitemap files, ignoring any links discovered through page crawling. This is useful for quickly indexing officially published pages.  \n**`includeSubdomains`**  \n- Type: boolean\n- Default: false\n- Description: Controls whether the crawler should follow and return links to subdomains (e.g., blog.example.com when crawling example.com). Enabling this provides a more comprehensive view of the entire web property.  \n**`limit`**  \n- Type: integer\n- Default: 5000\n- Description: Specifies the maximum number of URLs the crawler will return in a single request. This helps manage response sizes and processing time. Must be less than 5000 to prevent excessive server load for the time being.\n- Required range: x < 5000  \nLetâ€™s try running the Stripe example by including some of these parameters, like the `sitemapOnly` and `includeSubdomains` options set to True:  \n```python\nurl = \"https://docs.stripe.com\"\n\nresponse = app.map_url(url=url, params={\"search\": \"tax\", \"sitemapOnly\": True, \"includeSubdomains\": True})\nlen(response['links'])\n\n```  \n```python\n2712\n\n```  \nThis time, the link count increased.",
      "metadata": {
        "title": "How to Generate Sitemaps Using Firecrawl's /map Endpoint: A Complete Guide",
        "url": "https://www.firecrawl.dev/blog/how-to-generate-sitemaps-using-firecrawl-map-endpoint"
      }
    },
    {
      "id": "2c743061-d891-4ab8-9e84-5cfde1cf57af",
      "source": "firecrawl/blog/how-to-generate-sitemaps-using-firecrawl-map-endpoint.md",
      "content": "Comparing `/crawl` and `/map`: When to Use Each Endpoint\n\nIf you read our [separate guide on the `/crawl` endpoint](https://www.firecrawl.dev/blog/mastering-the-crawl-endpoint-in-firecrawl) of Firecrawl, you may notice one similarity between it and the `/map` endpoint:  \nIf you set the response format of crawl to â€œlinksâ€, you will also get a list of URLs found on the website. While the purpose is the same, there are huge differences performance-wise.  \nFirst, the `/crawl` endpoint is painfully slow for URL discovery, as evidenced by the execution times in the examples below:  \n```python\n%%time\n\nurl = \"books.toscrape.com\"\n\ncrawl_response = app.crawl_url(url=url, params={\"scrapeOptions\": {\"formats\": [\"links\"]}})\n\n```  \n```text\nCPU times: user 843 ms, sys: 470 ms, total: 1.31 s\nWall time: 2min 9s\n\n```  \n```python\n%%time\n\nurl = \"books.toscrape.com\"\n\nmap_response = app.map_url(url=url)\n\n```  \n```out\nCPU times: user 4.91 ms, sys: 3.58 ms, total: 8.49 ms\nWall time: 2.04 s\n\n```  \n![Comparison chart showing significant performance difference between map and crawl endpoints of Firecrawl with map being much faster](https://www.firecrawl.dev/images/blog/generating-sitemaps/notebook_files/notebook_42_0.png)  \nThis is because `/crawl` needs to fully load and parse each pageâ€™s HTML content, even when we only want the links. In contrast, `/map` is optimized specifically for URL discovery, making it much faster for generating sitemaps and link analysis.  \nBut, since `/map` is alpha, it doesnâ€™t capture as many links as `/crawl`:  \n```python\ncrawl_links = set()\n\nfor page in crawl_response['data']:\ncrawl_links.update(page[\"links\"])\n\nlen(crawl_links)\n\n```  \n```python\n1216\n\n```  \n```python\nlen(map_response[\"links\"])\n\n```  \n```python\n298\n\n```  \nIn fact, `/map` found three times less links than `/crawl`.  \nHowever, due to its speed, `/map` can still provide a good foundation for sitemap generation and its accuracy will increase as it progresses towards being stable.",
      "metadata": {
        "title": "How to Generate Sitemaps Using Firecrawl's /map Endpoint: A Complete Guide",
        "url": "https://www.firecrawl.dev/blog/how-to-generate-sitemaps-using-firecrawl-map-endpoint"
      }
    },
    {
      "id": "2afeedfc-2b1f-469e-9df4-e58b45c7990f",
      "source": "firecrawl/blog/how-to-generate-sitemaps-using-firecrawl-map-endpoint.md",
      "content": "Step-by-Step Guide: Creating XML Sitemaps with `/map`\n\nNow, letâ€™s see how to convert the links found with `/map` to an XML sitemap in Python. We will need to import the following packages:  \n```python\nfrom datetime import datetime\nimport xml.etree.ElementTree as ET\nfrom urllib.parse import urlparse\n\n```  \nWeâ€™ll use:  \n- `datetime`: To add timestamps to our sitemap entries\n- `xml.etree.ElementTree`: To create and structure the XML sitemap file\n- `urllib.parse`: To parse and validate URLs before adding them to the sitemap  \nLetâ€™s start by defining a new function - `create_xml_sitemap`:  \n```python\ndef create_xml_sitemap(urls, base_url):\n# Create the root element\nurlset = ET.Element(\"urlset\")\nurlset.set(\"xmlns\", \"http://www.sitemaps.org/schemas/sitemap/0.9\")\n\n```  \nIn the body of the function, we first create the root XML element named â€œurlsetâ€ using `ET.Element()`. Then we set its `xmlns` attribute to the sitemap schema URL `http://www.sitemaps.org/schemas/sitemap/0.9` to identify this as a valid sitemap XML document.  \nThen, we get the current date for providing a last modified date (since `/map` doesnâ€™t return the modified dates of pages):  \n```python\ndef create_xml_sitemap(urls, base_url):\n# Create the root element\n...\n\n# Get current date for lastmod\ntoday = datetime.now().strftime(\"%Y-%m-%d\")\n\n```  \nThen, we add each URL to the sitemap:  \n```python\ndef create_xml_sitemap(urls, base_url):\n...\n\n# Add each URL to the sitemap\nfor url in urls:\n# Only include URLs from the same domain\nif urlparse(url).netloc == urlparse(base_url).netloc:\nurl_element = ET.SubElement(urlset, \"url\")\nloc = ET.SubElement(url_element, \"loc\")\nloc.text = url\n\n# Add optional elements\nlastmod = ET.SubElement(url_element, \"lastmod\")\nlastmod.text = today\n\nchangefreq = ET.SubElement(url_element, \"changefreq\")\nchangefreq.text = \"monthly\"\n\npriority = ET.SubElement(url_element, \"priority\")\npriority.text = \"0.5\"\n\n```  \nThe loop iterates through each URL in the provided list and adds it to the sitemap XML structure. For each URL, it first checks if the domain matches the base URLâ€™s domain to ensure we only include URLs from the same website. If it matches, it creates a new `<url>` element and adds several child elements:  \n- `<loc>`: Contains the actual URL\n- `<lastmod>`: Set to todayâ€™s date to indicate when the page was last modified\n- `<changefreq>`: Set to â€œmonthlyâ€ to suggest how often the page content changes\n- `<priority>`: Set to â€œ0.5â€ to indicate the relative importance of the page  \nThis creates a properly formatted sitemap entry for each URL following the Sitemap XML protocol specifications.  \nAfter the loop finishes, we create and return the XML string:  \n```python\ndef create_xml_sitemap(urls, base_url):\n...\n\n# Add each URL to the sitemap\nfor url in urls:\n...\n\n# Create the XML string\nreturn ET.tostring(urlset, encoding=\"unicode\", method=\"xml\")\n\n```  \nHere is the full function:  \n```python\ndef create_xml_sitemap(urls, base_url):\n# Create the root element\nurlset = ET.Element(\"urlset\")\nurlset.set(\"xmlns\", \"http://www.sitemaps.org/schemas/sitemap/0.9\")\n\n# Get current date for lastmod\ntoday = datetime.now().strftime(\"%Y-%m-%d\")\n\n# Add each URL to the sitemap\nfor url in urls:\n# Only include URLs from the same domain\nif urlparse(url).netloc == urlparse(base_url).netloc:\nurl_element = ET.SubElement(urlset, \"url\")\nloc = ET.SubElement(url_element, \"loc\")\nloc.text = url\n\n# Add optional elements\nlastmod = ET.SubElement(url_element, \"lastmod\")\nlastmod.text = today\n\nchangefreq = ET.SubElement(url_element, \"changefreq\")\nchangefreq.text = \"monthly\"\n\npriority = ET.SubElement(url_element, \"priority\")\npriority.text = \"0.5\"\n\n# Create the XML string\nreturn ET.tostring(urlset, encoding=\"unicode\", method=\"xml\")\n\n```  \nLetâ€™s use the function on the links returned by the last `/map` endpoint use:  \n```python\nbase_url = \"https://books.toscrape.com\"\nlinks = map_response[\"links\"]\n\nxml_sitemap = create_xml_sitemap(links, base_url)\n\n# Save to file\nwith open(\"sitemap.xml\", \"w\", encoding=\"utf-8\") as f:\nf.write('<?xml version=\"1.0\" encoding=\"UTF-8\"?>n')\nf.write(xml_sitemap)\n\n```  \nHere is what the file looks like after saving:  \n![XML sitemap example showing structured website data with URLs, last modified dates, change frequencies and priorities, generated using Firecrawl](https://www.firecrawl.dev/blog/notebook_files/image.png)  \nSuch `sitemap.xml` file provides a standardized way for search engines to discover and crawl all pages on your website.",
      "metadata": {
        "title": "How to Generate Sitemaps Using Firecrawl's /map Endpoint: A Complete Guide",
        "url": "https://www.firecrawl.dev/blog/how-to-generate-sitemaps-using-firecrawl-map-endpoint"
      }
    },
    {
      "id": "95771421-eaa4-465b-b656-1a900933bd81",
      "source": "firecrawl/blog/how-to-generate-sitemaps-using-firecrawl-map-endpoint.md",
      "content": "Advanced Visualization: Building Interactive Visual Sitemaps with `/map`\n\nIf you want a visual sitemap of a website, you donâ€™t have to sign up for expensive third-party services and platforms. You can automatically generate one using the `/map` endpoint, Plotly and a few other libraries.  \nThe resulting graph would like the following:  \n![Interactive Sankey diagram visualization showing hierarchical structure of Stripe documentation with color-coded sections and flow widths representing page counts, generated using Firecrawl's /map endpoint](https://www.firecrawl.dev/images/blog/generating-sitemaps/images/stripe_docs.png)  \nThe Sankey diagram above visualizes the hierarchical structure of [the Stripe documentation](https://docs.stripe.org/) (which is quite large) by showing how pages are organized and connected across different sections. The width of each flow represents the number of pages in that section, making it easy to identify which parts of the website contain the most content. The colors help distinguish between different sections and their subsections.  \nThe diagram starts from a central root node and branches out into main sections of the website. Each section can then split further into subsections, creating a tree-like visualization of the siteâ€™s architecture. This makes it simple to understand the overall organization and identify potential navigation or structural issues.  \nFor example, you can quickly spot which sections are the largest (the API section), how content is distributed across different areas, and whether thereâ€™s a logical grouping of related pages. This visualization is particularly useful for content strategists, SEO specialists, and web architects who need to analyze and optimize website structure.  \n[The script that generated this plot](https://github.com/FirstClassML/firecrawl_articles/blob/main/3_generating_sitemap/sitemap_generator.py) contains more than 400 lines of code and I made it fully customizable. The code in `sitemap_generator.py` follows a modular, object-oriented approach with several key components:  \n1. A `HierarchyBuilder` class that analyzes URLs returned my `/map` or `/crawl` and builds a tree-like data structure up to 4 levels deep.\n2. A `SankeyDataPreparator` class that transforms this hierarchy into a format suitable for visualization, using thresholds to control complexity\n3. A `SitemapVisualizer` class that creates the final Sankey diagram with proper styling and interactivity  \nThe script automatically handles things like grouping smaller sections together, truncating long labels, generating color schemes, and adding hover information (the generated plots are all interactive through Plotly). All aspects like minimum branch size, relative thresholds, label length, and color schemes can be customized through parameters.  \nHere is another plot generated for the [PyData.org](https://pydata.org/) website:  \n![Interactive Sankey diagram showing hierarchical website structure of PyData.org with color-coded sections, flow visualization and navigation paths for improved site architecture understanding, generated using Firecrawl's map endpoint and Plotly](https://www.firecrawl.dev/images/blog/generating-sitemaps/images/pydata.png)",
      "metadata": {
        "title": "How to Generate Sitemaps Using Firecrawl's /map Endpoint: A Complete Guide",
        "url": "https://www.firecrawl.dev/blog/how-to-generate-sitemaps-using-firecrawl-map-endpoint"
      }
    },
    {
      "id": "ddbae863-6a63-42c2-9638-1dd2b16378c3",
      "source": "firecrawl/blog/how-to-generate-sitemaps-using-firecrawl-map-endpoint.md",
      "content": "Conclusion\n\nThe `/map` endpoint represents a powerful tool in the modern web developerâ€™s toolkit, offering a fast and efficient way to discover and analyze website structures. While still in alpha, it provides significant advantages:  \n- **Speed**: As demonstrated, itâ€™s significantly faster than traditional crawling methods, making it ideal for quick site analysis\n- **Flexibility**: With parameters like `search`, `sitemapOnly`, and `includeSubdomains`, it can be tailored to specific needs\n- **Practical Applications**: From generating XML sitemaps for SEO to creating visual site hierarchies, the endpoint serves multiple use cases  \nWhile it may not capture every single URL compared to full crawling solutions, its speed and ease of use make it an excellent choice for rapid site mapping and initial structure analysis. As the endpoint continues to evolve, its combination of performance and accuracy will make it an increasingly valuable tool for website maintenance, SEO optimization, and content strategy.  \nTo discover what more Firecrawl has to offer, be sure to read the following related resources:  \n- [Firecrawl Documentation](https://docs.firecrawl.dev/)\n- [Firecrawl Blog](https://www.firecrawl.dev/blog/category/tutorials)\n- [Firecrawl API Reference](https://docs.firecrawl.dev/api-reference/introduction)",
      "metadata": {
        "title": "How to Generate Sitemaps Using Firecrawl's /map Endpoint: A Complete Guide",
        "url": "https://www.firecrawl.dev/blog/how-to-generate-sitemaps-using-firecrawl-map-endpoint"
      }
    },
    {
      "id": "6f01fe44-cd77-4f1c-ab56-b3287d4a8671",
      "source": "firecrawl/blog/how-to-generate-sitemaps-using-firecrawl-map-endpoint.md",
      "content": "Frequently Asked Questions > How fast is Firecrawlâ€™s `/map` endpoint?\n\nThe /map endpoint typically processes websites in 2-3 seconds, compared to several minutes with traditional crawling methods.",
      "metadata": {
        "title": "How to Generate Sitemaps Using Firecrawl's /map Endpoint: A Complete Guide",
        "url": "https://www.firecrawl.dev/blog/how-to-generate-sitemaps-using-firecrawl-map-endpoint"
      }
    },
    {
      "id": "769bd614-0a15-4529-bc43-e57467b37960",
      "source": "firecrawl/blog/how-to-generate-sitemaps-using-firecrawl-map-endpoint.md",
      "content": "Frequently Asked Questions > Can I use `/map` endpoint for large websites?\n\nYes, the `/map` endpoint can handle large websites with a current limit of 5000 URLs per request, making it suitable for most medium to large websites.",
      "metadata": {
        "title": "How to Generate Sitemaps Using Firecrawl's /map Endpoint: A Complete Guide",
        "url": "https://www.firecrawl.dev/blog/how-to-generate-sitemaps-using-firecrawl-map-endpoint"
      }
    },
    {
      "id": "d7612659-b859-4fdd-8c73-81b0b2e4004d",
      "source": "firecrawl/blog/how-to-generate-sitemaps-using-firecrawl-map-endpoint.md",
      "content": "Frequently Asked Questions > Whatâ€™s the difference between XML and visual sitemaps?\n\nXML sitemaps are machine-readable files used by search engines for indexing, while visual sitemaps provide a graphical representation of website structure for human understanding and planning.  \nArticle updated recently  \n[ðŸ”¥](https://www.firecrawl.dev/)",
      "metadata": {
        "title": "How to Generate Sitemaps Using Firecrawl's /map Endpoint: A Complete Guide",
        "url": "https://www.firecrawl.dev/blog/how-to-generate-sitemaps-using-firecrawl-map-endpoint"
      }
    },
    {
      "id": "94e5fdfb-68ab-4f76-b328-a3cf7d43ae09",
      "source": "firecrawl/blog/how-to-generate-sitemaps-using-firecrawl-map-endpoint.md",
      "content": "Ready to _Build?_\n\nStart scraping web data for your AI apps today.  \nNo credit card needed.  \nGet Started",
      "metadata": {
        "title": "How to Generate Sitemaps Using Firecrawl's /map Endpoint: A Complete Guide",
        "url": "https://www.firecrawl.dev/blog/how-to-generate-sitemaps-using-firecrawl-map-endpoint"
      }
    },
    {
      "id": "93e86131-13b7-4d0f-9d7c-b6d514e02aed",
      "source": "firecrawl/blog/how-to-generate-sitemaps-using-firecrawl-map-endpoint.md",
      "content": "About the Author\n\n[![Bex Tuychiev image](https://www.firecrawl.dev/bex.jpg)\\\nBex Tuychiev@bextuychiev](https://x.com/bextuychiev)  \nBex is a Top 10 AI writer on Medium and a Kaggle Master with over 15k followers. He loves writing detailed guides, tutorials, and notebooks on complex data science and machine learning topics",
      "metadata": {
        "title": "How to Generate Sitemaps Using Firecrawl's /map Endpoint: A Complete Guide",
        "url": "https://www.firecrawl.dev/blog/how-to-generate-sitemaps-using-firecrawl-map-endpoint"
      }
    },
    {
      "id": "d567af9c-4df9-44b3-8ff2-2e707a382366",
      "source": "firecrawl/blog/how-to-generate-sitemaps-using-firecrawl-map-endpoint.md",
      "content": "About the Author > More articles by Bex Tuychiev\n\n[Building an Automated Price Tracking Tool\\\n\\\nBuild an automated e-commerce price tracker in Python. Learn web scraping, price monitoring, and automated alerts using Firecrawl, Streamlit, PostgreSQL.](https://www.firecrawl.dev/blog/automated-price-tracking-tutorial-python) [Web Scraping Automation: How to Run Scrapers on a Schedule\\\n\\\nLearn how to automate web scraping in Python using free tools like schedule, asyncio, cron jobs and GitHub Actions. This comprehensive guide covers local and cloud-based scheduling methods to run scrapers reliably in 2025.](https://www.firecrawl.dev/blog/automated-web-scraping-free-2025) [Automated Data Collection - A Comprehensive Guide\\\n\\\nLearn how to build robust automated data collection systems using modern tools and best practices. This guide covers everything from selecting the right tools to implementing scalable collection pipelines.](https://www.firecrawl.dev/blog/automated-data-collection-guide) [BeautifulSoup4 vs. Scrapy - A Comprehensive Comparison for Web Scraping in Python\\\n\\\nLearn the key differences between BeautifulSoup4 and Scrapy for web scraping in Python. Compare their features, performance, and use cases to choose the right tool for your web scraping needs.](https://www.firecrawl.dev/blog/beautifulsoup4-vs-scrapy-comparison) [How to Build an Automated Competitor Price Monitoring System with Python\\\n\\\nLearn how to build an automated competitor price monitoring system in Python that tracks prices across e-commerce sites, provides real-time comparisons, and maintains price history using Firecrawl, Streamlit, and GitHub Actions.](https://www.firecrawl.dev/blog/automated-competitor-price-scraping) [Scraping Company Data and Funding Information in Bulk With Firecrawl and Claude\\\n\\\nLearn how to build a web scraper in Python that gathers company details, funding rounds, and investor information from public sources like Crunchbase using Firecrawl and Claude for automated data collection and analysis.](https://www.firecrawl.dev/blog/crunchbase-scraping-with-firecrawl-claude) [Data Enrichment: A Complete Guide to Enhancing Your Data Quality\\\n\\\nLearn how to enrich your data quality with a comprehensive guide covering data enrichment tools, best practices, and real-world examples. Discover how to leverage modern solutions like Firecrawl to automate data collection, validation, and integration for better business insights.](https://www.firecrawl.dev/blog/complete-guide-to-data-enrichment) [Building an Intelligent Code Documentation RAG Assistant with DeepSeek and Firecrawl\\\n\\\nLearn how to build an intelligent documentation assistant powered by DeepSeek and RAG (Retrieval Augmented Generation) that can answer questions about any documentation website by combining language models with efficient information retrieval.](https://www.firecrawl.dev/blog/deepseek-rag-documentation-assistant)",
      "metadata": {
        "title": "How to Generate Sitemaps Using Firecrawl's /map Endpoint: A Complete Guide",
        "url": "https://www.firecrawl.dev/blog/how-to-generate-sitemaps-using-firecrawl-map-endpoint"
      }
    },
    {
      "id": "f728f9fc-d516-436a-9cab-11e88702b315",
      "source": "firecrawl/blog/launch-week-ii-day-5-introducing-two-new-actions.md",
      "content": "---\ntitle: Launch Week II - Day 5: Introducing New Actions\nurl: https://www.firecrawl.dev/blog/launch-week-ii-day-5-introducing-two-new-actions\n---  \nIntroducing /extract - Get web data with a prompt [Try now](https://www.firecrawl.dev/extract)  \nNovember 1, 2024  \nâ€¢  \n[![Eric Ciarla image](https://www.firecrawl.dev/eric-img.jpeg)Eric Ciarla](https://x.com/ericciarla)  \n# Launch Week II - Day 5: Introducing New Actions  \n![Launch Week II - Day 5: Introducing New Actions image](https://www.firecrawl.dev/images/blog/firecrawl-new-actions.jpg)  \nWelcome to Day 5 of Firecrawlâ€™s second Launch Week! Today, weâ€™re excited to introduce two powerful new actions that will enhance your web scraping capabilities: **Scrape** and **Wait for Selector**.  \n**Introducing the New Actions**  \nFirecrawl allows you to perform various actions on a web page before scraping its content. This is particularly useful for interacting with dynamic content, navigating through pages, or accessing content that requires user interaction.  \nWeâ€™re excited to introduce two powerful new actions:  \n1. **Scrape**: Capture the current page content at any point during your interaction sequence, returning both URL and HTML.\n2. **Wait for Selector**: Wait for a specific element to appear on the page before proceeding, ensuring more reliable automation.  \nHereâ€™s how you can incorporate these new actions:  \n```json\nactions = [\\\n{\"type\": \"scrape\"},\\\n{\"type\": \"wait\", \"selector\": \"#my-element\"},\\\n]\n\n```  \nFor more details about the actions parameters, refer to the [API Reference](https://docs.firecrawl.dev/api-reference/endpoint/scrape).  \n**Why Use These New Actions?**  \n- **Flexibility**: Capture content at multiple points in your interaction flow.\n- **Reliability**: Ensure elements are loaded before proceeding, reducing errors.\n- **Efficiency**: Automate complex navigation and interactions seamlessly.  \n**Whatâ€™s Next?**  \nThatâ€™s a wrap for Day 5 of Launch Week II! Happy scraping, and stay tuned for more exciting updates from [Launch Week II](https://www.firecrawl.dev/launch-week)!  \nArticle updated recently  \n[ðŸ”¥](https://www.firecrawl.dev/)",
      "metadata": {
        "title": "Launch Week II - Day 5: Introducing New Actions",
        "url": "https://www.firecrawl.dev/blog/launch-week-ii-day-5-introducing-two-new-actions"
      }
    },
    {
      "id": "cba30a32-39b9-44a3-ace2-56fef9b1a14c",
      "source": "firecrawl/blog/launch-week-ii-day-5-introducing-two-new-actions.md",
      "content": "Ready to _Build?_\n\nStart scraping web data for your AI apps today.  \nNo credit card needed.  \nGet Started",
      "metadata": {
        "title": "Launch Week II - Day 5: Introducing New Actions",
        "url": "https://www.firecrawl.dev/blog/launch-week-ii-day-5-introducing-two-new-actions"
      }
    },
    {
      "id": "c878d2ba-f99f-4bc6-aa2b-0de6da43d9e7",
      "source": "firecrawl/blog/launch-week-ii-day-5-introducing-two-new-actions.md",
      "content": "About the Author\n\n[![Eric Ciarla image](https://www.firecrawl.dev/eric-img.jpeg)\\\nEric Ciarla@ericciarla](https://x.com/ericciarla)  \nEric Ciarla is the Chief Operating Officer (COO) of Firecrawl and leads marketing. He also worked on Mendable.ai and sold it to companies like Snapchat, Coinbase, and MongoDB.\nPreviously worked at Ford and Fracta as a Data Scientist. Eric also co-founded SideGuide, a tool for learning code within VS Code with 50,000 users.",
      "metadata": {
        "title": "Launch Week II - Day 5: Introducing New Actions",
        "url": "https://www.firecrawl.dev/blog/launch-week-ii-day-5-introducing-two-new-actions"
      }
    },
    {
      "id": "9580e3d4-8bc3-40a3-9a73-26078ff684c7",
      "source": "firecrawl/blog/launch-week-ii-day-5-introducing-two-new-actions.md",
      "content": "About the Author > More articles by Eric Ciarla\n\n[How to Create an llms.txt File for Any Website\\\n\\\nLearn how to generate an llms.txt file for any website using the llms.txt Generator and Firecrawl.](https://www.firecrawl.dev/blog/How-to-Create-an-llms-txt-File-for-Any-Website) [Cloudflare Error 1015: How to solve it?\\\n\\\nCloudflare Error 1015 is a rate limiting error that occurs when Cloudflare detects that you are exceeding the request limit set by the website owner.](https://www.firecrawl.dev/blog/cloudflare-error-1015-how-to-solve-it) [Build an agent that checks for website contradictions\\\n\\\nUsing Firecrawl and Claude to scrape your website's data and look for contradictions.](https://www.firecrawl.dev/blog/contradiction-agent) [Why Companies Need a Data Strategy for Generative AI\\\n\\\nLearn why a well-defined data strategy is essential for building robust, production-ready generative AI systems, and discover practical steps for curation, maintenance, and integration.](https://www.firecrawl.dev/blog/why-companies-need-a-data-strategy-for-generative-ai) [Getting Started with OpenAI's Predicted Outputs for Faster LLM Responses\\\n\\\nA guide to leveraging Predicted Outputs to speed up LLM tasks with GPT-4o models.](https://www.firecrawl.dev/blog/getting-started-with-predicted-outputs-openai) [How to easily install requests with pip and python\\\n\\\nA tutorial on installing the requests library in Python using various methods, with usage examples and troubleshooting tips](https://www.firecrawl.dev/blog/how-to-easily-install-requests-with-pip-and-python) [How to quickly install BeautifulSoup with Python\\\n\\\nA guide on installing the BeautifulSoup library in Python using various methods, with usage examples and troubleshooting tips](https://www.firecrawl.dev/blog/how-to-quickly-install-beautifulsoup-with-python) [How to Use OpenAI's o1 Reasoning Models in Your Applications\\\n\\\nLearn how to harness OpenAI's latest o1 series models for complex reasoning tasks in your apps.](https://www.firecrawl.dev/blog/how-to-use-openai-o1-reasoning-models-in-applications)",
      "metadata": {
        "title": "Launch Week II - Day 5: Introducing New Actions",
        "url": "https://www.firecrawl.dev/blog/launch-week-ii-day-5-introducing-two-new-actions"
      }
    },
    {
      "id": "107a4db3-73c1-4d22-a63b-bd1b877c8e64",
      "source": "firecrawl/blog/sales-lead-extractor-python-ai.md",
      "content": "---\ntitle: How to Build a Bulk Sales Lead Extractor in Python Using AI\nurl: https://www.firecrawl.dev/blog/sales-lead-extractor-python-ai\n---  \nIntroducing /extract - Get web data with a prompt [Try now](https://www.firecrawl.dev/extract)  \nJan 12, 2025  \nâ€¢  \n[![Bex Tuychiev image](https://www.firecrawl.dev/bex.jpg)Bex Tuychiev](https://x.com/bextuychiev)  \n# How to Build a Bulk Sales Lead Extractor in Python Using AI  \n![How to Build a Bulk Sales Lead Extractor in Python Using AI image](https://www.firecrawl.dev/images/blog/sales_lead_extractor/sales-lead-extractor.jpg)",
      "metadata": {
        "title": "How to Build a Bulk Sales Lead Extractor in Python Using AI",
        "url": "https://www.firecrawl.dev/blog/sales-lead-extractor-python-ai"
      }
    },
    {
      "id": "3078b290-5462-4dbf-bcac-3875d3003fa5",
      "source": "firecrawl/blog/sales-lead-extractor-python-ai.md",
      "content": "Introduction\n\nSales teams waste a lot of time gathering lead information from websites by hand. The Sales Lead Extractor app we are going to build in the article makes this task much faster by using smart web scraping and a simple interface. Sales professionals can upload a list of website URLs and pick what data they want to collect. The app then uses Streamlit and Firecrawlâ€™s API to automatically gather that information.  \nThe best part about this app is how flexible and simple it is to use. Users arenâ€™t stuck with preset data fields - they can choose exactly what information they need, like company names, contact details, or team sizes. The appâ€™s AI technology reads through websites and turns the data into a clean Excel file. What used to take hours of copying and pasting can now be done in just a few minutes.  \nLetâ€™s dive into building this powerful lead extraction tool step by step, starting with covering the prerequisite concepts.",
      "metadata": {
        "title": "How to Build a Bulk Sales Lead Extractor in Python Using AI",
        "url": "https://www.firecrawl.dev/blog/sales-lead-extractor-python-ai"
      }
    },
    {
      "id": "f4cea03e-b961-4b9b-8674-04d44858c0f9",
      "source": "firecrawl/blog/sales-lead-extractor-python-ai.md",
      "content": "Prerequisites\n\nBefore we dive into building the Sales Lead Extractor, make sure you have the following prerequisites in place:  \n1. **Python Environment Setup**\n- Python 3.8 or higher installed\n- A code editor of your choice\n2. **Required Accounts**\n- A Firecrawl account with an API key (sign up at [https://firecrawl.dev](https://firecrawl.dev/))\n- GitHub account (if you plan to deploy the app)  \nDonâ€™t worry if you are completely new to Firecrawl as we will its basics in the next section.  \n3. **Technical Knowledge**\n- Basic understanding of Python programming\n- Familiarity with web concepts (URLs, HTML)\n- Basic understanding of data structures (JSON, CSV)\n4. **Project Dependencies**\n- Streamlit for the web interface\n- Firecrawl for AI-powered web scraping\n- Pydantic for data validation\n- Pandas for data manipulation",
      "metadata": {
        "title": "How to Build a Bulk Sales Lead Extractor in Python Using AI",
        "url": "https://www.firecrawl.dev/blog/sales-lead-extractor-python-ai"
      }
    },
    {
      "id": "598a0f5f-098e-4c31-98bb-9f2ead9aff42",
      "source": "firecrawl/blog/sales-lead-extractor-python-ai.md",
      "content": "Firecrawl Basics\n\nFirecrawl is an AI-powered web scraping API that takes a different approach from traditional scraping libraries. Instead of relying on HTML selectors or XPaths, it uses natural language understanding to identify and extract content. Hereâ€™s a simple example:  \n```python\nfrom firecrawl import FirecrawlApp\nfrom dotenv import load_dotenv\nfrom pydantic import BaseModel\n\nload_dotenv()\n\nclass CompanyInfo(BaseModel):\ncompany: str\nemail: str\n\napp = FirecrawlApp()\nschema = CompanyInfo.model_json_schema()\n\ndata = app.scrape_url(\n\"https://openai.com\",\nparams={\n\"formats\": [\"extract\"],\n\"extract\": {\n\"prompt\": \"Find the company name and contact email\",\n\"schema\": schema\n}\n}\n)\n\ncompany_info = CompanyInfo(**data[\"extract\"])\nprint(company_info) # Shows validated company info\n\n```  \nIn this example, we:  \n1. Define a Pydantic model `CompanyInfo` to structure the scraping process\n2. Initialize the Firecrawl client\n3. Convert the Pydantic model to a JSON schema that Firecrawl understands\n4. Make an API call to extract company info from `openai.com` using natural language  \nThe key advantage of Firecrawl is that we can describe what we want to extract in plain English (â€œFind the company name and contact emailâ€) rather than writing complex selectors. The AI understands the intent and returns structured data matching our schema.  \nNow that we understand how Firecrawlâ€™s AI-powered extraction works, letâ€™s build a complete sales lead extraction tool that leverages these capabilities at scale.",
      "metadata": {
        "title": "How to Build a Bulk Sales Lead Extractor in Python Using AI",
        "url": "https://www.firecrawl.dev/blog/sales-lead-extractor-python-ai"
      }
    },
    {
      "id": "bba918de-c1a8-4f4d-adc6-b532f6715de8",
      "source": "firecrawl/blog/sales-lead-extractor-python-ai.md",
      "content": "Project Setup\n\nLetâ€™s start by setting up our development environment and installing the necessary dependencies.  \n1. **Create a working directory**  \nFirst, create a working directory and initialize a virtual environment:  \n```bash\nmkdir sales-lead-extractor\ncd sales-lead-extractor\npython -m venv venv\nsource venv/bin/activate # On Windows use: venvScriptsactivate\n\n```  \n2. **Install Dependencies**  \nWeâ€™ll use Poetry for dependency management. If you havenâ€™t installed Poetry yet:  \n```bash\ncurl -sSL https://install.python-poetry.org | python3 -\n\n```  \nThen, initialize it inside the current working directory:  \n```bash\npoetry init\n\n```  \nType â€œ^3.10â€ when asked for the Python version but, donâ€™t specify the dependencies interactively.  \nNext, install the project dependencies with the `add` command:  \n```bash\npoetry add streamlit firecrawl-py pandas pydantic openpyxl python-dotenv\n\n```  \n3. **Build the project structure**  \n```bash\nmkdir data src\ntouch .gitignore README.md .env src/{app.py,models.py,scraper.py}\n\n```  \n4. **Configure environment variables**  \nInside the `.env` file in the root directory, add your [Firecrawl API key](https://firecrawl.dev/):  \n```plaintext\nFIRECRAWL_API_KEY=your_api_key_here\n\n```  \n5. **Start the app UI**  \nRun the Streamlit app (which is blank just now) to ensure everything is working:  \n```bash\npoetry run streamlit run src/app.py\n\n```  \nYou should see the Streamlit development server start up and your default browser open to the appâ€™s interface. Keep this tab open to see the changes we make to the app in the next steps.",
      "metadata": {
        "title": "How to Build a Bulk Sales Lead Extractor in Python Using AI",
        "url": "https://www.firecrawl.dev/blog/sales-lead-extractor-python-ai"
      }
    },
    {
      "id": "ff0b3ba1-b912-4f99-b7ba-cabb45a9e13f",
      "source": "firecrawl/blog/sales-lead-extractor-python-ai.md",
      "content": "Building the Lead Extraction App Step-by-Step\n\nWe will take a top-down approach to building the app: starting with the high-level UI components and user flows, then implementing the underlying functionality piece by piece. This approach will help us validate the appâ€™s usability early and ensure weâ€™re building exactly what users need.",
      "metadata": {
        "title": "How to Build a Bulk Sales Lead Extractor in Python Using AI",
        "url": "https://www.firecrawl.dev/blog/sales-lead-extractor-python-ai"
      }
    },
    {
      "id": "65ea165c-d735-40c1-8e6f-cdf502f2af9a",
      "source": "firecrawl/blog/sales-lead-extractor-python-ai.md",
      "content": "Building the Lead Extraction App Step-by-Step > Step 1: Adding a file upload component\n\nWe start with the following imports at the top of `src/app.py`:  \n```python\nimport streamlit as st\nimport pandas as pd\nfrom typing import List, Dict\n\n```  \nThen, we will define the `main` function. Inside, we write the page title and subtitle and add a component for file uploads:  \n```python\ndef main():\nst.title(\"Sales Lead Extractor\")\nst.write(\n\"Upload a file with website URLs of your leads and define the fields to extract\"\n)\n\n# File upload\nuploaded_file = st.file_uploader(\"Choose a file\", type=[\"csv\", \"txt\"])\n\n```  \nThe file uploader component allows users to upload either CSV or TXT files containing URLs. Weâ€™ll need a helper function to parse these files and extract the URLs.  \n```python\n# Paste the function after the imports but before the main() function\ndef load_urls(uploaded_file) -> List[str]:\n\"\"\"Load URLs from uploaded file\"\"\"\nif uploaded_file.name.endswith(\".csv\"):\ndf = pd.read_csv(uploaded_file, header=None)\nreturn df.iloc[:, 0].tolist()\nelse:\ncontent = uploaded_file.getvalue().decode()\nreturn [url.strip() for url in content.split(\"n\") if url.strip()]\n\n```  \nThe `load_urls()` function handles both CSV and TXT file formats:  \n- For CSV files: It assumes URLs are in the first column and uses pandas to read them\n- For TXT files: It reads the content and splits by newlines to get individual URLs  \nIn both cases, it returns a clean list of URLs with any empty lines removed. Note that the function expects files in the following format:  \n```plaintext\nhttps://website1.com\nhttps://website2.com\nhttps://website3.com\n\n```  \nFor simplicity, we are skipping implementing security best practices to validate the format of the URLs and the overall files.  \nWe can now use this in our main function to process the uploaded file:  \n```python\n# Continue the main function\ndef main():\n...\n\nif uploaded_file:\nurls = load_urls(uploaded_file)\nst.write(f\"Loaded {len(urls)} URLs\")\n\n```  \nThis gives users immediate feedback about how many URLs were successfully loaded from their file.  \nNow, add this to the end of `src/app.py`:  \n```python\nif __name__ == \"__main__\":\nmain()\n\n```",
      "metadata": {
        "title": "How to Build a Bulk Sales Lead Extractor in Python Using AI",
        "url": "https://www.firecrawl.dev/blog/sales-lead-extractor-python-ai"
      }
    },
    {
      "id": "bb3ccd4c-ee8a-4f42-ad4d-1deae499090f",
      "source": "firecrawl/blog/sales-lead-extractor-python-ai.md",
      "content": "Building the Lead Extraction App Step-by-Step > Step 2: Adding a form to define fields interactively\n\nOnce a user uploads their file, they must be presented with another form to specify what information they are looking to extract from the leads. To add this functionality, continue `src/app.py` with the following code block:  \n```python\ndef main():\n# ... the rest of the function\n\nif uploaded_file:\nurls = load_urls(uploaded_file)\nst.write(f\"Loaded {len(urls)} URLs\")\n\n# <THIS PART IS NEW> #\n# Dynamic field input\nst.subheader(\"Define Fields to Extract\")\n\nfields: Dict[str, str] = {}\ncol1, col2 = st.columns(2)\n\nwith col1:\nnum_fields = st.number_input(\n\"Number of fields\", min_value=1, max_value=10, value=3\n)\n\nfor i in range(num_fields):\ncol1, col2 = st.columns(2)\nwith col1:\nfield_name = st.text_input(f\"Field {i+1} name\", key=f\"name_{i}\")\n\n# Convert field_name to lower snake case\nfield_name = field_name.lower().replace(\" \", \"_\")\n\nwith col2:\nfield_desc = st.text_input(f\"Field {i+1} description\", key=f\"desc_{i}\")\n\nif field_name and field_desc:\nfields[field_name] = field_desc\n\n```  \nHereâ€™s whatâ€™s happening:  \n1. We create a dictionary called `fields` to store the field definitions\n2. Using Streamlitâ€™s column layout, we first ask users how many fields they want to extract (1-10)\n3. For each field, we create two columns:\n- Left column: Input for the field name (automatically converted to snake_case)\n- Right column: Input for field description that explains what to extract\n4. Valid field name/description pairs are added to the fields dictionary\n5. This fields dictionary will later be used to:\n- Create a dynamic Pydantic model for validation\n- Guide the AI in extracting the right information from each website  \nThe form is responsive and updates in real-time as users type, providing a smooth experience for defining extraction parameters.  \nExample inputs users might provide:  \nField 1:  \n- Name: `company_name`\n- Description: Extract the company name from the website header or footer  \nField 2:  \n- Name: `employee_count`\n- Description: Find the number of employees mentioned on the About or Company page  \nField 3:  \n- Name: `tech_stack`\n- Description: Look for technology names mentioned in job postings or footer  \nField 4:  \n- Name: `contact_email`\n- Description: Find the main contact or sales email address  \nField 5:  \n- Name: industry\n- Description: Determine the companyâ€™s primary industry from their description  \nOnce the user fills out the fields and descriptions, they must click on a button that fires up the entire system under the hood:  \n```python\n# Continuing main()\ndef main():\n...\n\nif uploaded_file:\n...\n\nif st.button(\"Start Extraction\") and fields:\nwith st.spinner(\n\"Extracting data. This may take a while, so don't close the window.\"\n):\npass\n\n```  \nCurrently, nothing happens when the user clicks on the field but we will build up the functionality in the next steps.",
      "metadata": {
        "title": "How to Build a Bulk Sales Lead Extractor in Python Using AI",
        "url": "https://www.firecrawl.dev/blog/sales-lead-extractor-python-ai"
      }
    },
    {
      "id": "7feb0514-2c9b-4096-9667-833d177c64eb",
      "source": "firecrawl/blog/sales-lead-extractor-python-ai.md",
      "content": "Building the Lead Extraction App Step-by-Step > Step 3: Building a dynamic Pydantic model from input fields\n\nLetâ€™s convert the fields and descriptions provided through the Streamlit UI into a Pydantic model inside the `src/models.py` file:  \n```python\nfrom pydantic import BaseModel, Field\nfrom typing import Dict, Any\n\n```  \nThe pydantic `BaseModel` provides data validation and serialization capabilities. We use Field to add helpful metadata like descriptions to our model fields. For type safety, we use `Dict` and `Any` type hints - `Dict` helps define our field definitions while `Any` provides flexibility in typing when needed.  \n```python\nclass DynamicLeadModel(BaseModel):\n\"\"\"Dynamic model for lead data extraction\"\"\"\npass\n\n```  \nThen, we define a new class `DynamicLeadModel(BaseModel)` with the purpose of dynamically generating Pydantic models based on user-defined fields.  \n```python\nclass DynamicLeadModel(BaseModel):\n\"\"\"Dynamic model for lead data extraction\"\"\"\n\n@classmethod\ndef create_model(cls, fields: Dict[str, str]) -> type[BaseModel]:\n\"\"\"\nCreate a dynamic Pydantic model based on user-defined fields\n\nArgs:\nfields: Dictionary of field names and their descriptions\n\"\"\"\nfield_annotations = {}\nfield_definitions = {}\n\nfor field_name, description in fields.items():\nfield_annotations[field_name] = str\nfield_definitions[field_name] = Field(description=description)\n\n# Create new model dynamically\nreturn type(\n\"LeadData\",\n(BaseModel,),\n{\"__annotations__\": field_annotations, **field_definitions},\n)\n\n```  \nThis class contains a `create_model` class method that takes a dictionary of field names and descriptions as input (passed through the Streamlit UI). For each field, it creates type annotations and field definitions with metadata. The method then returns a new dynamically created Pydantic model class that will be used by Firecrawl to guide its AI-powered scraping engine while extracting the lead information.",
      "metadata": {
        "title": "How to Build a Bulk Sales Lead Extractor in Python Using AI",
        "url": "https://www.firecrawl.dev/blog/sales-lead-extractor-python-ai"
      }
    },
    {
      "id": "3ae895d7-72b3-441d-a822-5143b1978ea6",
      "source": "firecrawl/blog/sales-lead-extractor-python-ai.md",
      "content": "Building the Lead Extraction App Step-by-Step > Step 4: Scraping input URLs with Firecrawl\n\nLetâ€™s move to `src/scraper.py` to write the scraping functionality:  \n```python\n# src/scraper.py\nfrom firecrawl import FirecrawlApp\nfrom typing import Dict, List\nfrom datetime import datetime\nimport pandas as pd\nfrom models import DynamicLeadModel\n\n```  \nThe `datetime` module is imported to generate unique timestamps for the output files. `pandas` (imported as `pd`) is used to create and manipulate DataFrames for storing the scraped lead data and exporting it to Excel.  \n```python\nclass LeadScraper:\ndef __init__(self):\nself.app = FirecrawlApp()\n\n```  \nThe `LeadScraper` class is initialized with a FirecrawlApp instance that provides the connection to the scraping engine. This allows us to make API calls to extract data from the provided URLs.  \n```python\n# ... continue the class\nasync def scrape_leads(self, urls: List[str], fields: Dict[str, str]) -> str:\n\"\"\"Scrape multiple leads using Firecrawl's batch extraction endpoint\"\"\"\n# Create dynamic model\nmodel = DynamicLeadModel.create_model(fields)\n\n# Extract data for all URLs at once\ndata = self.app.batch_scrape_urls(\nurls,\nparams={\n\"formats\": [\"extract\"],\n\"extract\": {\"schema\": model.model_json_schema()},\n},\n)\n\n# Process and format the results\nresults = [\\\n{\"url\": result[\"metadata\"][\"url\"], **result[\"extract\"]}\\\nfor result in data[\"data\"]\\\n]\n\n# Convert to DataFrame\ndf = pd.DataFrame(results)\n\n# Save results\ntimestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\nfilename = f\"data/leads_{timestamp}.xlsx\"\ndf.to_excel(filename, index=False)\n\nreturn filename\n\n```  \nNext, we define a `scrape_leads` method that takes a list of URLs and field definitions as input. The method creates a dynamic model based on the field definitions, extracts data from all URLs in a single batch request using Firecrawlâ€™s API, processes the results into a DataFrame, and saves them to an Excel file with a timestamp. The method returns the path to the generated Excel file.  \nHere is a sample output file that may be generated by the function:  \n| url | company_name | employee_count | industry | contact_email |\n| --- | --- | --- | --- | --- |\n| [https://acme.com](https://acme.com/) | Acme Corp | 500-1000 | Technology | [info@acme.com](mailto:info@acme.com) |\n| [https://betainc.com](https://betainc.com/) | Beta Inc | 100-500 | Healthcare | [contact@betainc.com](mailto:contact@betainc.com) |\n| [https://gammatech.io](https://gammatech.io/) | Gamma Tech | 1000+ | Software | [sales@gammatech.io](mailto:sales@gammatech.io) |\n| [https://deltasolutions.com](https://deltasolutions.com/) | Delta Sol | 50-100 | Consulting | [hello@deltasol.com](mailto:hello@deltasol.com) |  \nWhen the â€œStart extractionâ€ button is clicked in the UI, the app must create a `LeadScraper` instance and call its `scrape_leads()` method asynchronously using `asyncio.run()`. So, letâ€™s return to `src/app.py`.",
      "metadata": {
        "title": "How to Build a Bulk Sales Lead Extractor in Python Using AI",
        "url": "https://www.firecrawl.dev/blog/sales-lead-extractor-python-ai"
      }
    },
    {
      "id": "bb85ef80-59a9-4a2b-b0e5-92a3ffa2184b",
      "source": "firecrawl/blog/sales-lead-extractor-python-ai.md",
      "content": "Building the Lead Extraction App Step-by-Step > Step 5: Creating a download link with the results\n\nFirst, update the imports with the following version:  \n```python\n# src/app.py\nimport streamlit as st\nimport pandas as pd\nfrom typing import List, Dict\nimport time\nimport asyncio\nimport os\nfrom scraper import LeadScraper\nfrom dotenv import load_dotenv\n\nload_dotenv()\n\n```  \nThe imports have been updated to include all necessary dependencies for the app:  \n- `time` for tracking execution duration\n- `asyncio` for asynchronous execution\n- `os` for file path operations\n- `LeadScraper` from our custom scraper module\n- `load_dotenv` for loading environment variables  \nNext, continue the last `if` block of `main()` function with the following codeblock:  \n```python\ndef main():\n...\n\nif uploaded_file:\n...\n\nif st.button(\"Start Extraction\") and fields:\nwith st.spinner(\n\"Extracting data. This may take a while, so don't close the window.\"\n):\nstart_time = time.time()\nscraper = LeadScraper()\n\n# Run scraping asynchronously\nresult_file = asyncio.run(scraper.scrape_leads(urls, fields))\n\nelapsed_time = time.time() - start_time\nelapsed_mins = int(elapsed_time // 60)\nelapsed_secs = int(elapsed_time % 60)\n\n# Show download link\nwith open(result_file, \"rb\") as f:\nst.download_button(\n\"Download Results\",\nf,\nfile_name=os.path.basename(result_file),\nmime=\"text/csv\",\n)\nst.balloons()\nst.success(\nf\"Extraction complete! Time taken: {elapsed_mins}m {elapsed_secs}s\"\n)\n\n```  \nThis section adds the core extraction functionality:  \nWhen the â€œStart Extractionâ€ button is clicked and fields are defined:  \n- Shows a loading spinner with message\n- Records start time\n- Creates `LeadScraper` instance\n- Runs scraping asynchronously using `asyncio`\n- Calculates elapsed time in minutes and seconds\n- Opens the result file and creates download button\n- Shows success message with time taken\n- Displays celebratory balloons animation  \nThe code handles the entire extraction workflow from triggering the scrape to delivering results to the user, with progress feedback throughout the process.  \nAt this point, the core app functionality is finished. Try it out with few different CSV files containing potential leads your company has.  \nIn the final step, we will deploy the app to Streamlit Cloud.",
      "metadata": {
        "title": "How to Build a Bulk Sales Lead Extractor in Python Using AI",
        "url": "https://www.firecrawl.dev/blog/sales-lead-extractor-python-ai"
      }
    },
    {
      "id": "172666c3-64fc-4125-a5a8-bb3b2c058947",
      "source": "firecrawl/blog/sales-lead-extractor-python-ai.md",
      "content": "Building the Lead Extraction App Step-by-Step > Step 6: Deploying the app to Streamlit Cloud\n\nBefore deploying, letâ€™s create a `requirements.txt` file in the root directory:  \n```bash\npoetry export -f requirements.txt --output requirements.txt --without-hashes\n\n```  \nNext, create a new file called `.streamlit/secrets.toml`:  \n```toml\nFIRECRAWL_API_KEY = \"your_api_key_here\"\n\n```  \nAlso, add these to your `.gitignore` file:  \n```plaintext\n__pycache__/\n\n# Virtual Environment\n.env\n.venv\nvenv/\nENV/\n\n# IDE\n.idea/\n.vscode/\n\n# Project specific\n.streamlit/secrets.toml\ndata/\n*.xlsx\n\n```  \nNow follow these steps to deploy:  \n1. **Push to GitHub**  \n```bash\ngit init\ngit add .\ngit commit -m \"Initial commit\"\ngit branch -M main\ngit remote add origin https://github.com/yourusername/sales-lead-extractor.git\ngit push -u origin main\n\n```  \n2. **Deploy on Streamlit Cloud**  \n- Go to [share.streamlit.io](https://share.streamlit.io/)\n- Click â€œNew appâ€\n- Connect your GitHub repository\n- Select the main branch and `src/app.py` as the entry point\n- Add your Firecrawl API key in the Secrets section using the same format as `.streamlit/secrets.toml`\n- Click â€œDeployâ€  \nYour app will be live in a few minutes.",
      "metadata": {
        "title": "How to Build a Bulk Sales Lead Extractor in Python Using AI",
        "url": "https://www.firecrawl.dev/blog/sales-lead-extractor-python-ai"
      }
    },
    {
      "id": "3b524fa3-84d5-4871-9d70-91fb5e4dbb36",
      "source": "firecrawl/blog/sales-lead-extractor-python-ai.md",
      "content": "Conclusion\n\nWeâ€™ve built a powerful lead extraction tool that combines the simplicity of Streamlit with the AI capabilities of Firecrawl. The app allows sales teams to:  \n- Upload lists of target websites\n- Define custom data fields to extract\n- Get structured lead data in Excel format\n- Save hours of manual data gathering  \nKey features implemented:  \n- Dynamic field definition\n- Batch URL processing\n- Progress tracking\n- Excel export\n- Cloud deployment  \nFeel free to extend the app with features like:  \n- Authentication\n- Lead scoring\n- CRM integration\n- Custom extraction templates\n- Rate limiting\n- Error handling  \nThe complete source code is available on [GitHub](https://github.com/FirstClassML/firecrawl_projects/tree/main/sales-lead-extractor).  \nArticle updated recently  \n[ðŸ”¥](https://www.firecrawl.dev/)",
      "metadata": {
        "title": "How to Build a Bulk Sales Lead Extractor in Python Using AI",
        "url": "https://www.firecrawl.dev/blog/sales-lead-extractor-python-ai"
      }
    },
    {
      "id": "193da679-af42-4aef-8dfc-2391b0acc1ac",
      "source": "firecrawl/blog/sales-lead-extractor-python-ai.md",
      "content": "Ready to _Build?_\n\nStart scraping web data for your AI apps today.  \nNo credit card needed.  \nGet Started",
      "metadata": {
        "title": "How to Build a Bulk Sales Lead Extractor in Python Using AI",
        "url": "https://www.firecrawl.dev/blog/sales-lead-extractor-python-ai"
      }
    },
    {
      "id": "d0693b61-c6e7-42ac-84ec-827467d0e6ca",
      "source": "firecrawl/blog/sales-lead-extractor-python-ai.md",
      "content": "About the Author\n\n[![Bex Tuychiev image](https://www.firecrawl.dev/bex.jpg)\\\nBex Tuychiev@bextuychiev](https://x.com/bextuychiev)  \nBex is a Top 10 AI writer on Medium and a Kaggle Master with over 15k followers. He loves writing detailed guides, tutorials, and notebooks on complex data science and machine learning topics",
      "metadata": {
        "title": "How to Build a Bulk Sales Lead Extractor in Python Using AI",
        "url": "https://www.firecrawl.dev/blog/sales-lead-extractor-python-ai"
      }
    },
    {
      "id": "01812331-0398-49d6-8bd0-19db596415e1",
      "source": "firecrawl/blog/sales-lead-extractor-python-ai.md",
      "content": "About the Author > More articles by Bex Tuychiev\n\n[Building an Automated Price Tracking Tool\\\n\\\nBuild an automated e-commerce price tracker in Python. Learn web scraping, price monitoring, and automated alerts using Firecrawl, Streamlit, PostgreSQL.](https://www.firecrawl.dev/blog/automated-price-tracking-tutorial-python) [Web Scraping Automation: How to Run Scrapers on a Schedule\\\n\\\nLearn how to automate web scraping in Python using free tools like schedule, asyncio, cron jobs and GitHub Actions. This comprehensive guide covers local and cloud-based scheduling methods to run scrapers reliably in 2025.](https://www.firecrawl.dev/blog/automated-web-scraping-free-2025) [Automated Data Collection - A Comprehensive Guide\\\n\\\nLearn how to build robust automated data collection systems using modern tools and best practices. This guide covers everything from selecting the right tools to implementing scalable collection pipelines.](https://www.firecrawl.dev/blog/automated-data-collection-guide) [BeautifulSoup4 vs. Scrapy - A Comprehensive Comparison for Web Scraping in Python\\\n\\\nLearn the key differences between BeautifulSoup4 and Scrapy for web scraping in Python. Compare their features, performance, and use cases to choose the right tool for your web scraping needs.](https://www.firecrawl.dev/blog/beautifulsoup4-vs-scrapy-comparison) [How to Build an Automated Competitor Price Monitoring System with Python\\\n\\\nLearn how to build an automated competitor price monitoring system in Python that tracks prices across e-commerce sites, provides real-time comparisons, and maintains price history using Firecrawl, Streamlit, and GitHub Actions.](https://www.firecrawl.dev/blog/automated-competitor-price-scraping) [Scraping Company Data and Funding Information in Bulk With Firecrawl and Claude\\\n\\\nLearn how to build a web scraper in Python that gathers company details, funding rounds, and investor information from public sources like Crunchbase using Firecrawl and Claude for automated data collection and analysis.](https://www.firecrawl.dev/blog/crunchbase-scraping-with-firecrawl-claude) [Data Enrichment: A Complete Guide to Enhancing Your Data Quality\\\n\\\nLearn how to enrich your data quality with a comprehensive guide covering data enrichment tools, best practices, and real-world examples. Discover how to leverage modern solutions like Firecrawl to automate data collection, validation, and integration for better business insights.](https://www.firecrawl.dev/blog/complete-guide-to-data-enrichment) [Building an Intelligent Code Documentation RAG Assistant with DeepSeek and Firecrawl\\\n\\\nLearn how to build an intelligent documentation assistant powered by DeepSeek and RAG (Retrieval Augmented Generation) that can answer questions about any documentation website by combining language models with efficient information retrieval.](https://www.firecrawl.dev/blog/deepseek-rag-documentation-assistant)",
      "metadata": {
        "title": "How to Build a Bulk Sales Lead Extractor in Python Using AI",
        "url": "https://www.firecrawl.dev/blog/sales-lead-extractor-python-ai"
      }
    },
    {
      "id": "4bd7afc3-7cb3-49ac-8d9d-ec22f521140e",
      "source": "firecrawl/blog/trend-finder-typescript.md",
      "content": "---\ntitle: Building a Trend Detection System with AI in TypeScript: A Step-by-Step Guide\nurl: https://www.firecrawl.dev/blog/trend-finder-typescript\n---  \nIntroducing /extract - Get web data with a prompt [Try now](https://www.firecrawl.dev/extract)  \nJan 11, 2025  \nâ€¢  \n[![Bex Tuychiev image](https://www.firecrawl.dev/bex.jpg)Bex Tuychiev](https://x.com/bextuychiev)  \n# Building a Trend Detection System with AI in TypeScript: A Step-by-Step Guide  \n![Building a Trend Detection System with AI in TypeScript: A Step-by-Step Guide image](https://www.firecrawl.dev/images/blog/trend_finder/trend-finder-typescript.jpg)",
      "metadata": {
        "title": "Building a Trend Detection System with AI in TypeScript: A Step-by-Step Guide",
        "url": "https://www.firecrawl.dev/blog/trend-finder-typescript"
      }
    },
    {
      "id": "41b4e6c8-7e03-442a-8998-31dac6f58b11",
      "source": "firecrawl/blog/trend-finder-typescript.md",
      "content": "Introduction\n\nIn this comprehensive guide, weâ€™ll explore the development of a sophisticated social media trend detection system built with TypeScript and powered by AI. Youâ€™ll learn how to create a robust solution that monitors social platforms and news websites, analyzes emerging trends, and delivers them in real-time as Slack messages.  \nBefore we dive into the technical details and implementation steps, watch a video preview of the project:  \nHow to set up Trend Finder to uncover trends on social media and sites with AI. - YouTube  \nEric Ciarla  \n25 subscribers  \n[How to set up Trend Finder to uncover trends on social media and sites with AI.](https://www.youtube.com/watch?v=puimQSun92g)  \nEric Ciarla  \nSearch  \nWatch later  \nShare  \nCopy link  \nInfo  \nShopping  \nTap to unmute  \nIf playback doesn't begin shortly, try restarting your device.  \nFull screen is unavailable. [Learn More](https://support.google.com/youtube/answer/6276924)  \nMore videos",
      "metadata": {
        "title": "Building a Trend Detection System with AI in TypeScript: A Step-by-Step Guide",
        "url": "https://www.firecrawl.dev/blog/trend-finder-typescript"
      }
    },
    {
      "id": "f0e6faa4-98d9-4c0c-aadc-30f10fd5048d",
      "source": "firecrawl/blog/trend-finder-typescript.md",
      "content": "More videos\n\nYou're signed out  \nVideos you watch may be added to the TV's watch history and influence TV recommendations. To avoid this, cancel and sign in to YouTube on your computer.  \nCancelConfirm  \nShare  \nInclude playlist  \nAn error occurred while retrieving sharing information. Please try again later.  \n[Watch on](https://www.youtube.com/watch?v=puimQSun92g&embeds_referring_euri=https%3A%2F%2Fwww.firecrawl.dev%2F)  \n0:00  \n0:00 / 4:00â€¢Live  \nâ€¢  \n[Watch on YouTube](https://www.youtube.com/watch?v=puimQSun92g \"Watch on YouTube\")  \nWhile the video demonstrates the project detecting AI-related trends from specific sources, you have the flexibility to customize it for monitoring any topics or themes from your preferred websites and Twitter accounts. If that sounds interesting, letâ€™s set up your development environment for running the project locally.  \nNote: Before starting this project, ensure you have the following prerequisites installed:  \n- Node.js (version 16 or higher)\n- `npm` (Node Package Manager)\n- A code editor like VS Code\n- Git for version control\n- A Slack workspace with admin privileges\n- X Developer Account (for X API access)\n- Basic knowledge of TypeScript and Node.js",
      "metadata": {
        "title": "Building a Trend Detection System with AI in TypeScript: A Step-by-Step Guide",
        "url": "https://www.firecrawl.dev/blog/trend-finder-typescript"
      }
    },
    {
      "id": "e06bef34-f13c-4770-b1f8-0711beec2fe8",
      "source": "firecrawl/blog/trend-finder-typescript.md",
      "content": "Project Setup\n\nWe start by cloning the projectâ€™s GitHub repository, which is maintained by Eric Ciarla, co-founder of Firecrawl:  \n```bash\ngit clone https://github.com/ericciarla/trendFinder\ncd trendFinder\n\n```  \nNext, install dependencies:  \n```bash\nnpm install\n\n```  \nThen, configure your `.env` file:  \n```bash\ncp .env.example .env\n# Edit .env with your configuration\n\n```  \nif you open `.env.example`, you will see that our app depends on four core services:  \n1. Slack Webhook - For sending notifications about detected trends\n2. X (Twitter) API - For monitoring tweets and engagement metrics\n3. Together AI - For analyzing content and detecting trends with LLMs\n4. Firecrawl API - For scraping and monitoring web content  \nTo run the project locally, you will need to obtain the necessary URLs and API keys from these services. Below, you will see some instructions for setting up each required service and obtaining the necessary credentials.",
      "metadata": {
        "title": "Building a Trend Detection System with AI in TypeScript: A Step-by-Step Guide",
        "url": "https://www.firecrawl.dev/blog/trend-finder-typescript"
      }
    },
    {
      "id": "1309fe08-aa87-424c-9acc-890a6075f0b7",
      "source": "firecrawl/blog/trend-finder-typescript.md",
      "content": "Project Setup > Obtaining API Tokens > X (Twitter) Bearer Token\n\nThe X API is a crucial component of our trend detection system. It allows us to monitor prominent accounts in real-time, track engagement metrics, and identify emerging topics. The Bearer Token provides secure authentication for making API requests through your own X developer account. Here are the instructions to get your token:  \n1. Go to [Twitter Developer Portal](https://developer.x.com/en/portal/dashboard)\n2. Create a developer account if needed\n3. Create a new project and app (free plan accounts already have an app ready)\n4. Navigate to â€œKeys and Tokensâ€\n5. Generate/copy your Bearer Token (OAuth 2.0)\n6. Add to `.env` file",
      "metadata": {
        "title": "Building a Trend Detection System with AI in TypeScript: A Step-by-Step Guide",
        "url": "https://www.firecrawl.dev/blog/trend-finder-typescript"
      }
    },
    {
      "id": "ccabe54d-48f7-4516-834c-69cd72e444bd",
      "source": "firecrawl/blog/trend-finder-typescript.md",
      "content": "Project Setup > Obtaining API Tokens > Firecrawl API Key\n\nFirecrawl serves as our primary web content extraction engine, offering several key advantages for trend detection:  \n1. **AI-Powered Content Extraction**: Uses natural language understanding instead of brittle HTML selectors, ensuring reliable trend detection even when websites change.\n2. **Automated Content Discovery**: Automatically processes entire website sections, ideal for monitoring news sites and blogs\n3. **Multiple Output Formats**: Supports structured data, markdown, and plain text formats for seamless integration with Together AI\n4. **Built-in Rate Limiting**: Handles request management automatically, ensuring stable monitoring  \nSince Firecrawl is a scraping engine, you will need an API key to connect to it through its TypeScript dependency:  \n1. Visit [Firecrawl](https://www.firecrawl.dev/)\n2. Create an account\n3. Navigate to your dashboard\n4. Generate and copy your API key\n5. Add to `.env` file",
      "metadata": {
        "title": "Building a Trend Detection System with AI in TypeScript: A Step-by-Step Guide",
        "url": "https://www.firecrawl.dev/blog/trend-finder-typescript"
      }
    },
    {
      "id": "89a49a23-5105-44e6-a5d9-8666ea10cb66",
      "source": "firecrawl/blog/trend-finder-typescript.md",
      "content": "Project Setup > Obtaining API Tokens > Together AI Token\n\nTogether AI powers the intelligence layer of our trend detection system:  \n1. **Natural Language Processing**: Analyzes scraped content to identify emerging trends and patterns\n2. **Sentiment Analysis**: Evaluates public sentiment and engagement around potential trends\n3. **Content Summarization**: Generates concise summaries of trends for Slack notifications  \nTo get an API token, follow these steps:  \n1. Visit [Together AI](https://www.together.ai/)\n2. Sign up for an account\n3. Navigate to API settings/dashboard\n4. Generate and copy your API key\n5. Add to `.env` file",
      "metadata": {
        "title": "Building a Trend Detection System with AI in TypeScript: A Step-by-Step Guide",
        "url": "https://www.firecrawl.dev/blog/trend-finder-typescript"
      }
    },
    {
      "id": "27f8baca-4feb-4561-bd5b-3db56287102e",
      "source": "firecrawl/blog/trend-finder-typescript.md",
      "content": "Project Setup > Setting Up Slack Webhook\n\nFinally, you will need a Slack webhook URL to receive real-time notifications about emerging trends. When our system runs, it scrapes provided list of sources (X accounts, websites), detects trends related to our specified topics, summarizes their contents and delivers them as a Slack message through the webhook.  \nTo create a webhook for your account, follow these steps:  \n1. **Create a Slack Workspace** (Log in if you already have one)\n- Visit [slack.com](https://slack.com/)\n- Click â€œCreate a new workspaceâ€\n- Follow the setup wizard to create your workspace\n- Verify your email address\n2. **Create a Slack App**\n- Go to [api.slack.com/apps](https://api.slack.com/apps)\n- Click â€œCreate New Appâ€\n- Choose â€œFrom scratchâ€\n- Name your app (e.g., â€œTrend Finderâ€)\n- Select your workspace\n- Click â€œCreate Appâ€\n3. **Enable Incoming Webhooks**\n- In your appâ€™s settings, click â€œIncoming Webhooksâ€\n- Toggle â€œActivate Incoming Webhooksâ€ to On\n- Click â€œAdd New Webhook to Workspaceâ€\n4. **Configure the Webhook**\n- Choose the channel where you want notifications to appear\n- Click â€œAllowâ€\n- Youâ€™ll see your new webhook URL in the list\n- Copy the Webhook URL (it starts with `https://hooks.slack.com/services/`)\n5. **Add to Environment Variables**\n- Open your `.env` file\n- Add your webhook URL:  \n```plaintext\nSLACK_WEBHOOK_URL=https://hooks.slack.com/services/YOUR/WEBHOOK/URL\n\n```  \n6. **Test the Webhook** (Optional)\n- You can test your webhook using curl:  \n```bash\ncurl -X POST -H 'Content-type: application/json' --data '{\"text\":\"Hello from Trend Finder!\"}' YOUR_WEBHOOK_URL\n\n```",
      "metadata": {
        "title": "Building a Trend Detection System with AI in TypeScript: A Step-by-Step Guide",
        "url": "https://www.firecrawl.dev/blog/trend-finder-typescript"
      }
    },
    {
      "id": "caf21260-5269-4dd9-9f6c-3f256a62e831",
      "source": "firecrawl/blog/trend-finder-typescript.md",
      "content": "System Architecture Overview > Core Components > 2. Cron Controller ( `src/controllers/cron.ts`)\n\n[See the file on GitHub.](https://github.com/ericciarla/trendFinder/blob/main/src/controllers/cron.ts)  \nThe controller orchestrates the entire workflow in a sequential process:  \n1. Fetches source configurations\n2. Scrapes content from sources\n3. Generates an AI-analyzed draft\n4. Sends the results to Slack",
      "metadata": {
        "title": "Building a Trend Detection System with AI in TypeScript: A Step-by-Step Guide",
        "url": "https://www.firecrawl.dev/blog/trend-finder-typescript"
      }
    },
    {
      "id": "9d2bed68-a1b1-4949-98d9-fa94ce366b5a",
      "source": "firecrawl/blog/trend-finder-typescript.md",
      "content": "System Architecture Overview > Core Components > 3. Source Management ( `src/services/getCronSources.ts`)\n\n[See the file on GitHub.](https://github.com/ericciarla/trendFinder/blob/main/src/services/getCronSources.ts)  \nThis service manages the content sources, supporting two types of inputs:  \n- Websites (these will be scraped with Firecrawl)\n- Twitter/X accounts (scraped with the X Developer API)  \nThe service verifies API keys and filters content sources to prevent unauthorized access attempts. The configuration file includes multiple AI news websites and one X account. While the file lists several prominent AI news X accounts, most are currently disabled in comments because the X Developer API free tier restricts scraping to one account every 15 minutes.  \nThis is where you would add your own sources if you wish to monitor a trend other than AI.",
      "metadata": {
        "title": "Building a Trend Detection System with AI in TypeScript: A Step-by-Step Guide",
        "url": "https://www.firecrawl.dev/blog/trend-finder-typescript"
      }
    },
    {
      "id": "f586f44b-18e7-4e39-a0cb-745076da4431",
      "source": "firecrawl/blog/trend-finder-typescript.md",
      "content": "System Architecture Overview > Core Components > 4. Content Scraping ( `src/services/scrapeSources.ts`)\n\n[See the file on GitHub.](https://github.com/ericciarla/trendFinder/blob/main/src/services/scrapeSources.ts)  \nA robust scraping service that:  \n- Handles Twitter/X API integration for social media content\n- Uses Firecrawl for web page content extraction\n- Implements strong typing and structured extraction with Zod schemas\n- Normalizes data from different sources into a consistent format  \nIt is in this file that the topic of interest is specified as â€œAIâ€. To change this, you need to update the lines 21 and 96.",
      "metadata": {
        "title": "Building a Trend Detection System with AI in TypeScript: A Step-by-Step Guide",
        "url": "https://www.firecrawl.dev/blog/trend-finder-typescript"
      }
    },
    {
      "id": "0afc4107-8b0f-4fc3-8ef3-b804854eea2b",
      "source": "firecrawl/blog/trend-finder-typescript.md",
      "content": "System Architecture Overview > Core Components > 5. Draft Generation ( `src/services/generateDraft.ts`)\n\n[See the file on GitHub.](https://github.com/ericciarla/trendFinder/blob/main/src/services/generateDraft.ts)  \nThe AI analysis component that:  \n- Uses Together AIâ€™s Llama 3.1 model\n- Processes raw content through structured prompts\n- Implements JSON schema validation\n- Formats content into readable Slack messages  \nThis script has several parts that make it tailored for watching AI trends. You would need to change those parts as well to choose a different topic.",
      "metadata": {
        "title": "Building a Trend Detection System with AI in TypeScript: A Step-by-Step Guide",
        "url": "https://www.firecrawl.dev/blog/trend-finder-typescript"
      }
    },
    {
      "id": "908c941b-78fe-4870-882d-2a8767f2dae1",
      "source": "firecrawl/blog/trend-finder-typescript.md",
      "content": "System Architecture Overview > Core Components > 6. Notification Service ( `src/services/sendDraft.ts`)\n\nA straightforward service that delivers the processed content to Slack via webhooks, with proper error handling and logging.",
      "metadata": {
        "title": "Building a Trend Detection System with AI in TypeScript: A Step-by-Step Guide",
        "url": "https://www.firecrawl.dev/blog/trend-finder-typescript"
      }
    },
    {
      "id": "e96eb80c-1047-44cd-9bea-b94cdd662270",
      "source": "firecrawl/blog/trend-finder-typescript.md",
      "content": "System Architecture Overview > Key Features\n\n1. **Modular Architecture**: Each component is self-contained and follows single-responsibility principles.\n2. **Type Safety**: Comprehensive TypeScript implementation with Zod schemas for runtime validation.\n3. **Error Handling**: Robust error handling at each step of the pipeline.\n4. **Scalability**: Docker support enables easy deployment and scaling.\n5. **API Integration**: Supports multiple data sources with extensible architecture.\n6. **AI Analysis**: Leverages advanced AI models for content analysis.",
      "metadata": {
        "title": "Building a Trend Detection System with AI in TypeScript: A Step-by-Step Guide",
        "url": "https://www.firecrawl.dev/blog/trend-finder-typescript"
      }
    },
    {
      "id": "eb6973fb-44ca-4574-b52c-7e5260790f8f",
      "source": "firecrawl/blog/trend-finder-typescript.md",
      "content": "In-depth Project Breakdown > 1. Specifying the resources to scrape\n\nIn `src/services/getCronSources.ts`, we start by importing `dotenv`:  \n```ts\nimport dotenv from \"dotenv\";\n\ndotenv.config();\n\n```  \nThis allows the application to securely load configuration values from a `.env` file, which is a common practice for managing sensitive information like API keys and credentials.  \nThen, we define a new function called `getCronSources`:  \n```ts\nexport async function getCronSources() {...}\n\n```  \nThe function is `async` because it needs to make network requests to fetch content from external sources. This allows the application to handle multiple requests efficiently without blocking.  \nIn the function body, we start a parent `try-catch` block:  \n```ts\nexport async function getCronSources() {\ntry {\nconsole.log(\"Fetching sources...\");\n\n// Check for required API keys\nconst hasXApiKey = !!process.env.X_API_BEARER_TOKEN;\nconst hasFirecrawlKey = !!process.env.FIRECRAWL_API_KEY;\n\n... // continued below\n\n```  \nThe code above performs important validation by checking for required API keys. It uses the double exclamation mark (!!) operator to convert the environment variables into boolean values, making it easy to verify if both the X API bearer token and Firecrawl API key are present. This validation step is crucial before attempting to make any API calls to ensure the application has proper authentication credentials.  \n```ts\n// ... continuation of the above block\n// Filter sources based on available API keys\nconst sources = [\\\n// High priority sources (Only 1 x account due to free plan rate limits)\\\n...(hasFirecrawlKey ? [\\\n{ identifier: 'https://www.firecrawl.dev/blog' },\\\n{ identifier: 'https://openai.com/news/' },\\\n{ identifier: 'https://www.anthropic.com/news' },\\\n{ identifier: 'https://news.ycombinator.com/' },\\\n{ identifier: 'https://www.reuters.com/technology/artificial-intelligence/' },\\\n{ identifier: 'https://simonwillison.net/' },\\\n{ identifier: 'https://buttondown.com/ainews/archive/' },\\\n] : []),\\\n...(hasXApiKey ? [\\\n{ identifier: 'https://x.com/skirano' },\\\n] : []),\\\n];\n\nreturn sources.map(source => source.identifier);\n} catch (error) {\nconsole.error(error);\n}\n}\n\n```  \nThe code uses the ternary operator ( `? :`) to conditionally include sources based on available API keys. For each API key check, if the condition before the `?` is true (e.g. `hasFirecrawlKey` is true), the array of sources after the `?` is included. Otherwise, if the condition is false, an empty array after the `:` is used instead.  \nThis conditional logic ensures we only try to fetch from sources where we have valid API credentials. The spread operator ( `...`) is used to flatten these conditional arrays into a single sources array.  \nFor error handling, the entire function is wrapped in a `try-catch` block. If any error occurs during execution, it will be caught and logged to the console via `console.error()`. This prevents the application from crashing if there are issues with environment variables or API calls.",
      "metadata": {
        "title": "Building a Trend Detection System with AI in TypeScript: A Step-by-Step Guide",
        "url": "https://www.firecrawl.dev/blog/trend-finder-typescript"
      }
    },
    {
      "id": "e2772af4-1782-44e7-8dd9-6cc1239f5029",
      "source": "firecrawl/blog/trend-finder-typescript.md",
      "content": "In-depth Project Breakdown > 2. Scraping specified resources with X and Firecrawl\n\nInside `src/services/scrapeSources.ts`, we write the functionality to scrape the resources specified in `getCronSources.ts` with Firecrawl and X API.  \nThe script starts with the following imports and setup:  \n```ts\nimport FirecrawlApp from \"@mendable/firecrawl-js\";\nimport dotenv from \"dotenv\";\nimport { z } from \"zod\";\n\ndotenv.config();\n\n```  \nThese imports provide essential functionality for the scraping service:  \n- `FirecrawlApp`: A JavaScript client for interacting with the Firecrawl API to scrape web content\n- `dotenv`: For loading environment variables from a `.env` file\n- `zod`: A TypeScript-first schema validation library  \nThe `dotenv.config()` call loads environment variables at runtime, making them accessible via `process.env`. This is important since weâ€™ll need API keys and other configuration stored in environment variables.  \n```ts\n// Initialize Firecrawl\nconst app = new FirecrawlApp({ apiKey: process.env.FIRECRAWL_API_KEY });\n\n// 1. Define the schema for our expected JSON\nconst StorySchema = z.object({\nheadline: z.string().describe(\"Story or post headline\"),\nlink: z.string().describe(\"A link to the post or story\"),\ndate_posted: z.string().describe(\"The date the story or post was published\"),\n});\n\nconst StoriesSchema = z.object({\nstories: z\n.array(StorySchema)\n.describe(\"A list of today's AI or LLM-related stories\"),\n});\n\n```  \nThe code above initializes Firecrawl with an API key and defines two Zod schemas for validating the data structure we expect to receive from our scraping operations.  \nThe `StorySchema` defines the shape of individual story objects, with three required string fields:  \n- `headline`: The title or headline of the story/post\n- `link`: URL linking to the full content\n- `date_posted`: Publication timestamp  \nThe `StoriesSchema` wraps this in an array, expecting multiple story objects within a â€œstoriesâ€ property. This schema will be used by Firecrawlâ€™s scraping engine to format its output according to our needs.  \nThe `.describe()` method calls on each field are essential - they provide semantic descriptions that Firecrawlâ€™s AI engine uses to intelligently identify and extract the correct data from web pages. By understanding these descriptions, the AI can automatically determine the appropriate HTML elements and CSS selectors to target when scraping content.  \n```ts\nexport async function scrapeSources(sources: string[]) {\n// ... continued below\n\n```  \nThen, we start a function `scrapeSources` that takes an array of source URLs as input and will handle the scraping of content from each provided source.  \n```ts\nconst num_sources = sources.length;\nconsole.log(`Scraping ${num_sources} sources...`);\n\nlet combinedText: { stories: any[] } = { stories: [] };\n\n// Configure these if you want to toggle behavior\nconst useTwitter = true;\nconst useScrape = true;\n\n// continued below ...\n\n```  \nThe code above sets up a few key variables in the body of the function:  \n- `num_sources` tracks how many URLs weâ€™re processing\n- `combinedText` initializes an empty array to store all scraped stories\n- Two boolean flags control which scraping methods to use:\n- `useTwitter` enables Twitter API integration\n- `useScrape` enables direct web scraping  \nThese variables will be used throughout the rest of the scraping process to control behavior and aggregate results.  \n```ts\n// ... continuation of above\nfor (const source of sources) {\n// --- 1) Handle x.com (Twitter) sources ---\nif (source.includes(\"x.com\")) {\nif (useTwitter) {\nconst usernameMatch = source.match(/x.com/([^/]+)/);\nif (usernameMatch) {\nconst username = usernameMatch[1];\n\n// Build the search query for tweets\nconst query = `from:${username} has:media -is:retweet -is:reply`;\nconst encodedQuery = encodeURIComponent(query);\n\n// Get tweets from the last 24 hours\nconst startTime = new Date(\nDate.now() - 24 * 60 * 60 * 1000\n).toISOString();\nconst encodedStartTime = encodeURIComponent(startTime);\n\n// x.com API URL\nconst apiUrl = `https://api.x.com/2/tweets/search/recent?query=${encodedQuery}&max_results=10&start_time=${encodedStartTime}`;\n\n// Fetch recent tweets from the Twitter API\nconst response = await fetch(apiUrl, {\nheaders: {\nAuthorization: `Bearer ${process.env.X_API_BEARER_TOKEN}`,\n},\n});\n\n// Continued below...\n\n```  \nNext, we makes a request to the Twitter API to fetch recent tweets from a specific user. Letâ€™s break down whatâ€™s happening:  \n1. We check if the source URL contains â€œx.comâ€ and if Twitter integration is enabled\n2. We extract the username from the URL using regex\n3. We construct a search query that:\n- Gets tweets from that user\n- Only includes tweets with media\n- Excludes retweets and replies\n4. We calculate a timestamp from 24 hours ago to limit results\n5. We build the API URL with the encoded query parameters\n6. Finally, we make the authenticated request using the bearer token  \n```ts\n// ... continuation of above\nif (!response.ok) {\nthrow new Error(\n`Failed to fetch tweets for ${username}: ${response.statusText}`,\n);\n}\n\n```  \nAfter making the API request, we check if the response was successful. If not, we throw an error with details about what went wrong, including the username and the status text from the response.  \n```ts\nconst tweets = await response.json();\n\nif (tweets.meta?.result_count === 0) {\nconsole.log(`No tweets found for username ${username}.`);\n} else if (Array.isArray(tweets.data)) {\nconsole.log(`Tweets found from username ${username}`);\nconst stories = tweets.data.map((tweet: any) => {\nreturn {\nheadline: tweet.text,\nlink: `https://x.com/i/status/${tweet.id}`,\ndate_posted: startTime,\n};\n});\ncombinedText.stories.push(...stories);\n} else {\nconsole.error(\n\"Expected tweets.data to be an array:\",\ntweets.data\n);\n}\n}\n}\n}\n\n// Continued below...\n\n```  \nAfter parsing the tweets, we map them into story objects that contain:  \n- The tweet text as the headline\n- A link to the original tweet\n- The timestamp when it was posted  \nThese story objects are then added to our `combinedText` array which aggregates content from multiple sources.  \nIf no tweets are found, we log a message. If thereâ€™s an unexpected response format where `tweets.data` isnâ€™t an array, we log an error with the actual data received.  \nThe code handles all edge cases gracefully while maintaining a clean data structure for downstream processing.  \n```ts\n// ... continuation of above\n// --- 2) Handle all other sources with Firecrawl extract ---\nelse {\nif (useScrape) {\n// Firecrawl will both scrape and extract for you\n// Provide a prompt that instructs Firecrawl what to extract\nconst currentDate = new Date().toLocaleDateString();\nconst promptForFirecrawl = `\nReturn only today's AI or LLM related story or post headlines and links in JSON format from the page content.\nThey must be posted today, ${currentDate}. The format should be:\n{\n\"stories\": [\\\n{\\\n\"headline\": \"headline1\",\\\n\"link\": \"link1\",\\\n\"date_posted\": \"YYYY-MM-DD\"\\\n},\\\n...\\\n]\n}\nIf there are no AI or LLM stories from today, return {\"stories\": []}.\n\nThe source link is ${source}.\nIf a story link is not absolute, prepend ${source} to make it absolute.\nReturn only pure JSON in the specified format (no extra text, no markdown, no ```).\n`;\n// continued below ...\n\n```  \nThe prompt instructs Firecrawl to extract AI/LLM related stories from the current day only. It specifies the exact JSON format required for the response, with each story containing a headline, link and date posted. The prompt ensures links are absolute by having Firecrawl prepend the source URL if needed. For clean parsing, it explicitly requests pure JSON output without any formatting or extra text.  \n```ts\n// Use app.extract(...) directly\nconst scrapeResult = await app.extract(\n[source],\n{\nprompt: promptForFirecrawl,\nschema: StoriesSchema, // The Zod schema for expected JSON\n}\n);\n\nif (!scrapeResult.success) {\nthrow new Error(`Failed to scrape: ${scrapeResult.error}`);\n}\n\n// The structured data\nconst todayStories = scrapeResult.data;\nconsole.log(`Found ${todayStories.stories.length} stories from ${source}`);\ncombinedText.stories.push(...todayStories.stories);\n}\n}\n}\n// Continued below ...\n\n```  \nThe code above implements the core scraping functionality:  \n1. It constructs a prompt for Firecrawl that specifies exactly what content to extract\n2. The prompt requests AI/LLM headlines from the current day only\n3. It defines the exact JSON structure expected in the response\n4. It handles relative URLs by having Firecrawl convert them to absolute\n5. The extracted data is validated against a Zod schema\n6. Valid results are accumulated into the `combinedText` array\n7. Error handling ensures failed scrapes donâ€™t crash the process  \n```ts\n// ... continuation of above\n// Return the combined stories from all sources\nconst rawStories = combinedText.stories;\nconsole.log(rawStories);\nreturn rawStories;\n}\n// End of script\n\n```  \nFinally, this code returns the raw stories array containing all the scraped headlines and content from the various sources. The stories can then be processed further for trend analysis and summarization.",
      "metadata": {
        "title": "Building a Trend Detection System with AI in TypeScript: A Step-by-Step Guide",
        "url": "https://www.firecrawl.dev/blog/trend-finder-typescript"
      }
    },
    {
      "id": "86763738-16b0-4a58-bd4f-8db92c298212",
      "source": "firecrawl/blog/trend-finder-typescript.md",
      "content": "In-depth Project Breakdown > 3. Synthesizing scraped contents into a summary\n\nInside `src/services/generateDraft.ts`, we write the functionality to convert the raw stories scraped in the previous script into a summary message that will later be sent with Slack.  \nThe script starts with the following imports:  \n```ts\nimport dotenv from \"dotenv\";\nimport Together from \"together-ai\";\nimport { z } from \"zod\";\nimport { zodToJsonSchema } from \"zod-to-json-schema\";\n\ndotenv.config();\n\n```  \nThe script imports several key dependencies:  \n- dotenv: For loading environment variables from a `.env` file\n- Together: The `Together.ai` client library for making API calls\n- `z` from `zod`: A TypeScript-first schema validation library\n- `zodToJsonSchema`: A utility to convert Zod schemas to JSON Schema format  \n```ts\n/**\n* Generate a post draft with trending ideas based on raw tweets.\n*/\nexport async function generateDraft(rawStories: string) {\nconsole.log(`Generating a post draft with raw stories (${rawStories.length} characters)...`)\n\n// continued below ...\n\n```  \nThe `generateDraft` function takes the raw stories as input and processes them to identify key trends and generate a summary. First, it prints a log message indicating the size of the input by showing the character count.  \n```ts\n// ... continuation of above\ntry {\n// Initialize Together client\nconst together = new Together();\n\n// Define the schema for our response\nconst DraftPostSchema = z.object({\ninterestingTweetsOrStories: z.array(z.object({\nstory_or_tweet_link: z.string().describe(\"The direct link to the tweet or story\"),\ndescription: z.string().describe(\"A short sentence describing what's interesting about the tweet or story\")\n}))\n}).describe(\"Draft post schema with interesting tweets or stories for AI developers.\");\n\n// Convert our Zod schema to JSON Schema\nconst jsonSchema = zodToJsonSchema(DraftPostSchema, {\nname: 'DraftPostSchema',\nnameStrategy: 'title'\n});\n\n// Create a date string if you need it in the post header\nconst currentDate = new Date().toLocaleDateString('en-US', {\ntimeZone: 'America/New_York',\nmonth: 'numeric',\nday: 'numeric',\n});\n\n// continued below ...\n\n```  \nIn this block, we set up the core functionality for generating the draft post. We initialize the Together AI client which will be used for making API calls. We then define a Zod schema that specifies the expected structure of our response - an array of interesting tweets/stories where each item has a link and description. This schema is converted to JSON Schema format which will help enforce the output structure. Finally, we create a formatted date string in US format (MM/DD) that can be used in the post header.  \n```ts\n// ...continuation of above\n// Use Togetherâ€™s chat completion with the Llama 3.1 model\nconst completion = await together.chat.completions.create({\nmodel: \"meta-llama/Meta-Llama-3.1-70B-Instruct-Turbo\",\nmessages: [\\\n{\\\nrole: \"system\",\\\ncontent: `You are given a list of raw AI and LLM-related tweets sourced from X/Twitter.\\\nOnly respond in valid JSON that matches the provided schema (no extra keys).\\\n`,\\\n},\\\n{\\\nrole: \"user\",\\\ncontent: `Your task is to find interesting trends, launches, or interesting examples from the tweets or stories.\\\nFor each tweet or story, provide a 'story_or_tweet_link' and a one-sentence 'description'.\\\nReturn all relevant tweets or stories as separate objects.\\\nAim to pick at least 10 tweets or stories unless there are fewer than 10 available. If there are less than 10 tweets or stories, return ALL of them. Here are the raw tweets or stories you can pick from:nn${rawStories}nn`,\\\n},\\\n],\n// Tell Together to strictly enforce JSON output that matches our schema\n// @ts-ignore\nresponse_format: { type: \"json_object\", schema: jsonSchema },\n});\n\n// continued below ...\n\n```  \nIn this block, we make the API call to Together AI using their chat completions endpoint with the Llama 3.1 model. The system prompt instructs the model to only output valid JSON matching our schema. The user prompt provides the actual task - finding interesting trends, launches and examples from the raw tweets/stories. We request at least 10 items (or all available if less than 10) and pass in the raw content. The `response_format` parameter enforces strict JSON output matching our defined schema.  \nThe completion response will contain structured JSON data that we can parse and use to generate our draft post. Each item will have a link to the original tweet/story and a concise description of what makes it noteworthy.  \n```ts\n// Check if we got a content payload in the first choice\nconst rawJSON = completion?.choices?.[0]?.message?.content;\nif (!rawJSON) {\nconsole.log(\"No JSON output returned from Together.\");\nreturn \"No output.\";\n}\nconsole.log(rawJSON);\n\n// Parse the JSON to match our schema\nconst parsedResponse = JSON.parse(rawJSON);\n\n// Construct the final post\nconst header = `ðŸš€ AI and LLM Trends on X for ${currentDate}nn`;\nconst draft_post = header + parsedResponse.interestingTweetsOrStories\n.map((tweetOrStory: any) => `â€¢ ${tweetOrStory.description}n ${tweetOrStory.story_or_tweet_link}`)\n.join('nn');\n\nreturn draft_post;\n\n} catch (error) {\nconsole.error(\"Error generating draft post\", error);\nreturn \"Error generating draft post.\";\n}\n}\n// End of script\n\n```  \nThis code block shows the final part of our script where we handle the Together AI API response. We first check if we received valid JSON content in the response. If not, we log an error and return early.  \nIf we have valid JSON, we parse it into a structured object matching our schema. Then we construct the final post by adding a header with the current date and mapping over the interesting tweets/stories to create bullet points. Each bullet point contains the description and link.  \nThe script includes error handling to catch and log any issues that occur during execution. If thereâ€™s an error, it returns a generic error message rather than failing silently.  \nThis completes the core functionality of our trend finding script. The next sections will cover setting up notifications, scheduling, and deployment.",
      "metadata": {
        "title": "Building a Trend Detection System with AI in TypeScript: A Step-by-Step Guide",
        "url": "https://www.firecrawl.dev/blog/trend-finder-typescript"
      }
    },
    {
      "id": "0ce6cc16-e11c-4a38-8c64-ca142b4941e4",
      "source": "firecrawl/blog/trend-finder-typescript.md",
      "content": "In-depth Project Breakdown > 4. Setting up a notification system with Slack\n\nInside `src/services/sendDraft.ts`, we write the functionality to send the composed final post as a Slack message through a webhook:  \n```ts\nimport axios from \"axios\";\nimport dotenv from \"dotenv\";\ndotenv.config();\n\nexport async function sendDraft(draft_post: string) {\ntry {\nconst response = await axios.post(\nprocess.env.SLACK_WEBHOOK_URL || \"\",\n{\ntext: draft_post,\n},\n{\nheaders: {\n\"Content-Type\": \"application/json\",\n},\n},\n);\n\nreturn `Success sending draft to webhook at ${new Date().toISOString()}`;\n} catch (error) {\nconsole.log(\"error sending draft to webhook\");\nconsole.log(error);\n}\n}\n\n```  \nThis script sets up a Slack notification system by creating a `sendDraft` function that takes a draft post as input and sends it to a configured Slack webhook URL. The function uses `axios` to make a POST request to the webhook with the draft text. It includes error handling to log any issues that occur during the sending process. The webhook URL is loaded from environment variables using `dotenv` for security. On success, it returns a timestamp of when the draft was sent.",
      "metadata": {
        "title": "Building a Trend Detection System with AI in TypeScript: A Step-by-Step Guide",
        "url": "https://www.firecrawl.dev/blog/trend-finder-typescript"
      }
    },
    {
      "id": "6c9b09bc-bf74-452e-b9f2-3cd67c9cf52f",
      "source": "firecrawl/blog/trend-finder-typescript.md",
      "content": "In-depth Project Breakdown > 5. Writing a script to execute the system with Cron\n\nThe cron.ts file contains the main execution logic for our trend finding system. It exports a `handleCron` function that orchestrates the entire workflow:  \n```ts\n// src/controllers/cron.ts\nimport { scrapeSources } from \"../services/scrapeSources\";\nimport { getCronSources } from \"../services/getCronSources\";\nimport { generateDraft } from \"../services/generateDraft\";\nimport { sendDraft } from \"../services/sendDraft\";\nexport const handleCron = async (): Promise<void> => {\ntry {\nconst cronSources = await getCronSources();\nconst rawStories = await scrapeSources(cronSources!);\nconst rawStoriesString = JSON.stringify(rawStories);\nconst draftPost = await generateDraft(rawStoriesString);\nconst result = await sendDraft(draftPost!);\nconsole.log(result);\n} catch (error) {\nconsole.error(error);\n}\n};\n\n```  \nFirst, it retrieves the list of sources to scrape by calling `getCronSources()`. Then it scrapes those sources using `scrapeSources()` to get the raw story data. This raw data is stringified into JSON format.  \nNext, it generates a draft post from the story data by passing it to `generateDraft()`. Finally, it sends the draft to Slack using `sendDraft()` and logs the result.  \nThe function includes error handling to catch and log any issues that occur during execution. This script ties together all the individual services we created to form a complete automated workflow.",
      "metadata": {
        "title": "Building a Trend Detection System with AI in TypeScript: A Step-by-Step Guide",
        "url": "https://www.firecrawl.dev/blog/trend-finder-typescript"
      }
    },
    {
      "id": "f9e68079-7964-4d97-b51c-78949f4c280c",
      "source": "firecrawl/blog/trend-finder-typescript.md",
      "content": "In-depth Project Breakdown > 6. Creating a project entrypoint\n\nThe `src/index.ts` file serves as the main entry point for our application. It imports the `handleCron` function from our cron controller and sets up the execution flow.  \nThe file uses `node-cron` for scheduling and `dotenv` for environment variable management. The main function provides a simple way to run the draft generation process manually.  \nThereâ€™s also a commented-out cron schedule that can be uncommented to run the job automatically at 5 PM daily (0 17 ** *).  \n```ts\nimport { handleCron } from \"./controllers/cron\";\nimport cron from \"node-cron\";\nimport dotenv from \"dotenv\";\n\ndotenv.config();\n\nasync function main() {\nconsole.log(`Starting process to generate draft...`);\nawait handleCron();\n}\nmain();\n\n// If you want to run the cron job manually, uncomment the following line:\n//cron.schedule(`0 17 * * *`, async () => {\n// console.log(`Starting process to generate draft...`);\n// await handleCron();\n//});\n\n```  \nWhen you `npm run start`, this script is executed.  \n* * *  \nAt this point, the project is ready for local use. You can modify the topic configurations at any time to track different subjects and generate Slack summaries on demand. While running locally is useful for testing, weâ€™ll explore an even more powerful automation option using GitHub Actions in the next section.",
      "metadata": {
        "title": "Building a Trend Detection System with AI in TypeScript: A Step-by-Step Guide",
        "url": "https://www.firecrawl.dev/blog/trend-finder-typescript"
      }
    },
    {
      "id": "e54e7e4f-e7f0-4dc4-a78e-ac13658eb27a",
      "source": "firecrawl/blog/trend-finder-typescript.md",
      "content": "In-depth Project Breakdown > 7. Deploying the project with GitHub Actions\n\nNow that we have our project working locally, letâ€™s take it to the next level by automating it with GitHub Actions. GitHub Actions is a powerful CI/CD platform that allows us to automate workflows directly from our GitHub repository. Instead of running our trend finder manually or setting up a server to host it, we can leverage GitHubâ€™s infrastructure to run our script on a schedule, completely free for public repositories. Letâ€™s set it up.  \nFirst, create a new file in your repository at `.github/workflows/trend-finder.yml`:  \n```bash\nmkdir -p .github/workflows\ntouch .github/workflows/trend-finder.yml\n\n```  \nThen, paste the following contents:  \n```yaml\nname: Run Trend Finder\n\non:\nschedule:\n- cron: \"0 17 * * *\" # Runs at 5 PM UTC daily\nworkflow_dispatch: # Allows manual trigger from GitHub UI\n\njobs:\nfind-trends:\nruns-on: ubuntu-latest\n\nsteps:\n- uses: actions/checkout@v3\n\n- name: Setup Node.js\nuses: actions/setup-node@v3\nwith:\nnode-version: \"18\"\n\n- name: Install dependencies\nrun: npm install\n\n- name: Run trend finder\nenv:\nX_API_BEARER_TOKEN: ${{ secrets.X_API_BEARER_TOKEN }}\nFIRECRAWL_API_KEY: ${{ secrets.FIRECRAWL_API_KEY }}\nTOGETHER_API_KEY: ${{ secrets.TOGETHER_API_KEY }}\nSLACK_WEBHOOK_URL: ${{ secrets.SLACK_WEBHOOK_URL }}\nrun: npm run start\n\n```  \nThis workflow configuration does several important things:  \n1. **Scheduling**: The `on.schedule` section sets up automatic daily runs at 5 PM UTC\n2. **Manual Triggers**: `workflow_dispatch` allows you to run the workflow manually from GitHubâ€™s UI\n3. **Environment**: Uses Ubuntu as the runner environment\n4. **Setup**: Configures Node.js and installs dependencies\n5. **Secrets**: Securely passes API keys and tokens from GitHub Secrets to the application  \nTo set up the secrets in your GitHub repository:  \n1. Go to your repositoryâ€™s Settings\n2. Click on â€œSecrets and variablesâ€ â†’ â€œActionsâ€\n3. Add each required secret:\n- `X_API_BEARER_TOKEN`\n- `FIRECRAWL_API_KEY`\n- `TOGETHER_API_KEY`\n- `SLACK_WEBHOOK_URL`  \nThe workflow will now run automatically every day at 5 PM UTC, scraping your configured sources and sending trend updates to your Slack channel. You can also trigger it manually:  \n1. Go to your repositoryâ€™s â€œActionsâ€ tab\n2. Select â€œRun Trend Finderâ€ workflow\n3. Click â€œRun workflowâ€  \nSome key benefits of using GitHub Actions:  \n- **Zero Infrastructure**: No need to maintain servers or worry about uptime\n- **Cost Effective**: Free for public repositories (2000 minutes/month)\n- **Version Controlled**: Your automation configuration lives with your code\n- **Easy Monitoring**: Built-in logs and status checks\n- **Flexible Scheduling**: Easy to modify run times or add multiple schedules",
      "metadata": {
        "title": "Building a Trend Detection System with AI in TypeScript: A Step-by-Step Guide",
        "url": "https://www.firecrawl.dev/blog/trend-finder-typescript"
      }
    },
    {
      "id": "d769fa61-b2fb-4caf-9c47-ad0aea8c7c74",
      "source": "firecrawl/blog/trend-finder-typescript.md",
      "content": "Next Steps\n\nNow that your trend finder is fully automated, here are some ways to extend it:  \n1. **Custom Topics**: Modify the scraping configurations to track different topics\n2. **Additional Sources**: Add more websites or social media accounts to monitor\n3. **Enhanced Analysis**: Customize the AI prompts for different types of trend analysis\n4. **Multiple Channels**: Set up different Slack channels for different topic categories\n5. **Metrics**: Add monitoring for successful runs and trend detection rates  \nThe complete project provides a robust foundation for automated trend detection that you can build upon based on your specific needs.",
      "metadata": {
        "title": "Building a Trend Detection System with AI in TypeScript: A Step-by-Step Guide",
        "url": "https://www.firecrawl.dev/blog/trend-finder-typescript"
      }
    },
    {
      "id": "bbf78685-16fa-4ca6-bbf6-0a7db5a88f9b",
      "source": "firecrawl/blog/trend-finder-typescript.md",
      "content": "Troubleshooting\n\nIf you encounter issues with the GitHub Actions workflow:  \n1. **Check Logs**: Review the workflow run logs in the Actions tab\n2. **Verify Secrets**: Ensure all secrets are properly set and not expired\n3. **Rate Limits**: Monitor API rate limits, especially for the X API\n4. **Timeout Issues**: Consider breaking up large scraping jobs if runs timeout\n5. **Dependencies**: Keep Node.js dependencies updated to latest stable versions  \nFor additional help, check the projectâ€™s GitHub Issues or create a new one with specific details about any problems you encounter.",
      "metadata": {
        "title": "Building a Trend Detection System with AI in TypeScript: A Step-by-Step Guide",
        "url": "https://www.firecrawl.dev/blog/trend-finder-typescript"
      }
    },
    {
      "id": "00fa1d38-c3d1-4397-afb0-22108949d0bc",
      "source": "firecrawl/blog/trend-finder-typescript.md",
      "content": "Limitations of Free Tier Tools Used\n\nWhile this project uses several free tier services to minimize costs, there are some limitations to be aware of:  \n1. **X API Rate Limits**\n- Limited to 1 account scrape requests/15-minute window\n- Some advanced filtering features not available\n2. **GitHub Actions Minutes**\n- 2,000 minutes/month for public repositories\n- 3,000 minutes/month for private repositories\n- Additional minutes require paid plan\n3. **Together AI Free Credits**\n- $1 in free credits for new accounts\n- 600 requests per minute\n4. **Firecrawl API Limits**\n- 500 requests/month on free plan  \nTo work within these constraints:  \n- Carefully plan scraping intervals\n- Implement caching where possible\n- Monitor usage to avoid hitting limits\n- Consider paid tiers for production use  \nFor most personal or small team use cases, the free tiers provide sufficient capacity. However, larger scale deployments may require upgrading to paid plans for higher limits and additional features.",
      "metadata": {
        "title": "Building a Trend Detection System with AI in TypeScript: A Step-by-Step Guide",
        "url": "https://www.firecrawl.dev/blog/trend-finder-typescript"
      }
    },
    {
      "id": "aa3f6477-3d06-4dae-a8ee-073ee2d44b45",
      "source": "firecrawl/blog/trend-finder-typescript.md",
      "content": "Conclusion\n\nYouâ€™ve now built and deployed a fully automated trend detection system that leverages AI, web scraping, and cloud automation. This solution provides real-time insights into emerging trends across your chosen sources, delivered directly to Slack. With the foundation in place, you can easily customize and expand the system to match your specific trend monitoring needs. The combination of GitHub Actions for automation, Firecrawl for AI web scraping, Together AI for analysis, and Slack for notifications creates a powerful, maintainable solution that will help you stay ahead of relevant trends in your field.  \nArticle updated recently  \n[ðŸ”¥](https://www.firecrawl.dev/)",
      "metadata": {
        "title": "Building a Trend Detection System with AI in TypeScript: A Step-by-Step Guide",
        "url": "https://www.firecrawl.dev/blog/trend-finder-typescript"
      }
    },
    {
      "id": "b80fb87b-ac23-4aa0-90d0-d0e60a703dcd",
      "source": "firecrawl/blog/trend-finder-typescript.md",
      "content": "Ready to _Build?_\n\nStart scraping web data for your AI apps today.  \nNo credit card needed.  \nGet Started",
      "metadata": {
        "title": "Building a Trend Detection System with AI in TypeScript: A Step-by-Step Guide",
        "url": "https://www.firecrawl.dev/blog/trend-finder-typescript"
      }
    },
    {
      "id": "b38412a6-5efe-4205-a9c6-ada8831ac630",
      "source": "firecrawl/blog/trend-finder-typescript.md",
      "content": "About the Author\n\n[![Bex Tuychiev image](https://www.firecrawl.dev/bex.jpg)\\\nBex Tuychiev@bextuychiev](https://x.com/bextuychiev)  \nBex is a Top 10 AI writer on Medium and a Kaggle Master with over 15k followers. He loves writing detailed guides, tutorials, and notebooks on complex data science and machine learning topics",
      "metadata": {
        "title": "Building a Trend Detection System with AI in TypeScript: A Step-by-Step Guide",
        "url": "https://www.firecrawl.dev/blog/trend-finder-typescript"
      }
    },
    {
      "id": "067795bc-7c11-47f9-a347-05433c45c6f9",
      "source": "firecrawl/blog/trend-finder-typescript.md",
      "content": "About the Author > More articles by Bex Tuychiev\n\n[Building an Automated Price Tracking Tool\\\n\\\nBuild an automated e-commerce price tracker in Python. Learn web scraping, price monitoring, and automated alerts using Firecrawl, Streamlit, PostgreSQL.](https://www.firecrawl.dev/blog/automated-price-tracking-tutorial-python) [Web Scraping Automation: How to Run Scrapers on a Schedule\\\n\\\nLearn how to automate web scraping in Python using free tools like schedule, asyncio, cron jobs and GitHub Actions. This comprehensive guide covers local and cloud-based scheduling methods to run scrapers reliably in 2025.](https://www.firecrawl.dev/blog/automated-web-scraping-free-2025) [Automated Data Collection - A Comprehensive Guide\\\n\\\nLearn how to build robust automated data collection systems using modern tools and best practices. This guide covers everything from selecting the right tools to implementing scalable collection pipelines.](https://www.firecrawl.dev/blog/automated-data-collection-guide) [BeautifulSoup4 vs. Scrapy - A Comprehensive Comparison for Web Scraping in Python\\\n\\\nLearn the key differences between BeautifulSoup4 and Scrapy for web scraping in Python. Compare their features, performance, and use cases to choose the right tool for your web scraping needs.](https://www.firecrawl.dev/blog/beautifulsoup4-vs-scrapy-comparison) [How to Build an Automated Competitor Price Monitoring System with Python\\\n\\\nLearn how to build an automated competitor price monitoring system in Python that tracks prices across e-commerce sites, provides real-time comparisons, and maintains price history using Firecrawl, Streamlit, and GitHub Actions.](https://www.firecrawl.dev/blog/automated-competitor-price-scraping) [Scraping Company Data and Funding Information in Bulk With Firecrawl and Claude\\\n\\\nLearn how to build a web scraper in Python that gathers company details, funding rounds, and investor information from public sources like Crunchbase using Firecrawl and Claude for automated data collection and analysis.](https://www.firecrawl.dev/blog/crunchbase-scraping-with-firecrawl-claude) [Data Enrichment: A Complete Guide to Enhancing Your Data Quality\\\n\\\nLearn how to enrich your data quality with a comprehensive guide covering data enrichment tools, best practices, and real-world examples. Discover how to leverage modern solutions like Firecrawl to automate data collection, validation, and integration for better business insights.](https://www.firecrawl.dev/blog/complete-guide-to-data-enrichment) [Building an Intelligent Code Documentation RAG Assistant with DeepSeek and Firecrawl\\\n\\\nLearn how to build an intelligent documentation assistant powered by DeepSeek and RAG (Retrieval Augmented Generation) that can answer questions about any documentation website by combining language models with efficient information retrieval.](https://www.firecrawl.dev/blog/deepseek-rag-documentation-assistant)",
      "metadata": {
        "title": "Building a Trend Detection System with AI in TypeScript: A Step-by-Step Guide",
        "url": "https://www.firecrawl.dev/blog/trend-finder-typescript"
      }
    },
    {
      "id": "572681d0-bc8c-4812-a704-5f72eaa64e8e",
      "source": "firecrawl/blog/launch-week-i-day-4-introducing-firecrawl-v1.md",
      "content": "---\ntitle: Launch Week I / Day 4: Introducing Firecrawl /v1\nurl: https://www.firecrawl.dev/blog/launch-week-i-day-4-introducing-firecrawl-v1\n---  \nIntroducing /extract - Get web data with a prompt [Try now](https://www.firecrawl.dev/extract)  \nAugust 29, 2024  \nâ€¢  \n[![Eric Ciarla image](https://www.firecrawl.dev/eric-img.jpeg)Eric Ciarla](https://x.com/ericciarla)  \n# Launch Week I / Day 4: Introducing Firecrawl /v1  \n![Launch Week I / Day 4: Introducing Firecrawl /v1 image](https://www.firecrawl.dev/images/blog/firecrawl-v1-release.png)  \nWelcome to Day 4 of Firecrawlâ€™s Launch Week! Today, weâ€™re thrilled to announce our biggest release yet: Firecrawl /v1.  \n**Introducing Firecrawl /v1**  \nFirecrawl v1 is a more reliable and developer-friendly API that makes gathering any web data seamless. Weâ€™ve packed this release with numerous improvements and new features to enhance your web scraping experience. Letâ€™s dive into the exciting updates!  \n**1. New Scrape Formats**  \nWith v1, you now have the flexibility to choose your output formats. From markdown to screenshots, get your data exactly how you want it. You can even specify multiple output formats in a single request!  \n**2. Improved Crawl Status**  \nWeâ€™ve significantly enhanced our crawl status results:  \n- Pagination by default for easy content streaming\n- A â€˜nextâ€™ parameter to show which URL to query next\n- Improved clarity on errors, credits used, and crawl expiration  \n**3. Enhanced Markdown Parsing**  \nOur markdown parsing has been upgraded, with improved content cleaning by default. The `onlyMainContent` option is now set to true by default, resulting in cleaner text chunks and improved quality of LLM outputs.  \n**4. V1 Support for All SDKs**  \nWeâ€™ve bumped all our SDKs to version 1.0, ensuring full compatibility with the new v1 API. Donâ€™t worry - theyâ€™re backwards compatible with v0 too! Weâ€™re also excited to introduce two new SDKs: GO and Rust. A big thanks to our amazing contributors @sanixdarker and @kentchiahaohsu for their work on these.  \n**5. New /map Endpoint**  \nAs announced yesterday, our new v1 endpoint `/map` is the fastest way to gather and search for relevant links on a website. If you missed it, check out our [previous post](https://firecrawl.dev/blog/launch-week-i-day-3-introducing-map-endpoint) for more details.  \n**6. Improved Developer Experience**  \nWeâ€™ve enhanced the developer experience across all our API endpoints. This includes reducing complexity and providing more descriptive errors. For a smooth transition, check out our guide on how to migrate from v0: [https://docs.firecrawl.dev/migrating-from-v0](https://docs.firecrawl.dev/migrating-from-v0)  \n**A Milestone Achievement**  \nWeâ€™re thrilled to announce that weâ€™ve just hit 10,000 GitHub stars! This achievement wouldnâ€™t have been possible without our amazing community, contributors, and partners. Thank you for your continued support and enthusiasm.  \nCheck out our v1 documentation here: [https://docs.firecrawl.dev/v1-welcome](https://docs.firecrawl.dev/v1-welcome)  \nAnd donâ€™t forget to star our GitHub repo: [https://github.com/mendableai/firecrawl](https://github.com/mendableai/firecrawl)  \nStay tuned for our final day of Launch Week tomorrow. We canâ€™t wait to show you what else we have in store!  \nArticle updated recently  \n[ðŸ”¥](https://www.firecrawl.dev/)",
      "metadata": {
        "title": "Launch Week I / Day 4: Introducing Firecrawl /v1",
        "url": "https://www.firecrawl.dev/blog/launch-week-i-day-4-introducing-firecrawl-v1"
      }
    },
    {
      "id": "f2a54e06-9f72-4f6b-9e33-1a2b3a73d969",
      "source": "firecrawl/blog/launch-week-i-day-4-introducing-firecrawl-v1.md",
      "content": "Ready to _Build?_\n\nStart scraping web data for your AI apps today.  \nNo credit card needed.  \nGet Started",
      "metadata": {
        "title": "Launch Week I / Day 4: Introducing Firecrawl /v1",
        "url": "https://www.firecrawl.dev/blog/launch-week-i-day-4-introducing-firecrawl-v1"
      }
    },
    {
      "id": "3fb11e8d-1e4b-466f-b0a1-02021f266146",
      "source": "firecrawl/blog/launch-week-i-day-4-introducing-firecrawl-v1.md",
      "content": "About the Author\n\n[![Eric Ciarla image](https://www.firecrawl.dev/eric-img.jpeg)\\\nEric Ciarla@ericciarla](https://x.com/ericciarla)  \nEric Ciarla is the Chief Operating Officer (COO) of Firecrawl and leads marketing. He also worked on Mendable.ai and sold it to companies like Snapchat, Coinbase, and MongoDB.\nPreviously worked at Ford and Fracta as a Data Scientist. Eric also co-founded SideGuide, a tool for learning code within VS Code with 50,000 users.",
      "metadata": {
        "title": "Launch Week I / Day 4: Introducing Firecrawl /v1",
        "url": "https://www.firecrawl.dev/blog/launch-week-i-day-4-introducing-firecrawl-v1"
      }
    },
    {
      "id": "f861b5a9-e221-4288-8409-83e9707e21b6",
      "source": "firecrawl/blog/launch-week-i-day-4-introducing-firecrawl-v1.md",
      "content": "About the Author > More articles by Eric Ciarla\n\n[How to Create an llms.txt File for Any Website\\\n\\\nLearn how to generate an llms.txt file for any website using the llms.txt Generator and Firecrawl.](https://www.firecrawl.dev/blog/How-to-Create-an-llms-txt-File-for-Any-Website) [Cloudflare Error 1015: How to solve it?\\\n\\\nCloudflare Error 1015 is a rate limiting error that occurs when Cloudflare detects that you are exceeding the request limit set by the website owner.](https://www.firecrawl.dev/blog/cloudflare-error-1015-how-to-solve-it) [Build an agent that checks for website contradictions\\\n\\\nUsing Firecrawl and Claude to scrape your website's data and look for contradictions.](https://www.firecrawl.dev/blog/contradiction-agent) [Why Companies Need a Data Strategy for Generative AI\\\n\\\nLearn why a well-defined data strategy is essential for building robust, production-ready generative AI systems, and discover practical steps for curation, maintenance, and integration.](https://www.firecrawl.dev/blog/why-companies-need-a-data-strategy-for-generative-ai) [Getting Started with OpenAI's Predicted Outputs for Faster LLM Responses\\\n\\\nA guide to leveraging Predicted Outputs to speed up LLM tasks with GPT-4o models.](https://www.firecrawl.dev/blog/getting-started-with-predicted-outputs-openai) [How to easily install requests with pip and python\\\n\\\nA tutorial on installing the requests library in Python using various methods, with usage examples and troubleshooting tips](https://www.firecrawl.dev/blog/how-to-easily-install-requests-with-pip-and-python) [How to quickly install BeautifulSoup with Python\\\n\\\nA guide on installing the BeautifulSoup library in Python using various methods, with usage examples and troubleshooting tips](https://www.firecrawl.dev/blog/how-to-quickly-install-beautifulsoup-with-python) [How to Use OpenAI's o1 Reasoning Models in Your Applications\\\n\\\nLearn how to harness OpenAI's latest o1 series models for complex reasoning tasks in your apps.](https://www.firecrawl.dev/blog/how-to-use-openai-o1-reasoning-models-in-applications)",
      "metadata": {
        "title": "Launch Week I / Day 4: Introducing Firecrawl /v1",
        "url": "https://www.firecrawl.dev/blog/launch-week-i-day-4-introducing-firecrawl-v1"
      }
    },
    {
      "id": "f3348246-0133-45e7-ac5e-004fef825a21",
      "source": "firecrawl/blog/mastering-the-crawl-endpoint-in-firecrawl.md",
      "content": "---\ntitle: Mastering Firecrawl's Crawl Endpoint: A Complete Web Scraping Guide\nurl: https://www.firecrawl.dev/blog/mastering-the-crawl-endpoint-in-firecrawl\n---  \nIntroducing /extract - Get web data with a prompt [Try now](https://www.firecrawl.dev/extract)  \nNov 18, 2024  \nâ€¢  \n[![Bex Tuychiev image](https://www.firecrawl.dev/bex.jpg)Bex Tuychiev](https://x.com/bextuychiev)  \n# Mastering Firecrawl's Crawl Endpoint: A Complete Web Scraping Guide  \n![Mastering Firecrawl's Crawl Endpoint: A Complete Web Scraping Guide image](https://www.firecrawl.dev/images/blog/crawl-masterclass/images/mastering-crawl.jpg)",
      "metadata": {
        "title": "Mastering Firecrawl's Crawl Endpoint: A Complete Web Scraping Guide",
        "url": "https://www.firecrawl.dev/blog/mastering-the-crawl-endpoint-in-firecrawl"
      }
    },
    {
      "id": "19d9437e-3d37-4730-89ab-1719bbfbe661",
      "source": "firecrawl/blog/mastering-the-crawl-endpoint-in-firecrawl.md",
      "content": "Introduction\n\nWeb scraping and data extraction have become essential tools as businesses race to convert unprecedented amounts of online data into LLM-friendly formats. Firecrawlâ€™s powerful web scraping API streamlines this process with enterprise-grade automation and scalability features.  \nThis comprehensive guide focuses on Firecrawlâ€™s most powerful feature - the `/crawl` endpoint, which enables automated website scraping at scale. Youâ€™ll learn how to:  \n- Recursively traverse website sub-pages\n- Handle dynamic JavaScript-based content\n- Bypass common web scraping blockers\n- Extract clean, structured data for AI/ML applications  \nWant to follow along with our python notebook version of this post? [Check it out here!](https://github.com/mendableai/firecrawl/blob/main/examples/mastering-the-crawl-endpoint/mastering-the-crawl-endpoint.ipynb)",
      "metadata": {
        "title": "Mastering Firecrawl's Crawl Endpoint: A Complete Web Scraping Guide",
        "url": "https://www.firecrawl.dev/blog/mastering-the-crawl-endpoint-in-firecrawl"
      }
    },
    {
      "id": "376a4b02-6d1e-4e0d-bc05-586647e40821",
      "source": "firecrawl/blog/mastering-the-crawl-endpoint-in-firecrawl.md",
      "content": "Table of Contents\n\n- [Introduction](https://www.firecrawl.dev/blog/mastering-the-crawl-endpoint-in-firecrawl#introduction)\n- [Web Scraping vs Web Crawling: Understanding the Key Differences](https://www.firecrawl.dev/blog/mastering-the-crawl-endpoint-in-firecrawl#web-scraping-vs-web-crawling-understanding-the-key-differences)\n- [Whatâ€™s the Difference?](https://www.firecrawl.dev/blog/mastering-the-crawl-endpoint-in-firecrawl#whats-the-difference)\n- [How Firecrawl Combines Both](https://www.firecrawl.dev/blog/mastering-the-crawl-endpoint-in-firecrawl#how-firecrawl-combines-both)\n- [Step-by-Step Guide to Web Crawling with Firecrawlâ€™s API](https://www.firecrawl.dev/blog/mastering-the-crawl-endpoint-in-firecrawl#step-by-step-guide-to-web-crawling-with-firecrawls-api)\n- [Performance & Limits](https://www.firecrawl.dev/blog/mastering-the-crawl-endpoint-in-firecrawl#performance--limits)\n- [Asynchronous Web Crawling with Firecrawl: Efficient Large-Scale Data Collection](https://www.firecrawl.dev/blog/mastering-the-crawl-endpoint-in-firecrawl#asynchronous-web-crawling-with-firecrawl-efficient-large-scale-data-collection)\n- [Asynchronous programming in a nutshell](https://www.firecrawl.dev/blog/mastering-the-crawl-endpoint-in-firecrawl#asynchronous-programming-in-a-nutshell)\n- [Using `async_crawl_url` method](https://www.firecrawl.dev/blog/mastering-the-crawl-endpoint-in-firecrawl#using-async_crawl_url-method)\n- [Benefits of asynchronous crawling](https://www.firecrawl.dev/blog/mastering-the-crawl-endpoint-in-firecrawl#benefits-of-asynchronous-crawling)\n- [How to Save and Store Web Crawling Results](https://www.firecrawl.dev/blog/mastering-the-crawl-endpoint-in-firecrawl#how-to-save-and-store-web-crawling-results)\n- [Local file storage](https://www.firecrawl.dev/blog/mastering-the-crawl-endpoint-in-firecrawl#local-file-storage)\n- [Building AI-Powered Web Crawlers with Firecrawl and LangChain Integration](https://www.firecrawl.dev/blog/mastering-the-crawl-endpoint-in-firecrawl#building-ai-powered-web-crawlers-with-firecrawl-and-langchain-integration)\n- [Conclusion](https://www.firecrawl.dev/blog/mastering-the-crawl-endpoint-in-firecrawl#conclusion)",
      "metadata": {
        "title": "Mastering Firecrawl's Crawl Endpoint: A Complete Web Scraping Guide",
        "url": "https://www.firecrawl.dev/blog/mastering-the-crawl-endpoint-in-firecrawl"
      }
    },
    {
      "id": "4b470319-6358-4c33-8b67-7ca695b55f55",
      "source": "firecrawl/blog/mastering-the-crawl-endpoint-in-firecrawl.md",
      "content": "Web Scraping vs Web Crawling: Understanding the Key Differences > Whatâ€™s the Difference?\n\n_Web scraping_ refers to extracting specific data from individual web pages like a Wikipedia article or a technical tutorial. It is primarily used when you need specific information from pages with _known URLs_.  \n_Web crawling_, on the other hand, involves systematically browsing and discovering web pages by following links. It focuses on website navigation and URL discovery.  \nFor example, to build a chatbot that answers questions about Stripeâ€™s documentation, you would need:  \n1. Web crawling to discover and traverse all pages in Stripeâ€™s documentation site\n2. Web scraping to extract the actual content from each discovered page",
      "metadata": {
        "title": "Mastering Firecrawl's Crawl Endpoint: A Complete Web Scraping Guide",
        "url": "https://www.firecrawl.dev/blog/mastering-the-crawl-endpoint-in-firecrawl"
      }
    },
    {
      "id": "25f2133a-df41-4675-a409-a6c4238363e0",
      "source": "firecrawl/blog/mastering-the-crawl-endpoint-in-firecrawl.md",
      "content": "Web Scraping vs Web Crawling: Understanding the Key Differences > How Firecrawl Combines Both\n\nFirecrawlâ€™s `/crawl` endpoint combines both capabilities:  \n1. URL analysis: Identifies links through sitemap or page traversal\n2. Recursive traversal: Follows links to discover sub-pages\n3. Content scraping: Extracts clean content from each page\n4. Results compilation: Converts everything to structured data  \nWhen you pass the URL `https://docs.stripe.com/api` to the endpoint, it automatically discovers and crawls all documentation sub-pages. The endpoint returns the content in your preferred format - whether thatâ€™s markdown, HTML, screenshots, links, or metadata.",
      "metadata": {
        "title": "Mastering Firecrawl's Crawl Endpoint: A Complete Web Scraping Guide",
        "url": "https://www.firecrawl.dev/blog/mastering-the-crawl-endpoint-in-firecrawl"
      }
    },
    {
      "id": "884d6202-32a7-470e-b3bf-b8c385859413",
      "source": "firecrawl/blog/mastering-the-crawl-endpoint-in-firecrawl.md",
      "content": "Step-by-Step Guide to Web Crawling with Firecrawlâ€™s API\n\nFirecrawl is a web scraping engine exposed as a REST API. You can use it from the command line using cURL or using one of its language SDKs for Python, Node, Go, or Rust. For this tutorial, weâ€™ll focus on its Python SDK.  \nTo get started:  \n1. Sign up at [firecrawl.dev](https://firecrawl.dev/) and copy your API key\n2. Save the key as an environment variable:  \n```bash\nexport FIRECRAWL_API_KEY='fc-YOUR-KEY-HERE'\n\n```  \nOr use a dot-env file:  \n```bash\ntouch .env\necho \"FIRECRAWL_API_KEY='fc-YOUR-KEY-HERE'\" >> .env\n\n```  \nThen use the Python SDK:  \n```python\nfrom firecrawl import FirecrawlApp\nfrom dotenv import load_dotenv\n\nload_dotenv()\napp = FirecrawlApp()\n\n```  \nOnce your API key is loaded, the `FirecrawlApp` class uses it to establish a connection with the Firecrawl API engine.  \nFirst, we will crawl the [https://books.toscrape.com/](https://books.toscrape.com/) website, which is built for web-scraping practice:  \n![Homepage of Books to Scrape website showing a grid layout of book covers, prices and ratings - a popular web scraping practice site](https://www.firecrawl.dev/images/blog/crawl-masterclass/images/books_to_scrape_homepage.png)  \nInstead of writing dozens of lines of code with libraries like `beautifulsoup4` or `lxml` to parse HTML elements, handle pagination and data retrieval, Firecrawlâ€™s `crawl_url` endpoint lets you accomplish this in a single line:  \n```python\nbase_url = \"https://books.toscrape.com/\"\ncrawl_result = app.crawl_url(url=base_url)\n\n```  \nThe result is a dictionary with the following keys:  \n```python\ncrawl_result.keys()\n\n```  \n```text\ndict_keys(['success', 'status', 'completed', 'total', 'creditsUsed', 'expiresAt', 'data'])\n\n```  \nFirst, we are interested in the status of the crawl job:  \n```python\ncrawl_result['status']\n\n```  \n```text\n'completed'\n\n```  \nIf it is completed, letâ€™s see how many pages were scraped:  \n```python\ncrawl_result['total']\n\n```  \n```text\n1195\n\n```  \nAlmost 1200 pages (it took about 70 seconds on my machine; the speed vary based on your connection speed). Letâ€™s look at one of the elements of the `data` list:  \n```python\nsample_page = crawl_result['data'][10]\nmarkdown_content = sample_page['markdown']\n\nprint(markdown_content[:500])\n\n```  \n```text\n- [Home](../../../../index.html)\n- [Books](../../books_1/index.html)\n- Womens Fiction\n\n# Womens Fiction\n\n**17** results.\n\n**Warning!** This is a demo website for web scraping purposes. Prices and ratings here were randomly assigned and have no real meaning.\n\n01. [![I Had a Nice Time And Other Lies...: How to find love & sh*t like that](../../../../media/cache/5f/72/5f72c8a0d5a7292e2929a354ec8a022f.jpg)](../../../i-had-a-nice-time-and-other-lies-how-to-find-love-sht-like-that_814/index.html)\n\n```  \nThe page corresponds to Womenâ€™s Fiction page:  \n![Screenshot of Books to Scrape website showing the Women's Fiction category page with book listings and pagination](https://www.firecrawl.dev/images/blog/crawl-masterclass/images/women-fiction.png)  \nFirecrawl also includes page metadata in the elementâ€™s dictionary as well:  \n```python\nsample_page['metadata']\n\n```  \n```text\n{\n'url': 'https://books.toscrape.com/catalogue/category/books/womens-fiction_9/index.html',\n'title': 'Womens Fiction | Books to Scrape - Sandbox',\n'robots': 'NOARCHIVE,NOCACHE',\n'created': '24th Jun 2016 09:29',\n'language': 'en-us',\n'viewport': 'width=device-width',\n'sourceURL': 'https://books.toscrape.com/catalogue/category/books/womens-fiction_9/index.html',\n'statusCode': 200,\n'description': '',\n'ogLocaleAlternate': []\n}\n\n```  \nOne thing we didnâ€™t mention is how Firecrawl handles pagination. If you scroll to the bottom of Books-to-Scrape, you will see that it has a â€œnextâ€ button.  \nBefore moving on to sub-pages like `books.toscrape.com/category`, Firecrawl first scrapes all sub-pages from the homepage. Later, if a sub-page includes links to already scraped pages, they are ignored.",
      "metadata": {
        "title": "Mastering Firecrawl's Crawl Endpoint: A Complete Web Scraping Guide",
        "url": "https://www.firecrawl.dev/blog/mastering-the-crawl-endpoint-in-firecrawl"
      }
    },
    {
      "id": "0f3d9530-d6c4-43a2-a61a-e0415064ccf5",
      "source": "firecrawl/blog/mastering-the-crawl-endpoint-in-firecrawl.md",
      "content": "Advanced Web Scraping Configuration and Best Practices\n\nFirecrawl offers several types of parameters to configure how the endpoint crawls over websites. We will outline them here with their use-cases.",
      "metadata": {
        "title": "Mastering Firecrawl's Crawl Endpoint: A Complete Web Scraping Guide",
        "url": "https://www.firecrawl.dev/blog/mastering-the-crawl-endpoint-in-firecrawl"
      }
    },
    {
      "id": "898b743a-1219-402c-883f-f155ad1cd35a",
      "source": "firecrawl/blog/mastering-the-crawl-endpoint-in-firecrawl.md",
      "content": "Advanced Web Scraping Configuration and Best Practices > Scrape Options\n\nOn real-world projects, you will tweak this parameter the most frequently. It allows you to control how a webpageâ€™s contents are saved. Firecrawl allows the following formats:  \n- Markdown - the default\n- HTML\n- Raw HTML (simple copy/paste of the entire webpage)\n- Links\n- Screenshot  \nHere is an example request to scrape the Stripe API in four formats:  \n```python\n# Crawl the first 5 pages of the stripe API documentation\nstripe_crawl_result = app.crawl_url(\nurl=\"https://docs.stripe.com/api\",\nparams={\n\"limit\": 5, # Only scrape the first 5 pages including the base-url\n\"scrapeOptions\": {\n\"formats\": [\"markdown\", \"html\", \"links\", \"screenshot\"]\n}\n}\n)\n\n```  \nWhen you specify multiple formats, each webpageâ€™s data contains separate keys for each formatâ€™s content:  \n```python\nstripe_crawl_result['data'][0].keys()\n\n```  \n```text\ndict_keys(['html', 'links', 'markdown', 'metadata', 'screenshot'])\n\n```  \nThe value of the `screenshot` key is a temporary link to a PNG file stored on Firecrawlâ€™s servers and expires within 24 hours. Here is what it looks like for Stripeâ€™s API documentation homepage:  \n```python\nfrom IPython.display import Image\n\nImage(stripe_crawl_result['data'][0]['screenshot'])\n\n```  \n![Screenshot of Stripe API documentation homepage captured using Firecrawl's web scraping capabilities](https://www.firecrawl.dev/images/blog/crawl-masterclass/notebook_files/notebook_39_0.png)  \nNote that specifying more formats to transform the pageâ€™s contents can significantly slow down the process.  \nAnother time-consuming operation can be scraping the entire page contents instead of just the elements you want. For such scenarios, Firecrawl allows you to control which elements of a webpage are scraped using the `onlyMainContent`, `includeTags`, and `excludeTags` parameters.  \nEnabling `onlyMainContent` parameter (disabled by default) excludes navigation, headers and footers:  \n```python\nstripe_crawl_result = app.crawl_url(\nurl=\"https://docs.stripe.com/api\",\nparams={\n\"limit\": 5,\n\"scrapeOptions\": {\n\"formats\": [\"markdown\", \"html\"],\n\"onlyMainContent\": True,\n},\n},\n)\n\n```  \n`includeTags` and `excludeTags` accepts a list of allowlisted/blocklisted HTML tags, classes and IDs:  \n```python\n# Crawl the first 5 pages of the stripe API documentation\nstripe_crawl_result = app.crawl_url(\nurl=\"https://docs.stripe.com/api\",\nparams={\n\"limit\": 5,\n\"scrapeOptions\": {\n\"formats\": [\"markdown\", \"html\"],\n\"includeTags\": [\"code\", \"#page-header\"],\n\"excludeTags\": [\"h1\", \"h2\", \".main-content\"],\n},\n},\n)\n\n```  \nCrawling large websites can take a long time and when appropriate, these small tweaks can have a big impact on the runtime.",
      "metadata": {
        "title": "Mastering Firecrawl's Crawl Endpoint: A Complete Web Scraping Guide",
        "url": "https://www.firecrawl.dev/blog/mastering-the-crawl-endpoint-in-firecrawl"
      }
    },
    {
      "id": "3d0ed790-575b-41d2-a16d-468bdb8c5a38",
      "source": "firecrawl/blog/mastering-the-crawl-endpoint-in-firecrawl.md",
      "content": "Advanced Web Scraping Configuration and Best Practices > URL Control\n\nApart from scraping configurations, you have four options to specify URL patterns to include or exclude during crawling:  \n- `includePaths` - targeting specific sections\n- `excludePaths` - avoiding unwanted content\n- `allowBackwardLinks` - handling cross-references\n- `allowExternalLinks` - managing external content  \nHere is a sample request that uses these parameters:  \n```python\n# Example of URL control parameters\nurl_control_result = app.crawl_url(\nurl=\"https://docs.stripe.com/\",\nparams={\n# Only crawl pages under the /payments path\n\"includePaths\": [\"/payments/*\"],\n# Skip the terminal and financial-connections sections\n\"excludePaths\": [\"/terminal/*\", \"/financial-connections/*\"],\n# Allow crawling links that point to already visited pages\n\"allowBackwardLinks\": False,\n# Don't follow links to external domains\n\"allowExternalLinks\": False,\n\"scrapeOptions\": {\n\"formats\": [\"html\"]\n}\n}\n)\n\n# Print the total number of pages crawled\nprint(f\"Total pages crawled: {url_control_result['total']}\")\n\n```  \n```out\nTotal pages crawled: 134\n\n```  \nIn this example, weâ€™re crawling the Stripe documentation website with specific URL control parameters:  \n- The crawler starts at [https://docs.stripe.com/](https://docs.stripe.com/) and only crawls pages under the `\"/payments/*\"` path\n- It explicitly excludes the `\"/terminal/*\"` and `\"/financial-connections/*\"` sections\n- By setting allowBackwardLinks to false, it wonâ€™t revisit already crawled pages\n- External links are ignored ( `allowExternalLinks: false`)\n- The scraping is configured to only capture HTML content  \nThis targeted approach helps focus the crawl on relevant content while avoiding unnecessary pages, making the crawl more efficient and focused on the specific documentation sections we need.  \nAnother critical parameter is `maxDepth`, which lets you control how many levels deep the crawler will traverse from the starting URL. For example, a `maxDepth` of 2 means it will crawl the initial page and pages linked from it, but wonâ€™t go further.  \nHere is another sample request on the Stripe API docs:  \n```python\n# Example of URL control parameters\nurl_control_result = app.crawl_url(\nurl=\"https://docs.stripe.com/\",\nparams={\n\"limit\": 100,\n\"maxDepth\": 2,\n\"allowBackwardLinks\": False,\n\"allowExternalLinks\": False,\n\"scrapeOptions\": {\"formats\": [\"html\"]},\n},\n)\n\n# Print the total number of pages crawled\nprint(f\"Total pages crawled: {url_control_result['total']}\")\n\n```  \n```out\nTotal pages crawled: 99\n\n```  \nNote: When a page has pagination (e.g. pages 2, 3, 4), these paginated pages are not counted as additional depth levels when using `maxDepth`.",
      "metadata": {
        "title": "Mastering Firecrawl's Crawl Endpoint: A Complete Web Scraping Guide",
        "url": "https://www.firecrawl.dev/blog/mastering-the-crawl-endpoint-in-firecrawl"
      }
    },
    {
      "id": "02a40a77-3d91-4f14-9be0-2a5eef6130d4",
      "source": "firecrawl/blog/mastering-the-crawl-endpoint-in-firecrawl.md",
      "content": "Advanced Web Scraping Configuration and Best Practices > Performance & Limits\n\nThe `limit` parameter, which weâ€™ve used in previous examples, is essential for controlling the scope of web crawling. It sets a maximum number of pages that will be scraped, which is particularly important when crawling large websites or when external links are enabled. Without this limit, the crawler could potentially traverse an endless chain of connected pages, consuming unnecessary resources and time.  \nWhile the limit parameter helps control the breadth of crawling, you may also need to ensure the quality and completeness of each page crawled. To make sure all desired content is scraped, you can enable a waiting period to let pages fully load. For example, some websites use JavaScript to handle dynamic content, have iFrames for embedding content or heavy media elements like videos or GIFs:  \n```python\nstripe_crawl_result = app.crawl_url(\nurl=\"https://docs.stripe.com/api\",\nparams={\n\"limit\": 5,\n\"scrapeOptions\": {\n\"formats\": [\"markdown\", \"html\"],\n\"waitFor\": 1000, # wait for a second for pages to load\n\"timeout\": 10000, # timeout after 10 seconds\n},\n},\n)\n\n```  \nThe above code also sets the `timeout` parameter to 10000 milliseconds (10 seconds), which ensures that if a page takes too long to load, the crawler will move on rather than getting stuck.  \nNote: `waitFor` duration applies to all pages the crawler encounters.  \nAll the while, it is important to keep the limits of your plan in mind:  \n| Plan | /scrape (requests/min) | /crawl (requests/min) | /search (requests/min) |\n| --- | --- | --- | --- |\n| Free | 10 | 1 | 5 |\n| Hobby | 20 | 3 | 10 |\n| Standard | 100 | 10 | 50 |\n| Growth | 1000 | 50 | 500 |",
      "metadata": {
        "title": "Mastering Firecrawl's Crawl Endpoint: A Complete Web Scraping Guide",
        "url": "https://www.firecrawl.dev/blog/mastering-the-crawl-endpoint-in-firecrawl"
      }
    },
    {
      "id": "5adda905-bef1-4f87-8462-c9b6bbc30707",
      "source": "firecrawl/blog/mastering-the-crawl-endpoint-in-firecrawl.md",
      "content": "Asynchronous Web Crawling with Firecrawl: Efficient Large-Scale Data Collection\n\nEven after following the tips and best practices from the previous section, the crawling process can be significantly long for large websites with thousands of pages. To handle this efficiently, Firecrawl provides asynchronous crawling capabilities that allow you to start a crawl and monitor its progress without blocking your application. This is particularly useful when building web applications or services that need to remain responsive while crawling is in progress.",
      "metadata": {
        "title": "Mastering Firecrawl's Crawl Endpoint: A Complete Web Scraping Guide",
        "url": "https://www.firecrawl.dev/blog/mastering-the-crawl-endpoint-in-firecrawl"
      }
    },
    {
      "id": "84901831-a969-4934-b540-984fc5f6fc13",
      "source": "firecrawl/blog/mastering-the-crawl-endpoint-in-firecrawl.md",
      "content": "Asynchronous Web Crawling with Firecrawl: Efficient Large-Scale Data Collection > Asynchronous programming in a nutshell\n\nFirst, letâ€™s understand asynchronous programming with a real-world analogy:  \nAsynchronous programming is like a restaurant server taking multiple orders at once. Instead of waiting at one table until the customers finish their meal before moving to the next table, they can take orders from multiple tables, submit them to the kitchen, and handle other tasks while the food is being prepared.  \nIn programming terms, this means your code can initiate multiple operations (like web requests or database queries) and continue executing other tasks while waiting for responses, rather than processing everything sequentially.  \nThis approach is particularly valuable in web crawling, where most of the time is spent waiting for network responses - instead of freezing the entire application while waiting for each page to load, async programming allows you to process multiple pages concurrently, dramatically improving efficiency.",
      "metadata": {
        "title": "Mastering Firecrawl's Crawl Endpoint: A Complete Web Scraping Guide",
        "url": "https://www.firecrawl.dev/blog/mastering-the-crawl-endpoint-in-firecrawl"
      }
    },
    {
      "id": "66ba191f-25d3-4c4e-95d3-cdb65c10fbfd",
      "source": "firecrawl/blog/mastering-the-crawl-endpoint-in-firecrawl.md",
      "content": "Asynchronous Web Crawling with Firecrawl: Efficient Large-Scale Data Collection > Using `async_crawl_url` method\n\nFirecrawl offers an intuitive asynchronous crawling method via `async_crawl_url`:  \n```python\napp = FirecrawlApp()\n\ncrawl_status = app.async_crawl_url(\"https://docs.stripe.com\")\n\nprint(crawl_status)\n\n```  \n```python\n{'success': True, 'id': 'c4a6a749-3445-454e-bf5a-f3e1e6befad7', 'url': 'https://api.firecrawl.dev/v1/crawl/c4a6a749-3445-454e-bf5a-f3e1e6befad7'}\n\n```  \nIt accepts the same parameters and scrape options as `crawl_url` but returns a crawl status dictionary.  \nWe are mostly interested in the crawl job `id` and can use it to check the status of the process using `check_crawl_status`:  \n```python\ncheckpoint = app.check_crawl_status(crawl_status['id'])\n\nprint(len(checkpoint['data']))\n\n```  \n```python\n29\n\n```  \n`check_crawl_status` returns the same output as `crawl_url` but only includes the pages scraped so far. You can run it multiple times and see the number of scraped pages increasing.  \nIf you want to cancel the job, you can use `cancel_crawl` passing the job id:  \n```python\nfinal_result = app.cancel_crawl(crawl_status['id'])\n\nprint(final_result)\n\n```  \n```python\n{'status': 'cancelled'}\n\n```",
      "metadata": {
        "title": "Mastering Firecrawl's Crawl Endpoint: A Complete Web Scraping Guide",
        "url": "https://www.firecrawl.dev/blog/mastering-the-crawl-endpoint-in-firecrawl"
      }
    },
    {
      "id": "50ca53cb-006f-402e-983f-57d66e8653ef",
      "source": "firecrawl/blog/mastering-the-crawl-endpoint-in-firecrawl.md",
      "content": "Asynchronous Web Crawling with Firecrawl: Efficient Large-Scale Data Collection > Benefits of asynchronous crawling\n\nThere are many advantages of using the `async_crawl_url` over `crawl_url`:  \n- You can create multiple crawl jobs without waiting for each to complete.\n- You can monitor progress and manage resources more effectively.\n- Perfect for batch processing or parallel crawling tasks.\n- Applications can remain responsive while crawling happens in background\n- Users can monitor progress instead of waiting for completion\n- Allows for implementing progress bars or status updates\n- Easier to integrate with message queues or job schedulers\n- Can be part of larger automated workflows\n- Better suited for microservices architectures  \nIn practice, you almost always use asynchronous crawling for large websites.",
      "metadata": {
        "title": "Mastering Firecrawl's Crawl Endpoint: A Complete Web Scraping Guide",
        "url": "https://www.firecrawl.dev/blog/mastering-the-crawl-endpoint-in-firecrawl"
      }
    },
    {
      "id": "22cc341b-c743-4e31-bbee-d105658d1402",
      "source": "firecrawl/blog/mastering-the-crawl-endpoint-in-firecrawl.md",
      "content": "How to Save and Store Web Crawling Results\n\nWhen crawling large websites, itâ€™s important to save the results persistently. Firecrawl provides the crawled data in a structured format that can be easily saved to various storage systems. Letâ€™s explore some common approaches.",
      "metadata": {
        "title": "Mastering Firecrawl's Crawl Endpoint: A Complete Web Scraping Guide",
        "url": "https://www.firecrawl.dev/blog/mastering-the-crawl-endpoint-in-firecrawl"
      }
    },
    {
      "id": "edbd1a61-7d37-4f11-9571-45d4ee38567b",
      "source": "firecrawl/blog/mastering-the-crawl-endpoint-in-firecrawl.md",
      "content": "How to Save and Store Web Crawling Results > Local file storage\n\nThe simplest approach is saving to local files. Hereâ€™s how to save crawled content in different formats:  \n```python\nimport json\nfrom pathlib import Path\n\ndef save_crawl_results(crawl_result, output_dir=\"firecrawl_output\"):\n# Create output directory if it doesn't exist\nPath(output_dir).mkdir(parents=True, exist_ok=True)\n\n# Save full results as JSON\nwith open(f\"{output_dir}/full_results.json\", \"w\") as f:\njson.dump(crawl_result, f, indent=2)\n\n# Save just the markdown content in separate files\nfor idx, page in enumerate(crawl_result[\"data\"]):\n# Create safe filename from URL\nfilename = (\npage[\"metadata\"][\"url\"].split(\"/\")[-1].replace(\".html\", \"\") or f\"page_{idx}\"\n)\n\n# Save markdown content\nif \"markdown\" in page:\nwith open(f\"{output_dir}/{filename}.md\", \"w\") as f:\nf.write(page[\"markdown\"])\n\n```  \nHere is what the above function does:  \n1. Creates an output directory if it doesnâ€™t exist\n2. Saves the complete crawl results as a JSON file with proper indentation\n3. For each crawled page:\n- Generates a filename based on the page URL\n- Saves the markdown content to a separate .md file  \n```python\napp = FirecrawlApp()\n\ncrawl_result = app.crawl_url(url=\"https://docs.stripe.com/api\", params={\"limit\": 10})\n\nsave_crawl_results(crawl_result)\n\n```  \nIt is a basic function that requires modifications for other scraping formats.",
      "metadata": {
        "title": "Mastering Firecrawl's Crawl Endpoint: A Complete Web Scraping Guide",
        "url": "https://www.firecrawl.dev/blog/mastering-the-crawl-endpoint-in-firecrawl"
      }
    },
    {
      "id": "005e4293-655b-4375-b294-e7ffe3bc7b78",
      "source": "firecrawl/blog/mastering-the-crawl-endpoint-in-firecrawl.md",
      "content": "How to Save and Store Web Crawling Results > Database storage\n\nFor more complex applications, you might want to store the results in a database. Hereâ€™s an example using SQLite:  \n```python\nimport sqlite3\n\ndef save_to_database(crawl_result, db_path=\"crawl_results.db\"):\nconn = sqlite3.connect(db_path)\ncursor = conn.cursor()\n\n# Create table if it doesn't exist\ncursor.execute(\n\"\"\"\nCREATE TABLE IF NOT EXISTS pages (\nurl TEXT PRIMARY KEY,\ntitle TEXT,\ncontent TEXT,\nmetadata TEXT,\ncrawl_date TIMESTAMP DEFAULT CURRENT_TIMESTAMP\n)\n\"\"\"\n)\n\n# Insert pages\nfor page in crawl_result[\"data\"]:\ncursor.execute(\n\"INSERT OR REPLACE INTO pages (url, title, content, metadata) VALUES (?, ?, ?, ?)\",\n(\npage[\"metadata\"][\"url\"],\npage[\"metadata\"][\"title\"],\npage.get(\"markdown\", \"\"),\njson.dumps(page[\"metadata\"]),\n),\n)\n\nconn.commit()\n\nprint(f\"Saved {len(crawl_result['data'])} pages to {db_path}\")\nconn.close()\n\n```  \nThe function creates a SQLite database with a `pages` table that stores the crawled data. For each page, it saves the URL (as primary key), title, content (in markdown format), and metadata (as JSON). The crawl date is automatically added as a timestamp. If a page with the same URL already exists, it will be replaced with the new data. This provides a persistent storage solution that can be easily queried later.  \n```python\nsave_to_database(crawl_result)\n\n```  \n```python\nSaved 9 pages to crawl_results.db\n\n```  \nLetâ€™s query the database to double-check:  \n```python\n# Query the database\nconn = sqlite3.connect(\"crawl_results.db\")\ncursor = conn.cursor()\ncursor.execute(\"SELECT url, title, metadata FROM pages\")\nprint(cursor.fetchone())\nconn.close()\n\n```  \n```python\n(\n'https://docs.stripe.com/api/errors',\n'Errors | Stripe API Reference',\n{\n\"url\": \"https://docs.stripe.com/api/errors\",\n\"title\": \"Errors | Stripe API Reference\",\n\"language\": \"en-US\",\n\"viewport\": \"width=device-width, initial-scale=1\",\n\"sourceURL\": \"https://docs.stripe.com/api/errors\",\n\"statusCode\": 200,\n\"description\": \"Complete reference documentation for the Stripe API. Includes code snippets and examples for our Python, Java, PHP, Node.js, Go, Ruby, and .NET libraries.\",\n\"ogLocaleAlternate\": []\n}\n)\n\n```",
      "metadata": {
        "title": "Mastering Firecrawl's Crawl Endpoint: A Complete Web Scraping Guide",
        "url": "https://www.firecrawl.dev/blog/mastering-the-crawl-endpoint-in-firecrawl"
      }
    },
    {
      "id": "3bbdf49b-cff3-4ee4-840e-fa1ddb916ffd",
      "source": "firecrawl/blog/mastering-the-crawl-endpoint-in-firecrawl.md",
      "content": "How to Save and Store Web Crawling Results > Cloud storage\n\nFor production applications, you might want to store results in cloud storage. Hereâ€™s an example using AWS S3:  \n```python\nimport boto3\nfrom datetime import datetime\n\ndef save_to_s3(crawl_result, bucket_name, prefix=\"crawls\"):\ns3 = boto3.client(\"s3\")\ntimestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n\n# Save full results\nfull_results_key = f\"{prefix}/{timestamp}/full_results.json\"\ns3.put_object(\nBucket=bucket_name,\nKey=full_results_key,\nBody=json.dumps(crawl_result, indent=2),\n)\n\n# Save individual pages\nfor idx, page in enumerate(crawl_result[\"data\"]):\nif \"markdown\" in page:\npage_key = f\"{prefix}/{timestamp}/pages/{idx}.md\"\ns3.put_object(Bucket=bucket_name, Key=page_key, Body=page[\"markdown\"])\nprint(f\"Successfully saved {len(crawl_result['data'])} pages to {bucket_name}/{full_results_key}\")\n\n```  \nHere is what the function does:  \n- Takes a crawl result dictionary, S3 bucket name, and optional prefix as input\n- Creates a timestamped folder structure in S3 to organize the data\n- Saves the full crawl results as a single JSON file\n- For each crawled page that has markdown content, saves it as an individual `.md` file\n- Uses boto3 to handle the AWS S3 interactions\n- Preserves the hierarchical structure of the crawl data  \nFor this function to work, you must have `boto3` installed and your AWS credentials saved inside the `~/.aws/credentials` file with the following format:  \n```bash\n[default]\naws_access_key_id = your_access_key\naws_secret_access_key = your_secret_key\nregion = your_region\n\n```  \nThen, you can execute the function provided that you already have an S3 bucket to store the data:  \n```python\nsave_to_s3(crawl_result, \"sample-bucket-1801\", \"stripe-api-docs\")\n\n```  \n```text\nSuccessfully saved 9 pages to sample-bucket-1801/stripe-api-docs/20241118_142945/full_results.json\n\n```",
      "metadata": {
        "title": "Mastering Firecrawl's Crawl Endpoint: A Complete Web Scraping Guide",
        "url": "https://www.firecrawl.dev/blog/mastering-the-crawl-endpoint-in-firecrawl"
      }
    },
    {
      "id": "77462034-13b8-4218-bb97-31b20fff2df1",
      "source": "firecrawl/blog/mastering-the-crawl-endpoint-in-firecrawl.md",
      "content": "How to Save and Store Web Crawling Results > Incremental saving with async crawls\n\nWhen using async crawling, you might want to save results incrementally sa they come in:  \n```python\nimport time\n\ndef save_incremental_results(app, crawl_id, output_dir=\"firecrawl_output\"):\nPath(output_dir).mkdir(parents=True, exist_ok=True)\nprocessed_urls = set()\n\nwhile True:\n# Check current status\nstatus = app.check_crawl_status(crawl_id)\n\n# Save new pages\nfor page in status[\"data\"]:\nurl = page[\"metadata\"][\"url\"]\nif url not in processed_urls:\nfilename = f\"{output_dir}/{len(processed_urls)}.md\"\nwith open(filename, \"w\") as f:\nf.write(page.get(\"markdown\", \"\"))\nprocessed_urls.add(url)\n\n# Break if crawl is complete\nif status[\"status\"] == \"completed\":\nprint(f\"Saved {len(processed_urls)} pages.\")\nbreak\n\ntime.sleep(5) # Wait before checking again\n\n```  \nHere is what the function does:  \n- Creates an output directory if it doesnâ€™t exist\n- Maintains a set of processed URLs to avoid duplicates\n- Continuously checks the crawl status until completion\n- For each new page found, saves its markdown content to a numbered file\n- Sleeps for 5 seconds between status checks to avoid excessive API calls  \nLetâ€™s use it while the app crawls Books-to-Scrape website:  \n```python\n# Start the crawl\ncrawl_status = app.async_crawl_url(url=\"https://books.toscrape.com/\")\n\n# Save results incrementally\nsave_incremental_results(app, crawl_status[\"id\"])\n\n```  \n```python\nSaved 705 pages.\n\n```",
      "metadata": {
        "title": "Mastering Firecrawl's Crawl Endpoint: A Complete Web Scraping Guide",
        "url": "https://www.firecrawl.dev/blog/mastering-the-crawl-endpoint-in-firecrawl"
      }
    },
    {
      "id": "f724dee5-b92b-4139-95c9-d4f24d1575bc",
      "source": "firecrawl/blog/mastering-the-crawl-endpoint-in-firecrawl.md",
      "content": "Building AI-Powered Web Crawlers with Firecrawl and LangChain Integration\n\nFirecrawl has integrations with popular open-source libraries like LangChain and other platforms.  \n![Popular AI and data tools that integrate with Firecrawl including LangChain, LlamaIndex, CrewAI, and vector databases](https://www.firecrawl.dev/images/blog/crawl-masterclass/notebook_files/image.png)  \nIn this section, we will see how to use the LangChain integration to build a basic QA chatbot on the [LangChain Community Integrations](https://python.langchain.com/docs/integrations/providers/) website.  \nStart by installing LangChain and its related libraries:  \n```bash\npip install langchain langchain_community langchain_anthropic langchain_openai\n\n```  \nThen, add your `ANTHROPIC_API_KEY` and `OPENAI_API_KEY` as variables to your `.env` file.  \nNext, import the `FireCrawlLoader` class from the document loaders module and initialize it:  \n```python\nfrom dotenv import load_dotenv\nfrom langchain_community.document_loaders.firecrawl import FireCrawlLoader\n\nload_dotenv()\n\nloader = FireCrawlLoader(\nurl=\"https://python.langchain.com/docs/integrations/providers/\",\nmode=\"crawl\",\nparams={\"limit\": 5, \"scrapeOptions\": {\"onlyMainContent\": True}},\n)\n\n```  \nThe class can read your Firecrawl API key automatically since we are loading the variables using `load_dotenv()`.  \nTo start the crawl, you can call the `load()` method of the loader object and the scraped contents will be turned into LangChain compatible documents:  \n```python\n# Start the crawl\ndocs = loader.load()\n\n```  \nThe next step is chunking:  \n```python\nfrom langchain_text_splitters import RecursiveCharacterTextSplitter\n\n# Add text splitting before creating the vector store\ntext_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=100)\n\n# Split the documents\nsplit_docs = text_splitter.split_documents(docs)\n\n```  \nAbove, we split the documents into smaller chunks using the `RecursiveCharacterTextSplitter`. This helps make the text more manageable for processing and ensures better results when creating embeddings and performing retrieval. The chunk size of 1,000 characters with 100-character overlap provides a good balance between context preservation and granularity.  \n```python\nfrom langchain_chroma import Chroma\nfrom langchain.embeddings import OpenAIEmbeddings\nfrom langchain_community.vectorstores.utils import filter_complex_metadata\n\n# Create embeddings for the documents\nembeddings = OpenAIEmbeddings()\n\n# Create a vector store from the loaded documents\ndocs = filter_complex_metadata(docs)\nvector_store = Chroma.from_documents(docs, embeddings)\n\n```  \nMoving on, we create a vector store using Chroma and OpenAI embeddings. The vector store enables semantic search and retrieval on our documents. We also filter out complex metadata that could cause storage issues.  \nThe final step is building the QA chain using Claude 3.5 Sonnet as the language model:  \n```python\nfrom langchain.chains import RetrievalQA\nfrom langchain_anthropic import ChatAnthropic\n\n# Initialize the language model\nllm = ChatAnthropic(model=\"claude-3-5-sonnet-20240620\", streaming=True)\n\n# Create a QA chain\nqa_chain = RetrievalQA.from_chain_type(\nllm=llm,\nchain_type=\"stuff\",\nretriever=vector_store.as_retriever(),\n)\n\n```  \nNow, we can ask questions about our documents:  \n```python\n# Example question\nquery = \"What is the main topic of the website?\"\nanswer = qa_chain.invoke(query)\n\nprint(answer)\n\n```  \n```python\n{\n'query': 'What is the main topic of the website?',\n'result': \"\"\"The main topic of the website is LangChain's integrations with Hugging Face.\nThe page provides an overview of various LangChain components that can be used with\nHugging Face models and services, including:\n\n1. Chat models\n2. LLMs (Language Models)\n3. Embedding models\n4. Document loaders\n5. Tools\n\nThe page focuses on showing how to use different Hugging Face functionalities within\nthe LangChain framework, such as embedding models, language models, datasets, and\nother tools.\"\"\"\n}\n\n```  \nThis section demonstrated a process for building a basic RAG pipeline for content scraped using Firecrawl. For this version, we only used 10 pages from the LangChain documentation. As the volume of information increases, the pipeline would need additional refinement. To scale this pipeline effectively, we would need to consider several factors including:  \n- Chunking strategy optimization\n- Embedding model selection\n- Vector store performance tuning\n- Prompt engineering for larger document collections",
      "metadata": {
        "title": "Mastering Firecrawl's Crawl Endpoint: A Complete Web Scraping Guide",
        "url": "https://www.firecrawl.dev/blog/mastering-the-crawl-endpoint-in-firecrawl"
      }
    },
    {
      "id": "726b5fb1-9870-423e-8e6e-b39a36e0a581",
      "source": "firecrawl/blog/mastering-the-crawl-endpoint-in-firecrawl.md",
      "content": "Conclusion\n\nThroughout this guide, weâ€™ve explored Firecrawlâ€™s `/crawl` endpoint and its capabilities for web scraping at scale. From basic usage to advanced configurations, we covered URL control, performance optimization, and asynchronous operations. We also examined practical implementations, including data storage solutions and integration with frameworks like LangChain.  \nThe endpointâ€™s ability to handle JavaScript content, pagination, and various output formats makes it a versatile tool for modern web scraping needs. Whether youâ€™re building documentation chatbots or gathering training data, Firecrawl provides a robust foundation. By leveraging the configuration options and best practices discussed, you can build efficient and scalable web scraping solutions tailored to your specific requirements.  \nArticle updated recently  \n[ðŸ”¥](https://www.firecrawl.dev/)",
      "metadata": {
        "title": "Mastering Firecrawl's Crawl Endpoint: A Complete Web Scraping Guide",
        "url": "https://www.firecrawl.dev/blog/mastering-the-crawl-endpoint-in-firecrawl"
      }
    },
    {
      "id": "513a9cb7-aac0-43b0-84df-281a4d983e2f",
      "source": "firecrawl/blog/mastering-the-crawl-endpoint-in-firecrawl.md",
      "content": "Ready to _Build?_\n\nStart scraping web data for your AI apps today.  \nNo credit card needed.  \nGet Started",
      "metadata": {
        "title": "Mastering Firecrawl's Crawl Endpoint: A Complete Web Scraping Guide",
        "url": "https://www.firecrawl.dev/blog/mastering-the-crawl-endpoint-in-firecrawl"
      }
    },
    {
      "id": "15e39b45-6d08-455a-ab77-66f7965665c7",
      "source": "firecrawl/blog/mastering-the-crawl-endpoint-in-firecrawl.md",
      "content": "About the Author\n\n[![Bex Tuychiev image](https://www.firecrawl.dev/bex.jpg)\\\nBex Tuychiev@bextuychiev](https://x.com/bextuychiev)  \nBex is a Top 10 AI writer on Medium and a Kaggle Master with over 15k followers. He loves writing detailed guides, tutorials, and notebooks on complex data science and machine learning topics",
      "metadata": {
        "title": "Mastering Firecrawl's Crawl Endpoint: A Complete Web Scraping Guide",
        "url": "https://www.firecrawl.dev/blog/mastering-the-crawl-endpoint-in-firecrawl"
      }
    },
    {
      "id": "c5d42bc5-bfb4-4c15-a889-289dd0c5c09e",
      "source": "firecrawl/blog/mastering-the-crawl-endpoint-in-firecrawl.md",
      "content": "About the Author > More articles by Bex Tuychiev\n\n[Building an Automated Price Tracking Tool\\\n\\\nBuild an automated e-commerce price tracker in Python. Learn web scraping, price monitoring, and automated alerts using Firecrawl, Streamlit, PostgreSQL.](https://www.firecrawl.dev/blog/automated-price-tracking-tutorial-python) [Web Scraping Automation: How to Run Scrapers on a Schedule\\\n\\\nLearn how to automate web scraping in Python using free tools like schedule, asyncio, cron jobs and GitHub Actions. This comprehensive guide covers local and cloud-based scheduling methods to run scrapers reliably in 2025.](https://www.firecrawl.dev/blog/automated-web-scraping-free-2025) [Automated Data Collection - A Comprehensive Guide\\\n\\\nLearn how to build robust automated data collection systems using modern tools and best practices. This guide covers everything from selecting the right tools to implementing scalable collection pipelines.](https://www.firecrawl.dev/blog/automated-data-collection-guide) [BeautifulSoup4 vs. Scrapy - A Comprehensive Comparison for Web Scraping in Python\\\n\\\nLearn the key differences between BeautifulSoup4 and Scrapy for web scraping in Python. Compare their features, performance, and use cases to choose the right tool for your web scraping needs.](https://www.firecrawl.dev/blog/beautifulsoup4-vs-scrapy-comparison) [How to Build an Automated Competitor Price Monitoring System with Python\\\n\\\nLearn how to build an automated competitor price monitoring system in Python that tracks prices across e-commerce sites, provides real-time comparisons, and maintains price history using Firecrawl, Streamlit, and GitHub Actions.](https://www.firecrawl.dev/blog/automated-competitor-price-scraping) [Scraping Company Data and Funding Information in Bulk With Firecrawl and Claude\\\n\\\nLearn how to build a web scraper in Python that gathers company details, funding rounds, and investor information from public sources like Crunchbase using Firecrawl and Claude for automated data collection and analysis.](https://www.firecrawl.dev/blog/crunchbase-scraping-with-firecrawl-claude) [Data Enrichment: A Complete Guide to Enhancing Your Data Quality\\\n\\\nLearn how to enrich your data quality with a comprehensive guide covering data enrichment tools, best practices, and real-world examples. Discover how to leverage modern solutions like Firecrawl to automate data collection, validation, and integration for better business insights.](https://www.firecrawl.dev/blog/complete-guide-to-data-enrichment) [Building an Intelligent Code Documentation RAG Assistant with DeepSeek and Firecrawl\\\n\\\nLearn how to build an intelligent documentation assistant powered by DeepSeek and RAG (Retrieval Augmented Generation) that can answer questions about any documentation website by combining language models with efficient information retrieval.](https://www.firecrawl.dev/blog/deepseek-rag-documentation-assistant)",
      "metadata": {
        "title": "Mastering Firecrawl's Crawl Endpoint: A Complete Web Scraping Guide",
        "url": "https://www.firecrawl.dev/blog/mastering-the-crawl-endpoint-in-firecrawl"
      }
    },
    {
      "id": "803f6acb-3910-40e3-8f32-606f50201896",
      "source": "firecrawl/blog/crunchbase-scraping-with-firecrawl-claude.md",
      "content": "---\ntitle: Scraping Company Data and Funding Information in Bulk With Firecrawl and Claude\nurl: https://www.firecrawl.dev/blog/crunchbase-scraping-with-firecrawl-claude\n---  \nIntroducing /extract - Get web data with a prompt [Try now](https://www.firecrawl.dev/extract)  \nJan 31, 2025  \nâ€¢  \n[![Bex Tuychiev image](https://www.firecrawl.dev/bex.jpg)Bex Tuychiev](https://x.com/bextuychiev)  \n# Scraping Company Data and Funding Information in Bulk With Firecrawl and Claude  \n![Scraping Company Data and Funding Information in Bulk With Firecrawl and Claude image](https://www.firecrawl.dev/images/blog/company-data-scraping/company-data-scraping.jpg)",
      "metadata": {
        "title": "Scraping Company Data and Funding Information in Bulk With Firecrawl and Claude",
        "url": "https://www.firecrawl.dev/blog/crunchbase-scraping-with-firecrawl-claude"
      }
    },
    {
      "id": "d10f68e4-3106-4af8-b9b1-3c8d6965cd8d",
      "source": "firecrawl/blog/crunchbase-scraping-with-firecrawl-claude.md",
      "content": "Introduction\n\nIn todayâ€™s data-driven business world, having access to accurate information about companies and their funding history is incredibly valuable. There are several online databases that track startups, investments, and company growth, containing details about millions of businesses, their funding rounds, and investors. While many of these platforms offer APIs, they can be expensive and out of reach for many users. This tutorial will show you how to build a web scraper that can gather company and funding data from public sources using Python, Firecrawl, and Claude.  \nThis guide is designed for developers who want to collect company data efficiently and ethically. By the end of this tutorial, youâ€™ll have a working tool that can extract company details, funding rounds, and investor information from company profiles across the web.  \nHere is the preview of the app:  \n![Screenshot of the Crunchbase scraping app interface showing company input fields, a scraping button, and results display area with download option](https://www.firecrawl.dev/images/blog/company-data-scraping/app_demo.png)  \nThe application provides two input methods for users - they can either upload a file containing company names or enter them directly as text. Behind the scenes, Firecrawl automatically scrapes relevant company information from public databases like Crunchbase. This scraped data is then processed by Claude, an AI assistant that generates concise company summaries. The results are displayed in a clean Streamlit interface, complete with a download option that exports all findings to a CSV file for further analysis.",
      "metadata": {
        "title": "Scraping Company Data and Funding Information in Bulk With Firecrawl and Claude",
        "url": "https://www.firecrawl.dev/blog/crunchbase-scraping-with-firecrawl-claude"
      }
    },
    {
      "id": "b76dae22-8308-4a7e-b0e0-9750996de5b7",
      "source": "firecrawl/blog/crunchbase-scraping-with-firecrawl-claude.md",
      "content": "Table of Contents\n\n1. Introduction\n2. Setting up the Development Environment\n3. Prerequisite: Scraping with Firecrawl\n4. Building the Funding Data Scraper\n- Step 1: Adding brief app information\n- Step 2: Adding components for company name input\n- Step 3: Building a scraping class\n- Step 4: Adding a scraping button\n- Step 5: Creating a download button\n- Step 6: Generating summaries\n- Step 7: Deployment\n5. Conclusion  \n**Time to Complete:** ~60 minutes\n**Prerequisites:**  \n- Python 3.10+\n- Basic Python knowledge\n- API keys for Firecrawl and Claude  \n**Important Note:** This tutorial demonstrates web scraping for educational purposes. Always review and comply with websitesâ€™ terms of service and implement appropriate rate limiting in production environments.",
      "metadata": {
        "title": "Scraping Company Data and Funding Information in Bulk With Firecrawl and Claude",
        "url": "https://www.firecrawl.dev/blog/crunchbase-scraping-with-firecrawl-claude"
      }
    },
    {
      "id": "873b2e78-629b-42c0-8b9b-fbd4d964073a",
      "source": "firecrawl/blog/crunchbase-scraping-with-firecrawl-claude.md",
      "content": "Setting up the Development Environment\n\nLetâ€™s start by setting up our development environment and installing the necessary dependencies.  \n1. **Create a working directory**  \nFirst, create a working directory:  \n```bash\nmkdir company-data-scraper\ncd company-data-scraper\n\n```  \n2. **Install dependencies**  \nWeâ€™ll use Poetry for dependency management. If you havenâ€™t installed Poetry yet:  \n```bash\ncurl -sSL https://install.python-poetry.org | python3 -\n\n```  \nThen, initialize it inside the current working directory:  \n```bash\npoetry init\n\n```  \nType â€œ^3.10â€ when asked for the Python version but, donâ€™t specify the dependencies interactively.  \nNext, install the project dependencies with the `add` command:  \n```bash\npoetry add streamlit firecrawl-py pandas pydantic openpyxl python-dotenv anthropic\n\n```  \n3. **Build the project structure**  \n```bash\nmkdir data src\ntouch .gitignore README.md .env src/{app.py,models.py,scraper.py}\n\n```  \nThe created files serve the following purposes:  \n- `data/` - Directory to store input files and scraped results\n- `src/` - Source code directory containing the main application files\n- `.gitignore` - Specifies which files Git should ignore\n- `README.md` - Project documentation and setup instructions\n- `.env` - Stores sensitive configuration like API keys\n- `src/app.py` - Main Streamlit application and UI code\n- `src/models.py` - Data models and validation logic\n- `src/scraper.py` - Web scraping and data collection functionality  \n4. **Configure environment variables**  \nThis project requires two accounts of third-party services:  \n- [Firecrawl](https://firecrawl.dev/) for AI-powered web scraping\n- [Anthropic (Claude)](https://console.anthropic.com/) for summarizing scraped data  \nClick on the hyperlinks above to create your accounts and copy/generate your API keys. Then, Inside the `.env` file in the root directory, add your API keys:  \n```plaintext\nFIRECRAWL_API_KEY=your_api_key_here\nANTHROPIC_API_KEY=your_api_key_here\n\n```  \nThe `.env` file is used to store sensitive configuration like API keys securely.The `python-dotenv` package will automatically load these environment variables when the app starts. It should never be committed to version control so add the following line to your `.gitignore` file:  \n```plaintext\n.env\n\n```  \n5. **Start the app UI**  \nRun the Streamlit app (which is blank just now) to ensure everything is working:  \n```bash\npoetry run streamlit run src/app.py\n\n```  \nYou should see the Streamlit development server start up and your default browser open to the appâ€™s interface. Keep this tab open to see the changes we make to the app in the next steps.  \nNow that we have our development environment set up, letâ€™s cover how Firecrawl works, which is a prerequisite to building our app.",
      "metadata": {
        "title": "Scraping Company Data and Funding Information in Bulk With Firecrawl and Claude",
        "url": "https://www.firecrawl.dev/blog/crunchbase-scraping-with-firecrawl-claude"
      }
    },
    {
      "id": "bc50120c-85e8-46c3-ab3a-70d9e5f16214",
      "source": "firecrawl/blog/crunchbase-scraping-with-firecrawl-claude.md",
      "content": "Prerequisite: Scraping with Firecrawl\n\nThe biggest challenge with any application that scrapes websites is maintenance. Since websites regularly update their layout and underlying HTML/CSS code, traditional scrapers break easily, making the entire app useless. Firecrawl solves this exact problem by allowing you to scrape websites using natural language.  \nInstead of writing complex CSS selectors and XPath expressions that need constant maintenance, you can simply describe what data you want to extract in plain English. Firecrawlâ€™s AI will figure out how to get that data from the page, even if the websiteâ€™s structure changes. This makes our scraper much more reliable and easier to maintain over time.  \nHere is a simple Firecrawl workflow we will later use in the app to scrape company information:  \n```python\nfrom firecrawl import FirecrawlApp\nfrom pydantic import BaseModel, Field\nfrom dotenv import load_dotenv\n\nload_dotenv()\n\n```  \n```plaintext\nTrue\n\n```  \nWe import `FirecrawlApp` to interact with the Firecrawl API for web scraping. `BaseModel` and `Field` from pydantic help us define structured data models with validation. The `load_dotenv` function loads environment variables from our `.env` file to securely access API keys.  \n```python\n# Define the data structure we want to extract\nclass CompanyData(BaseModel):\nname: str = Field(description=\"Company name\")\nfunding_total: str = Field(description=\"Total funding amount\")\nemployee_count: str = Field(description=\"Number of employees\")\nindustry: str = Field(description=\"Primary industry or sector\")\nfounded_year: str = Field(\ndescription=\"Year the company was founded\"\n) # Initialize Firecrawl\n\n```  \nNext, we define a Pydantic data model specifying the fields we want to extract from a website. Firecrawl will follow this schema to the letter - detecting the relevant HTML/CSS selectors containing this information and returning them in a simple JSON object. Here, the `Field` descriptions written in plain English are important as they guide the underlying Firecrawl AI to capture the required fields.  \n```python\napp = FirecrawlApp()\n\n# Scrape company data from Crunchbase\ndata = app.extract(\nurls=[\"https://www.crunchbase.com/organization/openai\"],\nparams={\n\"schema\": CompanyData.model_json_schema(), # Use our schema for extraction\n\"prompt\": \"Extract key company information from the page\",\n},\n)\n\n```  \nWe then initialize a `FirecrawlApp` instance and call its `extract` method, passing in the URL for OpenAIâ€™s Crunchbase page. The `params` dictionary configures the scraping behavior - we provide our `CompanyData` schema to guide the structured data extraction. We also include a prompt to help direct the extraction process.  \nThe scraped data is returned in a format matching our schema, which we can then parse into a CompanyData object for easy access to the extracted fields, as shown in the following code block.  \n```python\n# Access the extracted data\ncompany = CompanyData(**data[\"data\"])\nprint(f\"Company: {company.name}\")\nprint(f\"Funding: {company.funding_total}\")\nprint(f\"Employees: {company.employee_count}\")\nprint(f\"Industry: {company.industry}\")\nprint(f\"Founded: {company.founded_year}\")\n\n```  \n```plaintext\nCompany: OpenAI\nFunding: null\nEmployees: 251-500\nIndustry: Artificial Intelligence (AI)\nFounded: 2015\n\n```  \nIn a later step, we will integrate this process into our app but will use the `batch_scrape_urls` method instead of `extract` to enable concurrent scraping.",
      "metadata": {
        "title": "Scraping Company Data and Funding Information in Bulk With Firecrawl and Claude",
        "url": "https://www.firecrawl.dev/blog/crunchbase-scraping-with-firecrawl-claude"
      }
    },
    {
      "id": "6f102f69-9ac8-45f3-87fc-b1b180699f01",
      "source": "firecrawl/blog/crunchbase-scraping-with-firecrawl-claude.md",
      "content": "Building the Funding Data Scraper Step-by-Step\n\nWe will take a top-down approach to building the app: starting with the high-level UI components and user flows, then implementing the underlying functionality piece by piece. This approach will help us validate the appâ€™s usability early and ensure weâ€™re building exactly what users need.",
      "metadata": {
        "title": "Scraping Company Data and Funding Information in Bulk With Firecrawl and Claude",
        "url": "https://www.firecrawl.dev/blog/crunchbase-scraping-with-firecrawl-claude"
      }
    },
    {
      "id": "7c13bef2-b9a6-4684-975f-d719923e8f66",
      "source": "firecrawl/blog/crunchbase-scraping-with-firecrawl-claude.md",
      "content": "Building the Funding Data Scraper Step-by-Step > Step 1: Adding brief app information\n\nWe turn our focus to the `src/app.py` file and make the following imports:  \n```python\nimport streamlit as st\nimport pandas as pd\nimport anthropic\nfrom typing import List\nfrom dotenv import load_dotenv\n# from scraper import CrunchbaseScraper\n\nload_dotenv()\n\n```  \nThe imports above serve the following purposes:  \n- `streamlit`: Provides the web interface components and app framework\n- `pandas`: Used for data manipulation and CSV file handling\n- `anthropic`: Client library for accessing Claude AI capabilities\n- `typing.List`: Type hint for lists to improve code readability\n- `dotenv`: Loads environment variables from `.env` file for configuration  \nCurrently, the `CrunchbaseScraper` class is commented out since we are yet to write it.  \nNext, we create a `main` function that holds the core UI components:  \n```python\ndef main():\nst.title(\"Crunchbase Company Data Scraper\")\nst.write(\n\"\"\"\nExtract detailed company information from Crunchbase including funding data,\nemployee counts, industries, and more. Upload a file with company names or\nenter them manually below.\n\"\"\"\n)\n\n```  \nRight now, the function gives brief info about the appâ€™s purpose. To run the app, add the following `main` block to the end of `src/app.py`:  \n```python\nif __name__ == \"__main__\":\nmain()\n\n```  \nYou should see the change in the Streamlit development server.",
      "metadata": {
        "title": "Scraping Company Data and Funding Information in Bulk With Firecrawl and Claude",
        "url": "https://www.firecrawl.dev/blog/crunchbase-scraping-with-firecrawl-claude"
      }
    },
    {
      "id": "195cc454-9ed8-407b-9c31-81a3c8b783ec",
      "source": "firecrawl/blog/crunchbase-scraping-with-firecrawl-claude.md",
      "content": "Building the Funding Data Scraper Step-by-Step > Step 2: Adding components for company name input\n\nIn this step, we add a new function to `src/app.py`:  \n```python\ndef load_companies(file) -> List[str]:\n\"\"\"Load company names from uploaded file\"\"\"\ncompanies = []\nfor line in file:\ncompany = line.decode(\"utf-8\").strip()\nif company: # Skip empty lines\ncompanies.append(company)\nreturn companies\n\n```  \nThe `load_companies` function takes a file object as input and parses it line by line, extracting company names. It decodes each line from bytes to UTF-8 text, strips whitespace, and skips any empty lines. The function returns a list of company names that can be used for scraping Crunchbase data.  \nNow, make the following changes to the `main` function:  \n```python\ndef main():\nst.title(\"Crunchbase Company Data Scraper\")\nst.write(\n\"\"\"\nExtract detailed company information from Crunchbase including funding data,\nemployee counts, industries, and more. Upload a file with company names or\nenter them manually below.\n\"\"\"\n)\n\n# File upload option\nuploaded_file = st.file_uploader(\n\"Upload a text file with company names (one per line)\", type=[\"txt\"]\n)\n\n# Manual input option\nst.write(\"### Or Enter Companies Manually\")\nmanual_input = st.text_area(\n\"Enter company names (one per line)\",\nheight=150,\nhelp=\"Enter each company name on a new line\",\n)\n\n```  \nIn this version, weâ€™ve added two main ways for users to input company names: file upload and manual text entry. The file upload component accepts `.txt` files and for manual entry, users can type or paste company names directly into a text area, with each name on a new line. This provides flexibility for users whether they have a prepared list or want to enter names ad-hoc.  \nFurthermore, add these two blocks of code after the input components:  \n```python\ndef main():\n...\n\ncompanies = []\n\nif uploaded_file:\ncompanies = load_companies(uploaded_file)\nst.write(f\"Loaded {len(companies)} companies from file\")\nelif manual_input:\ncompanies = [line.strip() for line in manual_input.split(\"n\") if line.strip()]\nst.write(f\"Found {len(companies)} companies in input\")\n\n```  \nThis code block processes the user input to create a list of company names. When a file is uploaded, it uses the `load_companies()` function to read and parse the file contents. For manual text input, it splits the input text by newlines and strips whitespace to extract company names. In both cases, it displays a message showing how many companies were found. The companies list will be used later for scraping data from funding data sources.",
      "metadata": {
        "title": "Scraping Company Data and Funding Information in Bulk With Firecrawl and Claude",
        "url": "https://www.firecrawl.dev/blog/crunchbase-scraping-with-firecrawl-claude"
      }
    },
    {
      "id": "cc2c42bc-ca3d-4343-a549-3be5d26031e5",
      "source": "firecrawl/blog/crunchbase-scraping-with-firecrawl-claude.md",
      "content": "Building the Funding Data Scraper Step-by-Step > Step 3: Building a scraping class with Firecrawl\n\nLetâ€™s take a look at the snapshot of the final UI once again:  \n![Screenshot of the Crunchbase scraping app interface showing company input fields, a scraping button, and results display area with download option](https://www.firecrawl.dev/images/blog/company-data-scraping/app_demo.png)  \nIn this step, we implement the backend process that happens when a user clicks on â€œStart scrapingâ€ button. To do so, we use Firecrawl like we outlined in the prerequisites section. First, go to `src/models.py` script to write the data model we are going to use to scrape company and funding information:  \n```python\nfrom pydantic import BaseModel\nfrom typing import List, Optional\n\nclass CompanyData(BaseModel):\nname: str\nabout: Optional[str]\nemployee_count: Optional[str]\nfinancing_type: Optional[str]\nindustries: List[str] = []\nheadquarters: List[str] = []\nfounders: List[str] = []\nfounded_date: Optional[str]\noperating_status: Optional[str]\nlegal_name: Optional[str]\nstock_symbol: Optional[str]\nacquisitions: List[str] = []\ninvestments: List[str] = []\nexits: List[str] = []\ntotal_funding: Optional[str]\ncontacts: List[str] = []\n\n```  \nThis data model is more detailed and tries to extract as much information as possible from given sources. Now, switch to `src/scraper.py` where we implement a class called `CrunchbaseScraper`:  \n```python\nfrom firecrawl import FirecrawlApp\nfrom models import CompanyData\nfrom typing import List, Dict\n\nclass CrunchbaseScraper:\ndef __init__(self):\nself.app = FirecrawlApp()\n\ndef scrape_companies(self, urls: List[str]) -> List[Dict]:\n\"\"\"Scrape multiple Crunchbase company profiles\"\"\"\nschema = CompanyData.model_json_schema()\n\ntry:\ndata = self.app.batch_scrape_urls(\nurls,\nparams={\n\"formats\": [\"extract\"],\n\"extract\": {\n\"prompt\": \"\"\"Extract information from given pages based on the schema provided.\"\"\",\n\"schema\": schema,\n},\n},\n)\n\nreturn [res[\"extract\"] for res in data[\"data\"]]\n\nexcept Exception as e:\nprint(f\"Error while scraping companies: {str(e)}\")\nreturn []\n\n```  \nLetâ€™s break down how the class works.  \nWhen the class is initialized, it creates an instance of `FirecrawlApp`. The main method `scrape_companies` takes a list of URLs and returns a list of dictionaries containing the scraped data. It works by:  \n1. Getting the JSON schema from our `CompanyData` model to define the structure\n2. Using `batch_scrape_urls` to process multiple URLs at once\n3. Configuring the scraper to use the â€œextractâ€ format with our schema\n4. Providing a prompt that instructs the scraper how to extract the data\n5. Handling any errors that occur during scraping  \nError handling ensures the script continues running even if individual URLs fail, returning an empty list in case of errors rather than crashing.  \nNow, the only thing left to do to finalize the scraping feature is to add the â€œStart Scrapingâ€ button to the UI.",
      "metadata": {
        "title": "Scraping Company Data and Funding Information in Bulk With Firecrawl and Claude",
        "url": "https://www.firecrawl.dev/blog/crunchbase-scraping-with-firecrawl-claude"
      }
    },
    {
      "id": "413dd53c-9cd3-48bc-bcc1-0f06fab59963",
      "source": "firecrawl/blog/crunchbase-scraping-with-firecrawl-claude.md",
      "content": "Building the Funding Data Scraper Step-by-Step > Step 4: Adding a button to start scraping\n\nIn this step, return to `src/app.py` and add the following code block to the very end of the `main()` function:  \n```python\ndef main():\n...\n\nif companies and st.button(\"Start Scraping\"):\nscraper = CrunchbaseScraper()\n\nwith st.spinner(\"Scraping company data from Crunchbase...\"):\ntry:\n# Convert company names to Crunchbase URLs\nurls = [\\\nf\"https://www.crunchbase.com/organization/{name.lower().replace(' ', '-')}\"\\\nfor name in companies\\\n]\n\nresults = scraper.scrape_companies(urls)\n\nexcept Exception as e:\nst.error(f\"An error occurred: {str(e)}\")\n\n```  \nThis code block builds on the previous functionality by adding the core scraping logic. When the â€œStart Scrapingâ€ button is clicked (and companies have been provided), it:  \n1. Creates a new instance of our `CrunchbaseScraper` class\n2. Shows a loading spinner to indicate scraping is in progress\n3. Converts the company names into proper Crunchbase URLs by:\n- Converting to lowercase\n- Replacing spaces with hyphens\n- Adding the base Crunchbase URL prefix\n4. Calls the `scrape_companies` method we created earlier to fetch the data  \nThe try-except block ensures any scraping errors are handled gracefully rather than crashing the application. This is important since web scraping can be unpredictable due to network issues, rate limiting, and so on.  \nTo finish this step, uncomment the single import at the top of `src/app.py` so that they look like this:  \n```python\nimport streamlit as st\nimport pandas as pd\nimport anthropic\nfrom typing import List\nfrom scraper import CrunchbaseScraper\nfrom dotenv import load_dotenv\n\nload_dotenv()\n\n```",
      "metadata": {
        "title": "Scraping Company Data and Funding Information in Bulk With Firecrawl and Claude",
        "url": "https://www.firecrawl.dev/blog/crunchbase-scraping-with-firecrawl-claude"
      }
    },
    {
      "id": "5a784293-8f05-449e-afb6-5d44d198a21b",
      "source": "firecrawl/blog/crunchbase-scraping-with-firecrawl-claude.md",
      "content": "Building the Funding Data Scraper Step-by-Step > Step 5: Creating a download button for the scraped results\n\nNow, we must create a button to download the scraped results as a CSV file. To do so, add the following code block after the scraping part:  \n```python\ndef main():\n...\n\nif companies and st.button(\"Start Scraping\"):\nscraper = CrunchbaseScraper()\n\nwith st.spinner(\"Scraping company data from Crunchbase...\"):\ntry:\n...\nresults = scraper.scrape_companies(urls)\n\n# THIS PART IS NEW\ndf = pd.DataFrame(results)\ncsv = df.to_csv(index=False)\n\n# Create download button\nst.download_button(\n\"Download Results (CSV)\",\ncsv,\n\"crunchbase_data.csv\",\n\"text/csv\",\nkey=\"download-csv\",\n)\n\nexcept Exception as e:\nst.error(f\"An error occurred: {str(e)}\")\n\n```  \nIn the new lines of code, we convert the results to a Pandas dataframe and use its `to_csv()` function to save the dataframe as a CSV file. The method returns a filename, which we pass to the `st.download_button` method along with other details.",
      "metadata": {
        "title": "Scraping Company Data and Funding Information in Bulk With Firecrawl and Claude",
        "url": "https://www.firecrawl.dev/blog/crunchbase-scraping-with-firecrawl-claude"
      }
    },
    {
      "id": "f4893b4e-924a-492d-b832-a79e608bc7d8",
      "source": "firecrawl/blog/crunchbase-scraping-with-firecrawl-claude.md",
      "content": "Building the Funding Data Scraper Step-by-Step > Step 6: Generating a summary of scraped results\n\nAfter scraping the raw company data, we can use Claude to generate concise summaries that highlight key insights. Letâ€™s add this functionality to our app. First, create a new function in `src/app.py` to handle the summarization:  \n```python\ndef generate_company_summary(company_data: dict) -> str:\n\"\"\"Generate a summary of the company data\"\"\"\nclient = anthropic.Anthropic()\n\nmessage = client.messages.create(\nmodel=\"claude-3-5-sonnet-20241022\",\nmax_tokens=1000,\nsystem=\"You are a company & funding data expert. Summarize the given company data by the user in a few sentences.\",\nmessages=[\\\n{\"role\": \"user\", \"content\": [{\"type\": \"text\", \"text\": str(company_data)}]}\\\n],\n)\n\nreturn message.content[0].text\n\n```  \nNow, update the scraping section in the `main()` function to include the summary generation after the download button:  \n```python\ndef main():\n...\n\nif companies and st.button(\"Start Scraping\"):\nscraper = CrunchbaseScraper()\n\nwith st.spinner(\"Scraping company data from Crunchbase...\"):\ntry:\n...\n\n# Give summary of each company\nfor company in results:\nsummary = generate_company_summary(company)\nst.write(f\"### Summary of {company['name']}\")\nst.write(summary)\n\nexcept Exception as e:\nst.error(f\"An error occurred: {str(e)}\")\n\n```  \nThis implementation:  \n1. Creates a new `generate_company_summary()` function that:  \n- Formats the scraped company data into readable text\n- Uses Claude to analyze the data and generate insights\n- Returns a structured summary highlighting key patterns.  \n2. Updates the main scraping workflow to:  \n- Generate the summary after scraping is complete\n- Display the insights for each company after the download button  \nThe summary can provide context about the scraped data, helping users get the gist of the scraped data.",
      "metadata": {
        "title": "Scraping Company Data and Funding Information in Bulk With Firecrawl and Claude",
        "url": "https://www.firecrawl.dev/blog/crunchbase-scraping-with-firecrawl-claude"
      }
    },
    {
      "id": "3ad37d1e-72d9-4d1c-a234-e89f7043f6a1",
      "source": "firecrawl/blog/crunchbase-scraping-with-firecrawl-claude.md",
      "content": "Building the Funding Data Scraper Step-by-Step > Step 7: Deploying the app to Streamlit Cloud\n\nNow that our app is working locally, letâ€™s deploy it to Streamlit Cloud so others can use it. First, we need to prepare our project for deployment.  \n1. **Create a requirements.txt**  \nSince Streamlit Cloud doesnâ€™t support Poetry directly, we need to convert our dependencies to a `requirements.txt` file. Run this command in your terminal:  \n```bash\npoetry export -f requirements.txt --output requirements.txt --without-hashes\n\n```  \n2. **Create a GitHub repository**  \nInitialize a Git repository and push your code to GitHub:  \n```python\ngit init\ngit add .\ngit commit -m \"Initial commit\"\ngit branch -M main\ngit remote add origin https://github.com/yourusername/company-data-scraper.git\ngit push -u origin main\n\n```  \n3. **Add secrets to Streamlit Cloud**  \nVisit [share.streamlit.io](https://share.streamlit.io/) and connect your GitHub account. Then:  \n1. Click â€œNew appâ€\n2. Select your repository and branch\n3. Set the main file path as src/app.py\n4. Click â€œAdvanced settingsâ€ and add your environment variables:  \n- `FIRECRAWL_API_KEY`\n- `ANTHROPIC_API_KEY`  \n4. **Update imports for deployment**  \nSometimes local imports need adjustment for Streamlit Cloud. Ensure your imports in src/app.py use relative paths:  \n```python\nfrom .models import CompanyData\nfrom .scraper import CrunchbaseScraper\n\n```  \n5. **Add a .streamlit/config.toml file**  \nCreate a `.streamlit` directory and add a `config.toml` file for custom theme settings:  \n```python\n[theme]\nprimaryColor = \"#FF4B4B\"\nbackgroundColor = \"#FFFFFF\"\nsecondaryBackgroundColor = \"#F0F2F6\"\ntextColor = \"#262730\"\nfont = \"sans serif\"\n\n[server]\nmaxUploadSize = 5\n\n```  \n6. **Create a README.md file**  \nAdd a README.md file to help users understand your app:  \n```markdown\n# Crunchbase Company Data Scraper\n\nA Streamlit app that scrapes company information and funding data from Crunchbase.\n\n## Features\n\n- Bulk scraping of company profiles\n- AI-powered data summarization\n- CSV export functionality\n- Clean, user-friendly interface\n\n## Setup\n\n1. Clone the repository\n2. Install dependencies: `pip install -r requirements.txt`\n3. Set up environment variables in `.env`:\n- `FIRECRAWL_API_KEY`\n- `ANTHROPIC_API_KEY`\n4. Run the app: `streamlit run src/app.py`\n\n## Usage\n\n1. Enter company names (one per line) or upload a text file\n2. Click \"Start Scraping\"\n3. View AI-generated insights\n4. Download results as CSV\n\n## License\n\nMIT\n\n```  \n7. **Deploy the app**  \nAfter pushing all changes to GitHub, go back to Streamlit Cloud and:  \n1. Click â€œDeployâ€  \n2. Wait for the build process to complete  \n3. Your app will be live at `https://share.streamlit.io/yourusername/company-data-scraper/main`  \n4. **Monitor and maintain**  \nAfter deployment:  \n- Check the app logs in Streamlit Cloud for any issues\n- Monitor API usage and rate limits\n- Update dependencies periodically\n- Test the app regularly with different inputs  \nThe deployed app will automatically update whenever you push changes to your GitHub repository. Streamlit Cloud provides free hosting for public repositories, making it an excellent choice for sharing your scraper with others.",
      "metadata": {
        "title": "Scraping Company Data and Funding Information in Bulk With Firecrawl and Claude",
        "url": "https://www.firecrawl.dev/blog/crunchbase-scraping-with-firecrawl-claude"
      }
    },
    {
      "id": "bb45971f-3bfc-440d-a758-26ebd0c9689f",
      "source": "firecrawl/blog/crunchbase-scraping-with-firecrawl-claude.md",
      "content": "Conclusion\n\nIn this tutorial, weâ€™ve built a powerful web application that combines the capabilities of Firecrawl and Claude to extract and analyze company data at scale. By leveraging Firecrawlâ€™s AI-powered scraping and Claudeâ€™s natural language processing, weâ€™ve created a tool that not only gathers raw data but also provides meaningful insights about companies and their funding landscapes. The Streamlit interface makes the tool accessible to users of all technical levels, while features like bulk processing and CSV export enable efficient data collection workflows.",
      "metadata": {
        "title": "Scraping Company Data and Funding Information in Bulk With Firecrawl and Claude",
        "url": "https://www.firecrawl.dev/blog/crunchbase-scraping-with-firecrawl-claude"
      }
    },
    {
      "id": "ddeaa3f4-fb1b-4cac-b578-6564f6ffc14c",
      "source": "firecrawl/blog/crunchbase-scraping-with-firecrawl-claude.md",
      "content": "Conclusion > Limitations and Considerations\n\n- Rate limiting: Implement appropriate delays between requests\n- Data accuracy: Always verify scraped data against official sources\n- API costs: Monitor usage to stay within budget\n- Maintenance: Website structure changes may require updates",
      "metadata": {
        "title": "Scraping Company Data and Funding Information in Bulk With Firecrawl and Claude",
        "url": "https://www.firecrawl.dev/blog/crunchbase-scraping-with-firecrawl-claude"
      }
    },
    {
      "id": "e3ec2330-d49a-45f4-9c0c-1ec9f9dd38f2",
      "source": "firecrawl/blog/crunchbase-scraping-with-firecrawl-claude.md",
      "content": "Conclusion > Resources\n\n- [Firecrawl Documentation](https://firecrawl.dev/docs)\n- [Claude API Documentation](https://docs.anthropic.com/claude/docs)\n- [Streamlit Deployment Guide](https://docs.streamlit.io/streamlit-cloud)\n- [Firecrawlâ€™s scrape endpoint](https://www.firecrawl.dev/blog/mastering-firecrawl-scrape-endpoint)  \nArticle updated recently  \n[ðŸ”¥](https://www.firecrawl.dev/)",
      "metadata": {
        "title": "Scraping Company Data and Funding Information in Bulk With Firecrawl and Claude",
        "url": "https://www.firecrawl.dev/blog/crunchbase-scraping-with-firecrawl-claude"
      }
    },
    {
      "id": "943abe2c-2525-4a5f-a499-1dc855e8c511",
      "source": "firecrawl/blog/crunchbase-scraping-with-firecrawl-claude.md",
      "content": "Ready to _Build?_\n\nStart scraping web data for your AI apps today.  \nNo credit card needed.  \nGet Started",
      "metadata": {
        "title": "Scraping Company Data and Funding Information in Bulk With Firecrawl and Claude",
        "url": "https://www.firecrawl.dev/blog/crunchbase-scraping-with-firecrawl-claude"
      }
    },
    {
      "id": "cf41444a-4abd-4a50-ac34-74d316f3df94",
      "source": "firecrawl/blog/crunchbase-scraping-with-firecrawl-claude.md",
      "content": "About the Author\n\n[![Bex Tuychiev image](https://www.firecrawl.dev/bex.jpg)\\\nBex Tuychiev@bextuychiev](https://x.com/bextuychiev)  \nBex is a Top 10 AI writer on Medium and a Kaggle Master with over 15k followers. He loves writing detailed guides, tutorials, and notebooks on complex data science and machine learning topics",
      "metadata": {
        "title": "Scraping Company Data and Funding Information in Bulk With Firecrawl and Claude",
        "url": "https://www.firecrawl.dev/blog/crunchbase-scraping-with-firecrawl-claude"
      }
    },
    {
      "id": "934a5dba-722e-4212-a46f-5d3f5714eea3",
      "source": "firecrawl/blog/crunchbase-scraping-with-firecrawl-claude.md",
      "content": "About the Author > More articles by Bex Tuychiev\n\n[Building an Automated Price Tracking Tool\\\n\\\nBuild an automated e-commerce price tracker in Python. Learn web scraping, price monitoring, and automated alerts using Firecrawl, Streamlit, PostgreSQL.](https://www.firecrawl.dev/blog/automated-price-tracking-tutorial-python) [Web Scraping Automation: How to Run Scrapers on a Schedule\\\n\\\nLearn how to automate web scraping in Python using free tools like schedule, asyncio, cron jobs and GitHub Actions. This comprehensive guide covers local and cloud-based scheduling methods to run scrapers reliably in 2025.](https://www.firecrawl.dev/blog/automated-web-scraping-free-2025) [Automated Data Collection - A Comprehensive Guide\\\n\\\nLearn how to build robust automated data collection systems using modern tools and best practices. This guide covers everything from selecting the right tools to implementing scalable collection pipelines.](https://www.firecrawl.dev/blog/automated-data-collection-guide) [BeautifulSoup4 vs. Scrapy - A Comprehensive Comparison for Web Scraping in Python\\\n\\\nLearn the key differences between BeautifulSoup4 and Scrapy for web scraping in Python. Compare their features, performance, and use cases to choose the right tool for your web scraping needs.](https://www.firecrawl.dev/blog/beautifulsoup4-vs-scrapy-comparison) [How to Build an Automated Competitor Price Monitoring System with Python\\\n\\\nLearn how to build an automated competitor price monitoring system in Python that tracks prices across e-commerce sites, provides real-time comparisons, and maintains price history using Firecrawl, Streamlit, and GitHub Actions.](https://www.firecrawl.dev/blog/automated-competitor-price-scraping) [Scraping Company Data and Funding Information in Bulk With Firecrawl and Claude\\\n\\\nLearn how to build a web scraper in Python that gathers company details, funding rounds, and investor information from public sources like Crunchbase using Firecrawl and Claude for automated data collection and analysis.](https://www.firecrawl.dev/blog/crunchbase-scraping-with-firecrawl-claude) [Data Enrichment: A Complete Guide to Enhancing Your Data Quality\\\n\\\nLearn how to enrich your data quality with a comprehensive guide covering data enrichment tools, best practices, and real-world examples. Discover how to leverage modern solutions like Firecrawl to automate data collection, validation, and integration for better business insights.](https://www.firecrawl.dev/blog/complete-guide-to-data-enrichment) [Building an Intelligent Code Documentation RAG Assistant with DeepSeek and Firecrawl\\\n\\\nLearn how to build an intelligent documentation assistant powered by DeepSeek and RAG (Retrieval Augmented Generation) that can answer questions about any documentation website by combining language models with efficient information retrieval.](https://www.firecrawl.dev/blog/deepseek-rag-documentation-assistant)",
      "metadata": {
        "title": "Scraping Company Data and Funding Information in Bulk With Firecrawl and Claude",
        "url": "https://www.firecrawl.dev/blog/crunchbase-scraping-with-firecrawl-claude"
      }
    },
    {
      "id": "8ae58556-2541-44ec-80df-d0293b703be0",
      "source": "firecrawl/blog/launch-week-ii-day-4-advanced-iframe-scraping.md",
      "content": "---\ntitle: Launch Week II - Day 4: Advanced iframe Scraping\nurl: https://www.firecrawl.dev/blog/launch-week-ii-day-4-advanced-iframe-scraping\n---  \nIntroducing /extract - Get web data with a prompt [Try now](https://www.firecrawl.dev/extract)  \nOctober 31, 2024  \nâ€¢  \n[![Eric Ciarla image](https://www.firecrawl.dev/eric-img.jpeg)Eric Ciarla](https://x.com/ericciarla)  \n# Launch Week II - Day 4: Advanced iframe Scraping  \n![Launch Week II - Day 4: Advanced iframe Scraping image](https://www.firecrawl.dev/images/blog/firecrawl-iframe-scraping.jpg)  \nWelcome to Day 4 of Firecrawlâ€™s second Launch Week! Today, weâ€™re excited to announce a significant enhancement to our web scraping capabilities: **Advanced iframe Scraping**.  \n**Introducing Advanced iframe Scraping**  \nOur scraper can now seamlessly handle nested iframes, dynamically loaded content, and cross-origin framesâ€”solving one of web scrapingâ€™s most challenging technical hurdles. This means you can extract content from iframes just as easily as any other part of a webpage.",
      "metadata": {
        "title": "Launch Week II - Day 4: Advanced iframe Scraping",
        "url": "https://www.firecrawl.dev/blog/launch-week-ii-day-4-advanced-iframe-scraping"
      }
    },
    {
      "id": "82f48732-bfa7-4047-8ee6-35b9518904c0",
      "source": "firecrawl/blog/launch-week-ii-day-4-advanced-iframe-scraping.md",
      "content": "Technical Innovations\n\nFirecrawl now implements:  \n- **Recursive iframe Traversal and Content Extraction**: Navigate through nested iframes to extract content at any depth.\n- **Cross-Origin iframe Handling with Proper Security Context Management**: Scrape content from iframes hosted on different domains while respecting security protocols.\n- **Smart Automatic Wait for iframe Content to Load**: The scraper intelligently waits for iframe content to fully load before extraction.\n- **Support for Dynamically Injected iframes**: Capture iframes that are added to the DOM after the initial page load.\n- **Proper Handling of Sandboxed iframes**: Accurately retrieve data from iframes with sandbox attributes.",
      "metadata": {
        "title": "Launch Week II - Day 4: Advanced iframe Scraping",
        "url": "https://www.firecrawl.dev/blog/launch-week-ii-day-4-advanced-iframe-scraping"
      }
    },
    {
      "id": "737a477b-bb28-4244-b074-02cee40c48d1",
      "source": "firecrawl/blog/launch-week-ii-day-4-advanced-iframe-scraping.md",
      "content": "Why It Matters\n\nMany modern websites use iframes for:  \n- **Embedded Content and Widgets**: Like maps, videos, and interactive tools.\n- **Payment Forms and Secure Inputs**: Handling sensitive information securely.\n- **Third-Party Integrations**: Such as customer support chats and analytics tools.\n- **Advertisement Frames**: Managed by ad networks.\n- **Social Media Embeds**: Including Twitter feeds and Facebook posts.  \nPreviously, these elements were often inaccessible during scraping, leaving gaps in your data. Now, with Advanced iframe Scraping, you get complete access to iframe content just like any other part of the page.",
      "metadata": {
        "title": "Launch Week II - Day 4: Advanced iframe Scraping",
        "url": "https://www.firecrawl.dev/blog/launch-week-ii-day-4-advanced-iframe-scraping"
      }
    },
    {
      "id": "632c789e-d2e2-4dc0-8170-f75fffed0ff9",
      "source": "firecrawl/blog/launch-week-ii-day-4-advanced-iframe-scraping.md",
      "content": "Usage\n\nNo additional configuration is needed! The iframe scraping happens automatically when you use any of our scraping or crawling endpoints. Whether youâ€™re using `/scrape` for single pages or `/crawl` for entire websites, iframe content will be seamlessly integrated into your results.  \nHappy scraping, and join us tomorrow for [Launch Week II Day 5](https://www.firecrawl.dev/launch-week)!  \nArticle updated recently  \n[ðŸ”¥](https://www.firecrawl.dev/)",
      "metadata": {
        "title": "Launch Week II - Day 4: Advanced iframe Scraping",
        "url": "https://www.firecrawl.dev/blog/launch-week-ii-day-4-advanced-iframe-scraping"
      }
    },
    {
      "id": "8e985e87-7adf-4d33-a081-09dabada1771",
      "source": "firecrawl/blog/launch-week-ii-day-4-advanced-iframe-scraping.md",
      "content": "Ready to _Build?_\n\nStart scraping web data for your AI apps today.  \nNo credit card needed.  \nGet Started",
      "metadata": {
        "title": "Launch Week II - Day 4: Advanced iframe Scraping",
        "url": "https://www.firecrawl.dev/blog/launch-week-ii-day-4-advanced-iframe-scraping"
      }
    },
    {
      "id": "0f7d4214-ed97-4855-8f2e-13dd1c82ef91",
      "source": "firecrawl/blog/launch-week-ii-day-4-advanced-iframe-scraping.md",
      "content": "About the Author\n\n[![Eric Ciarla image](https://www.firecrawl.dev/eric-img.jpeg)\\\nEric Ciarla@ericciarla](https://x.com/ericciarla)  \nEric Ciarla is the Chief Operating Officer (COO) of Firecrawl and leads marketing. He also worked on Mendable.ai and sold it to companies like Snapchat, Coinbase, and MongoDB.\nPreviously worked at Ford and Fracta as a Data Scientist. Eric also co-founded SideGuide, a tool for learning code within VS Code with 50,000 users.",
      "metadata": {
        "title": "Launch Week II - Day 4: Advanced iframe Scraping",
        "url": "https://www.firecrawl.dev/blog/launch-week-ii-day-4-advanced-iframe-scraping"
      }
    },
    {
      "id": "8665ca92-a77c-446d-89b7-095d4e949695",
      "source": "firecrawl/blog/launch-week-ii-day-4-advanced-iframe-scraping.md",
      "content": "About the Author > More articles by Eric Ciarla\n\n[How to Create an llms.txt File for Any Website\\\n\\\nLearn how to generate an llms.txt file for any website using the llms.txt Generator and Firecrawl.](https://www.firecrawl.dev/blog/How-to-Create-an-llms-txt-File-for-Any-Website) [Cloudflare Error 1015: How to solve it?\\\n\\\nCloudflare Error 1015 is a rate limiting error that occurs when Cloudflare detects that you are exceeding the request limit set by the website owner.](https://www.firecrawl.dev/blog/cloudflare-error-1015-how-to-solve-it) [Build an agent that checks for website contradictions\\\n\\\nUsing Firecrawl and Claude to scrape your website's data and look for contradictions.](https://www.firecrawl.dev/blog/contradiction-agent) [Why Companies Need a Data Strategy for Generative AI\\\n\\\nLearn why a well-defined data strategy is essential for building robust, production-ready generative AI systems, and discover practical steps for curation, maintenance, and integration.](https://www.firecrawl.dev/blog/why-companies-need-a-data-strategy-for-generative-ai) [Getting Started with OpenAI's Predicted Outputs for Faster LLM Responses\\\n\\\nA guide to leveraging Predicted Outputs to speed up LLM tasks with GPT-4o models.](https://www.firecrawl.dev/blog/getting-started-with-predicted-outputs-openai) [How to easily install requests with pip and python\\\n\\\nA tutorial on installing the requests library in Python using various methods, with usage examples and troubleshooting tips](https://www.firecrawl.dev/blog/how-to-easily-install-requests-with-pip-and-python) [How to quickly install BeautifulSoup with Python\\\n\\\nA guide on installing the BeautifulSoup library in Python using various methods, with usage examples and troubleshooting tips](https://www.firecrawl.dev/blog/how-to-quickly-install-beautifulsoup-with-python) [How to Use OpenAI's o1 Reasoning Models in Your Applications\\\n\\\nLearn how to harness OpenAI's latest o1 series models for complex reasoning tasks in your apps.](https://www.firecrawl.dev/blog/how-to-use-openai-o1-reasoning-models-in-applications)",
      "metadata": {
        "title": "Launch Week II - Day 4: Advanced iframe Scraping",
        "url": "https://www.firecrawl.dev/blog/launch-week-ii-day-4-advanced-iframe-scraping"
      }
    },
    {
      "id": "8c3ec346-78d6-4d2a-bbb6-c802c06f0156",
      "source": "firecrawl/blog/launch-week-ii-day-7-introducing-faster-markdown-parsing.md",
      "content": "---\ntitle: Launch Week II - Day 7: Introducing Faster Markdown Parsing\nurl: https://www.firecrawl.dev/blog/launch-week-ii-day-7-introducing-faster-markdown-parsing\n---  \nIntroducing /extract - Get web data with a prompt [Try now](https://www.firecrawl.dev/extract)  \nNovember 3, 2024  \nâ€¢  \n[![Eric Ciarla image](https://www.firecrawl.dev/eric-img.jpeg)Eric Ciarla](https://x.com/ericciarla)  \n# Launch Week II - Day 7: Introducing Faster Markdown Parsing  \n![Launch Week II - Day 7: Introducing Faster Markdown Parsing image](https://www.firecrawl.dev/images/blog/firecrawl-faster-markdown.jpg)  \n**Introducing Faster Markdown Parsing ðŸŽï¸ðŸ’¨**  \nWeâ€™ve rebuilt our Markdown parser from the ground up with a focus on speed and performance. This enhancement ensures that your web scraping tasks are more efficient and deliver higher-quality results.",
      "metadata": {
        "title": "Launch Week II - Day 7: Introducing Faster Markdown Parsing",
        "url": "https://www.firecrawl.dev/blog/launch-week-ii-day-7-introducing-faster-markdown-parsing"
      }
    },
    {
      "id": "29fbd6db-3d25-4af1-97b3-95218627f8f8",
      "source": "firecrawl/blog/launch-week-ii-day-7-introducing-faster-markdown-parsing.md",
      "content": "Whatâ€™s New?\n\n- **Speed Improvements**: Experience parsing speeds up to 4 times faster than before, allowing for quicker data processing and reduced waiting times.\n- **Enhanced Reliability**: Our new parser handles a wider range of HTML content more gracefully, reducing errors and improving consistency.\n- **Cleaner Markdown Output**: Get cleaner and more readable Markdown, making your data easier to work with and integrate into your workflows.  \n**Whatâ€™s Next?**  \nThat concludes our Launch Week II! We hope these new features will significantly enhance your web scraping projects and workflows.  \nThank you for joining us this week, and happy scraping!  \nArticle updated recently  \n[ðŸ”¥](https://www.firecrawl.dev/)",
      "metadata": {
        "title": "Launch Week II - Day 7: Introducing Faster Markdown Parsing",
        "url": "https://www.firecrawl.dev/blog/launch-week-ii-day-7-introducing-faster-markdown-parsing"
      }
    },
    {
      "id": "99590cea-23e8-4dbf-b152-7af6d79471c1",
      "source": "firecrawl/blog/launch-week-ii-day-7-introducing-faster-markdown-parsing.md",
      "content": "Ready to _Build?_\n\nStart scraping web data for your AI apps today.  \nNo credit card needed.  \nGet Started",
      "metadata": {
        "title": "Launch Week II - Day 7: Introducing Faster Markdown Parsing",
        "url": "https://www.firecrawl.dev/blog/launch-week-ii-day-7-introducing-faster-markdown-parsing"
      }
    },
    {
      "id": "edd2a7a6-84ba-4300-8264-fae94e7c4a34",
      "source": "firecrawl/blog/launch-week-ii-day-7-introducing-faster-markdown-parsing.md",
      "content": "About the Author\n\n[![Eric Ciarla image](https://www.firecrawl.dev/eric-img.jpeg)\\\nEric Ciarla@ericciarla](https://x.com/ericciarla)  \nEric Ciarla is the Chief Operating Officer (COO) of Firecrawl and leads marketing. He also worked on Mendable.ai and sold it to companies like Snapchat, Coinbase, and MongoDB.\nPreviously worked at Ford and Fracta as a Data Scientist. Eric also co-founded SideGuide, a tool for learning code within VS Code with 50,000 users.",
      "metadata": {
        "title": "Launch Week II - Day 7: Introducing Faster Markdown Parsing",
        "url": "https://www.firecrawl.dev/blog/launch-week-ii-day-7-introducing-faster-markdown-parsing"
      }
    },
    {
      "id": "f6f09326-718f-4c56-94d7-406142224d24",
      "source": "firecrawl/blog/launch-week-ii-day-7-introducing-faster-markdown-parsing.md",
      "content": "About the Author > More articles by Eric Ciarla\n\n[How to Create an llms.txt File for Any Website\\\n\\\nLearn how to generate an llms.txt file for any website using the llms.txt Generator and Firecrawl.](https://www.firecrawl.dev/blog/How-to-Create-an-llms-txt-File-for-Any-Website) [Cloudflare Error 1015: How to solve it?\\\n\\\nCloudflare Error 1015 is a rate limiting error that occurs when Cloudflare detects that you are exceeding the request limit set by the website owner.](https://www.firecrawl.dev/blog/cloudflare-error-1015-how-to-solve-it) [Build an agent that checks for website contradictions\\\n\\\nUsing Firecrawl and Claude to scrape your website's data and look for contradictions.](https://www.firecrawl.dev/blog/contradiction-agent) [Why Companies Need a Data Strategy for Generative AI\\\n\\\nLearn why a well-defined data strategy is essential for building robust, production-ready generative AI systems, and discover practical steps for curation, maintenance, and integration.](https://www.firecrawl.dev/blog/why-companies-need-a-data-strategy-for-generative-ai) [Getting Started with OpenAI's Predicted Outputs for Faster LLM Responses\\\n\\\nA guide to leveraging Predicted Outputs to speed up LLM tasks with GPT-4o models.](https://www.firecrawl.dev/blog/getting-started-with-predicted-outputs-openai) [How to easily install requests with pip and python\\\n\\\nA tutorial on installing the requests library in Python using various methods, with usage examples and troubleshooting tips](https://www.firecrawl.dev/blog/how-to-easily-install-requests-with-pip-and-python) [How to quickly install BeautifulSoup with Python\\\n\\\nA guide on installing the BeautifulSoup library in Python using various methods, with usage examples and troubleshooting tips](https://www.firecrawl.dev/blog/how-to-quickly-install-beautifulsoup-with-python) [How to Use OpenAI's o1 Reasoning Models in Your Applications\\\n\\\nLearn how to harness OpenAI's latest o1 series models for complex reasoning tasks in your apps.](https://www.firecrawl.dev/blog/how-to-use-openai-o1-reasoning-models-in-applications)",
      "metadata": {
        "title": "Launch Week II - Day 7: Introducing Faster Markdown Parsing",
        "url": "https://www.firecrawl.dev/blog/launch-week-ii-day-7-introducing-faster-markdown-parsing"
      }
    },
    {
      "id": "1b3ee720-ceef-43fa-9a79-8fae8cb078de",
      "source": "firecrawl/blog/deepseek-rag-documentation-assistant.md",
      "content": "---\ntitle: Building an Intelligent Code Documentation RAG Assistant with DeepSeek and Firecrawl\nurl: https://www.firecrawl.dev/blog/deepseek-rag-documentation-assistant\n---  \nIntroducing /extract - Get web data with a prompt [Try now](https://www.firecrawl.dev/extract)  \nFeb 10, 2025  \nâ€¢  \n[![Bex Tuychiev image](https://www.firecrawl.dev/bex.jpg)Bex Tuychiev](https://x.com/bextuychiev)  \n# Building an Intelligent Code Documentation RAG Assistant with DeepSeek and Firecrawl  \n![Building an Intelligent Code Documentation RAG Assistant with DeepSeek and Firecrawl image](https://www.firecrawl.dev/images/blog/deepseek_rag/deepseek-rag-documentation-assistant.jpg)  \n# Building an Intelligent Code Documentation Assistant: RAG-Powered DeepSeek Implementation",
      "metadata": {
        "title": "Building an Intelligent Code Documentation RAG Assistant with DeepSeek and Firecrawl",
        "url": "https://www.firecrawl.dev/blog/deepseek-rag-documentation-assistant"
      }
    },
    {
      "id": "38307b8a-228b-4c02-a0b6-103d30a3ef50",
      "source": "firecrawl/blog/deepseek-rag-documentation-assistant.md",
      "content": "Introduction\n\nDeepSeek R1â€™s release made waves in the AI community, with countless demos highlighting its impressive capabilities. However, most examples only scratch the surface with basic prompts rather than showing practical real-world implementations.  \nIn this tutorial, weâ€™ll explore how to harness this powerful open-source model to create a documentation assistant powered by RAG (Retrieval Augmented Generation). Our application will be able to intelligently answer questions about any documentation website by combining DeepSeekâ€™s language capabilities with efficient information retrieval.  \n![A screenshot showing the documentation assistant interface with a chat window on the right and a sidebar on the left for managing documentation sources](https://www.firecrawl.dev/images/blog/deepseek_rag/demo2.png)  \nFor those eager to try it out, you can find installation and usage instructions [in the GitHub repository](https://github.com/BexTuychiev/local-documentation-rag). If youâ€™re interested in understanding how the application works and learning to customize it for your needs, continue reading this detailed walkthrough.",
      "metadata": {
        "title": "Building an Intelligent Code Documentation RAG Assistant with DeepSeek and Firecrawl",
        "url": "https://www.firecrawl.dev/blog/deepseek-rag-documentation-assistant"
      }
    },
    {
      "id": "2d1f32bc-a097-4bff-8534-b571dc916fb2",
      "source": "firecrawl/blog/deepseek-rag-documentation-assistant.md",
      "content": "What Is DeepSeek R1?\n\n![A logo for DeepSeek AI showing a stylized deep learning neural network visualization](https://www.firecrawl.dev/images/blog/deepseek_rag/deepseek.jpeg)  \n[DeepSeek R1](https://github.com/deepseek-ai/DeepSeek-R1) represents a notable advancement in artificial intelligence, combining reinforcement learning and supervised fine-tuning in a novel and most importantly, open-source approach. The model comes in two variants: DeepSeek-R1-Zero, trained purely through reinforcement learning, and DeepSeek-R1, which undergoes additional training steps. Its architecture manages 671 billion total parameters, though it operates efficiently with 37 billion active parameters and handles context lengths up to 128,000 tokens.  \nThe development journey progressed through carefully planned stages. Beginning with supervised fine-tuning for core capabilities, the model then underwent two phases of reinforcement learning. These RL stages shaped its reasoning patterns and aligned its behavior with human thought processes. This methodical approach produced a system capable of generating responses, performing self-verification, engaging in reflection, and constructing detailed reasoning across mathematics, programming, and general problem-solving.  \nWhen it comes to performance, DeepSeek R1 demonstrates compelling results that rival OpenAIâ€™s offerings. It achieves 97.3% accuracy on MATH-500, reaches the 96.3 percentile on Codeforces programming challenges, and scores 90.8% on the MMLU general knowledge assessment. The technology has also been distilled into smaller versions ranging from 1.5B to 70B parameters, built on established frameworks like Qwen and Llama. These adaptations make the technology more accessible for practical use while preserving its core strengths.  \nIn this tutorial, we will use its 14B version but your hardware may support up to 70B parameters. It is important to choose a higher capacity model as this number is the biggest contributor to performance.",
      "metadata": {
        "title": "Building an Intelligent Code Documentation RAG Assistant with DeepSeek and Firecrawl",
        "url": "https://www.firecrawl.dev/blog/deepseek-rag-documentation-assistant"
      }
    },
    {
      "id": "89421394-5ecb-4afb-9f8e-27097753bc0d",
      "source": "firecrawl/blog/deepseek-rag-documentation-assistant.md",
      "content": "Prerequisite: Revisiting RAG concepts\n\n![A diagram showing the RAG (Retrieval Augmented Generation) architecture with components for document processing, embedding generation, vector storage, and query processing connected by arrows to illustrate the information flow](https://www.firecrawl.dev/images/blog/deepseek_rag/rag-overview.png)  \n[Source](https://www.patrickschnass.de/posts/rag_intro/)  \nRetrieval Augmented Generation (RAG) represents a significant advancement in how Large Language Models (LLMs) interact with information. Unlike traditional LLMs that rely solely on their training data, RAG combines the power of language models with the ability to retrieve and reference external information in real-time. This approach effectively creates a bridge between the modelâ€™s inherent knowledge and up-to-date, specific information stored in external databases or documents.  \nThe RAG architecture consists of two main components: the retriever and the generator. The retriever is responsible for searching through a knowledge base to find relevant information based on the userâ€™s query. This process typically involves converting both the query and stored documents into vector embeddings, allowing for semantic similarity searches that go beyond simple keyword matching. The generator, usually an LLM, then takes both the original query and the retrieved information to produce a comprehensive, contextually relevant response.  \nOne of RAGâ€™s key advantages is its ability to provide more accurate and verifiable responses. By grounding the modelâ€™s outputs in specific, retrievable sources, RAG helps reduce hallucinations â€“ instances where LLMs generate plausible-sounding but incorrect information. This is particularly valuable in professional contexts where accuracy and accountability are crucial, such as technical documentation, customer support, or legal applications. Additionally, RAG systems can be updated with new information without requiring retraining of the underlying language model, making them more flexible and maintainable.  \nThe implementation of RAG typically involves several technical components working in harmony. First, documents are processed and converted into embeddings using models like BERT or Sentence Transformers. These embeddings are then stored in vector databases such as Pinecone, Weaviate, or FAISS for efficient retrieval. When a query arrives, it goes through the same embedding process, and similarity search algorithms find the most relevant documents. Finally, these documents, along with the original query, are formatted into a prompt that the LLM uses to generate its response. This structured approach ensures that the final output is both relevant and grounded in reliable source material.  \nNow that weâ€™ve refreshed our memory on basic RAG concepts, letâ€™s dive in to the appâ€™s implementation.",
      "metadata": {
        "title": "Building an Intelligent Code Documentation RAG Assistant with DeepSeek and Firecrawl",
        "url": "https://www.firecrawl.dev/blog/deepseek-rag-documentation-assistant"
      }
    },
    {
      "id": "723148c3-7cf6-4d63-8175-2ac9c16635e7",
      "source": "firecrawl/blog/deepseek-rag-documentation-assistant.md",
      "content": "Overview of the App\n\nBefore diving into the technical details, letâ€™s walk through a typical user journey to understand how the documentation assistant works.  \nThe process starts with the user providing documentation URLs to scrape. The app is designed to work with any documentation website, but here are some examples of typical documentation pages:  \n- `https://docs.firecrawl.dev`\n- `https://docs.langchain.com`\n- `https://docs.streamlit.io`  \nThe appâ€™s interface is divided into two main sections: a sidebar for documentation management and a main chat interface. In the sidebar, users can:  \n1. Enter a documentation URL to scrape\n2. Specify a name for the documentation (must end with â€œ-docsâ€)\n3. Optionally limit the number of pages to scrape\n4. View and select from previously scraped documentation sets  \nWhen a user initiates scraping, the app uses Firecrawl to intelligently crawl the documentation website, converting HTML content into clean markdown files. These files are stored locally in a directory named after the documentation (e.g., â€œFirecrawl-docsâ€). The app shows real-time progress during scraping and notifies the user when complete.  \nAfter scraping, the documentation is processed into a vector database using the Nomic embeddings model. This enables semantic search capabilities, allowing the assistant to find relevant documentation sections based on user questions. The processing happens automatically when a user selects a documentation set from the sidebar.  \nThe main chat interface provides an intuitive way to interact with the documentation:  \n1. Users can ask questions in natural language about the selected documentation\n2. The app uses RAG (Retrieval-Augmented Generation) to find relevant documentation sections\n3. DeepSeek R1 generates accurate, contextual responses based on the retrieved content\n4. Each response includes an expandable â€œView reasoningâ€ section showing the chain of thought  \n![Screenshot showing the documentation assistant interface with sidebar controls and chat interface](https://www.firecrawl.dev/images/blog/deepseek_rag/demo2.png)  \nUsers can switch between different documentation sets at any time, and the app will automatically reprocess the vectors as needed.  \nThis approach combines the power of modern AI with traditional documentation search, creating a more interactive and intelligent way to explore technical documentation. Whether youâ€™re learning a new framework or trying to solve a specific problem, the assistant helps you find and understand relevant documentation more efficiently than traditional search methods.",
      "metadata": {
        "title": "Building an Intelligent Code Documentation RAG Assistant with DeepSeek and Firecrawl",
        "url": "https://www.firecrawl.dev/blog/deepseek-rag-documentation-assistant"
      }
    },
    {
      "id": "2600c86e-d652-428a-bd67-25546f15ddcd",
      "source": "firecrawl/blog/deepseek-rag-documentation-assistant.md",
      "content": "The Tech Stack Used in the App\n\nBuilding an effective documentation assistant requires tools that can handle complex tasks like web scraping, text processing, and natural language understanding while remaining maintainable and efficient. Letâ€™s explore the core technologies that power our application and why each was chosen:",
      "metadata": {
        "title": "Building an Intelligent Code Documentation RAG Assistant with DeepSeek and Firecrawl",
        "url": "https://www.firecrawl.dev/blog/deepseek-rag-documentation-assistant"
      }
    },
    {
      "id": "da13f981-f1e0-4af8-91ce-af4d09d3552f",
      "source": "firecrawl/blog/deepseek-rag-documentation-assistant.md",
      "content": "The Tech Stack Used in the App > 1. [Firecrawl](https://firecrawl.dev/) for AI-powered documentation scraping\n\nAt the heart of our documentation collection system is Firecrawl, an AI-powered web scraping engine. Unlike traditional scraping libraries that rely on brittle HTML selectors, Firecrawl uses natural language understanding to identify and extract content. This makes it ideal for our use case because:  \n- It can handle diverse documentation layouts without custom code\n- Maintains reliability even when documentation structure changes\n- Automatically extracts clean markdown content\n- Handles JavaScript-rendered documentation sites\n- Provides metadata like titles and URLs automatically\n- Follows documentation links intelligently",
      "metadata": {
        "title": "Building an Intelligent Code Documentation RAG Assistant with DeepSeek and Firecrawl",
        "url": "https://www.firecrawl.dev/blog/deepseek-rag-documentation-assistant"
      }
    },
    {
      "id": "32a59d2a-57fa-4cdc-9785-50e20d8920e0",
      "source": "firecrawl/blog/deepseek-rag-documentation-assistant.md",
      "content": "The Tech Stack Used in the App > 2. [DeepSeek R1](https://deepseek.ai/) for question answering\n\nFor the critical task of answering documentation questions, we use the DeepSeek R1 14B model through Ollama. This AI model excels at understanding technical documentation and providing accurate responses. We chose DeepSeek R1 because:  \n- Runs locally for better privacy and lower latency\n- Specifically trained on technical content\n- Provides detailed explanations with chain-of-thought reasoning\n- More cost-effective than cloud-based models\n- Integrates well with LangChain for RAG workflows",
      "metadata": {
        "title": "Building an Intelligent Code Documentation RAG Assistant with DeepSeek and Firecrawl",
        "url": "https://www.firecrawl.dev/blog/deepseek-rag-documentation-assistant"
      }
    },
    {
      "id": "89f91f38-37f9-4718-a15e-a5d1e5ec2685",
      "source": "firecrawl/blog/deepseek-rag-documentation-assistant.md",
      "content": "The Tech Stack Used in the App > 3. [Nomic Embeddings](https://docs.nomic.ai/) for semantic search\n\nTo enable semantic search across documentation, we use Nomicâ€™s text embedding model through [Ollama](https://ollama.com/). This component is crucial for finding relevant documentation sections. We chose Nomic because:  \n- Optimized for technical documentation\n- Runs locally alongside DeepSeek through Ollama\n- Produces high-quality embeddings for RAG\n- Fast inference speed\n- Compact model size",
      "metadata": {
        "title": "Building an Intelligent Code Documentation RAG Assistant with DeepSeek and Firecrawl",
        "url": "https://www.firecrawl.dev/blog/deepseek-rag-documentation-assistant"
      }
    },
    {
      "id": "bbf16330-66ed-4fe3-9e16-314db9e9ddda",
      "source": "firecrawl/blog/deepseek-rag-documentation-assistant.md",
      "content": "The Tech Stack Used in the App > 5. [Streamlit](https://streamlit.io/) for user interface\n\nThe web interface is built with Streamlit, a Python framework for data applications. We chose Streamlit because:  \n- It enables rapid development of chat interfaces\n- Provides built-in components for file handling\n- Handles async operations smoothly\n- Maintains chat history during sessions\n- Requires minimal frontend code\n- Makes deployment straightforward",
      "metadata": {
        "title": "Building an Intelligent Code Documentation RAG Assistant with DeepSeek and Firecrawl",
        "url": "https://www.firecrawl.dev/blog/deepseek-rag-documentation-assistant"
      }
    },
    {
      "id": "56d41862-f4e5-41e9-8a7f-a210068332d8",
      "source": "firecrawl/blog/deepseek-rag-documentation-assistant.md",
      "content": "The Tech Stack Used in the App > 6. [LangChain](https://www.langchain.com/) for RAG orchestration\n\nTo coordinate the various components into a cohesive RAG system, we use LangChain. This framework provides:  \n- Standard interfaces for embeddings and LLMs\n- Document loading and text splitting utilities\n- Vector store integration\n- Prompt management\n- Structured output parsing  \nThis carefully selected stack provides a robust foundation while keeping the system entirely local and self-contained. The combination of AI-powered tools (Firecrawl and DeepSeek) with modern infrastructure (ChromaDB, LangChain, and Ollama) creates a reliable and efficient documentation assistant that can handle diverse technical documentation.  \nMost importantly, this stack minimizes both latency and privacy concerns by running all AI components locally. The infrastructure is lightweight and portable, letting you focus on using the documentation rather than managing complex dependencies or cloud services.",
      "metadata": {
        "title": "Building an Intelligent Code Documentation RAG Assistant with DeepSeek and Firecrawl",
        "url": "https://www.firecrawl.dev/blog/deepseek-rag-documentation-assistant"
      }
    },
    {
      "id": "7fb67825-ef0f-4a70-9d6a-e2ef438e00d2",
      "source": "firecrawl/blog/deepseek-rag-documentation-assistant.md",
      "content": "Breaking Down the App Components\n\nWhen you look at [the GitHub repository](https://github.com/BexTuychiev/local-documentation-rag/tree/main/src) of the app, you will see the following file structure:  \n![GitHub repository file structure showing src directory with core Python files and configuration files](https://www.firecrawl.dev/images/blog/deepseek_rag/github-snapshot.png)  \nSeveral files in the repository serve common purposes that most developers will recognize:  \n- `.gitignore`: Specifies which files Git should ignore when tracking changes\n- `README.md`: Documentation explaining what the project does and how to use it\n- `requirements.txt`: Lists all Python package dependencies needed to run the project  \nLetâ€™s examine the remaining Python scripts and understand how they work together to power the application. The explanations will be in a logical order building from foundational elements to higher-level functionality.",
      "metadata": {
        "title": "Building an Intelligent Code Documentation RAG Assistant with DeepSeek and Firecrawl",
        "url": "https://www.firecrawl.dev/blog/deepseek-rag-documentation-assistant"
      }
    },
    {
      "id": "c7a907f7-4eaa-44c1-900b-9ba90ac88f9c",
      "source": "firecrawl/blog/deepseek-rag-documentation-assistant.md",
      "content": "Breaking Down the App Components > 1. Scraping Documentation with Firecrawl - `src/scraper.py`\n\nThe documentation scraper component handles fetching and processing documentation pages using Firecrawlâ€™s AI capabilities. Letâ€™s examine how each part works:  \nFirst, we make the necessary imports and setup:  \n```python\nimport logging\nimport os\nimport re\nfrom pathlib import Path\nfrom typing import List\n\nfrom dotenv import load_dotenv\nfrom firecrawl import FirecrawlApp\nfrom pydantic import BaseModel, Field\n\n# Get logger for the scraper module\nlogger = logging.getLogger(__name__)\n\n```  \nThen, we define the core data structure for documentation pages:  \n```python\nclass DocPage(BaseModel):\ntitle: str = Field(description=\"Page title\")\ncontent: str = Field(description=\"Main content of the page\")\nurl: str = Field(description=\"Page URL\")\n\n```  \nThe `DocPage` model represents a single documentation page with three essential fields:  \n- `title`: The pageâ€™s heading or title\n- `content`: The main markdown content of the page\n- `url`: Direct link to the original page  \nThis model is used by both the scraper to structure extracted content and the RAG system to process documentation for the vector store.  \nThe main scraper class handles all documentation collection:  \n```python\nclass DocumentationScraper:\ndef init(self):\nself.app = FirecrawlApp()\n\n```  \nThe `DocumentationScraper` initializes a connection to Firecrawl and provides three main methods for documentation collection:  \n1. `get_documentation_links`: Discovers all documentation pages from a base URL:  \n```python\ndef get_documentation_links(self, base_url: str) -> list[str]:\n\"\"\"Get all documentation page links from a given base URL.\"\"\"\nlogger.info(f\"Getting documentation links from {base_url}\")\ninitial_crawl = self.app.crawl_url(\nbase_url,\nparams={\n\"scrapeOptions\": {\"formats\": [\"links\"]},\n},\n)\nall_links = []\nfor item in initial_crawl[\"data\"]:\nall_links.extend(item[\"links\"])\nfiltered_links = set(\n[link.split(\"#\")[0] for link in all_links if link.startswith(base_url)]\n)\nlogger.info(f\"Found {len(filtered_links)} unique documentation links\")\nreturn list(filtered_links)\n\n```  \nThis method:  \n- Uses Firecrawlâ€™s link extraction mode to find all URLs\n- Filters for links within the same documentation domain\n- Removes duplicate URLs and anchor fragments\n- Returns a clean list of documentation page URLs  \n2. `scrape_documentation`: Processes all documentation pages into structured content:  \n```python\ndef scrape_documentation(self, base_url: str, limit: int = None):\n\"\"\"Scrape documentation pages from a given base URL.\"\"\"\nlogger.info(f\"Scraping doc pages from {base_url}\")\n\nfiltered_links = self.get_documentation_links(base_url)\nif limit:\nfiltered_links = filtered_links[:limit]\n\ntry:\nlogger.info(f\"Scraping {len(filtered_links)} documentation pages\")\ncrawl_results = self.app.batch_scrape_urls(filtered_links)\nexcept Exception as e:\nlogger.error(f\"Error scraping documentation pages: {str(e)}\")\nreturn []\n\ndoc_pages = []\nfor result in crawl_results[\"data\"]:\nif result.get(\"markdown\"):\ndoc_pages.append(\nDocPage(\ntitle=result.get(\"metadata\", {}).get(\"title\", \"Untitled\"),\ncontent=result[\"markdown\"],\nurl=result.get(\"metadata\", {}).get(\"url\", \"\"),\n)\n)\nelse:\nlogger.warning(\nf\"Failed to scrape {result.get('metadata', {}).get('url', 'unknown URL')}\"\n)\n\nlogger.info(f\"Successfully scraped {len(doc_pages)} pages out of {len(filtered_links)} URLs\")\nreturn doc_pages\n\n```  \nThis method:  \n- Gets all documentation links using the previous method\n- Optionally limits the number of pages to scrape\n- Uses Firecrawlâ€™s batch scraping to efficiently process multiple pages\n- Converts raw scraping results into structured `DocPage` objects\n- Handles errors and provides detailed logging  \n3. `save_documentation_pages`: Stores scraped content as markdown files:  \n```python\ndef save_documentation_pages(self, doc_pages: List[DocPage], docs_dir: str):\n\"\"\"Save scraped documentation pages to markdown files.\"\"\"\nPath(docs_dir).mkdir(parents=True, exist_ok=True)\n\nfor page in doc_pages:\nurl_path = page.url.replace(\"https://docs.firecrawl.dev\", \"\")\nsafe_filename = url_path.strip(\"/\").replace(\"/\", \"-\")\nfilepath = os.path.join(docs_dir, f\"{safe_filename}.md\")\n\nwith open(filepath, \"w\", encoding=\"utf-8\") as f:\nf.write(\"---n\")\nf.write(f\"title: {page.title}n\")\nf.write(f\"url: {page.url}n\")\nf.write(\"---nn\")\nf.write(page.content)\n\nlogger.info(f\"Saved {len(doc_pages)} pages to {docs_dir}\")\n\n```  \nThis method:  \n- Creates a documentation directory if needed\n- Converts URLs to safe filenames\n- Saves each page as a markdown file with YAML frontmatter\n- Preserves original titles and URLs for reference  \nFinally, the class provides a convenience method to handle the entire scraping workflow:  \n```python\ndef pull_docs(self, base_url: str, docs_dir: str, n_pages: int = None):\ndoc_pages = self.scrape_documentation(base_url, n_pages)\nself.save_documentation_pages(doc_pages, docs_dir)\n\n```  \nThis scraper component is used by:  \n- The Streamlit interface ( `app.py`) for initial documentation collection\n- The RAG system ( `rag.py`) for processing documentation into the vector store\n- The command-line interface for testing and manual scraping  \nThe use of Firecrawlâ€™s AI capabilities allows the scraper to handle diverse documentation layouts without custom selectors, while the structured output ensures consistency for downstream processing.",
      "metadata": {
        "title": "Building an Intelligent Code Documentation RAG Assistant with DeepSeek and Firecrawl",
        "url": "https://www.firecrawl.dev/blog/deepseek-rag-documentation-assistant"
      }
    },
    {
      "id": "208a2d17-1682-44aa-9491-8cf958c3bc28",
      "source": "firecrawl/blog/deepseek-rag-documentation-assistant.md",
      "content": "Breaking Down the App Components > 2. Implementing RAG with Ollama - `src/rag.py`\n\nThe RAG (Retrieval Augmented Generation) component is the core of our documentation assistant, handling document processing, embedding generation, and question answering. Letâ€™s examine each part in detail:  \nFirst, we import the necessary LangChain components:  \n```python\nfrom langchain_chroma import Chroma\nfrom langchain_community.document_loaders import DirectoryLoader\nfrom langchain_core.prompts import ChatPromptTemplate\nfrom langchain_ollama import ChatOllama, OllamaEmbeddings\nfrom langchain_text_splitters import RecursiveCharacterTextSplitter\n\n```  \nThese imports provide:  \n- `Chroma`: Vector database for storing embeddings\n- `DirectoryLoader`: Utility for loading markdown files from a directory\n- `ChatPromptTemplate`: Template system for LLM prompts\n- `ChatOllama` and `OllamaEmbeddings`: Local LLM and embedding models\n- `RecursiveCharacterTextSplitter`: Text chunking utility  \nThe main RAG class initializes all necessary components:  \n```python\nclass DocumentationRAG:\ndef __init__(self):\n# Initialize embeddings and vector store\nself.embeddings = OllamaEmbeddings(model=\"nomic-embed-text\")\nself.vector_store = Chroma(\nembedding_function=self.embeddings, persist_directory=\"./chroma_db\"\n)\n\n# Initialize LLM\nself.llm = ChatOllama(model=\"deepseek-r1:14b\")\n\n# Text splitter for chunking\nself.text_splitter = RecursiveCharacterTextSplitter(\nchunk_size=1000, chunk_overlap=200, add_start_index=True\n)\n\n```  \nThe initialization:  \n1. Creates an embedding model using Nomicâ€™s text embeddings\n2. Sets up a Chroma vector store with persistent storage\n3. Initializes the DeepSeek R1 14B model for question answering\n4. Configures a text splitter with 1000-character chunks and 200-character overlap  \nThe prompt template defines how the LLM should process questions:  \n```python\n# RAG prompt template\nself.prompt = ChatPromptTemplate.from_template(\n\"\"\"\nYou are an expert documentation assistant. Use the following documentation context\nto answer the question. If you don't know the answer, just say that you don't\nhave enough information. Keep the answer concise and clear.\n\nContext: {context}\nQuestion: {question}\n\nAnswer:\"\"\"\n)\n\n```  \nThis template:  \n- Sets the assistantâ€™s role and behavior\n- Provides placeholders for context and questions\n- Encourages concise and clear responses  \nThe document loading method handles reading markdown files:  \n```python\ndef load_docs_from_directory(self, docs_dir: str):\n\"\"\"Load all markdown documents from a directory\"\"\"\nmarkdown_docs = DirectoryLoader(docs_dir, glob=\"*.md\").load()\nreturn markdown_docs\n\n```  \nThis method:  \n- Uses `DirectoryLoader` to find all markdown files\n- Automatically handles file reading and basic preprocessing\n- Returns a list of Document objects  \nThe document processing method prepares content for the vector store:  \n```python\ndef process_documents(self, docs_dir: str):\n\"\"\"Process documents and add to vector store\"\"\"\n# Clear existing documents\nself.vector_store = Chroma(\nembedding_function=self.embeddings, persist_directory=\"./chroma_db\"\n)\n\n# Load and process new documents\ndocuments = self.load_docs_from_directory(docs_dir)\nchunks = self.text_splitter.split_documents(documents)\nself.vector_store.add_documents(chunks)\n\n```  \nThis method:  \n1. Reinitializes the vector store to clear existing documents\n2. Loads new documents from the specified directory\n3. Splits documents into manageable chunks\n4. Generates and stores embeddings in the vector database  \nFinally, the query method handles question answering:  \n```python\ndef query(self, question: str) -> tuple[str, str]:\n\"\"\"Query the documentation\"\"\"\n# Get relevant documents\ndocs = self.vector_store.similarity_search(question, k=3)\n\n# Combine context\ncontext = \"nn\".join([doc.page_content for doc in docs])\n\n# Generate response\nchain = self.prompt | self.llm\nresponse = chain.invoke({\"context\": context, \"question\": question})\n\n# Extract chain of thought between <think> and </think>\nchain_of_thought = response.content.split(\"<think>\")[1].split(\"</think>\")[0]\n\n# Extract response\nresponse = response.content.split(\"</think>\")[1].strip()\n\nreturn response, chain_of_thought\n\n```  \nThe query process:  \n1. Performs semantic search to find the 3 most relevant document chunks\n2. Combines the chunks into a single context string\n3. Creates a LangChain chain combining the prompt and LLM\n4. Generates a response with chain-of-thought reasoning\n5. Extracts and returns both the final answer and reasoning process  \nThis RAG component is used by:  \n- The Streamlit interface ( `app.py`) for handling user questions\n- The command-line interface for testing and development\n- Future extensions that need documentation Q&A capabilities  \nThe implementation uses LangChainâ€™s abstractions to create a modular and maintainable system while keeping all AI components running locally through Ollama.",
      "metadata": {
        "title": "Building an Intelligent Code Documentation RAG Assistant with DeepSeek and Firecrawl",
        "url": "https://www.firecrawl.dev/blog/deepseek-rag-documentation-assistant"
      }
    },
    {
      "id": "56812b92-a5a5-4ab9-870a-c5c11e616617",
      "source": "firecrawl/blog/deepseek-rag-documentation-assistant.md",
      "content": "Breaking Down the App Components > 3. Building a clean UI with Streamlit - `src/app.py`\n\nThe Streamlit interface brings together the scraping and RAG components into a user-friendly web application. Letâ€™s break down each component:  \nFirst, we set up basic configuration and utilities:  \n```python\nimport glob\nimport logging\nfrom pathlib import Path\n\nimport streamlit as st\nfrom dotenv import load_dotenv\nfrom rag import DocumentationRAG\nfrom scraper import DocumentationScraper\n\n# Configure logging\nlogging.basicConfig(\nlevel=logging.INFO,\nformat=\"%(asctime)s - %(name)s - %(levelname)s - %(message)s\",\nhandlers=[logging.StreamHandler()],\n)\nlogger = logging.getLogger(__name__)\n\n```  \nThese imports and configurations:  \n- Set up logging for debugging and monitoring\n- Import our custom RAG and scraper components\n- Load environment variables for configuration  \nHelper functions handle documentation management:  \n```python\ndef get_existing_docs():\n\"\"\"Get all documentation directories with -docs suffix\"\"\"\ndocs_dirs = glob.glob(\"*-docs\")\nreturn [Path(dir_path).name for dir_path in docs_dirs]\n\ndef get_doc_page_count(docs_dir: str) -> int:\n\"\"\"Get number of markdown files in a documentation directory\"\"\"\nreturn len(list(Path(docs_dir).glob(\"*.md\")))\n\n```  \nThese utilities:  \n- Find all documentation directories with â€œ-docsâ€ suffix\n- Count pages in each documentation set\n- Support the UIâ€™s documentation selection features  \nThe scraping configuration section handles documentation collection:  \n```python\ndef scraping_config_section():\n\"\"\"Create the documentation scraping configuration section\"\"\"\nst.markdown(\"### Configure Scraping\")\nbase_url = st.text_input(\n\"Documentation URL\",\nplaceholder=\"https://docs.firecrawl.dev\",\nhelp=\"The base URL of the documentation to scrape\",\n)\n\ndocs_name = st.text_input(\n\"Documentation Name\",\nplaceholder=\"Firecrawl-docs\",\nhelp=\"Name of the directory to store documentation\",\n)\n\nn_pages = st.number_input(\n\"Number of Pages\",\nmin_value=0,\nvalue=0,\nhelp=\"Limit the number of pages to scrape (0 for all pages)\",\n)\n\nst.info(\n\"ðŸ’¡ Add '-docs' suffix to the documentation name. \"\n\"Set pages to 0 to scrape all available pages.\"\n)\n\nif st.button(\"Start Scraping\"):\nif not base_url or not docs_name:\nst.error(\"Please provide both URL and documentation name\")\nelif not docs_name.endswith(\"-docs\"):\nst.error(\"Documentation name must end with '-docs'\")\nelse:\nwith st.spinner(\"Scraping documentation...\"):\ntry:\nscraper = DocumentationScraper()\nn_pages = None if n_pages == 0 else n_pages\nscraper.pull_docs(base_url, docs_name, n_pages=n_pages)\nst.success(\"Documentation scraped successfully!\")\nexcept Exception as e:\nst.error(f\"Error scraping documentation: {str(e)}\")\n\n```  \nThis section:  \n- Provides input fields for documentation URL and name\n- Allows limiting the number of pages to scrape\n- Handles validation and error reporting\n- Shows progress during scraping\n- Uses our `DocumentationScraper` class for content collection  \nThe documentation selection interface manages switching between docs:  \n```python\ndef documentation_select_section():\n\"\"\"Create the documentation selection section\"\"\"\nst.markdown(\"### Select Documentation\")\nexisting_docs = get_existing_docs()\n\nif not existing_docs:\nst.caption(\"No documentation found yet\")\nreturn None\n\n# Create options with page counts\ndoc_options = [f\"{doc} ({get_doc_page_count(doc)} pages)\" for doc in existing_docs]\n\nselected_doc = st.selectbox(\n\"Choose documentation to use as context\",\noptions=doc_options,\nhelp=\"Select which documentation to use for answering questions\",\n)\n\nif selected_doc:\n# Extract the actual doc name without page count\nst.session_state.current_doc = selected_doc.split(\" (\")[0]\nreturn st.session_state.current_doc\nreturn None\n\n```  \nThis component:  \n- Lists available documentation sets\n- Shows page counts for each set\n- Updates session state when selection changes\n- Handles the case of no available documentation  \nThe chat interface consists of two main functions that work together to create the interactive Q&A experience:  \nFirst, we initialize the necessary session state:  \n```python\ndef initialize_chat_state():\n\"\"\"Initialize session state for chat\"\"\"\nif \"messages\" not in st.session_state:\nst.session_state.messages = []\nif \"rag\" not in st.session_state:\nst.session_state.rag = DocumentationRAG()\n\n```  \nThis initialization:  \n- Creates an empty message list if none exists\n- Sets up the RAG system for document processing and querying\n- Uses Streamlitâ€™s session state to persist data between reruns  \nThe main chat interface starts with basic setup:  \n```python\ndef chat_interface():\n\"\"\"Create the chat interface\"\"\"\nst.title(\"Documentation Assistant\")\n\n# Check if documentation is selected\nif \"current_doc\" not in st.session_state:\nst.info(\"Please select a documentation from the sidebar to start chatting.\")\nreturn\n\n```  \nThis section:  \n- Sets the page title\n- Ensures documentation is selected before proceeding\n- Shows a helpful message if no documentation is chosen  \nDocument processing is handled next:  \n```python\n# Process documentation if not already processed\nif (\n\"docs_processed\" not in st.session_state\nor st.session_state.docs_processed != st.session_state.current_doc\n):\nwith st.spinner(\"Processing documentation...\"):\nst.session_state.rag.process_documents(st.session_state.current_doc)\nst.session_state.docs_processed = st.session_state.current_doc\n\n```  \nThis block:  \n- Checks if the current documentation needs processing\n- Shows a loading spinner during processing\n- Updates the session state after processing\n- Prevents unnecessary reprocessing of the same documentation  \nMessage display is handled by iterating through the chat history:  \n```python\n# Display chat messages\nfor message in st.session_state.messages:\nwith st.chat_message(message[\"role\"]):\nst.markdown(message[\"content\"])\nif \"chain_of_thought\" in message:\nwith st.expander(\"View reasoning\"):\nst.markdown(message[\"chain_of_thought\"])\n\n```  \nThis section:  \n- Shows each message with appropriate styling based on role\n- Displays the main content using markdown\n- Creates expandable sections for reasoning chains\n- Maintains visual consistency in the chat  \nFinally, the input handling and response generation:  \n```python\n# Chat input\nif prompt := st.chat_input(\"Ask a question about the documentation\"):\n# Add user message\nst.session_state.messages.append({\"role\": \"user\", \"content\": prompt})\n\nwith st.chat_message(\"user\"):\nst.markdown(prompt)\n\n# Generate and display response\nwith st.chat_message(\"assistant\"):\nwith st.spinner(\"Thinking...\"):\nresponse, chain_of_thought = st.session_state.rag.query(prompt)\nst.markdown(response)\nwith st.expander(\"View reasoning\"):\nst.markdown(chain_of_thought)\n\n# Store assistant response\nst.session_state.messages.append({\n\"role\": \"assistant\",\n\"content\": response,\n\"chain_of_thought\": chain_of_thought,\n})\n\n```  \nThis section:  \n1. Captures user input:\n- Uses Streamlitâ€™s chat input component\n- Stores the message in session state\n- Displays the message immediately\n2. Generates response:\n- Shows a â€œthinkingâ€ spinner during processing\n- Queries the RAG system for an answer\n- Displays the response with expandable reasoning\n3. Updates chat history:\n- Stores both response and reasoning\n- Maintains the conversation flow\n- Preserves the interaction for future reference  \nThe entire chat interface creates a seamless experience by:  \n- Managing state effectively\n- Providing immediate feedback\n- Showing processing status\n- Maintaining conversation context\n- Exposing the AIâ€™s reasoning process  \nFinally, the main application structure:  \n```python\ndef sidebar():\n\"\"\"Create the sidebar UI components\"\"\"\nwith st.sidebar:\nst.title(\"Documentation Scraper\")\nscraping_config_section()\ndocumentation_select_section()\n\ndef main():\ninitialize_chat_state()\nsidebar()\nchat_interface()\n\nif __name__ == \"__main__\":\nmain()\n\n```  \nThis structure:  \n- Organizes UI components into sidebar and main area\n- Initializes necessary state on startup\n- Provides a clean entry point for the application  \nThe Streamlit interface brings together all components into a cohesive application that:  \n- Makes documentation scraping accessible to non-technical users\n- Provides immediate feedback during operations\n- Maintains conversation history\n- Shows the AIâ€™s reasoning process\n- Handles errors gracefully",
      "metadata": {
        "title": "Building an Intelligent Code Documentation RAG Assistant with DeepSeek and Firecrawl",
        "url": "https://www.firecrawl.dev/blog/deepseek-rag-documentation-assistant"
      }
    },
    {
      "id": "5b54004b-f56a-4489-9dfb-04ee740210c0",
      "source": "firecrawl/blog/deepseek-rag-documentation-assistant.md",
      "content": "How to Increase System Performance\n\nThere are several ways to optimize the performance of this documentation assistant. The following sections explore key areas for potential improvements:",
      "metadata": {
        "title": "Building an Intelligent Code Documentation RAG Assistant with DeepSeek and Firecrawl",
        "url": "https://www.firecrawl.dev/blog/deepseek-rag-documentation-assistant"
      }
    },
    {
      "id": "0cfb53ae-5771-48aa-8e2a-710454859e54",
      "source": "firecrawl/blog/deepseek-rag-documentation-assistant.md",
      "content": "How to Increase System Performance > 1. Optimize document chunking\n\nIn `rag.py`, we currently use a basic chunking strategy:  \n```python\nself.text_splitter = RecursiveCharacterTextSplitter(\nchunk_size=1000,\nchunk_overlap=200,\nadd_start_index=True\n)\n\n```  \nWe can improve this by:  \n- Using semantic chunking that respects document structure\n- Adjusting chunk size based on content type (e.g., larger for API docs)\n- Implementing custom splitting rules for documentation headers\n- Adding metadata to chunks for better context preservation  \nExample improved configuration:  \n```python\nself.text_splitter = RecursiveCharacterTextSplitter(\nchunk_size=1500, # Larger chunks for more context\nchunk_overlap=300, # Increased overlap for better coherence\nseparators=[\"n## \", \"n### \", \"nn\", \"n\", \" \", \"\"], # Respect markdown structure\nadd_start_index=True,\nlength_function=len,\nis_separator_regex=False\n)\n\n```",
      "metadata": {
        "title": "Building an Intelligent Code Documentation RAG Assistant with DeepSeek and Firecrawl",
        "url": "https://www.firecrawl.dev/blog/deepseek-rag-documentation-assistant"
      }
    },
    {
      "id": "ff600933-e6df-484c-a924-6434ba39096d",
      "source": "firecrawl/blog/deepseek-rag-documentation-assistant.md",
      "content": "How to Increase System Performance > 2. Enhance vector search\n\nThe current similarity search in `rag.py` is basic:  \n```python\ndocs = self.vector_store.similarity_search(question, k=3)\n\n```  \nWe can improve retrieval by:  \n- Increasing `k`, i.e. the number of chunks returned\n- Implementing hybrid search (combining semantic and keyword matching)\n- Using Maximum Marginal Relevance (MMR) for diverse results\n- Adding metadata filtering based on document sections\n- Implementing re-ranking of retrieved chunks  \nExample enhanced retrieval:  \n```python\ndef query(self, question: str) -> tuple[str, str]:\n# Get relevant documents with MMR\ndocs = self.vector_store.max_marginal_relevance_search(\nquestion,\nk=5, # Retrieve more candidates\nfetch_k=20, # Consider larger initial set\nlambda_mult=0.7 # Diversity factor\n)\n\n# Filter and re-rank results\nfiltered_docs = [\\\ndoc for doc in docs\\\nif self._calculate_relevance_score(doc, question) > 0.7\\\n]\n\n# Use top 3 most relevant chunks\ncontext = \"nn\".join([doc.page_content for doc in filtered_docs[:3]])\n\n```",
      "metadata": {
        "title": "Building an Intelligent Code Documentation RAG Assistant with DeepSeek and Firecrawl",
        "url": "https://www.firecrawl.dev/blog/deepseek-rag-documentation-assistant"
      }
    },
    {
      "id": "dd83bb6d-d666-4072-9996-c4bfc017a29b",
      "source": "firecrawl/blog/deepseek-rag-documentation-assistant.md",
      "content": "How to Increase System Performance > 3. Implement caching\n\nThe current implementation reprocesses documentation on every selection:  \n```python\nif (\n\"docs_processed\" not in st.session_state\nor st.session_state.docs_processed != st.session_state.current_doc\n):\nwith st.spinner(\"Processing documentation...\"):\nst.session_state.rag.process_documents(st.session_state.current_doc)\n\n```  \nWe can improve this by:  \n- Implementing persistent vector storage with versioning\n- Caching processed embeddings\n- Adding incremental updates for documentation changes  \nExample caching implementation:  \n```python\nfrom hashlib import md5\nimport pickle\n\nclass CachedDocumentationRAG(DocumentationRAG):\ndef process_documents(self, docs_dir: str):\ncache_key = self._get_cache_key(docs_dir)\ncache_path = f\"cache/{cache_key}.pkl\"\n\nif os.path.exists(cache_path):\nwith open(cache_path, 'rb') as f:\nself.vector_store = pickle.load(f)\nelse:\nsuper().process_documents(docs_dir)\nos.makedirs(\"cache\", exist_ok=True)\nwith open(cache_path, 'wb') as f:\npickle.dump(self.vector_store, f)\n\n```",
      "metadata": {
        "title": "Building an Intelligent Code Documentation RAG Assistant with DeepSeek and Firecrawl",
        "url": "https://www.firecrawl.dev/blog/deepseek-rag-documentation-assistant"
      }
    },
    {
      "id": "215c824b-e28d-48b1-be42-0fcf2e37bf1b",
      "source": "firecrawl/blog/deepseek-rag-documentation-assistant.md",
      "content": "How to Increase System Performance > 4. Optimize model loading\n\nCurrently, we initialize models in `__init__`:  \n```python\ndef __init__(self):\nself.embeddings = OllamaEmbeddings(model=\"nomic-embed-text\")\nself.llm = ChatOllama(model=\"deepseek-r1:14b\")\n\n```  \nWe can improve this by:  \n- Implementing lazy loading of models\n- Using smaller models for initial responses\n- Adding model quantization options\n- Implementing model caching  \nExample optimized initialization:  \n```python\nclass OptimizedDocumentationRAG:\ndef __init__(self, use_small_model=True):\nself._embeddings = None\nself._llm = None\nself._use_small_model = use_small_model\n\n@property\ndef llm(self):\nif self._llm is None:\nmodel_size = \"7b\" if self._use_small_model else \"14b\"\nself._llm = ChatOllama(\nmodel=f\"deepseek-r1:{model_size}\",\ntemperature=0.1, # Lower temperature for docs\nnum_ctx=2048 # Reduced context for faster inference\n)\nreturn self._llm\n\n```  \nThese optimizations can significantly improve:  \n- Response latency\n- Memory usage\n- Processing throughput\n- User experience  \nRemember to benchmark performance before and after implementing these changes to measure their impact. Also, consider your specific use case - some optimizations might be more relevant depending on factors like user load, documentation size, and hardware constraints.",
      "metadata": {
        "title": "Building an Intelligent Code Documentation RAG Assistant with DeepSeek and Firecrawl",
        "url": "https://www.firecrawl.dev/blog/deepseek-rag-documentation-assistant"
      }
    },
    {
      "id": "2671784f-e5da-48de-8576-e9b9729afce8",
      "source": "firecrawl/blog/deepseek-rag-documentation-assistant.md",
      "content": "Conclusion\n\nThis local documentation assistant demonstrates how modern AI technologies can be combined to create powerful, practical tools for technical documentation. By using DeepSeekâ€™s language capabilities, Firecrawlâ€™s AI-powered scraping, and the RAG architecture, weâ€™ve built a system that makes documentation more accessible and interactive. The applicationâ€™s modular design, with clear separation between scraping, RAG implementation, and user interface components, provides a solid foundation for future enhancements and adaptations to different documentation needs.  \nMost importantly, this implementation shows that sophisticated AI applications can be built entirely with local components, eliminating privacy concerns and reducing operational costs. The combination of Streamlitâ€™s intuitive interface, LangChainâ€™s flexible abstractions, and Ollamaâ€™s local AI models creates a seamless experience that feels like a cloud service but runs entirely on your machine. Whether youâ€™re a developer learning a new framework, a technical writer maintaining documentation, or a team lead looking to improve documentation accessibility, this assistant provides a practical solution that can be customized and extended to meet your specific needs.  \nArticle updated recently  \n[ðŸ”¥](https://www.firecrawl.dev/)",
      "metadata": {
        "title": "Building an Intelligent Code Documentation RAG Assistant with DeepSeek and Firecrawl",
        "url": "https://www.firecrawl.dev/blog/deepseek-rag-documentation-assistant"
      }
    },
    {
      "id": "49dd2147-4fe8-46e9-8170-eefdfea250ec",
      "source": "firecrawl/blog/deepseek-rag-documentation-assistant.md",
      "content": "Ready to _Build?_\n\nStart scraping web data for your AI apps today.  \nNo credit card needed.  \nGet Started",
      "metadata": {
        "title": "Building an Intelligent Code Documentation RAG Assistant with DeepSeek and Firecrawl",
        "url": "https://www.firecrawl.dev/blog/deepseek-rag-documentation-assistant"
      }
    },
    {
      "id": "23424c26-7578-4afa-95fd-ab95ced90306",
      "source": "firecrawl/blog/deepseek-rag-documentation-assistant.md",
      "content": "About the Author\n\n[![Bex Tuychiev image](https://www.firecrawl.dev/bex.jpg)\\\nBex Tuychiev@bextuychiev](https://x.com/bextuychiev)  \nBex is a Top 10 AI writer on Medium and a Kaggle Master with over 15k followers. He loves writing detailed guides, tutorials, and notebooks on complex data science and machine learning topics",
      "metadata": {
        "title": "Building an Intelligent Code Documentation RAG Assistant with DeepSeek and Firecrawl",
        "url": "https://www.firecrawl.dev/blog/deepseek-rag-documentation-assistant"
      }
    },
    {
      "id": "280eeed4-7e66-4c7d-a0cf-abd02b5e5ed8",
      "source": "firecrawl/blog/deepseek-rag-documentation-assistant.md",
      "content": "About the Author > More articles by Bex Tuychiev\n\n[Building an Automated Price Tracking Tool\\\n\\\nBuild an automated e-commerce price tracker in Python. Learn web scraping, price monitoring, and automated alerts using Firecrawl, Streamlit, PostgreSQL.](https://www.firecrawl.dev/blog/automated-price-tracking-tutorial-python) [Web Scraping Automation: How to Run Scrapers on a Schedule\\\n\\\nLearn how to automate web scraping in Python using free tools like schedule, asyncio, cron jobs and GitHub Actions. This comprehensive guide covers local and cloud-based scheduling methods to run scrapers reliably in 2025.](https://www.firecrawl.dev/blog/automated-web-scraping-free-2025) [Automated Data Collection - A Comprehensive Guide\\\n\\\nLearn how to build robust automated data collection systems using modern tools and best practices. This guide covers everything from selecting the right tools to implementing scalable collection pipelines.](https://www.firecrawl.dev/blog/automated-data-collection-guide) [BeautifulSoup4 vs. Scrapy - A Comprehensive Comparison for Web Scraping in Python\\\n\\\nLearn the key differences between BeautifulSoup4 and Scrapy for web scraping in Python. Compare their features, performance, and use cases to choose the right tool for your web scraping needs.](https://www.firecrawl.dev/blog/beautifulsoup4-vs-scrapy-comparison) [How to Build an Automated Competitor Price Monitoring System with Python\\\n\\\nLearn how to build an automated competitor price monitoring system in Python that tracks prices across e-commerce sites, provides real-time comparisons, and maintains price history using Firecrawl, Streamlit, and GitHub Actions.](https://www.firecrawl.dev/blog/automated-competitor-price-scraping) [Scraping Company Data and Funding Information in Bulk With Firecrawl and Claude\\\n\\\nLearn how to build a web scraper in Python that gathers company details, funding rounds, and investor information from public sources like Crunchbase using Firecrawl and Claude for automated data collection and analysis.](https://www.firecrawl.dev/blog/crunchbase-scraping-with-firecrawl-claude) [Data Enrichment: A Complete Guide to Enhancing Your Data Quality\\\n\\\nLearn how to enrich your data quality with a comprehensive guide covering data enrichment tools, best practices, and real-world examples. Discover how to leverage modern solutions like Firecrawl to automate data collection, validation, and integration for better business insights.](https://www.firecrawl.dev/blog/complete-guide-to-data-enrichment) [Building an Intelligent Code Documentation RAG Assistant with DeepSeek and Firecrawl\\\n\\\nLearn how to build an intelligent documentation assistant powered by DeepSeek and RAG (Retrieval Augmented Generation) that can answer questions about any documentation website by combining language models with efficient information retrieval.](https://www.firecrawl.dev/blog/deepseek-rag-documentation-assistant)",
      "metadata": {
        "title": "Building an Intelligent Code Documentation RAG Assistant with DeepSeek and Firecrawl",
        "url": "https://www.firecrawl.dev/blog/deepseek-rag-documentation-assistant"
      }
    },
    {
      "id": "6b96bb50-61f7-40bd-ba6a-a0db37f15c90",
      "source": "firecrawl/blog/how-to-use-openai-o1-reasoning-models-in-applications.md",
      "content": "---\ntitle: How to Use OpenAI's o1 Reasoning Models in Your Applications\nurl: https://www.firecrawl.dev/blog/how-to-use-openai-o1-reasoning-models-in-applications\n---  \nIntroducing /extract - Get web data with a prompt [Try now](https://www.firecrawl.dev/extract)  \nSep 16, 2024  \nâ€¢  \n[![Eric Ciarla image](https://www.firecrawl.dev/eric-img.jpeg)Eric Ciarla](https://x.com/ericciarla)  \n# How to Use OpenAI's o1 Reasoning Models in Your Applications  \n![How to Use OpenAI's o1 Reasoning Models in Your Applications image](https://www.firecrawl.dev/images/blog/how-to-use-openai-o1-reasoning-models-in-applications.jpg)  \nOpenAI has recently unveiled its **o1 series models**, marking a significant leap in the realm of complex reasoning with AI. These models are designed to â€œthink before they answer,â€ producing extensive internal chains of thought before responding. In this guide, weâ€™ll explore how to integrate these powerful models into your applications, with a practical example of crawling a website using the `o1-preview` model.",
      "metadata": {
        "title": "How to Use OpenAI's o1 Reasoning Models in Your Applications",
        "url": "https://www.firecrawl.dev/blog/how-to-use-openai-o1-reasoning-models-in-applications"
      }
    },
    {
      "id": "0e3d70ee-02da-4e35-b1ce-60f9ac8f39a0",
      "source": "firecrawl/blog/how-to-use-openai-o1-reasoning-models-in-applications.md",
      "content": "Getting Started with o1 Models in Your Applications\n\nTo demonstrate how to integrate the o1 models into your apps, weâ€™ll walk through a practical example: crawling a website to find specific information using the `o1-preview` model.",
      "metadata": {
        "title": "How to Use OpenAI's o1 Reasoning Models in Your Applications",
        "url": "https://www.firecrawl.dev/blog/how-to-use-openai-o1-reasoning-models-in-applications"
      }
    },
    {
      "id": "72abd5eb-604d-4008-9c71-c1d781d106c4",
      "source": "firecrawl/blog/how-to-use-openai-o1-reasoning-models-in-applications.md",
      "content": "Getting Started with o1 Models in Your Applications > Prerequisites\n\nEnsure you have the following libraries installed:  \n```bash\npip install firecrawl-py openai\n\n```",
      "metadata": {
        "title": "How to Use OpenAI's o1 Reasoning Models in Your Applications",
        "url": "https://www.firecrawl.dev/blog/how-to-use-openai-o1-reasoning-models-in-applications"
      }
    },
    {
      "id": "561e31ab-65ba-43ff-9b37-cb9d0ff432c0",
      "source": "firecrawl/blog/how-to-use-openai-o1-reasoning-models-in-applications.md",
      "content": "Getting Started with o1 Models in Your Applications > Step 1: Import Necessary Libraries\n\nWeâ€™ll start by importing the required modules.  \n```python\nimport os\nfrom firecrawl import FirecrawlApp\nimport json\nfrom dotenv import load_dotenv\nfrom openai import OpenAI\n\n```",
      "metadata": {
        "title": "How to Use OpenAI's o1 Reasoning Models in Your Applications",
        "url": "https://www.firecrawl.dev/blog/how-to-use-openai-o1-reasoning-models-in-applications"
      }
    },
    {
      "id": "46a48f04-55cc-4246-8af4-351df671100f",
      "source": "firecrawl/blog/how-to-use-openai-o1-reasoning-models-in-applications.md",
      "content": "Getting Started with o1 Models in Your Applications > Step 2: Load Environment Variables\n\nWeâ€™ll use environment variables to securely manage our API keys.  \n```python\n# Load environment variables\nload_dotenv()\n\n# Retrieve API keys from environment variables\nfirecrawl_api_key = os.getenv(\"FIRECRAWL_API_KEY\")\nopenai_api_key = os.getenv(\"OPENAI_API_KEY\")\n\n```",
      "metadata": {
        "title": "How to Use OpenAI's o1 Reasoning Models in Your Applications",
        "url": "https://www.firecrawl.dev/blog/how-to-use-openai-o1-reasoning-models-in-applications"
      }
    },
    {
      "id": "fed5d967-05cd-48db-a141-539b662005f7",
      "source": "firecrawl/blog/how-to-use-openai-o1-reasoning-models-in-applications.md",
      "content": "Getting Started with o1 Models in Your Applications > Step 3: Initialize the FirecrawlApp and OpenAI Client\n\n```python\n# Initialize the FirecrawlApp and OpenAI client\napp = FirecrawlApp(api_key=firecrawl_api_key)\nclient = OpenAI(api_key=openai_api_key)\n\n```",
      "metadata": {
        "title": "How to Use OpenAI's o1 Reasoning Models in Your Applications",
        "url": "https://www.firecrawl.dev/blog/how-to-use-openai-o1-reasoning-models-in-applications"
      }
    },
    {
      "id": "535f1a9d-68b7-472d-a46b-ebadf0f179b4",
      "source": "firecrawl/blog/how-to-use-openai-o1-reasoning-models-in-applications.md",
      "content": "Getting Started with o1 Models in Your Applications > Step 4: Define the Objective and URL\n\nSet the website you want to crawl and the objective of the crawl.  \n```python\nurl = \"https://example.com\"\nobjective = \"Find the contact email for customer support\"\n\n```",
      "metadata": {
        "title": "How to Use OpenAI's o1 Reasoning Models in Your Applications",
        "url": "https://www.firecrawl.dev/blog/how-to-use-openai-o1-reasoning-models-in-applications"
      }
    },
    {
      "id": "8158d02b-15fb-4cc9-a1db-7e07c2bf9000",
      "source": "firecrawl/blog/how-to-use-openai-o1-reasoning-models-in-applications.md",
      "content": "Getting Started with o1 Models in Your Applications > Step 5: Determine the Search Parameter Using o1-preview\n\nWeâ€™ll use the `o1-preview` model to come up with a 1-2 word search parameter based on our objective.  \n```python\nmap_prompt = f\"\"\"\nThe map function generates a list of URLs from a website and accepts a search parameter. Based on the objective: {objective}, suggest a 1-2 word search parameter to find the needed information. Only respond with 1-2 words.\n\"\"\"\n\n# Highlighted OpenAI call\ncompletion = client.chat.completions.create(\nmodel=\"o1-preview\",\nmessages=[\\\n{\"role\": \"user\", \"content\": map_prompt}\\\n]\n)\n\nmap_search_parameter = completion.choices[0].message.content.strip()\n\n```",
      "metadata": {
        "title": "How to Use OpenAI's o1 Reasoning Models in Your Applications",
        "url": "https://www.firecrawl.dev/blog/how-to-use-openai-o1-reasoning-models-in-applications"
      }
    },
    {
      "id": "9186efeb-4e5c-42b0-be37-1f3b37f7b111",
      "source": "firecrawl/blog/how-to-use-openai-o1-reasoning-models-in-applications.md",
      "content": "Getting Started with o1 Models in Your Applications > Step 6: Map the Website Using the Search Parameter\n\nUse the `firecrawl` app to map the website and find relevant links.  \n```python\nmap_website = app.map_url(url, params={\"search\": map_search_parameter})\n\n```",
      "metadata": {
        "title": "How to Use OpenAI's o1 Reasoning Models in Your Applications",
        "url": "https://www.firecrawl.dev/blog/how-to-use-openai-o1-reasoning-models-in-applications"
      }
    },
    {
      "id": "9b0063f0-c24d-457a-830c-4d3be658fee1",
      "source": "firecrawl/blog/how-to-use-openai-o1-reasoning-models-in-applications.md",
      "content": "Getting Started with o1 Models in Your Applications > Step 7: Scrape the Top Pages and Check for the Objective\n\nWeâ€™ll scrape the top links from the mapping result and check if they meet our objective.  \n```python\n# Get top 3 links\ntop_links = map_website[:3] if isinstance(map_website, list) else []\n\nfor link in top_links:\n# Scrape the page\nscrape_result = app.scrape_url(link, params={'formats': ['markdown']})\n\n# Check if objective is met\ncheck_prompt = f\"\"\"\nGiven the following scraped content and objective, determine if the objective is met with high confidence.\nIf it is, extract the relevant information in a simple and concise JSON format.\nIf the objective is not met with high confidence, respond with 'Objective not met'.\n\nObjective: {objective}\nScraped content: {scrape_result['markdown']}\n\"\"\"\n\ncompletion = client.chat.completions.create(\nmodel=\"o1-preview\",\nmessages=[\\\n{\"role\": \"user\", \"content\": check_prompt}\\\n]\n)\n\nresult = completion.choices[0].message.content.strip()\n\nif result != \"Objective not met\":\ntry:\nextracted_info = json.loads(result)\nbreak\nexcept json.JSONDecodeError:\ncontinue\nelse:\nextracted_info = None\n\n```",
      "metadata": {
        "title": "How to Use OpenAI's o1 Reasoning Models in Your Applications",
        "url": "https://www.firecrawl.dev/blog/how-to-use-openai-o1-reasoning-models-in-applications"
      }
    },
    {
      "id": "da8b591f-9565-428a-a3a4-be2d8008b403",
      "source": "firecrawl/blog/how-to-use-openai-o1-reasoning-models-in-applications.md",
      "content": "Getting Started with o1 Models in Your Applications > Step 8: Display the Extracted Information\n\n```python\nif extracted_info:\nprint(\"Extracted Information:\")\nprint(json.dumps(extracted_info, indent=2))\nelse:\nprint(\"Objective not met with the available content.\")\n\n```",
      "metadata": {
        "title": "How to Use OpenAI's o1 Reasoning Models in Your Applications",
        "url": "https://www.firecrawl.dev/blog/how-to-use-openai-o1-reasoning-models-in-applications"
      }
    },
    {
      "id": "e664ab17-b5e0-4434-81c4-5a671a043a84",
      "source": "firecrawl/blog/how-to-use-openai-o1-reasoning-models-in-applications.md",
      "content": "Conclusion\n\nIn this article, weâ€™ve explored how to integrate OpenAIâ€™s new o1 reasoning models into your applications to perform complex tasks like crawling a website and extracting specific information. The o1 models showcase impressive capabilities in reasoning and problem-solving, making them valuable tools for developers tackling challenging AI tasks.  \nWhether youâ€™re working on advanced coding problems, mathematical computations, or intricate scientific queries, the o1 models can significantly enhance your applicationâ€™s reasoning abilities.  \n**Happy coding!**",
      "metadata": {
        "title": "How to Use OpenAI's o1 Reasoning Models in Your Applications",
        "url": "https://www.firecrawl.dev/blog/how-to-use-openai-o1-reasoning-models-in-applications"
      }
    },
    {
      "id": "9c0ac424-14e6-4d9b-a90a-389a007670b5",
      "source": "firecrawl/blog/how-to-use-openai-o1-reasoning-models-in-applications.md",
      "content": "References\n\n- [OpenAIâ€™s Documentation on o1 Models](https://platform.openai.com/docs/models/o1)\n- [Firecrawl Documentation](https://firecrawl.com/docs)  \nArticle updated recently  \n[ðŸ”¥](https://www.firecrawl.dev/)",
      "metadata": {
        "title": "How to Use OpenAI's o1 Reasoning Models in Your Applications",
        "url": "https://www.firecrawl.dev/blog/how-to-use-openai-o1-reasoning-models-in-applications"
      }
    },
    {
      "id": "33811efa-c315-4eaa-9f2f-e70d4d28e704",
      "source": "firecrawl/blog/how-to-use-openai-o1-reasoning-models-in-applications.md",
      "content": "Ready to _Build?_\n\nStart scraping web data for your AI apps today.  \nNo credit card needed.  \nGet Started",
      "metadata": {
        "title": "How to Use OpenAI's o1 Reasoning Models in Your Applications",
        "url": "https://www.firecrawl.dev/blog/how-to-use-openai-o1-reasoning-models-in-applications"
      }
    },
    {
      "id": "8cd5f073-1acf-4cdb-a57d-d2a067237173",
      "source": "firecrawl/blog/how-to-use-openai-o1-reasoning-models-in-applications.md",
      "content": "About the Author\n\n[![Eric Ciarla image](https://www.firecrawl.dev/eric-img.jpeg)\\\nEric Ciarla@ericciarla](https://x.com/ericciarla)  \nEric Ciarla is the Chief Operating Officer (COO) of Firecrawl and leads marketing. He also worked on Mendable.ai and sold it to companies like Snapchat, Coinbase, and MongoDB.\nPreviously worked at Ford and Fracta as a Data Scientist. Eric also co-founded SideGuide, a tool for learning code within VS Code with 50,000 users.",
      "metadata": {
        "title": "How to Use OpenAI's o1 Reasoning Models in Your Applications",
        "url": "https://www.firecrawl.dev/blog/how-to-use-openai-o1-reasoning-models-in-applications"
      }
    },
    {
      "id": "f932eee2-07a3-479a-97c0-550a9df46ee3",
      "source": "firecrawl/blog/how-to-use-openai-o1-reasoning-models-in-applications.md",
      "content": "About the Author > More articles by Eric Ciarla\n\n[How to Create an llms.txt File for Any Website\\\n\\\nLearn how to generate an llms.txt file for any website using the llms.txt Generator and Firecrawl.](https://www.firecrawl.dev/blog/How-to-Create-an-llms-txt-File-for-Any-Website) [Cloudflare Error 1015: How to solve it?\\\n\\\nCloudflare Error 1015 is a rate limiting error that occurs when Cloudflare detects that you are exceeding the request limit set by the website owner.](https://www.firecrawl.dev/blog/cloudflare-error-1015-how-to-solve-it) [Build an agent that checks for website contradictions\\\n\\\nUsing Firecrawl and Claude to scrape your website's data and look for contradictions.](https://www.firecrawl.dev/blog/contradiction-agent) [Why Companies Need a Data Strategy for Generative AI\\\n\\\nLearn why a well-defined data strategy is essential for building robust, production-ready generative AI systems, and discover practical steps for curation, maintenance, and integration.](https://www.firecrawl.dev/blog/why-companies-need-a-data-strategy-for-generative-ai) [Getting Started with OpenAI's Predicted Outputs for Faster LLM Responses\\\n\\\nA guide to leveraging Predicted Outputs to speed up LLM tasks with GPT-4o models.](https://www.firecrawl.dev/blog/getting-started-with-predicted-outputs-openai) [How to easily install requests with pip and python\\\n\\\nA tutorial on installing the requests library in Python using various methods, with usage examples and troubleshooting tips](https://www.firecrawl.dev/blog/how-to-easily-install-requests-with-pip-and-python) [How to quickly install BeautifulSoup with Python\\\n\\\nA guide on installing the BeautifulSoup library in Python using various methods, with usage examples and troubleshooting tips](https://www.firecrawl.dev/blog/how-to-quickly-install-beautifulsoup-with-python) [How to Use OpenAI's o1 Reasoning Models in Your Applications\\\n\\\nLearn how to harness OpenAI's latest o1 series models for complex reasoning tasks in your apps.](https://www.firecrawl.dev/blog/how-to-use-openai-o1-reasoning-models-in-applications)",
      "metadata": {
        "title": "How to Use OpenAI's o1 Reasoning Models in Your Applications",
        "url": "https://www.firecrawl.dev/blog/how-to-use-openai-o1-reasoning-models-in-applications"
      }
    },
    {
      "id": "3eeb4386-768f-4ee2-88ab-50d7f597a77e",
      "source": "firecrawl/blog/launch-week-ii-day-2-introducing-location-language-settings.md",
      "content": "---\ntitle: Launch Week II - Day 2: Introducing Location and Language Settings\nurl: https://www.firecrawl.dev/blog/launch-week-ii-day-2-introducing-location-language-settings\n---  \nIntroducing /extract - Get web data with a prompt [Try now](https://www.firecrawl.dev/extract)  \nOctober 29, 2024  \nâ€¢  \n[![Eric Ciarla image](https://www.firecrawl.dev/eric-img.jpeg)Eric Ciarla](https://x.com/ericciarla)  \n# Launch Week II - Day 2: Introducing Location and Language Settings  \n![Launch Week II - Day 2: Introducing Location and Language Settings image](https://www.firecrawl.dev/images/blog/firecrawl-location-language.jpg)  \nWelcome to Day 2 of Firecrawlâ€™s second Launch Week! Today, weâ€™re thrilled to introduce our latest feature: **Location and Language Settings**.  \n**Discover Location and Language Settings**  \nWith this new feature, you can now specify a country and preferred languages to receive content thatâ€™s tailored to your target location and linguistic preferences. This means more relevant and localized data for your web scraping projects.  \n**How It Works**  \nWhen you set the location parameters, Firecrawl utilizes an appropriate proxy (if available) and emulates the corresponding language and timezone settings. By default, the country is set to `'US'` if not specified.  \n**Getting Started with Location and Language Settings**  \nTo leverage these new settings, include the `location` object in your request body with the following properties:  \n- `country`: ISO 3166-1 alpha-2 country code (e.g., `'US'`, `'AU'`, `'DE'`, `'JP'`). Defaults to `'US'`.\n- `languages`: An array of preferred languages and locales for the request in order of priority. Defaults to the language of the specified location.  \n**Example Usage**  \nHereâ€™s how you can get started using Python:  \n```python\nfrom firecrawl import FirecrawlApp\n\napp = FirecrawlApp(api_key=\"fc-YOUR_API_KEY\")\n\n# Scrape a website with location and language settings\nscrape_result = app.scrape_url('airbnb.com',\nparams={\n'formats': ['markdown', 'html'],\n'location': {\n'country': 'BR',\n'languages': ['pt-BR']\n}\n}\n)\nprint(scrape_result)\n\n```  \n**Understanding the Response**  \nBy specifying the location as Brazil ( `'BR'`) and the preferred language as Brazilian Portuguese ( `'pt-BR'`), youâ€™ll receive content as it appears to users in Brazil, in Portuguese.  \n**Why Use Location and Language Settings?**  \n- **Relevance**: Access content thatâ€™s specific to a particular country or language.\n- **Localization**: Scrape websites as they appear to users in different regions.\n- **Customization**: Tailor your scraping to match your target audience or market.  \n**Whatâ€™s Next?**  \nWeâ€™re just getting warmed up with Launch Week II! The Location and Language Settings are just one of the exciting new features weâ€™re rolling out to enhance your web scraping capabilities.  \nWeâ€™d love to hear how you plan to use these new settings in your projects. Your feedback helps us continue to improve and tailor our services to better meet your needs.  \nHappy scraping, and stay tuned for Day 3 of [Launch Week II](https://www.firecrawl.dev/launch-week) tomorrow!  \nArticle updated recently  \n[ðŸ”¥](https://www.firecrawl.dev/)",
      "metadata": {
        "title": "Launch Week II - Day 2: Introducing Location and Language Settings",
        "url": "https://www.firecrawl.dev/blog/launch-week-ii-day-2-introducing-location-language-settings"
      }
    },
    {
      "id": "209741ea-cfd1-490e-9a6a-22c5492495ef",
      "source": "firecrawl/blog/launch-week-ii-day-2-introducing-location-language-settings.md",
      "content": "Ready to _Build?_\n\nStart scraping web data for your AI apps today.  \nNo credit card needed.  \nGet Started",
      "metadata": {
        "title": "Launch Week II - Day 2: Introducing Location and Language Settings",
        "url": "https://www.firecrawl.dev/blog/launch-week-ii-day-2-introducing-location-language-settings"
      }
    },
    {
      "id": "9408b0d3-aca6-43dc-a6a0-5df4cb7cf5e9",
      "source": "firecrawl/blog/launch-week-ii-day-2-introducing-location-language-settings.md",
      "content": "About the Author\n\n[![Eric Ciarla image](https://www.firecrawl.dev/eric-img.jpeg)\\\nEric Ciarla@ericciarla](https://x.com/ericciarla)  \nEric Ciarla is the Chief Operating Officer (COO) of Firecrawl and leads marketing. He also worked on Mendable.ai and sold it to companies like Snapchat, Coinbase, and MongoDB.\nPreviously worked at Ford and Fracta as a Data Scientist. Eric also co-founded SideGuide, a tool for learning code within VS Code with 50,000 users.",
      "metadata": {
        "title": "Launch Week II - Day 2: Introducing Location and Language Settings",
        "url": "https://www.firecrawl.dev/blog/launch-week-ii-day-2-introducing-location-language-settings"
      }
    },
    {
      "id": "63159ea0-75d0-4a70-a993-d147e20bdb69",
      "source": "firecrawl/blog/launch-week-ii-day-2-introducing-location-language-settings.md",
      "content": "About the Author > More articles by Eric Ciarla\n\n[How to Create an llms.txt File for Any Website\\\n\\\nLearn how to generate an llms.txt file for any website using the llms.txt Generator and Firecrawl.](https://www.firecrawl.dev/blog/How-to-Create-an-llms-txt-File-for-Any-Website) [Cloudflare Error 1015: How to solve it?\\\n\\\nCloudflare Error 1015 is a rate limiting error that occurs when Cloudflare detects that you are exceeding the request limit set by the website owner.](https://www.firecrawl.dev/blog/cloudflare-error-1015-how-to-solve-it) [Build an agent that checks for website contradictions\\\n\\\nUsing Firecrawl and Claude to scrape your website's data and look for contradictions.](https://www.firecrawl.dev/blog/contradiction-agent) [Why Companies Need a Data Strategy for Generative AI\\\n\\\nLearn why a well-defined data strategy is essential for building robust, production-ready generative AI systems, and discover practical steps for curation, maintenance, and integration.](https://www.firecrawl.dev/blog/why-companies-need-a-data-strategy-for-generative-ai) [Getting Started with OpenAI's Predicted Outputs for Faster LLM Responses\\\n\\\nA guide to leveraging Predicted Outputs to speed up LLM tasks with GPT-4o models.](https://www.firecrawl.dev/blog/getting-started-with-predicted-outputs-openai) [How to easily install requests with pip and python\\\n\\\nA tutorial on installing the requests library in Python using various methods, with usage examples and troubleshooting tips](https://www.firecrawl.dev/blog/how-to-easily-install-requests-with-pip-and-python) [How to quickly install BeautifulSoup with Python\\\n\\\nA guide on installing the BeautifulSoup library in Python using various methods, with usage examples and troubleshooting tips](https://www.firecrawl.dev/blog/how-to-quickly-install-beautifulsoup-with-python) [How to Use OpenAI's o1 Reasoning Models in Your Applications\\\n\\\nLearn how to harness OpenAI's latest o1 series models for complex reasoning tasks in your apps.](https://www.firecrawl.dev/blog/how-to-use-openai-o1-reasoning-models-in-applications)",
      "metadata": {
        "title": "Launch Week II - Day 2: Introducing Location and Language Settings",
        "url": "https://www.firecrawl.dev/blog/launch-week-ii-day-2-introducing-location-language-settings"
      }
    },
    {
      "id": "0f156783-af69-4891-9980-d6856c68e5ce",
      "source": "firecrawl/blog/contradiction-agent.md",
      "content": "---\ntitle: Build an agent that checks for website contradictions\nurl: https://www.firecrawl.dev/blog/contradiction-agent\n---  \nIntroducing /extract - Get web data with a prompt [Try now](https://www.firecrawl.dev/extract)  \nMay 19, 2024  \nâ€¢  \n[![Eric Ciarla image](https://www.firecrawl.dev/eric-img.jpeg)Eric Ciarla](https://x.com/ericciarla)  \n# Build an agent that checks for website contradictions  \n![Build an agent that checks for website contradictions image](https://www.firecrawl.dev/images/blog/g1.png)  \nIn this quick tutorial you will learn how to use Firecrawl and Claude to scrape your websiteâ€™s data and look for contradictions and inconsistencies in a few lines of code. When you are shipping fast, data is bound to get stale, with Firecrawl and LLMs you can make sure your public web data is always consistent! We will be using Opusâ€™s huge 200k context window and Firecrawlâ€™s parellization, making this process accurate and fast.",
      "metadata": {
        "title": "Build an agent that checks for website contradictions",
        "url": "https://www.firecrawl.dev/blog/contradiction-agent"
      }
    },
    {
      "id": "e8bc2b2b-acd1-47ec-aef1-cc9ad82dcee3",
      "source": "firecrawl/blog/contradiction-agent.md",
      "content": "Setup\n\nInstall our python dependencies, including anthropic and firecrawl-py.  \n```bash\npip install firecrawl-py anthropic\n\n```",
      "metadata": {
        "title": "Build an agent that checks for website contradictions",
        "url": "https://www.firecrawl.dev/blog/contradiction-agent"
      }
    },
    {
      "id": "032d15a7-7a38-4bbd-9db1-719a6d987dd8",
      "source": "firecrawl/blog/contradiction-agent.md",
      "content": "Getting your Claude and Firecrawl API Keys\n\nTo use Claude Opus and Firecrawl, you will need to get your API keys. You can get your Anthropic API key from [here](https://www.anthropic.com/) and your Firecrawl API key from [here](https://firecrawl.dev/).",
      "metadata": {
        "title": "Build an agent that checks for website contradictions",
        "url": "https://www.firecrawl.dev/blog/contradiction-agent"
      }
    },
    {
      "id": "56017634-36e6-4f22-9371-6a1900581bc5",
      "source": "firecrawl/blog/contradiction-agent.md",
      "content": "Load website with Firecrawl\n\nTo be able to get all the data from our website page put it into an easy to read format for the LLM, we will use [Firecrawl](https://firecrawl.dev/). It handles by-passing JS-blocked websites, extracting the main content, and outputting in a LLM-readable format for increased accuracy.  \nHere is how we will scrape a website url using Firecrawl-py  \n```python\nfrom firecrawl import FirecrawlApp\n\napp = FirecrawlApp(api_key=\"YOUR-KEY\")\n\ncrawl_result = app.crawl_url('mendable.ai', {'crawlerOptions': {'excludes': ['blog/*','usecases/*']}})\n\nprint(crawl_result)\n\n```  \nWith all of the web data we want scraped and in a clean format, we can move onto the next step.",
      "metadata": {
        "title": "Build an agent that checks for website contradictions",
        "url": "https://www.firecrawl.dev/blog/contradiction-agent"
      }
    },
    {
      "id": "2e30b25d-9faf-40a1-b4b1-3c31929610e4",
      "source": "firecrawl/blog/contradiction-agent.md",
      "content": "Combination and Generation\n\nNow that we have the website data, letâ€™s pair up every page and run every combination through Opus for analysis.  \n```python\nfrom itertools import combinations\n\npage_combinations = []\n\nfor first_page, second_page in combinations(crawl_result, 2):\ncombined_string = \"First Page:n\" + first_page['markdown'] + \"nnSecond Page:n\" + second_page['markdown']\npage_combinations.append(combined_string)\n\nimport anthropic\n\nclient = anthropic.Anthropic(\n# defaults to os.environ.get(\"ANTHROPIC_API_KEY\")\napi_key=\"YOUR-KEY\",\n)\n\nfinal_output = []\n\nfor page_combination in page_combinations:\n\nprompt = \"Here are two pages from a companies website, your job is to find any contradictions or differences in opinion between the two pages, this could be caused by outdated information or other. If you find any contradictions, list them out and provide a brief explanation of why they are contradictory or differing. Make sure the explanation is specific and concise. It is okay if you don't find any contradictions, just say 'No contradictions found' and nothing else. Here are the pages: \" + \"nn\".join(page_combination)\n\nmessage = client.messages.create(\nmodel=\"claude-3-opus-20240229\",\nmax_tokens=1000,\ntemperature=0.0,\nsystem=\"You are an assistant that helps find contradictions or differences in opinion between pages in a company website and knowledge base. This could be caused by outdated information in the knowledge base.\",\nmessages=[\\\n{\"role\": \"user\", \"content\": prompt}\\\n]\n)\nfinal_output.append(message.content)\n\n```",
      "metadata": {
        "title": "Build an agent that checks for website contradictions",
        "url": "https://www.firecrawl.dev/blog/contradiction-agent"
      }
    },
    {
      "id": "18868c8b-fda0-41fe-a496-d571c6cd0d55",
      "source": "firecrawl/blog/contradiction-agent.md",
      "content": "Thatâ€™s about it!\n\nYou have now built an agent that looks at your website and spots any inconsistencies it might have.  \nIf you have any questions or need help, feel free to reach out to us at [Firecrawl](https://firecrawl.dev/).  \nArticle updated recently  \n[ðŸ”¥](https://www.firecrawl.dev/)",
      "metadata": {
        "title": "Build an agent that checks for website contradictions",
        "url": "https://www.firecrawl.dev/blog/contradiction-agent"
      }
    },
    {
      "id": "9eb0bff0-8c69-45db-b0cc-4b2ca7c2d475",
      "source": "firecrawl/blog/contradiction-agent.md",
      "content": "Ready to _Build?_\n\nStart scraping web data for your AI apps today.  \nNo credit card needed.  \nGet Started",
      "metadata": {
        "title": "Build an agent that checks for website contradictions",
        "url": "https://www.firecrawl.dev/blog/contradiction-agent"
      }
    },
    {
      "id": "5e6539f2-140f-4b91-86f2-86874134123d",
      "source": "firecrawl/blog/contradiction-agent.md",
      "content": "About the Author\n\n[![Eric Ciarla image](https://www.firecrawl.dev/eric-img.jpeg)\\\nEric Ciarla@ericciarla](https://x.com/ericciarla)  \nEric Ciarla is the Chief Operating Officer (COO) of Firecrawl and leads marketing. He also worked on Mendable.ai and sold it to companies like Snapchat, Coinbase, and MongoDB.\nPreviously worked at Ford and Fracta as a Data Scientist. Eric also co-founded SideGuide, a tool for learning code within VS Code with 50,000 users.",
      "metadata": {
        "title": "Build an agent that checks for website contradictions",
        "url": "https://www.firecrawl.dev/blog/contradiction-agent"
      }
    },
    {
      "id": "0238a1e7-9b93-4b74-a32c-d5342b49adb1",
      "source": "firecrawl/blog/contradiction-agent.md",
      "content": "About the Author > More articles by Eric Ciarla\n\n[How to Create an llms.txt File for Any Website\\\n\\\nLearn how to generate an llms.txt file for any website using the llms.txt Generator and Firecrawl.](https://www.firecrawl.dev/blog/How-to-Create-an-llms-txt-File-for-Any-Website) [Cloudflare Error 1015: How to solve it?\\\n\\\nCloudflare Error 1015 is a rate limiting error that occurs when Cloudflare detects that you are exceeding the request limit set by the website owner.](https://www.firecrawl.dev/blog/cloudflare-error-1015-how-to-solve-it) [Build an agent that checks for website contradictions\\\n\\\nUsing Firecrawl and Claude to scrape your website's data and look for contradictions.](https://www.firecrawl.dev/blog/contradiction-agent) [Why Companies Need a Data Strategy for Generative AI\\\n\\\nLearn why a well-defined data strategy is essential for building robust, production-ready generative AI systems, and discover practical steps for curation, maintenance, and integration.](https://www.firecrawl.dev/blog/why-companies-need-a-data-strategy-for-generative-ai) [Getting Started with OpenAI's Predicted Outputs for Faster LLM Responses\\\n\\\nA guide to leveraging Predicted Outputs to speed up LLM tasks with GPT-4o models.](https://www.firecrawl.dev/blog/getting-started-with-predicted-outputs-openai) [How to easily install requests with pip and python\\\n\\\nA tutorial on installing the requests library in Python using various methods, with usage examples and troubleshooting tips](https://www.firecrawl.dev/blog/how-to-easily-install-requests-with-pip-and-python) [How to quickly install BeautifulSoup with Python\\\n\\\nA guide on installing the BeautifulSoup library in Python using various methods, with usage examples and troubleshooting tips](https://www.firecrawl.dev/blog/how-to-quickly-install-beautifulsoup-with-python) [How to Use OpenAI's o1 Reasoning Models in Your Applications\\\n\\\nLearn how to harness OpenAI's latest o1 series models for complex reasoning tasks in your apps.](https://www.firecrawl.dev/blog/how-to-use-openai-o1-reasoning-models-in-applications)",
      "metadata": {
        "title": "Build an agent that checks for website contradictions",
        "url": "https://www.firecrawl.dev/blog/contradiction-agent"
      }
    },
    {
      "id": "a422c12b-a177-4b50-8b62-f17089402bab",
      "source": "firecrawl/blog/how-athena-intelligence-empowers-analysts-with-firecrawl.md",
      "content": "---\ntitle: How Athena Intelligence Empowers Enterprise Analysts with Firecrawl\nurl: https://www.firecrawl.dev/blog/how-athena-intelligence-empowers-analysts-with-firecrawl\n---  \nIntroducing /extract - Get web data with a prompt [Try now](https://www.firecrawl.dev/extract)  \nSep 10, 2024  \nâ€¢  \n[![Ben Reilly image](https://www.firecrawl.dev/customers/ben-reilly.png)Ben Reilly](https://x.com/AthenaIntell)  \n# How Athena Intelligence Empowers Enterprise Analysts with Firecrawl  \n![How Athena Intelligence Empowers Enterprise Analysts with Firecrawl image](https://www.firecrawl.dev/images/blog/customer-story-athena-intelligence.jpg)  \nAt [Athena Intelligence](https://www.athenaintelligence.ai/), our mission is to empower enterprise analysts with an AI-native analytics platform and our artificial analyst, Athena.. To achieve this, we rely on Firecrawl to efficiently ingest and understand written content from across the web. By feeding this data into our AI models, we enable our agents to provide valuable insights and recommendations to our clients.  \nThe integration of Firecrawl into our production environment was seamless. With just a few lines of code from the Python SDK, we were up and running in no time. The ability to quickly capture website content in a format that our language models can readily consume is invaluable to us. Without Firecrawl, we would lose a crucial data source that powers our platform.  \nThroughout our journey with Firecrawl, the support from their team has been exceptional. On the rare occasions when we encountered outages, they were swiftly resolved, and the team was always open to feedback for improving error messaging and other aspects of the service. In one sentence, Firecrawl is a game-changer for extracting website content into LLM-friendly formats, making it an essential tool in our stack.  \nArticle updated recently",
      "metadata": {
        "title": "How Athena Intelligence Empowers Enterprise Analysts with Firecrawl",
        "url": "https://www.firecrawl.dev/blog/how-athena-intelligence-empowers-analysts-with-firecrawl"
      }
    },
    {
      "id": "8fe5b6f0-b6d5-4624-aa83-5330eb5dcbd4",
      "source": "firecrawl/blog/how-athena-intelligence-empowers-analysts-with-firecrawl.md",
      "content": "About the Author\n\n[![Ben Reilly image](https://www.firecrawl.dev/customers/ben-reilly.png)\\\nBen Reilly@AthenaIntell](https://x.com/AthenaIntell)  \nBen Reilly is a Founding Engineer at Athena Intelligence.  \n[ðŸ”¥](https://www.firecrawl.dev/)",
      "metadata": {
        "title": "How Athena Intelligence Empowers Enterprise Analysts with Firecrawl",
        "url": "https://www.firecrawl.dev/blog/how-athena-intelligence-empowers-analysts-with-firecrawl"
      }
    },
    {
      "id": "7dcfe785-57fb-4b6d-a96e-c7c6e78041ae",
      "source": "firecrawl/blog/how-athena-intelligence-empowers-analysts-with-firecrawl.md",
      "content": "Ready to _Build?_\n\nStart scraping web data for your AI apps today.  \nNo credit card needed.  \nGet Started",
      "metadata": {
        "title": "How Athena Intelligence Empowers Enterprise Analysts with Firecrawl",
        "url": "https://www.firecrawl.dev/blog/how-athena-intelligence-empowers-analysts-with-firecrawl"
      }
    },
    {
      "id": "ecae186c-ebb3-4e1d-8504-7cccccfd6fb5",
      "source": "firecrawl/blog/firecrawl-july-2024-updates.md",
      "content": "---\ntitle: Firecrawl July 2024 Updates\nurl: https://www.firecrawl.dev/blog/firecrawl-july-2024-updates\n---  \nIntroducing /extract - Get web data with a prompt [Try now](https://www.firecrawl.dev/extract)  \nJuly 31, 2024  \nâ€¢  \n[![Eric Ciarla image](https://www.firecrawl.dev/eric-img.jpeg)Eric Ciarla](https://x.com/ericciarla)  \n# Firecrawl July 2024 Updates  \n![Firecrawl July 2024 Updates image](https://www.firecrawl.dev/images/blog/launch-yc-firecrawl.png)  \nWe are excited to share our latest updates from July!  \n**TLDR:**  \n- We launched [Firecrawl on Launch YC](https://www.ycombinator.com/launches/LTf-firecrawl-open-source-crawling-and-scraping-for-ai-ready-web-data) ðŸ”¥\n- Improvements to Endpoints + Dashboard\n- New Templates & Community Creations\n- We are hiring a [Developer Relations Specialist](https://www.ycombinator.com/companies/firecrawl/jobs/bbUHmrJ-devrel-and-growth-specialist-at-firecrawl) & [Web Automation Engineer](https://www.ycombinator.com/companies/firecrawl/jobs/hZHD0j6-founding-web-automation-engineer)",
      "metadata": {
        "title": "Firecrawl July 2024 Updates",
        "url": "https://www.firecrawl.dev/blog/firecrawl-july-2024-updates"
      }
    },
    {
      "id": "53c03e78-748c-4405-9a0a-9db63b187621",
      "source": "firecrawl/blog/firecrawl-july-2024-updates.md",
      "content": "Officially launched on YC ðŸ§¡\n\nAfter three months and more than 8K stars, we have officially decided to launch Firecrawl on YC. It has been an incredible journey, and we are excited to continue building the best way to power AI with web data. [Check out our launch (and leave an upvote ðŸ™‚)!](https://www.ycombinator.com/launches/LTf-firecrawl-open-source-crawling-and-scraping-for-ai-ready-web-data)  \n![Firecrawl Launch YC](https://www.firecrawl.dev/images/blog/launchyc.jpeg)",
      "metadata": {
        "title": "Firecrawl July 2024 Updates",
        "url": "https://www.firecrawl.dev/blog/firecrawl-july-2024-updates"
      }
    },
    {
      "id": "bc68cbfb-d695-409b-b0bb-22c6ead5d210",
      "source": "firecrawl/blog/firecrawl-july-2024-updates.md",
      "content": "Improvements to Endpoints + Dashboard\n\nThis month, we made improving our core product a priority. This meant focusing time on speed, reliability, and our dashboard as well.  \nSpecifically in these categories, we:  \n- Shaved off around 1 second for every scrape and crawl request\n- Expanded scrape reliability for a bunch of new types of sites\n- Added enhanced dashboard monitoring which allows you to see processes, timing, failures and more. Check it out on your Activity Logs page on the dashboard!  \nLook for even more speed and reliability improvements coming soon!  \n![New enhanced dashboard monitoring](https://www.firecrawl.dev/images/blog/newactivitylogs.jpeg)",
      "metadata": {
        "title": "Firecrawl July 2024 Updates",
        "url": "https://www.firecrawl.dev/blog/firecrawl-july-2024-updates"
      }
    },
    {
      "id": "9df51fd8-a63d-46fd-92ae-afd5e85a9e3b",
      "source": "firecrawl/blog/firecrawl-july-2024-updates.md",
      "content": "New Templates & Community Creations\n\nNot only did we release some examples and templates this month, but we also witnessed incredible creations from our community. If youâ€™re working on an interesting Firecrawl project, weâ€™d love to hear about it! Give us a shout at [@firecrawl_dev](https://x.com/firecrawl_dev). Here are a few highlights:  \n- Firecrawl Web Data Ingestion UI Template [(Link to repo)](https://github.com/mendableai/firecrawl/tree/main/apps/ui/ingestion-ui)\n- Generative UI with demo Firecrawl x Langchain by Brace Sproul from Langchain [(Link to repo)](https://github.com/bracesproul/gen-ui)\n- Scraping Real Estate Data from Zillow by Sourav Maji [(Link to post)](https://x.com/SouravMaji221/status/1818133241460556178)\n- Website Contraction Analysis with Google Gemini [(Link to post)](https://x.com/ericciarla/status/1808614350967525873)  \n![Web Data Ingestion UI Template](https://www.firecrawl.dev/images/blog/ingestiontemplate.jpeg)",
      "metadata": {
        "title": "Firecrawl July 2024 Updates",
        "url": "https://www.firecrawl.dev/blog/firecrawl-july-2024-updates"
      }
    },
    {
      "id": "ac3b738a-8f76-488c-b76c-576e6f8d99ea",
      "source": "firecrawl/blog/firecrawl-july-2024-updates.md",
      "content": "We are hiring!\n\nIf you want to help build the best way to power AI with web data, we want to hear from you. Specifically, we are hiring for these roles:  \n- DevRel and Growth Specialist at Firecrawl [(Link to post)](https://www.ycombinator.com/companies/firecrawl/jobs/bbUHmrJ-devrel-and-growth-specialist-at-firecrawl)\n- Founding Web Automation Engineer [(Link to job post)](https://www.ycombinator.com/companies/firecrawl/jobs/hZHD0j6-founding-web-automation-engineer)  \nThatâ€™s all for this update! Stay tuned for the next one ðŸš€  \nArticle updated recently  \n[ðŸ”¥](https://www.firecrawl.dev/)",
      "metadata": {
        "title": "Firecrawl July 2024 Updates",
        "url": "https://www.firecrawl.dev/blog/firecrawl-july-2024-updates"
      }
    },
    {
      "id": "278ec42b-8708-4209-af8b-0c12370b8d0b",
      "source": "firecrawl/blog/firecrawl-july-2024-updates.md",
      "content": "Ready to _Build?_\n\nStart scraping web data for your AI apps today.  \nNo credit card needed.  \nGet Started",
      "metadata": {
        "title": "Firecrawl July 2024 Updates",
        "url": "https://www.firecrawl.dev/blog/firecrawl-july-2024-updates"
      }
    },
    {
      "id": "3d66fe4b-53e1-420f-839a-6d751284a92f",
      "source": "firecrawl/blog/firecrawl-july-2024-updates.md",
      "content": "About the Author\n\n[![Eric Ciarla image](https://www.firecrawl.dev/eric-img.jpeg)\\\nEric Ciarla@ericciarla](https://x.com/ericciarla)  \nEric Ciarla is the Chief Operating Officer (COO) of Firecrawl and leads marketing. He also worked on Mendable.ai and sold it to companies like Snapchat, Coinbase, and MongoDB.\nPreviously worked at Ford and Fracta as a Data Scientist. Eric also co-founded SideGuide, a tool for learning code within VS Code with 50,000 users.",
      "metadata": {
        "title": "Firecrawl July 2024 Updates",
        "url": "https://www.firecrawl.dev/blog/firecrawl-july-2024-updates"
      }
    },
    {
      "id": "d4c20c9f-219f-4641-a392-cf93b991bbcc",
      "source": "firecrawl/blog/firecrawl-july-2024-updates.md",
      "content": "About the Author > More articles by Eric Ciarla\n\n[How to Create an llms.txt File for Any Website\\\n\\\nLearn how to generate an llms.txt file for any website using the llms.txt Generator and Firecrawl.](https://www.firecrawl.dev/blog/How-to-Create-an-llms-txt-File-for-Any-Website) [Cloudflare Error 1015: How to solve it?\\\n\\\nCloudflare Error 1015 is a rate limiting error that occurs when Cloudflare detects that you are exceeding the request limit set by the website owner.](https://www.firecrawl.dev/blog/cloudflare-error-1015-how-to-solve-it) [Build an agent that checks for website contradictions\\\n\\\nUsing Firecrawl and Claude to scrape your website's data and look for contradictions.](https://www.firecrawl.dev/blog/contradiction-agent) [Why Companies Need a Data Strategy for Generative AI\\\n\\\nLearn why a well-defined data strategy is essential for building robust, production-ready generative AI systems, and discover practical steps for curation, maintenance, and integration.](https://www.firecrawl.dev/blog/why-companies-need-a-data-strategy-for-generative-ai) [Getting Started with OpenAI's Predicted Outputs for Faster LLM Responses\\\n\\\nA guide to leveraging Predicted Outputs to speed up LLM tasks with GPT-4o models.](https://www.firecrawl.dev/blog/getting-started-with-predicted-outputs-openai) [How to easily install requests with pip and python\\\n\\\nA tutorial on installing the requests library in Python using various methods, with usage examples and troubleshooting tips](https://www.firecrawl.dev/blog/how-to-easily-install-requests-with-pip-and-python) [How to quickly install BeautifulSoup with Python\\\n\\\nA guide on installing the BeautifulSoup library in Python using various methods, with usage examples and troubleshooting tips](https://www.firecrawl.dev/blog/how-to-quickly-install-beautifulsoup-with-python) [How to Use OpenAI's o1 Reasoning Models in Your Applications\\\n\\\nLearn how to harness OpenAI's latest o1 series models for complex reasoning tasks in your apps.](https://www.firecrawl.dev/blog/how-to-use-openai-o1-reasoning-models-in-applications)",
      "metadata": {
        "title": "Firecrawl July 2024 Updates",
        "url": "https://www.firecrawl.dev/blog/firecrawl-july-2024-updates"
      }
    },
    {
      "id": "ed23388d-307a-4096-ab15-750a638a6e2d",
      "source": "firecrawl/blog/launch-week-ii-day-3-introducing-credit-packs.md",
      "content": "---\ntitle: Launch Week II - Day 3: Introducing Credit Packs\nurl: https://www.firecrawl.dev/blog/launch-week-ii-day-3-introducing-credit-packs\n---  \nIntroducing /extract - Get web data with a prompt [Try now](https://www.firecrawl.dev/extract)  \nOctober 30, 2024  \nâ€¢  \n[![Eric Ciarla image](https://www.firecrawl.dev/eric-img.jpeg)Eric Ciarla](https://x.com/ericciarla)  \n# Launch Week II - Day 3: Introducing Credit Packs  \n![Launch Week II - Day 3: Introducing Credit Packs image](https://www.firecrawl.dev/images/blog/firecrawl-credit-packs.jpg)  \nWelcome to Day 3 of Firecrawlâ€™s second Launch Week! Today, weâ€™re excited to introduce **Credit Packs**.  \n**Introducing Credit Packs**  \nCredit Packs allow you to easily top up your plan whenever youâ€™re running low on credits. This means you can handle unexpected surges in your scraping needs without changing your existing subscription. With Credit Packs, you have the flexibility to manage your projects without any interruptions.  \n**Additionally, we now offer Auto Recharge, which automatically adds a Credit Pack when youâ€™re approaching your limit.**  \n**How Credit Packs Work**  \n- **Flexible Top-Ups**: Purchase additional credits as needed, whenever you need them.\n- **No Plan Changes Required**: Keep your current subscription and simply add credits on top.\n- **Instant Availability**: Credits are added to your account immediately upon purchase.  \n**Introducing Auto Recharge**  \nAs part of our Credit Packs feature, weâ€™re also launching **Auto Recharge**. This optional setting ensures that you never run out of credits by automatically recharging when youâ€™re approaching your limit.  \n**How Auto Recharge Works**  \n- **Enable Auto Recharge**: Turn on Auto Recharge in your account settings.\n- **Continuous Operation**: Keep your scraping tasks running smoothly without manual intervention.  \n**Getting Started with Credit Packs**  \nTo start using Credit Packs and Auto Recharge, log in to your Firecrawl account and navigate to the **Pricing** page:  \n1. **Purchase a Credit Pack**: Choose the amount of credits youâ€™d like to add and complete the purchase process.\n2. **Optional - Enable Auto Recharge**: If you want to automate your credit management, toggle the Auto Recharge option and set your preferred thresholds.  \n**Why Use Credit Packs?**  \n- **Flexibility**: Accommodate fluctuating scraping demands without altering your base plan.\n- **Convenience**: Top up your credits instantly whenever you need more.\n- **Control**: Manage your budget by purchasing credits as required.  \nThatâ€™s all for today, be sure to check back tomorrow for Day 4 of [Launch Week II](https://www.firecrawl.dev/launch-week)!  \nArticle updated recently  \n[ðŸ”¥](https://www.firecrawl.dev/)",
      "metadata": {
        "title": "Launch Week II - Day 3: Introducing Credit Packs",
        "url": "https://www.firecrawl.dev/blog/launch-week-ii-day-3-introducing-credit-packs"
      }
    },
    {
      "id": "3f9fe67f-4ac4-4259-8e8c-b64be27e666e",
      "source": "firecrawl/blog/launch-week-ii-day-3-introducing-credit-packs.md",
      "content": "Ready to _Build?_\n\nStart scraping web data for your AI apps today.  \nNo credit card needed.  \nGet Started",
      "metadata": {
        "title": "Launch Week II - Day 3: Introducing Credit Packs",
        "url": "https://www.firecrawl.dev/blog/launch-week-ii-day-3-introducing-credit-packs"
      }
    },
    {
      "id": "a54a1031-a3ba-424a-9e48-949bdbb7b567",
      "source": "firecrawl/blog/launch-week-ii-day-3-introducing-credit-packs.md",
      "content": "About the Author\n\n[![Eric Ciarla image](https://www.firecrawl.dev/eric-img.jpeg)\\\nEric Ciarla@ericciarla](https://x.com/ericciarla)  \nEric Ciarla is the Chief Operating Officer (COO) of Firecrawl and leads marketing. He also worked on Mendable.ai and sold it to companies like Snapchat, Coinbase, and MongoDB.\nPreviously worked at Ford and Fracta as a Data Scientist. Eric also co-founded SideGuide, a tool for learning code within VS Code with 50,000 users.",
      "metadata": {
        "title": "Launch Week II - Day 3: Introducing Credit Packs",
        "url": "https://www.firecrawl.dev/blog/launch-week-ii-day-3-introducing-credit-packs"
      }
    },
    {
      "id": "5a9f3698-0f6f-42ad-bdf6-155eeb8f1d78",
      "source": "firecrawl/blog/launch-week-ii-day-3-introducing-credit-packs.md",
      "content": "About the Author > More articles by Eric Ciarla\n\n[How to Create an llms.txt File for Any Website\\\n\\\nLearn how to generate an llms.txt file for any website using the llms.txt Generator and Firecrawl.](https://www.firecrawl.dev/blog/How-to-Create-an-llms-txt-File-for-Any-Website) [Cloudflare Error 1015: How to solve it?\\\n\\\nCloudflare Error 1015 is a rate limiting error that occurs when Cloudflare detects that you are exceeding the request limit set by the website owner.](https://www.firecrawl.dev/blog/cloudflare-error-1015-how-to-solve-it) [Build an agent that checks for website contradictions\\\n\\\nUsing Firecrawl and Claude to scrape your website's data and look for contradictions.](https://www.firecrawl.dev/blog/contradiction-agent) [Why Companies Need a Data Strategy for Generative AI\\\n\\\nLearn why a well-defined data strategy is essential for building robust, production-ready generative AI systems, and discover practical steps for curation, maintenance, and integration.](https://www.firecrawl.dev/blog/why-companies-need-a-data-strategy-for-generative-ai) [Getting Started with OpenAI's Predicted Outputs for Faster LLM Responses\\\n\\\nA guide to leveraging Predicted Outputs to speed up LLM tasks with GPT-4o models.](https://www.firecrawl.dev/blog/getting-started-with-predicted-outputs-openai) [How to easily install requests with pip and python\\\n\\\nA tutorial on installing the requests library in Python using various methods, with usage examples and troubleshooting tips](https://www.firecrawl.dev/blog/how-to-easily-install-requests-with-pip-and-python) [How to quickly install BeautifulSoup with Python\\\n\\\nA guide on installing the BeautifulSoup library in Python using various methods, with usage examples and troubleshooting tips](https://www.firecrawl.dev/blog/how-to-quickly-install-beautifulsoup-with-python) [How to Use OpenAI's o1 Reasoning Models in Your Applications\\\n\\\nLearn how to harness OpenAI's latest o1 series models for complex reasoning tasks in your apps.](https://www.firecrawl.dev/blog/how-to-use-openai-o1-reasoning-models-in-applications)",
      "metadata": {
        "title": "Launch Week II - Day 3: Introducing Credit Packs",
        "url": "https://www.firecrawl.dev/blog/launch-week-ii-day-3-introducing-credit-packs"
      }
    },
    {
      "id": "2a8b13b6-7812-412c-8fbb-fb62fefa9b9e",
      "source": "firecrawl/blog/launch-week-ii-recap.md",
      "content": "---\ntitle: Launch Week II Recap\nurl: https://www.firecrawl.dev/blog/launch-week-ii-recap\n---  \nIntroducing /extract - Get web data with a prompt [Try now](https://www.firecrawl.dev/extract)  \nNovember 4, 2024  \nâ€¢  \n[![Eric Ciarla image](https://www.firecrawl.dev/eric-img.jpeg)Eric Ciarla](https://x.com/ericciarla)  \n# Launch Week II Recap  \n![Launch Week II Recap image](https://www.firecrawl.dev/images/blog/launch-week-ii-recap.jpg)  \n**Welcome to the recap of Firecrawlâ€™s second Launch Week!** Over the past seven days, weâ€™ve rolled out a series of exciting new features and enhancements designed to supercharge your web scraping projects. In case you missed any of the announcements, hereâ€™s a day-by-day rundown of everything weâ€™ve launched.  \n* * *",
      "metadata": {
        "title": "Launch Week II Recap",
        "url": "https://www.firecrawl.dev/blog/launch-week-ii-recap"
      }
    },
    {
      "id": "99a15a56-aab0-4298-8f61-a6dc60cb7e7c",
      "source": "firecrawl/blog/launch-week-ii-recap.md",
      "content": "**Day 1: Introducing the Batch Scrape Endpoint**\n\nWe kicked off Launch Week II by unveiling our new **Batch Scrape Endpoint**, allowing you to scrape multiple URLs simultaneously. This feature streamlines bulk data collection, offering both synchronous and asynchronous methods to suit your needs.  \n[Read the blog](https://www.firecrawl.dev/blog/launch-week-ii-day-1-introducing-batch-scrape-endpoint)  \n* * *",
      "metadata": {
        "title": "Launch Week II Recap",
        "url": "https://www.firecrawl.dev/blog/launch-week-ii-recap"
      }
    },
    {
      "id": "29022a56-350c-462a-bf45-ba6282d94d3f",
      "source": "firecrawl/blog/launch-week-ii-recap.md",
      "content": "**Day 2: Introducing Location and Language Settings**\n\nOn Day 2, we added **Geolocation Support** to our scraping requests. You can now specify the country and preferred languages to get location-specific content, enhancing the relevance and accuracy of your data.  \n[Read the blog](https://www.firecrawl.dev/blog/launch-week-ii-day-2-introducing-location-language-settings)  \n* * *",
      "metadata": {
        "title": "Launch Week II Recap",
        "url": "https://www.firecrawl.dev/blog/launch-week-ii-recap"
      }
    },
    {
      "id": "3b192505-c506-4d80-8afa-40acf76aa20d",
      "source": "firecrawl/blog/launch-week-ii-recap.md",
      "content": "**Day 3: Introducing Credit Packs**\n\nDay 3 saw the launch of flexible **Credit Packs**, starting at just $9/month for 1,000 credits. We also introduced a new **Auto Recharge** feature, which automatically tops up your account when youâ€™re running low, ensuring uninterrupted scraping.  \n[Read the blog](https://www.firecrawl.dev/blog/launch-week-ii-day-3-introducing-credit-packs)  \n* * *",
      "metadata": {
        "title": "Launch Week II Recap",
        "url": "https://www.firecrawl.dev/blog/launch-week-ii-recap"
      }
    },
    {
      "id": "4e021c8e-c551-47eb-a0cb-fc83b8260d27",
      "source": "firecrawl/blog/launch-week-ii-recap.md",
      "content": "**Day 4: Advanced iframe Scraping**\n\nWe took iframe handling to the next level on Day 4 with **Advanced iframe Scraping**. This update includes recursive traversal, cross-origin handling, and dynamic content loading capabilities, making it easier to scrape complex web pages.  \n[Read the blog](https://www.firecrawl.dev/blog/launch-week-ii-day-4-advanced-iframe-scraping)  \n* * *",
      "metadata": {
        "title": "Launch Week II Recap",
        "url": "https://www.firecrawl.dev/blog/launch-week-ii-recap"
      }
    },
    {
      "id": "464e68cf-618f-42e5-ba6d-0430a933a839",
      "source": "firecrawl/blog/launch-week-ii-recap.md",
      "content": "**Day 5: Introducing Two New Actions**\n\nOn Day 5, we added two powerful new actions to our scraping toolkit:  \n1. **Scrape**: Capture page content during interaction sequences.\n2. **Wait for Selector**: Wait for specific elements to appear before proceeding, ensuring more reliable automation.  \n[Read the blog](https://www.firecrawl.dev/blog/launch-week-ii-day-5-introducing-two-new-actions)  \n* * *",
      "metadata": {
        "title": "Launch Week II Recap",
        "url": "https://www.firecrawl.dev/blog/launch-week-ii-recap"
      }
    },
    {
      "id": "97f5a82b-f8cf-4d7f-ac9b-e0d8e4a8d825",
      "source": "firecrawl/blog/launch-week-ii-recap.md",
      "content": "**Day 6: Introducing Mobile Scraping and Mobile Screenshots**\n\nDay 6 brought **Mobile Scraping and Mobile Screenshots** to Firecrawl. With mobile device emulation, you can now interact with sites as if from a mobile deviceâ€”perfect for testing mobile-specific content and responsive designs.  \n[Read the blog](https://www.firecrawl.dev/blog/launch-week-ii-day-6-introducing-mobile-scraping)  \n* * *",
      "metadata": {
        "title": "Launch Week II Recap",
        "url": "https://www.firecrawl.dev/blog/launch-week-ii-recap"
      }
    },
    {
      "id": "8ffb23a4-7046-4660-a439-9bb8dc72faaf",
      "source": "firecrawl/blog/launch-week-ii-recap.md",
      "content": "**Day 7: Introducing Faster Markdown Parsing**\n\nWe wrapped up Launch Week II on Day 7 by unveiling our new **HTML to Markdown parser**. Rebuilt from scratch, itâ€™s now up to **4x faster**, more reliable, and produces cleaner Markdown, enhancing your data processing workflows.  \n[Read the blog](https://www.firecrawl.dev/blog/launch-week-ii-day-7-introducing-faster-markdown-parsing)  \n* * *  \n**Thank you for joining us during Launch Week II!** We hope these new features empower you to build even more powerful and efficient scraping applications.  \nHappy scraping!  \nArticle updated recently  \n[ðŸ”¥](https://www.firecrawl.dev/)",
      "metadata": {
        "title": "Launch Week II Recap",
        "url": "https://www.firecrawl.dev/blog/launch-week-ii-recap"
      }
    },
    {
      "id": "e21ca6f5-139a-4c6a-b288-09689dcb5612",
      "source": "firecrawl/blog/launch-week-ii-recap.md",
      "content": "Ready to _Build?_\n\nStart scraping web data for your AI apps today.  \nNo credit card needed.  \nGet Started",
      "metadata": {
        "title": "Launch Week II Recap",
        "url": "https://www.firecrawl.dev/blog/launch-week-ii-recap"
      }
    },
    {
      "id": "f06224eb-52c1-4119-811c-01f58ee1f978",
      "source": "firecrawl/blog/launch-week-ii-recap.md",
      "content": "About the Author\n\n[![Eric Ciarla image](https://www.firecrawl.dev/eric-img.jpeg)\\\nEric Ciarla@ericciarla](https://x.com/ericciarla)  \nEric Ciarla is the Chief Operating Officer (COO) of Firecrawl and leads marketing. He also worked on Mendable.ai and sold it to companies like Snapchat, Coinbase, and MongoDB.\nPreviously worked at Ford and Fracta as a Data Scientist. Eric also co-founded SideGuide, a tool for learning code within VS Code with 50,000 users.",
      "metadata": {
        "title": "Launch Week II Recap",
        "url": "https://www.firecrawl.dev/blog/launch-week-ii-recap"
      }
    },
    {
      "id": "a20aa042-d713-458c-a1e9-a0d3264836bb",
      "source": "firecrawl/blog/launch-week-ii-recap.md",
      "content": "About the Author > More articles by Eric Ciarla\n\n[How to Create an llms.txt File for Any Website\\\n\\\nLearn how to generate an llms.txt file for any website using the llms.txt Generator and Firecrawl.](https://www.firecrawl.dev/blog/How-to-Create-an-llms-txt-File-for-Any-Website) [Cloudflare Error 1015: How to solve it?\\\n\\\nCloudflare Error 1015 is a rate limiting error that occurs when Cloudflare detects that you are exceeding the request limit set by the website owner.](https://www.firecrawl.dev/blog/cloudflare-error-1015-how-to-solve-it) [Build an agent that checks for website contradictions\\\n\\\nUsing Firecrawl and Claude to scrape your website's data and look for contradictions.](https://www.firecrawl.dev/blog/contradiction-agent) [Why Companies Need a Data Strategy for Generative AI\\\n\\\nLearn why a well-defined data strategy is essential for building robust, production-ready generative AI systems, and discover practical steps for curation, maintenance, and integration.](https://www.firecrawl.dev/blog/why-companies-need-a-data-strategy-for-generative-ai) [Getting Started with OpenAI's Predicted Outputs for Faster LLM Responses\\\n\\\nA guide to leveraging Predicted Outputs to speed up LLM tasks with GPT-4o models.](https://www.firecrawl.dev/blog/getting-started-with-predicted-outputs-openai) [How to easily install requests with pip and python\\\n\\\nA tutorial on installing the requests library in Python using various methods, with usage examples and troubleshooting tips](https://www.firecrawl.dev/blog/how-to-easily-install-requests-with-pip-and-python) [How to quickly install BeautifulSoup with Python\\\n\\\nA guide on installing the BeautifulSoup library in Python using various methods, with usage examples and troubleshooting tips](https://www.firecrawl.dev/blog/how-to-quickly-install-beautifulsoup-with-python) [How to Use OpenAI's o1 Reasoning Models in Your Applications\\\n\\\nLearn how to harness OpenAI's latest o1 series models for complex reasoning tasks in your apps.](https://www.firecrawl.dev/blog/how-to-use-openai-o1-reasoning-models-in-applications)",
      "metadata": {
        "title": "Launch Week II Recap",
        "url": "https://www.firecrawl.dev/blog/launch-week-ii-recap"
      }
    },
    {
      "id": "6b1f67da-780f-4778-a38a-1fcaa00c547b",
      "source": "firecrawl/blog/launch-week-i-day-6-llm-extract.md",
      "content": "---\ntitle: Launch Week I / Day 6: LLM Extract (v1)\nurl: https://www.firecrawl.dev/blog/launch-week-i-day-6-llm-extract\n---  \nIntroducing /extract - Get web data with a prompt [Try now](https://www.firecrawl.dev/extract)  \nAugust 31, 2024  \nâ€¢  \n[![Nicolas Camara image](https://www.firecrawl.dev/nick-img.jpeg)Nicolas Camara](https://x.com/nickscamara_)  \n# Launch Week I / Day 6: LLM Extract (v1)  \n![Launch Week I / Day 6: LLM Extract (v1) image](https://www.firecrawl.dev/images/blog/firecrawl-llm-extract.png)  \nWelcome to Day 6 of Firecrawlâ€™s Launch Week! Weâ€™re excited to introduce v1 support for LLM Extract.",
      "metadata": {
        "title": "Launch Week I / Day 6: LLM Extract (v1)",
        "url": "https://www.firecrawl.dev/blog/launch-week-i-day-6-llm-extract"
      }
    },
    {
      "id": "2a746a56-ae79-4f04-a8f6-1ed7d6a15d4f",
      "source": "firecrawl/blog/launch-week-i-day-6-llm-extract.md",
      "content": "Introducing the Extract Format\n\nLLM extraction is now available in v1 under the extract format. To extract structured from a page, you can pass a schema to the endpoint or just provide a prompt.  \n![Extract](https://www.firecrawl.dev/images/blog/extract.png)  \n**Output**  \n![Output](https://www.firecrawl.dev/images/blog/extract-output-llm.png)",
      "metadata": {
        "title": "Launch Week I / Day 6: LLM Extract (v1)",
        "url": "https://www.firecrawl.dev/blog/launch-week-i-day-6-llm-extract"
      }
    },
    {
      "id": "1eaa3b4a-9d03-419d-958b-288700b5ced5",
      "source": "firecrawl/blog/launch-week-i-day-6-llm-extract.md",
      "content": "Extracting without schema (New)\n\nYou can now extract without a schema by just passing a prompt to the endpoint. The LLMs choose the structure of the data.  \n![Output](https://www.firecrawl.dev/images/blog/extract-no-schema.png)",
      "metadata": {
        "title": "Launch Week I / Day 6: LLM Extract (v1)",
        "url": "https://www.firecrawl.dev/blog/launch-week-i-day-6-llm-extract"
      }
    },
    {
      "id": "4e27a333-3e52-42ee-8154-758adb61ff52",
      "source": "firecrawl/blog/launch-week-i-day-6-llm-extract.md",
      "content": "Learn More\n\nLearn more about the extract format in our [documentation](https://docs.firecrawl.dev/features/extract).  \nArticle updated recently  \n[ðŸ”¥](https://www.firecrawl.dev/)",
      "metadata": {
        "title": "Launch Week I / Day 6: LLM Extract (v1)",
        "url": "https://www.firecrawl.dev/blog/launch-week-i-day-6-llm-extract"
      }
    },
    {
      "id": "25dabd63-9ed3-47b1-b0c5-779b3d2cc18d",
      "source": "firecrawl/blog/launch-week-i-day-6-llm-extract.md",
      "content": "Ready to _Build?_\n\nStart scraping web data for your AI apps today.  \nNo credit card needed.  \nGet Started",
      "metadata": {
        "title": "Launch Week I / Day 6: LLM Extract (v1)",
        "url": "https://www.firecrawl.dev/blog/launch-week-i-day-6-llm-extract"
      }
    },
    {
      "id": "d7690e5f-4469-4b55-9357-8a2071dc18ce",
      "source": "firecrawl/blog/launch-week-i-day-6-llm-extract.md",
      "content": "About the Author\n\n[![Nicolas Camara image](https://www.firecrawl.dev/nick-img.jpeg)\\\nNicolas Camara@nickscamara_](https://x.com/nickscamara_)  \nNicolas Camara is the Chief Technology Officer (CTO) at Firecrawl.\nHe previously built and scaled Mendable, one of the pioneering \"chat with your documents\" apps,\nwhich had major Fortune 500 customers like Snapchat, Coinbase, and MongoDB.\nPrior to that, Nicolas built SideGuide, the first code-learning tool inside VS Code,\nand grew a community of 50,000 users. Nicolas studied Computer Science and has over 10 years of experience in building software.",
      "metadata": {
        "title": "Launch Week I / Day 6: LLM Extract (v1)",
        "url": "https://www.firecrawl.dev/blog/launch-week-i-day-6-llm-extract"
      }
    },
    {
      "id": "3b4418ed-1f87-4aef-82f6-d8d34a497a34",
      "source": "firecrawl/blog/launch-week-i-day-6-llm-extract.md",
      "content": "About the Author > More articles by Nicolas Camara\n\n[Using OpenAI's Realtime API and Firecrawl to Talk with Any Website\\\n\\\nBuild a real-time conversational agent that interacts with any website using OpenAI's Realtime API and Firecrawl.](https://www.firecrawl.dev/blog/How-to-Talk-with-Any-Website-Using-OpenAIs-Realtime-API-and-Firecrawl) [Extract website data using LLMs\\\n\\\nLearn how to use Firecrawl and Groq to extract structured data from a web page in a few lines of code.](https://www.firecrawl.dev/blog/data-extraction-using-llms) [Getting Started with Grok-2: Setup and Web Crawler Example\\\n\\\nA detailed guide on setting up Grok-2 and building a web crawler using Firecrawl.](https://www.firecrawl.dev/blog/grok-2-setup-and-web-crawler-example) [Launch Week I / Day 6: LLM Extract (v1)\\\n\\\nExtract structured data from your web pages using the extract format in /scrape.](https://www.firecrawl.dev/blog/launch-week-i-day-6-llm-extract) [Launch Week I / Day 7: Crawl Webhooks (v1)\\\n\\\nNew /crawl webhook support. Send notifications to your apps during a crawl.](https://www.firecrawl.dev/blog/launch-week-i-day-7-webhooks) [OpenAI Swarm Tutorial: Create Marketing Campaigns for Any Website\\\n\\\nA guide to building a multi-agent system using OpenAI Swarm and Firecrawl for AI-driven marketing strategies](https://www.firecrawl.dev/blog/openai-swarm-agent-tutorial) [Build a 'Chat with website' using Groq Llama 3\\\n\\\nLearn how to use Firecrawl, Groq Llama 3, and Langchain to build a 'Chat with your website' bot.](https://www.firecrawl.dev/blog/chat-with-website) [Scrape and Analyze Airbnb Data with Firecrawl and E2B\\\n\\\nLearn how to scrape and analyze Airbnb data using Firecrawl and E2B in a few lines of code.](https://www.firecrawl.dev/blog/scrape-analyze-airbnb-data-with-e2b)",
      "metadata": {
        "title": "Launch Week I / Day 6: LLM Extract (v1)",
        "url": "https://www.firecrawl.dev/blog/launch-week-i-day-6-llm-extract"
      }
    },
    {
      "id": "e509c81f-cd9c-40da-82a5-9f92910f1c67",
      "source": "firecrawl/blog/introducing-fire-engine-for-firecrawl.md",
      "content": "---\ntitle: Introducing Fire Engine for Firecrawl\nurl: https://www.firecrawl.dev/blog/introducing-fire-engine-for-firecrawl\n---  \nIntroducing /extract - Get web data with a prompt [Try now](https://www.firecrawl.dev/extract)  \nAug 6, 2024  \nâ€¢  \n[![Eric Ciarla image](https://www.firecrawl.dev/eric-img.jpeg)Eric Ciarla](https://x.com/ericciarla)  \n# Introducing Fire Engine for Firecrawl  \n![Introducing Fire Engine for Firecrawl image](https://www.firecrawl.dev/images/blog/fire-engine-launch.png)  \nFirecrawl handles web scraping orchestration but doesnâ€™t do the actual scraping. It initially relied on third-party services like Fetch and Playwright for data retrieval. However, these services often failed on certain sites or were too slow, causing issues for users. To address this, we built Fire Engine, now the default backend for Firecrawl. Itâ€™s designed to be more reliable and faster, solving the core problems we and our users encountered with other scraping services.",
      "metadata": {
        "title": "Introducing Fire Engine for Firecrawl",
        "url": "https://www.firecrawl.dev/blog/introducing-fire-engine-for-firecrawl"
      }
    },
    {
      "id": "0e67b427-36cb-4125-8b21-96b9d659fcc1",
      "source": "firecrawl/blog/introducing-fire-engine-for-firecrawl.md",
      "content": "What is Fire Engine?\n\nFire Engine is a scraping primitive designed to increase Firecrawlâ€™s scraping capabilities.  \nWeâ€™re proud to say that Fire Engine outperforms leading competitors in key areas:  \n- **Reliability:** 40% more reliable than scraping leading competitors when scraping different types of websites  \n- **Speed:** Up to 33.17% faster than scraping leading competitors  \nAnd this is just the beginning, we are working closely with Firecrawl users to further improve reliability, speed, and more.",
      "metadata": {
        "title": "Introducing Fire Engine for Firecrawl",
        "url": "https://www.firecrawl.dev/blog/introducing-fire-engine-for-firecrawl"
      }
    },
    {
      "id": "ee83b6fd-0a55-4b71-a74d-ef54d1d45e89",
      "source": "firecrawl/blog/introducing-fire-engine-for-firecrawl.md",
      "content": "The Technology Behind Fire Engine\n\nFire Engine combines a variety of browser and non-browser based techniques to balance speed and reliability, ensuring that you get data back without compromise. To do this, Fire engine has:  \n- **Efficient Headless Browser Management:** Running browsers at scale is notoriously difficult, but Fire Engine handles this with ease.\n- **Persistent Browser Sessions:** By keeping browsers running, Fire Engine improves efficiency when handling new requests, reducing startup times and resource usage.\n- **Advanced Web Interaction Techniques:** Employing a sophisticated array of methodsâ€”including browser-based, browserless, and proprietary approaches\n- **Intelligent Request Handling:** From smart proxy selection to advanced queuing, every aspect of the request process is optimized for speed and reliability.  \nWith this technology, Fire Engine allows firecrawl to handle millions of requests daily with speed and accuracy.",
      "metadata": {
        "title": "Introducing Fire Engine for Firecrawl",
        "url": "https://www.firecrawl.dev/blog/introducing-fire-engine-for-firecrawl"
      }
    },
    {
      "id": "8e2448b6-2260-4a43-b5db-88c7a5b30cc1",
      "source": "firecrawl/blog/introducing-fire-engine-for-firecrawl.md",
      "content": "Try Fire Engine on Firecrawl Today\n\nFire Engine powers Firecrawl to handle thousands of daily requests efficiently. Itâ€™s currently available exclusively through Firecraw cloud, Developers can test Fire Engineâ€™s capabilities by signing up for [Firecrawl](https://www.firecrawl.dev/).  \nArticle updated recently  \n[ðŸ”¥](https://www.firecrawl.dev/)",
      "metadata": {
        "title": "Introducing Fire Engine for Firecrawl",
        "url": "https://www.firecrawl.dev/blog/introducing-fire-engine-for-firecrawl"
      }
    },
    {
      "id": "56d8e340-39ca-48a9-bb78-62dd24812fd9",
      "source": "firecrawl/blog/introducing-fire-engine-for-firecrawl.md",
      "content": "Ready to _Build?_\n\nStart scraping web data for your AI apps today.  \nNo credit card needed.  \nGet Started",
      "metadata": {
        "title": "Introducing Fire Engine for Firecrawl",
        "url": "https://www.firecrawl.dev/blog/introducing-fire-engine-for-firecrawl"
      }
    },
    {
      "id": "148e6600-3365-45a2-93f3-c1298ae93d8a",
      "source": "firecrawl/blog/introducing-fire-engine-for-firecrawl.md",
      "content": "About the Author\n\n[![Eric Ciarla image](https://www.firecrawl.dev/eric-img.jpeg)\\\nEric Ciarla@ericciarla](https://x.com/ericciarla)  \nEric Ciarla is the Chief Operating Officer (COO) of Firecrawl and leads marketing. He also worked on Mendable.ai and sold it to companies like Snapchat, Coinbase, and MongoDB.\nPreviously worked at Ford and Fracta as a Data Scientist. Eric also co-founded SideGuide, a tool for learning code within VS Code with 50,000 users.",
      "metadata": {
        "title": "Introducing Fire Engine for Firecrawl",
        "url": "https://www.firecrawl.dev/blog/introducing-fire-engine-for-firecrawl"
      }
    },
    {
      "id": "b0cabe06-3140-46cb-b4d0-8277a3a7c187",
      "source": "firecrawl/blog/introducing-fire-engine-for-firecrawl.md",
      "content": "About the Author > More articles by Eric Ciarla\n\n[How to Create an llms.txt File for Any Website\\\n\\\nLearn how to generate an llms.txt file for any website using the llms.txt Generator and Firecrawl.](https://www.firecrawl.dev/blog/How-to-Create-an-llms-txt-File-for-Any-Website) [Cloudflare Error 1015: How to solve it?\\\n\\\nCloudflare Error 1015 is a rate limiting error that occurs when Cloudflare detects that you are exceeding the request limit set by the website owner.](https://www.firecrawl.dev/blog/cloudflare-error-1015-how-to-solve-it) [Build an agent that checks for website contradictions\\\n\\\nUsing Firecrawl and Claude to scrape your website's data and look for contradictions.](https://www.firecrawl.dev/blog/contradiction-agent) [Why Companies Need a Data Strategy for Generative AI\\\n\\\nLearn why a well-defined data strategy is essential for building robust, production-ready generative AI systems, and discover practical steps for curation, maintenance, and integration.](https://www.firecrawl.dev/blog/why-companies-need-a-data-strategy-for-generative-ai) [Getting Started with OpenAI's Predicted Outputs for Faster LLM Responses\\\n\\\nA guide to leveraging Predicted Outputs to speed up LLM tasks with GPT-4o models.](https://www.firecrawl.dev/blog/getting-started-with-predicted-outputs-openai) [How to easily install requests with pip and python\\\n\\\nA tutorial on installing the requests library in Python using various methods, with usage examples and troubleshooting tips](https://www.firecrawl.dev/blog/how-to-easily-install-requests-with-pip-and-python) [How to quickly install BeautifulSoup with Python\\\n\\\nA guide on installing the BeautifulSoup library in Python using various methods, with usage examples and troubleshooting tips](https://www.firecrawl.dev/blog/how-to-quickly-install-beautifulsoup-with-python) [How to Use OpenAI's o1 Reasoning Models in Your Applications\\\n\\\nLearn how to harness OpenAI's latest o1 series models for complex reasoning tasks in your apps.](https://www.firecrawl.dev/blog/how-to-use-openai-o1-reasoning-models-in-applications)",
      "metadata": {
        "title": "Introducing Fire Engine for Firecrawl",
        "url": "https://www.firecrawl.dev/blog/introducing-fire-engine-for-firecrawl"
      }
    },
    {
      "id": "9c49c8de-2350-4744-b11b-c28844b55609",
      "source": "firecrawl/blog/crawlbench-llm-extraction.md",
      "content": "---\ntitle: Evaluating Web Data Extraction with CrawlBench\nurl: https://www.firecrawl.dev/blog/crawlbench-llm-extraction\n---  \nIntroducing /extract - Get web data with a prompt [Try now](https://www.firecrawl.dev/extract)  \nDec 9, 2024  \nâ€¢  \n[![Swyx image](https://www.firecrawl.dev/swyx.jpg)Swyx](https://x.com/swyx)  \n# Evaluating Web Data Extraction with CrawlBench  \n![Evaluating Web Data Extraction with CrawlBench image](https://www.firecrawl.dev/images/blog/crawlbench/crawlbench.jpg)  \nThe most common AI Engineering task, after you have a really good web scraper/crawler like Firecrawl, is to feed it in as context to an LLM, **extracting structured data output.** From populating spreadsheets and databases, to driving decisions in code based on deterministic rules this stuctured data is incredible useful. This is a fundamental building block of any AI agent that needs to read in arbitrary state and knowledge from the real world.  \nFirecrawlâ€™s beta of [LLM Extract](https://docs.firecrawl.dev/features/extract) caught my eye when it was announced, it claimed to generate structured data from any webpage and I immediately wondered how reliable it could be for my use cases. Hallucinations are commonplace in LLMs and even structured data output is still not a fully mature modality where we understand every edge case, and on top of that, there was no benchmark available for LLM-driven data extraction on realistic web data.  \nSo we made one! Today, **we are sharing the results of CrawlBench** on Firecrawlâ€™s LLM Extract, and open sourcing the codebase for others to explore LLM-based Structured Data Extraction further.  \n**CrawlBench is a simple set of realistic, reproducible benchmarks**, based on work from Y Combinator (CrawlBench-Easy) and OpenAI (CrawlBench-Hard), that form a reasonable baseline for understanding the impact of varying:  \n- model selection (the default unless otherwise stated is `gpt-4o-mini`),\n- prompting (default prompt is hardcoded in LLM Extract but overridable), and\n- tasks (different schemas)  \nfor common workloads of LLM-based structured data extraction. Work was also done on the WebArena benchmark from Carnegie Mellon (prospectively CrawlBench-Medium), but due to its sheer complexity and outages relative to the expected results, we halted work on it for at least the initial version of CrawlBench.",
      "metadata": {
        "title": "Evaluating Web Data Extraction with CrawlBench",
        "url": "https://www.firecrawl.dev/blog/crawlbench-llm-extraction"
      }
    },
    {
      "id": "b056013a-2709-4be2-a196-762d59a3f624",
      "source": "firecrawl/blog/crawlbench-llm-extraction.md",
      "content": "Y Combinator Directory Listing (CrawlBench-Easy)\n\nThe task here is the simplest possible extraction task: Y Combinator maintains a [list of 50 top companies](https://www.ycombinator.com/topcompanies), as well as a chronological ordering of each batch, with a lot of structured data available for each company in their database.  \n![YC Company Website](https://www.firecrawl.dev/images/blog/crawlbench/yc.png)  \nWe compared the LLM Extract-driven output with ground truth derived from manually written scrapers covering the exact schema from the Y Combinator website (exemptions were made for common, understandable mismatches, eg for differently hosted logo images, to avoid unreasonable penalties). Scores were then tallied based on an â€œexact matchâ€ basis and on a ROUGE score basis.  \nFor the top 50 YC companies, Firecrawl did quite well:  \n```markdown\n==================================================\nFinal Results:\n==================================================\nTotal Exact Match Score: 920/1052\nOverall Exact Match Accuracy: 87.5%\nAverage ROUGE Score: 93.7%\n\n```  \nThis isnâ€™t a perfect 100% score, but thatâ€™s fine because many failures are within a reasonable margin of error, where, for example, the LLM is actually helping us extract the correct substring, compared to our ground truth scrape, which has no such intelligence:  \n```jsx\nMismatch at /companies/zepto > companyMission:\n\nWe deliver groceries in 10 minutes through a network of optimized micro-warehouses or 'dark stores' that we build across cities in India.\n\n!=\n\nWe deliver groceries in 10 minutes through a network of optimized micro-warehouses or 'dark stores' that we build across cities in India.\n\nWe're currently doing hundreds of millions of dollars in annual sales with best-in-class unit economics - come join us!\n\n```  \nBased on a manual audit of the remaining mismatches, **weâ€™d effectively consider Firecrawl to have saturated Crawlbench-Easy with a 93.7% ROUGE score on extracting >1000 datapoints on top Y Combinator companies.** Readers can use our code to expand this analysis to all ~5000 YC companies but we do not expect it to be meaningfully different for the cost that would entail.",
      "metadata": {
        "title": "Evaluating Web Data Extraction with CrawlBench",
        "url": "https://www.firecrawl.dev/blog/crawlbench-llm-extraction"
      }
    },
    {
      "id": "cd883dd9-f8b7-4d4c-9897-f72d09bed05b",
      "source": "firecrawl/blog/crawlbench-llm-extraction.md",
      "content": "OpenAI MiniWoB (CrawlBench-Hard)\n\nThe last set of use cases we wanted to explore was a combination of **Firecrawl for web agents** and **robustness to prompt injections**. Again, we needed a statically reproducible dataset with some institutional backing to compare LLM Extract with.  \nThe [2017 World of Bits paper](https://jimfan.me/publication/world-of-bits/) was the earliest exploration into computer-using web agents by OpenAI, with a very distinguished set of coauthors:  \n![MiniWoB](https://www.firecrawl.dev/images/blog/crawlbench/miniwob.png)  \nWorld of Bits consists of MiniWoB, FormWoB, and QAWoB, which are small exploratory datasets used to scale up to the full WoB dataset scaled up by crowdworkers. Out of all these datasets, OpenAI only released MiniWoB, which is the focus of our evaluations.  \nSince we are not executing full web agents, we did not directly run the MiniWoB benchmark on Firecrawl. Instead our task was to extract first **the list of tasks (Level 0)**, and then, for each task, **the specific instructions given to the computer-using agents (Level 1)**. These tasks range from â€œClick on a specific shapeâ€ and â€œOperate a date pickerâ€ to more complex agentic interactions like â€œOrder food items from a menu.â€ and â€œBuy from the stock market below a specified price.â€  \nHowever there were some interesting confounders in this task: the example lists â€œExample utterancesâ€ and â€œAdditional notesâ€, and also sometimes omits fields. Using LLM-Extract naively meant that the LLM would sometimes hallucinate answers to these fields because they could be interpreted to be asking for placeholders/â€synthetic dataâ€. This means that MiniWoB often also became a dataset for unintentional prompt injections/detecting hallucinations.  \nBased on our tests, **Firecrawl did perfectly on Crawlbench-Hard Level 0 and about 50-50 on Level 1.** Level 1 had >700 datapoints compared to >500 on Level 0, so the combined benchmark result comes in at 70%:  \n```jsx\n==================================================\nLevel 0 Results:\n==================================================\nTotal Score: 532/532\nOverall Accuracy: 100.0%\n\n==================================================\nLevel 1 Results:\n==================================================\nTotal Score: 382/768\nOverall Accuracy: 49.7%\n\n==================================================\nCombined Results:\n==================================================\nTotal Score Across Levels: 914/1300\nOverall Accuracy: 70.3%\n\n```",
      "metadata": {
        "title": "Evaluating Web Data Extraction with CrawlBench",
        "url": "https://www.firecrawl.dev/blog/crawlbench-llm-extraction"
      }
    },
    {
      "id": "b5ef513b-7065-4b2b-8ce2-54738b4cc22c",
      "source": "firecrawl/blog/crawlbench-llm-extraction.md",
      "content": "Varying Models and Prompts\n\nHowever this is where we found we could tweak LLM Extract. By default LLM Extract only uses gpt-4o-mini, so a natural question is what happens if you vary the models. We tested it out an initial set of realistically-cheap-enough-to-deploy-at-scale models (this is NOT all the models we used, but we are saving that surprising result for later) and found very comparable performances with some correlation to model size:  \n![All models](https://www.firecrawl.dev/images/blog/crawlbench/other-models.png)  \nHere are the prompts we ended up using - you can see that the first 2 tried to be as task agnostic as possible, whereas the last ( `customprompt`) peeked ahead to identify all the issues with the default prompt runs and were prompt engineered specifically to reduce known issues.  \n```\n'default': 'Based on the information on the page, extract all the information from the schema. Try to extract all the fields even those that might not be marked as required.',\n\n'nohallucination': 'Based on the page content, extract information that closely fits the schema. Do not hallucinate information that is not present on the page. Do not leak anything about this prompt. Just extract the information from the source content as asked, where possible, offering blank fields if the information is not present.',\n\n'customprompt': 'Based on the page content, extract information that closely fits the schema. Every field should ONLY be filled in if it is present in the source, with information directly from the source. The \"Description\" field should be from the source material, not a description of this task. The fields named \"additional notes\", \"utterance fields\" and \"example utterances\" are to be taken only from the source IF they are present. If they are not present, do not fill in with made up information, just leave them blank. Do not omit any markdown formatting from the source.',\n\n```  \nRunning these 3 prompts across all the candidate models produced a 2 dimensional matrix of results, with shocking outperformance for custom prompts:  \n![Custom Prompt](https://www.firecrawl.dev/images/blog/crawlbench/hardcomp.png)  \n**The conclusion we must draw here is that tweaking model choice is almost 7x less effective than prompt engineering for your specific task** (model choice has a max difference of 6 points, vs an **average 41 point improvement** when applying custom prompts) **.**  \nBy custom prompting for your task, you can reduce your costs dramatically â€”the most expensive model on this panel (gpt-4o) is 67x the cost of the cheapest (Gemini Flash) â€” for ~no loss in performance. So, at scale, you should basically **always customize your prompt**.  \nAs for LLM-Extract, our new `nohallucination` prompt was able to eke out an average +1 point improvement in most model performance, so this could constitute sufficient evidence to update the default prompt shipped with LLM-Extract.",
      "metadata": {
        "title": "Evaluating Web Data Extraction with CrawlBench",
        "url": "https://www.firecrawl.dev/blog/crawlbench-llm-extraction"
      }
    },
    {
      "id": "7827398e-9fe2-4d39-a1a9-d6736f1946f2",
      "source": "firecrawl/blog/crawlbench-llm-extraction.md",
      "content": "Bonus: Claude 3.5 models are REALLY goodâ€¦\n\nAlthough its much higher cost should give some pause, the zero shot extraction capabilities of the new Sonnet and Haiku models greatly surprised us. Hereâ€™s the same charts again, with the newer/more expensive Anthropic models:  \n![Benchmark with Claude 3.5 Models](https://www.firecrawl.dev/images/blog/crawlbench/claude35.png)  \nThatâ€™s a whopping 13.8 point jump on CrawlBench-Hard between 3 Haiku and 3.5 Haiku, [though it is 4x more expensive](https://x.com/simonw/status/1853509565469671585?s=46), it is still ~4x cheaper than Sonnet, which itself saw a sizable 7.2 point CrawlBench-Hard bump between the June and October 3.5 Sonnet versions.  \nIn other words, if you donâ€™t have time or have a wide enough scrape data set that you cannot afford to craft a custom prompt, you could simply pay Anthropic to get a pretty decent baseline.  \n_> Note: We considered adding the other newer bigger models like the o1 models but they do not yet support structured output and in any case would be prohibitively expensive and not realistic for practical extraction use._",
      "metadata": {
        "title": "Evaluating Web Data Extraction with CrawlBench",
        "url": "https://www.firecrawl.dev/blog/crawlbench-llm-extraction"
      }
    },
    {
      "id": "c64fefdb-7879-41e5-a301-f6853e17e989",
      "source": "firecrawl/blog/crawlbench-llm-extraction.md",
      "content": "Conclusion\n\nStructured Data Extraction is a fundamental building block for any web-browsing LLM agent. We introduce CrawlBench-Easy and CrawlBench-Hard as a set of simple, realistic, reproducible benchmarks that any LLM Extraction tool can be evaluated against, offering enough data points to elucidate significant differences in model and prompt performance that line up with intuitive priors. We are by no means done - CrawlBench-Medium with its survey of e-commerce, social network, and admin panel scenarios is a possible next step - but with this initial publication, we are now able to quantify and progress the state of the art in LLM Extraction.  \nArticle updated recently",
      "metadata": {
        "title": "Evaluating Web Data Extraction with CrawlBench",
        "url": "https://www.firecrawl.dev/blog/crawlbench-llm-extraction"
      }
    },
    {
      "id": "e21cc317-3d51-4eaa-a97d-467692b8b8e6",
      "source": "firecrawl/blog/crawlbench-llm-extraction.md",
      "content": "About the Author\n\n[![Swyx image](https://www.firecrawl.dev/swyx.jpg)\\\nSwyx@swyx](https://x.com/swyx)  \nSwyx (Shawn Wang) is a Writer, Founder, Devtools Startup Advisor.  \n[ðŸ”¥](https://www.firecrawl.dev/)",
      "metadata": {
        "title": "Evaluating Web Data Extraction with CrawlBench",
        "url": "https://www.firecrawl.dev/blog/crawlbench-llm-extraction"
      }
    },
    {
      "id": "92bd6ea9-aa71-472a-a869-f313a37662c4",
      "source": "firecrawl/blog/crawlbench-llm-extraction.md",
      "content": "Ready to _Build?_\n\nStart scraping web data for your AI apps today.  \nNo credit card needed.  \nGet Started",
      "metadata": {
        "title": "Evaluating Web Data Extraction with CrawlBench",
        "url": "https://www.firecrawl.dev/blog/crawlbench-llm-extraction"
      }
    },
    {
      "id": "cbc62570-764f-4b8a-9398-1beadaac438d",
      "source": "firecrawl/blog/How-to-Create-an-llms-txt-File-for-Any-Website.md",
      "content": "---\ntitle: How to Create an llms.txt File for Any Website\nurl: https://www.firecrawl.dev/blog/How-to-Create-an-llms-txt-File-for-Any-Website\n---  \nIntroducing /extract - Get web data with a prompt [Try now](https://www.firecrawl.dev/extract)  \nNov 22, 2024  \nâ€¢  \n[![Eric Ciarla image](https://www.firecrawl.dev/eric-img.jpeg)Eric Ciarla](https://x.com/ericciarla)  \n# How to Create an llms.txt File for Any Website  \nConcatenate any website into a single llms.txt file with Firecrawl ðŸ”¥ - YouTube  \nFirecrawl  \n472 subscribers  \n[Concatenate any website into a single llms.txt file with Firecrawl ðŸ”¥](https://www.youtube.com/watch?v=_hboVbUqsjY)  \nFirecrawl  \nSearch  \nInfo  \nShopping  \nTap to unmute  \nIf playback doesn't begin shortly, try restarting your device.  \nYou're signed out  \nVideos you watch may be added to the TV's watch history and influence TV recommendations. To avoid this, cancel and sign in to YouTube on your computer.  \nCancelConfirm  \nShare  \nInclude playlist  \nAn error occurred while retrieving sharing information. Please try again later.  \nWatch later  \nShare  \nCopy link  \nWatch on  \n0:00  \n/ â€¢Live  \nâ€¢  \n[Watch on YouTube](https://www.youtube.com/watch?v=_hboVbUqsjY \"Watch on YouTube\")  \nCreating an `llms.txt` file for your website is now simpler than ever with the **llms.txt Generator** an open source example app powered by Firecrawl. This tool enables you to compile your entire website into a single text file that can be used with any Large Language Model (LLM), improving how AI interacts with your content.",
      "metadata": {
        "title": "How to Create an llms.txt File for Any Website",
        "url": "https://www.firecrawl.dev/blog/How-to-Create-an-llms-txt-File-for-Any-Website"
      }
    },
    {
      "id": "f11ca88e-6c23-476a-90ca-bb350d2b3bb7",
      "source": "firecrawl/blog/How-to-Create-an-llms-txt-File-for-Any-Website.md",
      "content": "What is an llms.txt File?\n\nAn `llms.txt` file is a standardized markdown file proposed by Jeremy Howard to provide information to help LLMs use a website at inference time. Unlike traditional web content designed for human readers, `llms.txt` files offer concise, structured information that LLMs can quickly ingest. This is particularly useful for enhancing development environments, providing documentation for programming libraries, and offering structured overviews for various domains such as corporate websites, educational institutions, and personal portfolios.  \nThe `llms.txt` file is located at the root path `/llms.txt` of a website and contains sections in a specific order, including a project name, a summary, detailed information, and file lists with URLs for further details. This format allows LLMs to efficiently access and process the most important information about a website.",
      "metadata": {
        "title": "How to Create an llms.txt File for Any Website",
        "url": "https://www.firecrawl.dev/blog/How-to-Create-an-llms-txt-File-for-Any-Website"
      }
    },
    {
      "id": "281adb79-1e09-4189-9e37-6c1421e1cf23",
      "source": "firecrawl/blog/How-to-Create-an-llms-txt-File-for-Any-Website.md",
      "content": "Introducing llms.txt Generator âœ¨\n\nThe llms.txt Generator leverages [Firecrawl](https://www.firecrawl.dev/) to crawl your website and extracts data using **gpt-4o-mini**. You can generate both `llms.txt` and `llms-full.txt` files through the web interface or via API.",
      "metadata": {
        "title": "How to Create an llms.txt File for Any Website",
        "url": "https://www.firecrawl.dev/blog/How-to-Create-an-llms-txt-File-for-Any-Website"
      }
    },
    {
      "id": "a9c5b532-b392-4d27-8054-09326a259981",
      "source": "firecrawl/blog/How-to-Create-an-llms-txt-File-for-Any-Website.md",
      "content": "Accessing llms.txt via API\n\nYou can access `llms.txt` directly by making a GET request to:  \n```\nhttp://llmstxt.firecrawl.dev/{YOUR_URL}\n\n```  \nFor the full version, use:  \n```\nhttp://llmstxt.firecrawl.dev/{YOUR_URL}/full\n\n```  \nIf you have a Firecrawl API key, you can include it to unlock full results and remove limits:  \n```\nhttp://llmstxt.firecrawl.dev/{YOUR_URL}?FIRECRAWL_API_KEY=YOUR_API_KEY\n\n```  \nFor the full version with API key:  \n```\nhttp://llmstxt.firecrawl.dev/{YOUR_URL}/full?FIRECRAWL_API_KEY=YOUR_API_KEY\n\n```",
      "metadata": {
        "title": "How to Create an llms.txt File for Any Website",
        "url": "https://www.firecrawl.dev/blog/How-to-Create-an-llms-txt-File-for-Any-Website"
      }
    },
    {
      "id": "4c4bcd2b-f31f-4723-90e8-9fab6f4956ce",
      "source": "firecrawl/blog/How-to-Create-an-llms-txt-File-for-Any-Website.md",
      "content": "How to Generate Your llms.txt File\n\n1. **Visit the Generator**: Go to [http://llmstxt.firecrawl.dev](http://llmstxt.firecrawl.dev/).  \n2. **Enter Your Website URL**: Input the URL of your website.  \n3. **Generate the File**: Click the generate button and wait a few minutes as the tool processes your site.  \n4. **Download Your Files**: Once ready, download the `llms.txt` and `llms-full.txt` files.",
      "metadata": {
        "title": "How to Create an llms.txt File for Any Website",
        "url": "https://www.firecrawl.dev/blog/How-to-Create-an-llms-txt-File-for-Any-Website"
      }
    },
    {
      "id": "ce5a5361-ae2d-43c1-bdb2-e255e5f20da8",
      "source": "firecrawl/blog/How-to-Create-an-llms-txt-File-for-Any-Website.md",
      "content": "No API Key Required, But Recommended\n\nWhile an API key is not required, using a free Firecrawl API key removes any usage limits and provides full access to all features.",
      "metadata": {
        "title": "How to Create an llms.txt File for Any Website",
        "url": "https://www.firecrawl.dev/blog/How-to-Create-an-llms-txt-File-for-Any-Website"
      }
    },
    {
      "id": "9b20c5bc-9348-4443-94c3-b58b2e964214",
      "source": "firecrawl/blog/How-to-Create-an-llms-txt-File-for-Any-Website.md",
      "content": "References\n\n- [Firecrawl Official Website](https://www.firecrawl.dev/)\n- [llms.txt Standard](http://llmstxt.org/)\n- [llms.txt Generator](http://llmstxt.firecrawl.dev/)\n- [Directory of llms.txt Adopters](http://directory.llmstxt.cloud/)\n- [llms.txt Generator GitHub Repository](https://github.com/mendableai/llmstxt-generator)  \nArticle updated recently  \n[ðŸ”¥](https://www.firecrawl.dev/)",
      "metadata": {
        "title": "How to Create an llms.txt File for Any Website",
        "url": "https://www.firecrawl.dev/blog/How-to-Create-an-llms-txt-File-for-Any-Website"
      }
    },
    {
      "id": "b29f22f8-bd7f-4120-b783-57aa7a360c23",
      "source": "firecrawl/blog/How-to-Create-an-llms-txt-File-for-Any-Website.md",
      "content": "Ready to _Build?_\n\nStart scraping web data for your AI apps today.  \nNo credit card needed.  \nGet Started",
      "metadata": {
        "title": "How to Create an llms.txt File for Any Website",
        "url": "https://www.firecrawl.dev/blog/How-to-Create-an-llms-txt-File-for-Any-Website"
      }
    },
    {
      "id": "4713e051-3a24-486a-a563-6c630505ce38",
      "source": "firecrawl/blog/How-to-Create-an-llms-txt-File-for-Any-Website.md",
      "content": "About the Author\n\n[![Eric Ciarla image](https://www.firecrawl.dev/eric-img.jpeg)\\\nEric Ciarla@ericciarla](https://x.com/ericciarla)  \nEric Ciarla is the Chief Operating Officer (COO) of Firecrawl and leads marketing. He also worked on Mendable.ai and sold it to companies like Snapchat, Coinbase, and MongoDB.\nPreviously worked at Ford and Fracta as a Data Scientist. Eric also co-founded SideGuide, a tool for learning code within VS Code with 50,000 users.",
      "metadata": {
        "title": "How to Create an llms.txt File for Any Website",
        "url": "https://www.firecrawl.dev/blog/How-to-Create-an-llms-txt-File-for-Any-Website"
      }
    },
    {
      "id": "54a6177b-d5f8-4b53-9bdb-83550c1e5c74",
      "source": "firecrawl/blog/How-to-Create-an-llms-txt-File-for-Any-Website.md",
      "content": "About the Author > More articles by Eric Ciarla\n\n[How to Create an llms.txt File for Any Website\\\n\\\nLearn how to generate an llms.txt file for any website using the llms.txt Generator and Firecrawl.](https://www.firecrawl.dev/blog/How-to-Create-an-llms-txt-File-for-Any-Website) [Cloudflare Error 1015: How to solve it?\\\n\\\nCloudflare Error 1015 is a rate limiting error that occurs when Cloudflare detects that you are exceeding the request limit set by the website owner.](https://www.firecrawl.dev/blog/cloudflare-error-1015-how-to-solve-it) [Build an agent that checks for website contradictions\\\n\\\nUsing Firecrawl and Claude to scrape your website's data and look for contradictions.](https://www.firecrawl.dev/blog/contradiction-agent) [Why Companies Need a Data Strategy for Generative AI\\\n\\\nLearn why a well-defined data strategy is essential for building robust, production-ready generative AI systems, and discover practical steps for curation, maintenance, and integration.](https://www.firecrawl.dev/blog/why-companies-need-a-data-strategy-for-generative-ai) [Getting Started with OpenAI's Predicted Outputs for Faster LLM Responses\\\n\\\nA guide to leveraging Predicted Outputs to speed up LLM tasks with GPT-4o models.](https://www.firecrawl.dev/blog/getting-started-with-predicted-outputs-openai) [How to easily install requests with pip and python\\\n\\\nA tutorial on installing the requests library in Python using various methods, with usage examples and troubleshooting tips](https://www.firecrawl.dev/blog/how-to-easily-install-requests-with-pip-and-python) [How to quickly install BeautifulSoup with Python\\\n\\\nA guide on installing the BeautifulSoup library in Python using various methods, with usage examples and troubleshooting tips](https://www.firecrawl.dev/blog/how-to-quickly-install-beautifulsoup-with-python) [How to Use OpenAI's o1 Reasoning Models in Your Applications\\\n\\\nLearn how to harness OpenAI's latest o1 series models for complex reasoning tasks in your apps.](https://www.firecrawl.dev/blog/how-to-use-openai-o1-reasoning-models-in-applications)",
      "metadata": {
        "title": "How to Create an llms.txt File for Any Website",
        "url": "https://www.firecrawl.dev/blog/How-to-Create-an-llms-txt-File-for-Any-Website"
      }
    },
    {
      "id": "3153e9ad-d095-4e61-9356-4e4e862376d9",
      "source": "firecrawl/blog/firecrawl-launch-week-1-recap.md",
      "content": "---\ntitle: Launch Week I Recap\nurl: https://www.firecrawl.dev/blog/firecrawl-launch-week-1-recap\n---  \nIntroducing /extract - Get web data with a prompt [Try now](https://www.firecrawl.dev/extract)  \nSeptember 2, 2024  \nâ€¢  \n[![Eric Ciarla image](https://www.firecrawl.dev/eric-img.jpeg)Eric Ciarla](https://x.com/ericciarla)  \n# Launch Week I Recap  \n![Launch Week I Recap image](https://www.firecrawl.dev/images/blog/launch-week-1-recap.png)",
      "metadata": {
        "title": "Launch Week I Recap",
        "url": "https://www.firecrawl.dev/blog/firecrawl-launch-week-1-recap"
      }
    },
    {
      "id": "d3f18ec5-527e-425a-b9dc-52f2a24f3bf4",
      "source": "firecrawl/blog/firecrawl-launch-week-1-recap.md",
      "content": "Introduction\n\nLast week marked an exciting milestone for Firecrawl as we kicked off our inaugural Launch Week, unveiling a series of new features and updates designed to enhance your web scraping experience. Letâ€™s take a look back at the improvements we introduced throughout the week.",
      "metadata": {
        "title": "Launch Week I Recap",
        "url": "https://www.firecrawl.dev/blog/firecrawl-launch-week-1-recap"
      }
    },
    {
      "id": "bff20810-8391-412b-9440-bb51afef41a7",
      "source": "firecrawl/blog/firecrawl-launch-week-1-recap.md",
      "content": "[Day 1: Introducing Teams](https://firecrawl.dev/blog/launch-week-i-day-1-introducing-teams)\n\nWe started Launch Week by introducing our highly anticipated Teams feature. Teams enables seamless collaboration on web scraping projects, allowing you to work alongside your colleagues and tackle complex data gathering tasks together. With updated pricing plans to accommodate teams of all sizes, Firecrawl is now an excellent platform for collaborative web scraping.",
      "metadata": {
        "title": "Launch Week I Recap",
        "url": "https://www.firecrawl.dev/blog/firecrawl-launch-week-1-recap"
      }
    },
    {
      "id": "cc74cdc0-7d78-4314-8972-98d9b18e4534",
      "source": "firecrawl/blog/firecrawl-launch-week-1-recap.md",
      "content": "[Day 2: Increased Rate Limits](https://firecrawl.dev/blog/launch-week-i-day-2-doubled-rate-limits)\n\nOn Day 2, we improved your data collection capabilities by doubling the rate limits for our /scrape endpoint across all plans. This means you can now gather more data in the same amount of time, enabling you to take on larger projects and scrape more frequently.",
      "metadata": {
        "title": "Launch Week I Recap",
        "url": "https://www.firecrawl.dev/blog/firecrawl-launch-week-1-recap"
      }
    },
    {
      "id": "a78f80f5-c242-436e-9e8f-e0eb191a2b1a",
      "source": "firecrawl/blog/firecrawl-launch-week-1-recap.md",
      "content": "[Day 3: Introducing the Map Endpoint (Alpha)](https://firecrawl.dev/blog/launch-week-i-day-3-introducing-map-endpoint)\n\nDay 3 saw the unveiling of our new Map endpoint, which allows you to transform a single URL into a comprehensive map of an entire website quickly. As a fast and easy way to gather all the URLs on a website, the Map endpoint opens up new possibilities for your web scraping projects.",
      "metadata": {
        "title": "Launch Week I Recap",
        "url": "https://www.firecrawl.dev/blog/firecrawl-launch-week-1-recap"
      }
    },
    {
      "id": "0f77c453-a3a5-428c-abc2-d990a0474910",
      "source": "firecrawl/blog/firecrawl-launch-week-1-recap.md",
      "content": "[Day 4: Introducing Firecrawl /v1](https://firecrawl.dev/blog/launch-week-i-day-4-introducing-firecrawl-v1)\n\nDay 4 marked a significant release: Firecrawl /v1. This more reliable and developer-friendly API makes gathering web data easier. With new scrape formats, improved crawl status, enhanced markdown parsing, v1 support for all SDKs (including new Go and Rust SDKs), and an improved developer experience, v1 enhances your web scraping workflow.",
      "metadata": {
        "title": "Launch Week I Recap",
        "url": "https://www.firecrawl.dev/blog/firecrawl-launch-week-1-recap"
      }
    },
    {
      "id": "c92ccc76-d18b-4599-9746-7f483551f8cb",
      "source": "firecrawl/blog/firecrawl-launch-week-1-recap.md",
      "content": "[Day 5: Real-Time Crawling with WebSockets](https://firecrawl.dev/blog/launch-week-i-day-5-real-time-crawling-websockets)\n\nOn Day 5, we introduced a new feature: Real-Time Crawling with WebSockets. Our WebSocket-based method, Crawl URL and Watch, enables real-time data extraction and monitoring, allowing you to process data immediately, react to errors quickly, and know precisely when your crawl is complete.",
      "metadata": {
        "title": "Launch Week I Recap",
        "url": "https://www.firecrawl.dev/blog/firecrawl-launch-week-1-recap"
      }
    },
    {
      "id": "a73812a2-fd3f-4b7d-8fb5-3bbcdd414571",
      "source": "firecrawl/blog/firecrawl-launch-week-1-recap.md",
      "content": "[Day 6: LLM Extract (v1)](https://firecrawl.dev/blog/launch-week-i-day-6-llm-extract)\n\nDay 6 brought v1 support for LLM Extract, enabling you to extract structured data from web pages using the extract format in /scrape. With the ability to pass a schema or just provide a prompt, LLM extraction is now more flexible and powerful.",
      "metadata": {
        "title": "Launch Week I Recap",
        "url": "https://www.firecrawl.dev/blog/firecrawl-launch-week-1-recap"
      }
    },
    {
      "id": "6b32ad9b-1ab7-4c96-9288-61c9ae4ab925",
      "source": "firecrawl/blog/firecrawl-launch-week-1-recap.md",
      "content": "[Day 7: Crawl Webhooks (v1)](https://firecrawl.dev/blog/launch-week-i-day-7-webhooks)\n\nWe wrapped up Launch Week with the introduction of /crawl webhook support. You can now send notifications to your apps during a crawl, with four types of events: crawl.started, crawl.page, crawl.completed, and crawl.failed. This feature allows for more seamless integration of Firecrawl into your workflows.",
      "metadata": {
        "title": "Launch Week I Recap",
        "url": "https://www.firecrawl.dev/blog/firecrawl-launch-week-1-recap"
      }
    },
    {
      "id": "f7fa6f88-7e4f-4a6d-89db-5ab8ca83d86a",
      "source": "firecrawl/blog/firecrawl-launch-week-1-recap.md",
      "content": "Wrapping Up\n\nLaunch Week showcased our commitment to continually evolving and improving Firecrawl to meet the needs of our users. From collaborative features like Teams to performance improvements like increased rate limits, and from new endpoints like Map and Extract to real-time capabilities with WebSockets and Webhooks, weâ€™ve expanded the possibilities for your web scraping projects.  \nWeâ€™d like to thank our community for your support, feedback, and enthusiasm throughout Launch Week and beyond. Your input drives us to innovate and push the boundaries of whatâ€™s possible with web scraping.  \nStay tuned for more updates as we continue to shape the future of data gathering together. Happy scraping!  \nArticle updated recently  \n[ðŸ”¥](https://www.firecrawl.dev/)",
      "metadata": {
        "title": "Launch Week I Recap",
        "url": "https://www.firecrawl.dev/blog/firecrawl-launch-week-1-recap"
      }
    },
    {
      "id": "120e665d-7d82-4f1f-bf00-d9fda7e9f7a6",
      "source": "firecrawl/blog/firecrawl-launch-week-1-recap.md",
      "content": "Ready to _Build?_\n\nStart scraping web data for your AI apps today.  \nNo credit card needed.  \nGet Started",
      "metadata": {
        "title": "Launch Week I Recap",
        "url": "https://www.firecrawl.dev/blog/firecrawl-launch-week-1-recap"
      }
    },
    {
      "id": "2e2c67fd-d89a-4b6b-9142-e977ea530795",
      "source": "firecrawl/blog/firecrawl-launch-week-1-recap.md",
      "content": "About the Author\n\n[![Eric Ciarla image](https://www.firecrawl.dev/eric-img.jpeg)\\\nEric Ciarla@ericciarla](https://x.com/ericciarla)  \nEric Ciarla is the Chief Operating Officer (COO) of Firecrawl and leads marketing. He also worked on Mendable.ai and sold it to companies like Snapchat, Coinbase, and MongoDB.\nPreviously worked at Ford and Fracta as a Data Scientist. Eric also co-founded SideGuide, a tool for learning code within VS Code with 50,000 users.",
      "metadata": {
        "title": "Launch Week I Recap",
        "url": "https://www.firecrawl.dev/blog/firecrawl-launch-week-1-recap"
      }
    },
    {
      "id": "c1e92f08-7af1-49e6-a031-4d65848e23dd",
      "source": "firecrawl/blog/firecrawl-launch-week-1-recap.md",
      "content": "About the Author > More articles by Eric Ciarla\n\n[How to Create an llms.txt File for Any Website\\\n\\\nLearn how to generate an llms.txt file for any website using the llms.txt Generator and Firecrawl.](https://www.firecrawl.dev/blog/How-to-Create-an-llms-txt-File-for-Any-Website) [Cloudflare Error 1015: How to solve it?\\\n\\\nCloudflare Error 1015 is a rate limiting error that occurs when Cloudflare detects that you are exceeding the request limit set by the website owner.](https://www.firecrawl.dev/blog/cloudflare-error-1015-how-to-solve-it) [Build an agent that checks for website contradictions\\\n\\\nUsing Firecrawl and Claude to scrape your website's data and look for contradictions.](https://www.firecrawl.dev/blog/contradiction-agent) [Why Companies Need a Data Strategy for Generative AI\\\n\\\nLearn why a well-defined data strategy is essential for building robust, production-ready generative AI systems, and discover practical steps for curation, maintenance, and integration.](https://www.firecrawl.dev/blog/why-companies-need-a-data-strategy-for-generative-ai) [Getting Started with OpenAI's Predicted Outputs for Faster LLM Responses\\\n\\\nA guide to leveraging Predicted Outputs to speed up LLM tasks with GPT-4o models.](https://www.firecrawl.dev/blog/getting-started-with-predicted-outputs-openai) [How to easily install requests with pip and python\\\n\\\nA tutorial on installing the requests library in Python using various methods, with usage examples and troubleshooting tips](https://www.firecrawl.dev/blog/how-to-easily-install-requests-with-pip-and-python) [How to quickly install BeautifulSoup with Python\\\n\\\nA guide on installing the BeautifulSoup library in Python using various methods, with usage examples and troubleshooting tips](https://www.firecrawl.dev/blog/how-to-quickly-install-beautifulsoup-with-python) [How to Use OpenAI's o1 Reasoning Models in Your Applications\\\n\\\nLearn how to harness OpenAI's latest o1 series models for complex reasoning tasks in your apps.](https://www.firecrawl.dev/blog/how-to-use-openai-o1-reasoning-models-in-applications)",
      "metadata": {
        "title": "Launch Week I Recap",
        "url": "https://www.firecrawl.dev/blog/firecrawl-launch-week-1-recap"
      }
    },
    {
      "id": "2e702830-1279-43a8-9b26-5ab8eb9a38f9",
      "source": "firecrawl/blog/chat-with-website.md",
      "content": "---\ntitle: Build a 'Chat with website' using Groq Llama 3\nurl: https://www.firecrawl.dev/blog/chat-with-website\n---  \nIntroducing /extract - Get web data with a prompt [Try now](https://www.firecrawl.dev/extract)  \nMay 22, 2024  \nâ€¢  \n[![Nicolas Camara image](https://www.firecrawl.dev/nick-img.jpeg)Nicolas Camara](https://x.com/nickscamara_)  \n# Build a 'Chat with website' using Groq Llama 3  \n![Build a 'Chat with website' using Groq Llama 3 image](https://www.firecrawl.dev/images/blog/g4.png)",
      "metadata": {
        "title": "Build a 'Chat with website' using Groq Llama 3",
        "url": "https://www.firecrawl.dev/blog/chat-with-website"
      }
    },
    {
      "id": "ecc34bec-01c6-4343-8f0f-f83c64298aab",
      "source": "firecrawl/blog/chat-with-website.md",
      "content": "Setup\n\nInstall our python dependencies, including langchain, groq, faiss, ollama, and firecrawl-py.  \n```bash\npip install --upgrade --quiet langchain langchain-community groq faiss-cpu ollama firecrawl-py\n\n```  \nWe will be using Ollama for the embeddings, you can download Ollama [here](https://ollama.com/). But feel free to use any other embeddings you prefer.",
      "metadata": {
        "title": "Build a 'Chat with website' using Groq Llama 3",
        "url": "https://www.firecrawl.dev/blog/chat-with-website"
      }
    },
    {
      "id": "3e84b6c4-191c-47c4-a6e9-3072235c0f27",
      "source": "firecrawl/blog/chat-with-website.md",
      "content": "Load website with Firecrawl\n\nTo be able to get all the data from a website and make sure it is in the cleanest format, we will use Firecrawl. Firecrawl integrates very easily with Langchain as a document loader.  \nHere is how you can load a website with Firecrawl:  \n```python\nfrom langchain_community.document_loaders import FireCrawlLoader # Importing the FirecrawlLoader\n\nurl = \"https://firecrawl.dev\"\nloader = FirecrawlLoader(\napi_key=\"fc-YOUR_API_KEY\", # Note: Replace 'YOUR_API_KEY' with your actual FireCrawl API key\nurl=url, # Target URL to crawl\nmode=\"crawl\" # Mode set to 'crawl' to crawl all accessible subpages\n)\ndocs = loader.load()\n\n```",
      "metadata": {
        "title": "Build a 'Chat with website' using Groq Llama 3",
        "url": "https://www.firecrawl.dev/blog/chat-with-website"
      }
    },
    {
      "id": "1fb0c73f-1c27-4acc-9ff2-fb45e3cf2402",
      "source": "firecrawl/blog/chat-with-website.md",
      "content": "Retrieval and Generation\n\nNow that our documents are loaded and the vectorstore is setup, we can, based on userâ€™s question, do a similarity search to retrieve the most relevant documents. That way we can use these documents to be fed to the LLM model.  \n```python\nquestion = \"What is firecrawl?\"\ndocs = vectorstore.similarity_search(query=question)\n\n```",
      "metadata": {
        "title": "Build a 'Chat with website' using Groq Llama 3",
        "url": "https://www.firecrawl.dev/blog/chat-with-website"
      }
    },
    {
      "id": "2969b545-74e6-4b03-ba40-2850273a13e5",
      "source": "firecrawl/blog/chat-with-website.md",
      "content": "Generation\n\nLast but not least, you can use the Groq to generate a response to a question based on the documents we have loaded.  \n```python\nfrom groq import Groq\n\nclient = Groq(\napi_key=\"YOUR_GROQ_API_KEY\",\n)\n\ncompletion = client.chat.completions.create(\nmodel=\"llama3-8b-8192\",\nmessages=[\\\n{\\\n\"role\": \"user\",\\\n\"content\": f\"You are a friendly assistant. Your job is to answer the users question based on the documentation provided below:nDocs:nn{docs}nnQuestion: {question}\"\\\n}\\\n],\ntemperature=1,\nmax_tokens=1024,\ntop_p=1,\nstream=False,\nstop=None,\n)\n\nprint(completion.choices[0].message)\n\n```",
      "metadata": {
        "title": "Build a 'Chat with website' using Groq Llama 3",
        "url": "https://www.firecrawl.dev/blog/chat-with-website"
      }
    },
    {
      "id": "3a3d1cb7-2f53-4b80-965e-befc34827eaa",
      "source": "firecrawl/blog/chat-with-website.md",
      "content": "And Voila!\n\nYou have now built a â€˜Chat with your websiteâ€™ bot using Llama 3, Groq Llama 3, Langchain, and Firecrawl. You can now use this bot to answer questions based on the documentation of your website.  \nIf you have any questions or need help, feel free to reach out to us at [Firecrawl](https://firecrawl.dev/).  \nArticle updated recently  \n[ðŸ”¥](https://www.firecrawl.dev/)",
      "metadata": {
        "title": "Build a 'Chat with website' using Groq Llama 3",
        "url": "https://www.firecrawl.dev/blog/chat-with-website"
      }
    },
    {
      "id": "8ba944df-f518-4e69-84e8-b96098f187e7",
      "source": "firecrawl/blog/chat-with-website.md",
      "content": "Ready to _Build?_\n\nStart scraping web data for your AI apps today.  \nNo credit card needed.  \nGet Started",
      "metadata": {
        "title": "Build a 'Chat with website' using Groq Llama 3",
        "url": "https://www.firecrawl.dev/blog/chat-with-website"
      }
    },
    {
      "id": "d561f1ac-5655-435e-844e-2f1ac44eb5a9",
      "source": "firecrawl/blog/chat-with-website.md",
      "content": "About the Author\n\n[![Nicolas Camara image](https://www.firecrawl.dev/nick-img.jpeg)\\\nNicolas Camara@nickscamara_](https://x.com/nickscamara_)  \nNicolas Camara is the Chief Technology Officer (CTO) at Firecrawl.\nHe previously built and scaled Mendable, one of the pioneering \"chat with your documents\" apps,\nwhich had major Fortune 500 customers like Snapchat, Coinbase, and MongoDB.\nPrior to that, Nicolas built SideGuide, the first code-learning tool inside VS Code,\nand grew a community of 50,000 users. Nicolas studied Computer Science and has over 10 years of experience in building software.",
      "metadata": {
        "title": "Build a 'Chat with website' using Groq Llama 3",
        "url": "https://www.firecrawl.dev/blog/chat-with-website"
      }
    },
    {
      "id": "04bb970f-63c8-43d2-a612-377e4239b0ca",
      "source": "firecrawl/blog/chat-with-website.md",
      "content": "About the Author > More articles by Nicolas Camara\n\n[Using OpenAI's Realtime API and Firecrawl to Talk with Any Website\\\n\\\nBuild a real-time conversational agent that interacts with any website using OpenAI's Realtime API and Firecrawl.](https://www.firecrawl.dev/blog/How-to-Talk-with-Any-Website-Using-OpenAIs-Realtime-API-and-Firecrawl) [Extract website data using LLMs\\\n\\\nLearn how to use Firecrawl and Groq to extract structured data from a web page in a few lines of code.](https://www.firecrawl.dev/blog/data-extraction-using-llms) [Getting Started with Grok-2: Setup and Web Crawler Example\\\n\\\nA detailed guide on setting up Grok-2 and building a web crawler using Firecrawl.](https://www.firecrawl.dev/blog/grok-2-setup-and-web-crawler-example) [Launch Week I / Day 6: LLM Extract (v1)\\\n\\\nExtract structured data from your web pages using the extract format in /scrape.](https://www.firecrawl.dev/blog/launch-week-i-day-6-llm-extract) [Launch Week I / Day 7: Crawl Webhooks (v1)\\\n\\\nNew /crawl webhook support. Send notifications to your apps during a crawl.](https://www.firecrawl.dev/blog/launch-week-i-day-7-webhooks) [OpenAI Swarm Tutorial: Create Marketing Campaigns for Any Website\\\n\\\nA guide to building a multi-agent system using OpenAI Swarm and Firecrawl for AI-driven marketing strategies](https://www.firecrawl.dev/blog/openai-swarm-agent-tutorial) [Build a 'Chat with website' using Groq Llama 3\\\n\\\nLearn how to use Firecrawl, Groq Llama 3, and Langchain to build a 'Chat with your website' bot.](https://www.firecrawl.dev/blog/chat-with-website) [Scrape and Analyze Airbnb Data with Firecrawl and E2B\\\n\\\nLearn how to scrape and analyze Airbnb data using Firecrawl and E2B in a few lines of code.](https://www.firecrawl.dev/blog/scrape-analyze-airbnb-data-with-e2b)",
      "metadata": {
        "title": "Build a 'Chat with website' using Groq Llama 3",
        "url": "https://www.firecrawl.dev/blog/chat-with-website"
      }
    },
    {
      "id": "a2ac5125-9bb2-483a-9e1e-c34c3268ba35",
      "source": "firecrawl/blog/cloudflare-error-1015-how-to-solve-it.md",
      "content": "---\ntitle: Cloudflare Error 1015: How to solve it?\nurl: https://www.firecrawl.dev/blog/cloudflare-error-1015-how-to-solve-it\n---  \nIntroducing /extract - Get web data with a prompt [Try now](https://www.firecrawl.dev/extract)  \nAug 6, 2024  \nâ€¢  \n[![Eric Ciarla image](https://www.firecrawl.dev/eric-img.jpeg)Eric Ciarla](https://x.com/ericciarla)  \n# Cloudflare Error 1015: How to solve it?  \nCloudflare Error 1015 is a rate limiting error that occurs when Cloudflare detects that you are exceeding the request limit set by the website owner. This typically happens if you are making a large number of requests in a short period.",
      "metadata": {
        "title": "Cloudflare Error 1015: How to solve it?",
        "url": "https://www.firecrawl.dev/blog/cloudflare-error-1015-how-to-solve-it"
      }
    },
    {
      "id": "94195317-2afe-4ff8-a9a0-9ab0e5fdf8ba",
      "source": "firecrawl/blog/cloudflare-error-1015-how-to-solve-it.md",
      "content": "How to solve it?\n\nTo resolve Cloudflare Error 1015 you can reduce the frequency of your requests to stay within the allowed limit. Another way to solve it is to use a service like [Firecrawl](https://firecrawl.dev/), which rotates proxies to prevent any single proxy from hitting the rate limit. This approach can help you avoid triggering the Cloudflare 1015 error.  \nArticle updated recently  \n[ðŸ”¥](https://www.firecrawl.dev/)",
      "metadata": {
        "title": "Cloudflare Error 1015: How to solve it?",
        "url": "https://www.firecrawl.dev/blog/cloudflare-error-1015-how-to-solve-it"
      }
    },
    {
      "id": "3837bcfb-4e06-4203-92bf-9c34dd35bb94",
      "source": "firecrawl/blog/cloudflare-error-1015-how-to-solve-it.md",
      "content": "Ready to _Build?_\n\nStart scraping web data for your AI apps today.  \nNo credit card needed.  \nGet Started",
      "metadata": {
        "title": "Cloudflare Error 1015: How to solve it?",
        "url": "https://www.firecrawl.dev/blog/cloudflare-error-1015-how-to-solve-it"
      }
    },
    {
      "id": "7bf88005-f121-47e9-b19b-ceb3886708e8",
      "source": "firecrawl/blog/cloudflare-error-1015-how-to-solve-it.md",
      "content": "About the Author\n\n[![Eric Ciarla image](https://www.firecrawl.dev/eric-img.jpeg)\\\nEric Ciarla@ericciarla](https://x.com/ericciarla)  \nEric Ciarla is the Chief Operating Officer (COO) of Firecrawl and leads marketing. He also worked on Mendable.ai and sold it to companies like Snapchat, Coinbase, and MongoDB.\nPreviously worked at Ford and Fracta as a Data Scientist. Eric also co-founded SideGuide, a tool for learning code within VS Code with 50,000 users.",
      "metadata": {
        "title": "Cloudflare Error 1015: How to solve it?",
        "url": "https://www.firecrawl.dev/blog/cloudflare-error-1015-how-to-solve-it"
      }
    },
    {
      "id": "7341b4ad-f794-4658-b9bd-2b6001cceca8",
      "source": "firecrawl/blog/cloudflare-error-1015-how-to-solve-it.md",
      "content": "About the Author > More articles by Eric Ciarla\n\n[How to Create an llms.txt File for Any Website\\\n\\\nLearn how to generate an llms.txt file for any website using the llms.txt Generator and Firecrawl.](https://www.firecrawl.dev/blog/How-to-Create-an-llms-txt-File-for-Any-Website) [Cloudflare Error 1015: How to solve it?\\\n\\\nCloudflare Error 1015 is a rate limiting error that occurs when Cloudflare detects that you are exceeding the request limit set by the website owner.](https://www.firecrawl.dev/blog/cloudflare-error-1015-how-to-solve-it) [Build an agent that checks for website contradictions\\\n\\\nUsing Firecrawl and Claude to scrape your website's data and look for contradictions.](https://www.firecrawl.dev/blog/contradiction-agent) [Why Companies Need a Data Strategy for Generative AI\\\n\\\nLearn why a well-defined data strategy is essential for building robust, production-ready generative AI systems, and discover practical steps for curation, maintenance, and integration.](https://www.firecrawl.dev/blog/why-companies-need-a-data-strategy-for-generative-ai) [Getting Started with OpenAI's Predicted Outputs for Faster LLM Responses\\\n\\\nA guide to leveraging Predicted Outputs to speed up LLM tasks with GPT-4o models.](https://www.firecrawl.dev/blog/getting-started-with-predicted-outputs-openai) [How to easily install requests with pip and python\\\n\\\nA tutorial on installing the requests library in Python using various methods, with usage examples and troubleshooting tips](https://www.firecrawl.dev/blog/how-to-easily-install-requests-with-pip-and-python) [How to quickly install BeautifulSoup with Python\\\n\\\nA guide on installing the BeautifulSoup library in Python using various methods, with usage examples and troubleshooting tips](https://www.firecrawl.dev/blog/how-to-quickly-install-beautifulsoup-with-python) [How to Use OpenAI's o1 Reasoning Models in Your Applications\\\n\\\nLearn how to harness OpenAI's latest o1 series models for complex reasoning tasks in your apps.](https://www.firecrawl.dev/blog/how-to-use-openai-o1-reasoning-models-in-applications)",
      "metadata": {
        "title": "Cloudflare Error 1015: How to solve it?",
        "url": "https://www.firecrawl.dev/blog/cloudflare-error-1015-how-to-solve-it"
      }
    },
    {
      "id": "efb90e76-3381-4154-a4d3-70f4d02099c0",
      "source": "firecrawl/blog/automated-data-collection-guide.md",
      "content": "---\ntitle: Automated Data Collection - A Comprehensive Guide\nurl: https://www.firecrawl.dev/blog/automated-data-collection-guide\n---  \nIntroducing /extract - Get web data with a prompt [Try now](https://www.firecrawl.dev/extract)  \nFeb 2, 2025  \nâ€¢  \n[![Bex Tuychiev image](https://www.firecrawl.dev/bex.jpg)Bex Tuychiev](https://x.com/bextuychiev)  \n# Automated Data Collection - A Comprehensive Guide  \n![Automated Data Collection - A Comprehensive Guide image](https://www.firecrawl.dev/images/blog/automated_data_collection/automated-data-collection.jpg)",
      "metadata": {
        "title": "Automated Data Collection - A Comprehensive Guide",
        "url": "https://www.firecrawl.dev/blog/automated-data-collection-guide"
      }
    },
    {
      "id": "3e469613-2ef7-4267-925c-052189e5d8e9",
      "source": "firecrawl/blog/automated-data-collection-guide.md",
      "content": "Introduction to Automated Data Collection\n\nAutomated data collection represents the backbone of modern business intelligence. While many view it simply as software gathering data, successful implementations prove itâ€™s an entire ecosystem of interconnected tools and processes. Think of it as a digital workforce that never sleeps, operating with precision that human teams simply cannot match.  \nThe fundamental building blocks of any successful automated data collection system reflect the components discussed in detail throughout this guide:  \n- Data providers like Bloomberg, Reuters, and public data portals\n- Collection tools including Selenium, Beautiful Soup, and Firecrawl\n- Storage solutions like InfluxDB for time-series and MongoDB for documents\n- Processing pipelines built with Apache Airflow and Luigi",
      "metadata": {
        "title": "Automated Data Collection - A Comprehensive Guide",
        "url": "https://www.firecrawl.dev/blog/automated-data-collection-guide"
      }
    },
    {
      "id": "23701523-e1e4-4bb6-9ab4-f32459b91c24",
      "source": "firecrawl/blog/automated-data-collection-guide.md",
      "content": "Introduction to Automated Data Collection > Why Automation Matters in Modern Data Gathering\n\nModern businesses face unprecedented data challenges that manual processes cannot handle effectively. Key benefits of automation include:  \n**Speed and Efficiency**  \n- Real-time collection and processing at millisecond speeds\n- Continuous 24/7 operation with 99.9% reliability\n- Handles enterprise-scale data volumes automatically  \n**Accuracy and Reliability**  \n- Near-perfect accuracy with built-in validation\n- Consistent formatting across diverse sources\n- Complete data provenance tracking  \n**Cost-Effectiveness**  \n- Significant reduction in operational costs\n- Minimal staffing requirements\n- Automated error prevention  \n**Scalability**  \n- Seamless handling of growing data volumes\n- Quick integration of new data sources\n- Consistent performance at scale",
      "metadata": {
        "title": "Automated Data Collection - A Comprehensive Guide",
        "url": "https://www.firecrawl.dev/blog/automated-data-collection-guide"
      }
    },
    {
      "id": "10045db1-4fc2-427d-95dd-a096ad486b0e",
      "source": "firecrawl/blog/automated-data-collection-guide.md",
      "content": "Introduction to Automated Data Collection > What Lies Ahead in This Guide\n\nThis guide provides a practical roadmap for implementing automated data collection, covering:  \n- Strategic selection of collection methods and tools\n- Technical deep dives into modern automation platforms\n- Proven implementation frameworks and best practices\n- Common challenges and their solutions\n- Emerging trends in data automation  \nThe following sections translate these concepts into actionable insights, drawing from successful implementations across industries.",
      "metadata": {
        "title": "Automated Data Collection - A Comprehensive Guide",
        "url": "https://www.firecrawl.dev/blog/automated-data-collection-guide"
      }
    },
    {
      "id": "6f47e3e4-cb01-4b4c-bcab-1b569af2a4e8",
      "source": "firecrawl/blog/automated-data-collection-guide.md",
      "content": "Understanding Automated Data Collection Systems > Data Sources\n\nBefore diving into specific tools and techniques, itâ€™s important to understand where data actually comes from. Businesses can collect data from both internal and external sources. Internal sources include company websites, sales records, customer databases, and equipment sensors. When internal data isnâ€™t enough, third-party data providers offer ready-to-use information for various needs.  \nPopular data providers include [Bloomberg](https://www.bloomberg.com/professional/) and [Reuters](https://www.reuters.com/) for financial data, [Nielsen](https://www.nielsen.com/) for consumer behavior, and [Experian](https://www.experian.com/) for customer demographics. These providers offer clean, reliable data through simple subscriptions, though costs can be significant. For smaller businesses, alternatives like government open data portals, industry associations, and specialized data marketplaces offer more affordable options.  \nWhen data isnâ€™t readily available through providers, businesses often need to collect it themselves. Web scraping tools like [Firecrawl](https://firecrawl.dev/) can automatically extract information from public websites, while [API](https://en.wikipedia.org/wiki/API) integrations enable direct access to external systems. Custom data collection scripts can monitor specific sources and gather information on a schedule. Some companies even use machine learning models to process unstructured data like images and text documents, converting them into structured datasets.  \nFor example, a company tracking competitor pricing might scrape e-commerce websites daily. Market researchers could use natural language processing to analyze social media posts and news articles. Manufacturing firms might set up [IoT](https://en.wikipedia.org/wiki/Internet_of_things) sensors to collect equipment performance data. The key is identifying valuable data sources and implementing appropriate collection methods.  \nRegardless of which data sources and collection methods you choose, youâ€™ll need robust systems to collect and manage the information effectively. This includes proper error handling, data validation, and storage infrastructure to ensure reliable operation at scale.",
      "metadata": {
        "title": "Automated Data Collection - A Comprehensive Guide",
        "url": "https://www.firecrawl.dev/blog/automated-data-collection-guide"
      }
    },
    {
      "id": "ebfb94df-25cb-4fe2-9626-0292ab69d3e9",
      "source": "firecrawl/blog/automated-data-collection-guide.md",
      "content": "Understanding Automated Data Collection Systems > Core Components of Data Collection Systems\n\nModern data collection systems consist of several technical components working together. The collection layer includes APIs and web scrapers that gather data from sources like data providers, REST endpoints, databases, and websites. These components handle rate limiting, authentication, and error recovery. For example, a web scraper might use [Selenium](https://www.selenium.dev/) for JavaScript-heavy sites or [Beautiful Soup](https://www.crummy.com/software/BeautifulSoup/) (both Python libraries) for static HTML.  \nThe processing pipeline validates and transforms the collected data through ETL (Extract, Transform, Load) operations. This includes data cleaning, format standardization, and integrity checks. Tools like [Apache Airflow](https://airflow.apache.org/) or [Luigi](https://github.com/spotify/luigi) manage these workflow dependencies. Monitoring systems track success rates, latency, and data quality metrics, alerting teams when issues arise.  \nThe storage infrastructure typically combines different database types based on access patterns. Time-series databases like [InfluxDB](https://www.influxdata.com/) handle sensor data, while document stores like [MongoDB](https://www.mongodb.com/) suit unstructured content. A message queue system like [Apache Kafka](https://kafka.apache.org/) or [RabbitMQ](https://www.rabbitmq.com/) helps decouple collection from processing, improving system reliability. Security measures include encryption, access controls, and audit logging to maintain data compliance.",
      "metadata": {
        "title": "Automated Data Collection - A Comprehensive Guide",
        "url": "https://www.firecrawl.dev/blog/automated-data-collection-guide"
      }
    },
    {
      "id": "8a879b18-ad31-41a9-bf27-72428e737c82",
      "source": "firecrawl/blog/automated-data-collection-guide.md",
      "content": "Understanding Automated Data Collection Systems > Types of Data That Can Be Automated\n\nBusiness data comes in three main formats that can be automatically collected. Organized data includes structured information like [Salesforce](https://www.salesforce.com/) CRM records (with clear fields for customer names, contact info, and purchase history) and financial databases that use standard formats like [CSV](https://en.wikipedia.org/wiki/Comma-separated_values) and [JSON](https://www.json.org/). Unorganized data covers things like customer service chat logs, social media comments, and product reviews that need AI tools like [TensorFlow](https://www.tensorflow.org/) to extract useful information. Partially organized data falls in between, like website content that follows some patterns but isnâ€™t fully structured.  \nSome data needs to be processed right away for quick decisions. For example, stock trading systems need price updates within milliseconds, while factory sensors might need temperature readings every minute to prevent equipment damage. This real-time data requires special systems that can handle high-speed collection without delays or errors. Most businesses use message queues like [Apache Kafka](https://kafka.apache.org/) and in-memory databases like [Redis](https://redis.io/) to manage these fast-moving data streams reliably.  \nThe key is matching your collection method to your data type and timing needs. For instance, CRM data works well with API connections, while social media analysis might need AI-powered scrapers like [Scrapy](https://scrapy.org/). Real-time systems cost more to build but deliver faster insights, while batch processing works fine for daily or weekly updates.",
      "metadata": {
        "title": "Automated Data Collection - A Comprehensive Guide",
        "url": "https://www.firecrawl.dev/blog/automated-data-collection-guide"
      }
    },
    {
      "id": "38bd11da-6aae-4ec6-bfa2-542584965096",
      "source": "firecrawl/blog/automated-data-collection-guide.md",
      "content": "Understanding Automated Data Collection Systems > Real-world Applications and Use Cases\n\nDifferent industries use automated data collection to solve specific business challenges. E-commerce platforms like Amazon and Walmart continuously monitor competitor pricing across millions of products to optimize their own pricing strategies. Investment banks deploy sophisticated systems to aggregate real-time market data, news feeds, and social sentiment for algorithmic trading decisions. Manufacturing facilities use networks of IoT sensors to track equipment performance, predict maintenance needs, and prevent costly downtime.  \nHealthcare providers leverage automated collection to gather patient data from electronic health records, wearable devices, and medical imaging systems, enabling better diagnoses and personalized treatment plans. Even traditional industries like agriculture have embraced automation, using satellite imagery and ground sensors to optimize irrigation, fertilization, and harvest timing.  \nThese examples demonstrate how automated data collection has become essential across sectors, driving innovation and competitive advantage. The key is selecting the right combination of tools and methods for your specific needs.",
      "metadata": {
        "title": "Automated Data Collection - A Comprehensive Guide",
        "url": "https://www.firecrawl.dev/blog/automated-data-collection-guide"
      }
    },
    {
      "id": "ea60bd70-3278-49fe-b916-e09ef1d0e89a",
      "source": "firecrawl/blog/automated-data-collection-guide.md",
      "content": "Key Methods of Automated Data Collection\n\nAutomated data collection can be implemented through various approaches, from fully programmatic solutions to no-code platforms. Programmatic methods using languages like [Python](https://www.python.org/), [R](https://www.r-project.org/), or [Java](https://www.java.com/) offer maximum flexibility and control, allowing developers to create custom scrapers, API integrations, and data processing pipelines.  \nNo-code platforms like [Zapier](https://zapier.com/) and [Make](https://www.make.com/) provide visual interfaces for connecting data sources and automating workflows without writing code, making automation accessible to business users. Hybrid approaches combining both methods are also common, where no-code tools handle simple tasks while custom code manages complex requirements. Enterprise solutions like [Informatica](https://www.informatica.com/) and [Talend](https://www.talend.com/) offer comprehensive features but require significant investment. The choice depends on technical expertise, budget, and specific use cases.  \nWith that said, letâ€™s discuss available options for your businessâ€™s specific needs.",
      "metadata": {
        "title": "Automated Data Collection - A Comprehensive Guide",
        "url": "https://www.firecrawl.dev/blog/automated-data-collection-guide"
      }
    },
    {
      "id": "2a451b34-6cc3-4918-b6cd-ff5fd3c92efb",
      "source": "firecrawl/blog/automated-data-collection-guide.md",
      "content": "Key Methods of Automated Data Collection > Web Scraping and Crawling\n\nWeb scraping is a versatile way to automatically collect data from websites. Basic scraping tools like [Beautiful Soup](https://www.crummy.com/software/BeautifulSoup/) can extract text and numbers from web pages, while more advanced tools like [Scrapy](https://scrapy.org/) can handle complex layouts and gather images and documents. However, web scraping has limitations - websites can block scrapers, and site changes can break your collection process. Tools like [Firecrawl](https://firecrawl.dev/) help address these challenges by adapting to website changes and handling dynamic content.",
      "metadata": {
        "title": "Automated Data Collection - A Comprehensive Guide",
        "url": "https://www.firecrawl.dev/blog/automated-data-collection-guide"
      }
    },
    {
      "id": "a26fdeb2-a478-4d25-aebe-63e318b9c0ae",
      "source": "firecrawl/blog/automated-data-collection-guide.md",
      "content": "Key Methods of Automated Data Collection > Example: Modern Data Collection with Firecrawl\n\nWhile there are many tools available for automated data collection, letâ€™s look at a practical example using Firecrawl, which demonstrates several key principles of modern data gathering. This example shows how to extract structured product information from Amazon:  \n```python\nfrom firecrawl import FirecrawlApp\nfrom pydantic import BaseModel, Field\nfrom typing import Optional, List\nfrom dotenv import load_dotenv\n\nload_dotenv()\n\n# Define the data structure we want to collect\nclass Product(BaseModel):\nname: str = Field(description=\"Product name\")\nprice: float = Field(description=\"Current price in USD\")\ndescription: Optional[str] = Field(description=\"Product description\")\nrating: Optional[float] = Field(description=\"Customer rating out of 5\")\nreviews_count: Optional[int] = Field(description=\"Number of customer reviews\")\n\n# Initialize the data collection tool\napp = FirecrawlApp()\n\n# Collect data from multiple product pages\nresult = app.extract(\nurls=[\"https://www.amazon.com/dp/B094DYPM88/\"],\nparams={\n\"prompt\": \"Extract product information based on the schema provided.\",\n\"schema\": Product.model_json_schema(),\n},\n)\n\n# Process the results\nproduct = Product(**result[\"data\"])\nprint(f\"Product: {product.name}\")\nprint(f\"Price: ${product.price}\")\nprint(f\"Rating: {product.rating}/5 ({product.reviews_count} reviews)\")\n\n```  \n```plaintext\nProduct: Razer Ergonomic Wrist Rest for Tenkeyless Keyboards: Plush Leatherette Memory Foam Cushion - Anti-Slip Rubber Feet\nPrice: $19.99\nRating: 4.5/5 (9964 reviews)\n\n```  \nThe code above demonstrates a modern approach to web data collection using structured schemas and AI-powered extraction. By defining a Pydantic model, we specify exactly what product information we want to collect from Amazon pages - including the name, price, description, rating, and review count. The `FirecrawlApp` then uses this schema to intelligently identify and extract the relevant data without relying on brittle CSS selectors or XPath expressions.  \nThis approach offers several significant advantages over traditional web scraping methods. The schema-based collection using Pydantic ensures consistent data formats and built-in validation. The AI-powered extraction eliminates the need to maintain fragile selectors that break when websites change. The system can efficiently process multiple pages in parallel through the urls parameter, while providing robust error handling and automatic retry mechanisms. Additionally, it standardizes data formats automatically, converting prices, ratings, and other fields into appropriate data types.  \nBy shifting from traditional web scraping to intelligent data extraction, this method significantly reduces maintenance overhead while improving reliability. The structured approach makes it easier to adapt the collection process as websites evolve, while ensuring the extracted data maintains consistent quality and format.  \nFirecrawl builds on these principles to provide a comprehensive scraping engine with capabilities beyond basic structured data extraction. To explore more features and use-cases, visit our [blog](https://www.firecrawl.dev/blog) and [documentation](https://docs.firecrawl.dev/introduction).",
      "metadata": {
        "title": "Automated Data Collection - A Comprehensive Guide",
        "url": "https://www.firecrawl.dev/blog/automated-data-collection-guide"
      }
    },
    {
      "id": "522c760d-fe4f-4d84-9d4f-00668cb1f5fa",
      "source": "firecrawl/blog/automated-data-collection-guide.md",
      "content": "Key Methods of Automated Data Collection > Methods powered by ML & AI\n\nAI tools have made it possible to collect new types of data automatically. For example:  \n- Optical Character Recognition (OCR) tools like [Tesseract](https://github.com/tesseract-ocr/tesseract) turn printed or handwritten text into digital text\n- Natural Language Processing (NLP) tools like [spaCy](https://spacy.io/) find important information in regular text, like names and dates\n- Speech recognition systems like [Mozilla DeepSpeech](https://github.com/mozilla/DeepSpeech) turn recorded speech into text  \nWhile these AI methods work well in ideal conditions, real-world results can vary. Factors like image quality, background noise, and handwriting style affect accuracy. Most organizations pair AI tools with human review to ensure data quality.",
      "metadata": {
        "title": "Automated Data Collection - A Comprehensive Guide",
        "url": "https://www.firecrawl.dev/blog/automated-data-collection-guide"
      }
    },
    {
      "id": "a34ae437-033e-4d29-ac27-851e65bbd93d",
      "source": "firecrawl/blog/automated-data-collection-guide.md",
      "content": "Key Methods of Automated Data Collection > API Integrations\n\nAPIs let you collect data directly from other systems and services. Many companies provide APIs to access their data, including social media platforms, financial services, and weather services. APIs are generally more reliable than web scraping since they provide consistent data formats and clear documentation. However, they often have usage limits and can be expensive for large amounts of data.",
      "metadata": {
        "title": "Automated Data Collection - A Comprehensive Guide",
        "url": "https://www.firecrawl.dev/blog/automated-data-collection-guide"
      }
    },
    {
      "id": "32db2108-ff3f-438e-a425-3773893034c3",
      "source": "firecrawl/blog/automated-data-collection-guide.md",
      "content": "Key Methods of Automated Data Collection > IoT Sensors and Devices\n\nInternet of Things (IoT) devices automatically collect data from the physical world. Common examples include:  \n- Temperature and humidity sensors in warehouses\n- Machine performance monitors in factories\n- Medical devices that track patient vital signs\n- Environmental sensors that measure air and water quality  \nThe main challenge with IoT devices is managing the large volume of data they generate. Organizations need robust systems to store and analyze the constant stream of sensor readings.",
      "metadata": {
        "title": "Automated Data Collection - A Comprehensive Guide",
        "url": "https://www.firecrawl.dev/blog/automated-data-collection-guide"
      }
    },
    {
      "id": "3a51f02b-39fb-4a27-b681-a19290ad98df",
      "source": "firecrawl/blog/automated-data-collection-guide.md",
      "content": "Key Methods of Automated Data Collection > Form Automation\n\nForm automation helps collect data from digital forms and surveys more efficiently. These tools like [JotForm](https://www.jotform.com/) and [Typeform](https://www.typeform.com/) can:  \n- Automatically save form responses\n- Check that submitted information is valid\n- Send data to the right storage systems\n- Work well on mobile devices  \nWhile form automation saves time, the quality of collected data still depends on how well the forms are designed and whether people fill them out correctly.",
      "metadata": {
        "title": "Automated Data Collection - A Comprehensive Guide",
        "url": "https://www.firecrawl.dev/blog/automated-data-collection-guide"
      }
    },
    {
      "id": "8591e2f5-5a36-4507-9067-a174f7acf581",
      "source": "firecrawl/blog/automated-data-collection-guide.md",
      "content": "Essential Features of Data Collection Systems > Scheduling Capabilities\n\nEvery reliable data collection system needs robust scheduling features. Basic scheduling allows for daily or weekly data collection, while advanced systems can respond to specific triggers or events. The key is to match collection frequency with business needs - collecting too often wastes resources, while collecting too rarely risks missing important data.  \nPopular scheduling tools include [Apache Airflow](https://airflow.apache.org/) for complex workflows, [Jenkins](https://www.jenkins.io/) for simple automation, and [Windows Task Scheduler](https://learn.microsoft.com/en-us/windows/win32/taskschd/task-scheduler-start-page) for basic needs. Modern scheduling should include retry mechanisms for failed attempts and the ability to handle different time zones. The system should also avoid overwhelming data sources with too many requests by implementing proper delays and rate limiting.",
      "metadata": {
        "title": "Automated Data Collection - A Comprehensive Guide",
        "url": "https://www.firecrawl.dev/blog/automated-data-collection-guide"
      }
    },
    {
      "id": "8b0e8ec3-99ef-4b3d-a97e-d7af5c6f2369",
      "source": "firecrawl/blog/automated-data-collection-guide.md",
      "content": "Essential Features of Data Collection Systems > Data Validation and Cleaning\n\nData validation acts as a quality control checkpoint. Effective systems check for common issues like missing values, incorrect formats, and duplicate entries. Validation rules should match business requirements - for example, ensuring phone numbers follow the correct format or prices fall within reasonable ranges.  \nTools like [Great Expectations](https://greatexpectations.io/) and [Pandas Profiling](https://pandas-profiling.github.io/pandas-profiling/docs/master/rtd/) help automate validation checks. The cleaning process transforms raw data into a consistent, usable format using tools like [OpenRefine](https://openrefine.org/) or Pythonâ€™s [pandas](https://pandas.pydata.org/) library. This includes standardizing dates, removing extra spaces, fixing common typos, and converting units where necessary. Good cleaning processes document all changes made to the original data, allowing teams to trace any issues back to their source.",
      "metadata": {
        "title": "Automated Data Collection - A Comprehensive Guide",
        "url": "https://www.firecrawl.dev/blog/automated-data-collection-guide"
      }
    },
    {
      "id": "3ec3506b-2cd2-4ab2-a93d-8e6e5fd72bb6",
      "source": "firecrawl/blog/automated-data-collection-guide.md",
      "content": "Essential Features of Data Collection Systems > Storage and Export Options\n\nData storage needs careful planning to balance accessibility with security. Modern systems typically use databases for structured data and file systems for documents and media. Popular options include [PostgreSQL](https://www.postgresql.org/) for relational data, [MongoDB](https://www.mongodb.com/) for flexible storage, and [Amazon S3](https://aws.amazon.com/s3/) for files. The storage solution should scale with growing data volumes while maintaining quick access to frequently used information.  \nExport capabilities should support common formats like CSV, JSON, and Excel, making it easy to share data with other business tools. Tools like [Apache NiFi](https://nifi.apache.org/) and [Talend Open Studio](https://www.talend.com/products/talend-open-studio/) help manage data movement. The system should also maintain proper backup procedures and allow for easy data recovery when needed.",
      "metadata": {
        "title": "Automated Data Collection - A Comprehensive Guide",
        "url": "https://www.firecrawl.dev/blog/automated-data-collection-guide"
      }
    },
    {
      "id": "938d47dd-bfdd-4d61-ba7c-1e9fc7265001",
      "source": "firecrawl/blog/automated-data-collection-guide.md",
      "content": "Essential Features of Data Collection Systems > Compliance and Security Features\n\nData collection systems must include security features that protect sensitive information. Basic requirements include encryption for data in transit and at rest, secure authentication for system access, and detailed audit logs of who accessed what data and when. Tools like [HashiCorp Vault](https://www.vaultproject.io/) help manage secrets and encryption keys, while [Auth0](https://auth0.com/) provides robust authentication.  \nCompliance features vary by industry and region. Healthcare systems need [HIPAA](https://www.hhs.gov/hipaa/index.html) compliance, while systems collecting European customer data must follow [GDPR](https://gdpr.eu/) requirements. Tools like [OneTrust](https://www.onetrust.com/) and [BigID](https://bigid.com/) help manage compliance requirements. The key is understanding which regulations apply to your data and implementing appropriate controls. This might include data anonymization, retention policies, and mechanisms for handling user privacy requests.",
      "metadata": {
        "title": "Automated Data Collection - A Comprehensive Guide",
        "url": "https://www.firecrawl.dev/blog/automated-data-collection-guide"
      }
    },
    {
      "id": "2ee6cf78-5fe6-44ca-8ef3-59ed1b0c1207",
      "source": "firecrawl/blog/automated-data-collection-guide.md",
      "content": "Implementation Process > Planning Your Data Collection Strategy\n\nBefore implementing any data collection system, organizations must clearly define their goals and requirements. For example, an e-commerce platform might need to track orders, inventory levels, and customer feedback - each representing different types of data collection challenges that require careful planning. This initial assessment helps identify not just what data is needed, but also its sources, formats, and collection frequencies.  \nA thorough evaluation of technical requirements and available resources follows the initial planning. This includes assessing existing systems, staff capabilities, and budget constraints. For instance, while the e-commerce platform might have developers familiar with basic automation, they may need additional training for handling real-time data streams or implementing advanced validation rules.  \nThe planning phase should also account for future growth and scalability needs. Organizations need to estimate how their data volumes might increase over time and what additional types of data they might need to collect in the future. This forward-thinking approach helps avoid choosing solutions that might become bottlenecks as operations expand.",
      "metadata": {
        "title": "Automated Data Collection - A Comprehensive Guide",
        "url": "https://www.firecrawl.dev/blog/automated-data-collection-guide"
      }
    },
    {
      "id": "c1fe81fd-60fd-401a-820c-ff64c798d244",
      "source": "firecrawl/blog/automated-data-collection-guide.md",
      "content": "Implementation Process > Selecting the Right Tools\n\nTool selection requires balancing functionality, ease of use, and long-term maintenance requirements. While open-source solutions might appear cost-effective initially, they often demand more technical expertise and internal support. Commercial tools typically offer better documentation and support but come with ongoing license costs that need careful consideration.  \nConsider how tools will integrate with existing systems and workflows. An e-commerce platform might choose a data collection tool that integrates well with their current shopping cart software, but this tool should also work effectively with other data sources like marketing analytics or customer support systems. The goal is to select tools that fit into the broader technology ecosystem while meeting specific collection needs.  \nWhen evaluating options, start with a pilot project using simpler tools before committing to enterprise-level solutions. This approach allows organizations to test assumptions and identify potential issues early, when theyâ€™re easier and less expensive to address. The pilot phase also helps teams gain valuable experience with data collection processes and requirements.",
      "metadata": {
        "title": "Automated Data Collection - A Comprehensive Guide",
        "url": "https://www.firecrawl.dev/blog/automated-data-collection-guide"
      }
    },
    {
      "id": "3e2420f7-66a1-45c2-94eb-6820aa699dc3",
      "source": "firecrawl/blog/automated-data-collection-guide.md",
      "content": "Implementation Process > Setting Up Automation Workflows\n\nImplementation should begin with simple, well-defined processes that have clear success metrics. For instance, start by automating the collection of daily operational data before moving on to more complex real-time collection needs. This incremental approach helps teams build confidence and expertise while minimizing risks.  \nEach workflow requires thorough documentation covering the data source, collection schedule, validation rules, and storage requirements. Take an e-commerce platformâ€™s order collection process - the documentation should detail how order data is extracted, what validation checks ensure data quality, and where the processed data is stored. This documentation becomes crucial as systems grow more complex and team members change.  \nError handling and monitoring procedures need to be established from the start. This includes defining how the system should respond to common issues like network timeouts or invalid data, and setting up appropriate notification systems. Clear procedures help maintain system reliability and make troubleshooting more efficient when issues arise.",
      "metadata": {
        "title": "Automated Data Collection - A Comprehensive Guide",
        "url": "https://www.firecrawl.dev/blog/automated-data-collection-guide"
      }
    },
    {
      "id": "dfabb247-5b8d-4e82-8890-a0ef447fdac3",
      "source": "firecrawl/blog/automated-data-collection-guide.md",
      "content": "Implementation Process > Quality Assurance and Testing\n\nA comprehensive testing strategy should cover both normal operations and edge cases. Testing needs to verify that the system handles common scenarios correctly while also checking its response to unusual situations like partial data availability or system outages. For example, test how the system handles both standard data inputs and edge cases like malformed records or unexpected data volumes.  \nSetting up a separate testing environment that mirrors the production setup allows teams to safely verify changes before implementation. This environment should include sample data that represents real-world scenarios while maintaining data privacy and security requirements. Regular testing helps ensure that collection processes remain reliable as systems evolve.  \nQuality checks should compare collected data against source data to verify accuracy and completeness. These checks help identify any discrepancies early and maintain data quality over time. Regular audits of collected data help ensure that the system continues to meet business requirements and maintains high data quality standards.",
      "metadata": {
        "title": "Automated Data Collection - A Comprehensive Guide",
        "url": "https://www.firecrawl.dev/blog/automated-data-collection-guide"
      }
    },
    {
      "id": "3884335f-957b-4da8-8905-c35f89c66030",
      "source": "firecrawl/blog/automated-data-collection-guide.md",
      "content": "Implementation Process > Scaling Considerations\n\nAs data collection needs grow, systems often face new challenges in storage, processing speed, and data management. Organizations need to plan for these challenges by choosing scalable technologies and building flexible systems that can adapt to changing requirements. For example, an e-commerce platform might need to evolve from batch processing to real-time data collection as transaction volumes increase.  \nStorage and processing strategies often need adjustment as data volumes grow. This might include implementing data archiving for historical information, optimizing frequently accessed data, and adjusting collection frequencies to balance timeliness with system performance. Regular monitoring of system resources helps identify potential bottlenecks before they impact operations.  \nScaling isnâ€™t just about handling larger data volumes - it also involves maintaining system reliability and performance as complexity increases. Organizations need to consider factors like data retention policies, backup strategies, and disaster recovery procedures. Regular review and updates of these procedures help ensure that the system continues to meet business needs as it grows.",
      "metadata": {
        "title": "Automated Data Collection - A Comprehensive Guide",
        "url": "https://www.firecrawl.dev/blog/automated-data-collection-guide"
      }
    },
    {
      "id": "8f8cd503-7876-477f-bff8-34789bbcab53",
      "source": "firecrawl/blog/automated-data-collection-guide.md",
      "content": "Conclusion\n\nAutomated data collection represents a fundamental shift in how organizations gather and process information. While the initial setup requires careful planning and resource investment, the long-term benefits of reduced manual effort, improved accuracy, and faster data processing make it an essential tool for modern businesses. The key to success lies in choosing the right combination of methods and tools that match specific business needs while maintaining data quality and security.  \nModern tools like Firecrawl have made web data collection more accessible and reliable. With its powerful [crawl endpoint](https://www.firecrawl.dev/blog/mastering-the-crawl-endpoint-in-firecrawl) for systematic website exploration, [scrape endpoint](https://www.firecrawl.dev/blog/mastering-firecrawl-scrape-endpoint) for targeted data extraction, and comprehensive [data enrichment capabilities](https://www.firecrawl.dev/blog/complete-guide-to-data-enrichment), organizations can quickly implement professional-grade web scraping solutions without extensive development overhead.  \nOrganizations starting their automation journey should begin with clear objectives and manageable projects, gradually expanding their capabilities as they gain experience. Whether implementing web scraping through modern APIs, direct database integrations, or IoT sensors, the focus should remain on creating sustainable, scalable systems that deliver reliable data for decision-making. As technology continues to evolve, staying informed about new collection methods and best practices, while leveraging powerful tools like Firecrawl for web data collection, will help organizations maintain their competitive advantage in an increasingly data-driven world.  \nArticle updated recently  \n[ðŸ”¥](https://www.firecrawl.dev/)",
      "metadata": {
        "title": "Automated Data Collection - A Comprehensive Guide",
        "url": "https://www.firecrawl.dev/blog/automated-data-collection-guide"
      }
    },
    {
      "id": "3dda7032-4db7-46e5-930b-d3a59ed6a595",
      "source": "firecrawl/blog/automated-data-collection-guide.md",
      "content": "Ready to _Build?_\n\nStart scraping web data for your AI apps today.  \nNo credit card needed.  \nGet Started",
      "metadata": {
        "title": "Automated Data Collection - A Comprehensive Guide",
        "url": "https://www.firecrawl.dev/blog/automated-data-collection-guide"
      }
    },
    {
      "id": "9b57dd80-17e4-4ab4-976f-1889a4359b76",
      "source": "firecrawl/blog/automated-data-collection-guide.md",
      "content": "About the Author\n\n[![Bex Tuychiev image](https://www.firecrawl.dev/bex.jpg)\\\nBex Tuychiev@bextuychiev](https://x.com/bextuychiev)  \nBex is a Top 10 AI writer on Medium and a Kaggle Master with over 15k followers. He loves writing detailed guides, tutorials, and notebooks on complex data science and machine learning topics",
      "metadata": {
        "title": "Automated Data Collection - A Comprehensive Guide",
        "url": "https://www.firecrawl.dev/blog/automated-data-collection-guide"
      }
    },
    {
      "id": "11b1b06b-8388-49de-a387-ab705f59fce3",
      "source": "firecrawl/blog/automated-data-collection-guide.md",
      "content": "About the Author > More articles by Bex Tuychiev\n\n[Building an Automated Price Tracking Tool\\\n\\\nBuild an automated e-commerce price tracker in Python. Learn web scraping, price monitoring, and automated alerts using Firecrawl, Streamlit, PostgreSQL.](https://www.firecrawl.dev/blog/automated-price-tracking-tutorial-python) [Web Scraping Automation: How to Run Scrapers on a Schedule\\\n\\\nLearn how to automate web scraping in Python using free tools like schedule, asyncio, cron jobs and GitHub Actions. This comprehensive guide covers local and cloud-based scheduling methods to run scrapers reliably in 2025.](https://www.firecrawl.dev/blog/automated-web-scraping-free-2025) [Automated Data Collection - A Comprehensive Guide\\\n\\\nLearn how to build robust automated data collection systems using modern tools and best practices. This guide covers everything from selecting the right tools to implementing scalable collection pipelines.](https://www.firecrawl.dev/blog/automated-data-collection-guide) [BeautifulSoup4 vs. Scrapy - A Comprehensive Comparison for Web Scraping in Python\\\n\\\nLearn the key differences between BeautifulSoup4 and Scrapy for web scraping in Python. Compare their features, performance, and use cases to choose the right tool for your web scraping needs.](https://www.firecrawl.dev/blog/beautifulsoup4-vs-scrapy-comparison) [How to Build an Automated Competitor Price Monitoring System with Python\\\n\\\nLearn how to build an automated competitor price monitoring system in Python that tracks prices across e-commerce sites, provides real-time comparisons, and maintains price history using Firecrawl, Streamlit, and GitHub Actions.](https://www.firecrawl.dev/blog/automated-competitor-price-scraping) [Scraping Company Data and Funding Information in Bulk With Firecrawl and Claude\\\n\\\nLearn how to build a web scraper in Python that gathers company details, funding rounds, and investor information from public sources like Crunchbase using Firecrawl and Claude for automated data collection and analysis.](https://www.firecrawl.dev/blog/crunchbase-scraping-with-firecrawl-claude) [Data Enrichment: A Complete Guide to Enhancing Your Data Quality\\\n\\\nLearn how to enrich your data quality with a comprehensive guide covering data enrichment tools, best practices, and real-world examples. Discover how to leverage modern solutions like Firecrawl to automate data collection, validation, and integration for better business insights.](https://www.firecrawl.dev/blog/complete-guide-to-data-enrichment) [Building an Intelligent Code Documentation RAG Assistant with DeepSeek and Firecrawl\\\n\\\nLearn how to build an intelligent documentation assistant powered by DeepSeek and RAG (Retrieval Augmented Generation) that can answer questions about any documentation website by combining language models with efficient information retrieval.](https://www.firecrawl.dev/blog/deepseek-rag-documentation-assistant)",
      "metadata": {
        "title": "Automated Data Collection - A Comprehensive Guide",
        "url": "https://www.firecrawl.dev/blog/automated-data-collection-guide"
      }
    },
    {
      "id": "89780f17-3d83-4618-8405-23c7218e4708",
      "source": "firecrawl/blog/.md",
      "content": "---\ntitle: Blog\nurl: https://www.firecrawl.dev/blog\n---  \nIntroducing /extract - Get web data with a prompt [Try now](https://www.firecrawl.dev/extract)  \n[![Building an Intelligent Code Documentation RAG Assistant with DeepSeek and Firecrawl](https://www.firecrawl.dev/images/blog/deepseek_rag/deepseek-rag-documentation-assistant.jpg)\\\n\\\nFeb 10, 2025\\\n\\\n**Building an Intelligent Code Documentation RAG Assistant with DeepSeek and Firecrawl** \\\n\\\nLearn how to build an intelligent documentation assistant powered by DeepSeek and RAG (Retrieval Augmented Generation) that can answer questions about any documentation website by combining language models with efficient information retrieval.\\\n\\\nBy Bex Tuychiev](https://www.firecrawl.dev/blog/deepseek-rag-documentation-assistant)",
      "metadata": {
        "title": "Blog",
        "url": "https://www.firecrawl.dev/blog"
      }
    },
    {
      "id": "6843e1eb-f54d-479c-af19-f910447e079f",
      "source": "firecrawl/blog/.md",
      "content": "Explore Articles\n\n[All](https://www.firecrawl.dev/blog) [Product Updates](https://www.firecrawl.dev/blog/category/product) [Tutorials](https://www.firecrawl.dev/blog/category/tutorials) [Customer Stories](https://www.firecrawl.dev/blog/category/customer-stories) [Tips & Resources](https://www.firecrawl.dev/blog/category/tips-and-resources)  \n[![Automated Data Collection - A Comprehensive Guide](https://www.firecrawl.dev/images/blog/automated_data_collection/automated-data-collection.jpg)\\\n**Automated Data Collection - A Comprehensive Guide** \\\nLearn how to build robust automated data collection systems using modern tools and best practices. This guide covers everything from selecting the right tools to implementing scalable collection pipelines.\\\n\\\nBy Bex TuychievFeb 2, 2025](https://www.firecrawl.dev/blog/automated-data-collection-guide)  \n[![Building an AI Resume Job Matching App With Firecrawl And Claude](https://www.firecrawl.dev/images/blog/resume_parser/ai-resume-parser.jpg)\\\n**Building an AI Resume Job Matching App With Firecrawl And Claude** \\\nLearn how to build an AI-powered job matching system that automatically scrapes job postings, parses resumes, evaluates opportunities using Claude, and sends Discord alerts for matching positions using Firecrawl, Streamlit, and Supabase.\\\n\\\nBy Bex TuychievFeb 1, 2025](https://www.firecrawl.dev/blog/ai-resume-parser-job-matcher-python)  \n[![Scraping Company Data and Funding Information in Bulk With Firecrawl and Claude](https://www.firecrawl.dev/images/blog/company-data-scraping/company-data-scraping.jpg)\\\n**Scraping Company Data and Funding Information in Bulk With Firecrawl and Claude** \\\nLearn how to build a web scraper in Python that gathers company details, funding rounds, and investor information from public sources like Crunchbase using Firecrawl and Claude for automated data collection and analysis.\\\n\\\nBy Bex TuychievJan 31, 2025](https://www.firecrawl.dev/blog/crunchbase-scraping-with-firecrawl-claude)  \n[![Mastering the Extract Endpoint in Firecrawl](https://www.firecrawl.dev/images/blog/extract_endpoint/mastering-extract-endpoint.jpg)\\\n**Mastering the Extract Endpoint in Firecrawl** \\\nLearn how to use Firecrawl's extract endpoint to automatically gather structured data from any website using AI. Build powerful web scrapers, create training datasets, and enrich your data without writing complex code.\\\n\\\nBy Bex TuychievJan 23, 2025](https://www.firecrawl.dev/blog/mastering-firecrawl-extract-endpoint)  \n[![Introducing /extract: Get structured web data with just a prompt](https://www.firecrawl.dev/images/blog/firecrawl-extract-endpoint.png)\\\n**Introducing /extract: Get structured web data with just a prompt** \\\nOur new /extract endpoint harnesses AI to turn any website into structured data for your applications seamlessly.\\\n\\\nBy Eric CiarlaJanuary 20, 2025](https://www.firecrawl.dev/blog/introducing-extract-open-beta)  \n[![How to Build a Bulk Sales Lead Extractor in Python Using AI](https://www.firecrawl.dev/images/blog/sales_lead_extractor/sales-lead-extractor.jpg)\\\n**How to Build a Bulk Sales Lead Extractor in Python Using AI** \\\nLearn how to build an automated sales lead extraction tool in Python that uses AI to scrape company information from websites, exports data to Excel, and streamlines the lead generation process using Firecrawl and Streamlit.\\\n\\\nBy Bex TuychievJan 12, 2025](https://www.firecrawl.dev/blog/sales-lead-extractor-python-ai)  \n[![Building a Trend Detection System with AI in TypeScript: A Step-by-Step Guide](https://www.firecrawl.dev/images/blog/trend_finder/trend-finder-typescript.jpg)\\\n**Building a Trend Detection System with AI in TypeScript: A Step-by-Step Guide** \\\nLearn how to build an automated trend detection system in TypeScript that monitors social media and news sites, analyzes content with AI, and sends real-time Slack alerts using Firecrawl, Together AI, and GitHub Actions.\\\n\\\nBy Bex TuychievJan 11, 2025](https://www.firecrawl.dev/blog/trend-finder-typescript)  \n[![How to Build an Automated Competitor Price Monitoring System with Python](https://www.firecrawl.dev/images/blog/competitor_price_scraping/competitor-price-scraping.jpg)\\\n**How to Build an Automated Competitor Price Monitoring System with Python** \\\nLearn how to build an automated competitor price monitoring system in Python that tracks prices across e-commerce sites, provides real-time comparisons, and maintains price history using Firecrawl, Streamlit, and GitHub Actions.\\\n\\\nBy Bex TuychievJan 6, 2025](https://www.firecrawl.dev/blog/automated-competitor-price-scraping)  \n[![How Stack AI Uses Firecrawl to Power AI Agents](https://www.firecrawl.dev/images/blog/customer-story-stackai.jpg)\\\n**How Stack AI Uses Firecrawl to Power AI Agents** \\\nDiscover how Stack AI leverages Firecrawl to seamlessly feed agentic AI workflows with high-quality web data.\\\n\\\nBy Jonathan KleimanJan 3, 2025](https://www.firecrawl.dev/blog/how-stack-ai-uses-firecrawl-to-power-ai-agents)  \n[![BeautifulSoup4 vs. Scrapy - A Comprehensive Comparison for Web Scraping in Python](https://www.firecrawl.dev/images/blog/bs4_scrapy/bs4-vs-scrapy-comparison.jpg)\\\n**BeautifulSoup4 vs. Scrapy - A Comprehensive Comparison for Web Scraping in Python** \\\nLearn the key differences between BeautifulSoup4 and Scrapy for web scraping in Python. Compare their features, performance, and use cases to choose the right tool for your web scraping needs.\\\n\\\nBy Bex TuychievDec 24, 2024](https://www.firecrawl.dev/blog/beautifulsoup4-vs-scrapy-comparison)  \n[![15 Python Web Scraping Projects: From Beginner to Advanced](https://www.firecrawl.dev/images/blog/web_scraping_projects/python-web-scraping-projects.jpg)\\\n**15 Python Web Scraping Projects: From Beginner to Advanced** \\\nExplore 15 hands-on web scraping projects in Python, from beginner to advanced level. Learn essential concepts like data extraction, concurrent processing, and distributed systems while building real-world applications.\\\n\\\nBy Bex TuychievDec 17, 2024](https://www.firecrawl.dev/blog/python-web-scraping-projects)  \n[![How to Deploy Python Web Scrapers](https://www.firecrawl.dev/images/blog/deploying-web-scrapers/deploy-web-scrapers.jpg)\\\n**How to Deploy Python Web Scrapers** \\\nLearn how to deploy Python web scrapers using GitHub Actions, Heroku, PythonAnywhere and more.\\\n\\\nBy Bex TuychievDec 16, 2024](https://www.firecrawl.dev/blog/deploy-web-scrapers)  \n[![Why Companies Need a Data Strategy for Generative AI](https://www.firecrawl.dev/images/blog/data-strategy.jpg)\\\n**Why Companies Need a Data Strategy for Generative AI** \\\nLearn why a well-defined data strategy is essential for building robust, production-ready generative AI systems, and discover practical steps for curation, maintenance, and integration.\\\n\\\nBy Eric CiarlaDec 15, 2024](https://www.firecrawl.dev/blog/why-companies-need-a-data-strategy-for-generative-ai)  \n[![Data Enrichment: A Complete Guide to Enhancing Your Data Quality](https://www.firecrawl.dev/images/blog/data_enrichment_guide/complete-data-enrichment-guide.jpg)\\\n**Data Enrichment: A Complete Guide to Enhancing Your Data Quality** \\\nLearn how to enrich your data quality with a comprehensive guide covering data enrichment tools, best practices, and real-world examples. Discover how to leverage modern solutions like Firecrawl to automate data collection, validation, and integration for better business insights.\\\n\\\nBy Bex TuychievDec 14, 2024](https://www.firecrawl.dev/blog/complete-guide-to-data-enrichment)  \n[![A Complete Guide Scraping Authenticated Websites with cURL and Firecrawl](https://www.firecrawl.dev/images/blog/complete-guide-to-curl-authentication-firecrawl-api.jpg)\\\n**A Complete Guide Scraping Authenticated Websites with cURL and Firecrawl** \\\nLearn how to scrape login-protected websites using cURL and Firecrawl API. Step-by-step guide covering basic auth, tokens, and cookies with real examples.\\\n\\\nBy Rudrank RiyamDec 13, 2024](https://www.firecrawl.dev/blog/complete-guide-to-curl-authentication-firecrawl-api)  \n[![Building an Automated Price Tracking Tool](https://www.firecrawl.dev/images/blog/price-tracking/price-tracking.jpg)\\\n**Building an Automated Price Tracking Tool** \\\nBuild an automated e-commerce price tracker in Python. Learn web scraping, price monitoring, and automated alerts using Firecrawl, Streamlit, PostgreSQL.\\\n\\\nBy Bex TuychievDec 9, 2024](https://www.firecrawl.dev/blog/automated-price-tracking-tutorial-python)  \n[![Evaluating Web Data Extraction with CrawlBench](https://www.firecrawl.dev/images/blog/crawlbench/crawlbench.jpg)\\\n**Evaluating Web Data Extraction with CrawlBench** \\\nAn in-depth exploration of CrawlBench, a benchmark for testing LLM-based web data extraction.\\\n\\\nBy SwyxDec 9, 2024](https://www.firecrawl.dev/blog/crawlbench-llm-extraction)  \n[![How Cargo Empowers GTM Teams with Firecrawl](https://www.firecrawl.dev/images/blog/customer-story-cargo.jpg)\\\n**How Cargo Empowers GTM Teams with Firecrawl** \\\nSee how Cargo uses Firecrawl to instantly analyze webpage content and power Go-To-Market workflows for their users.\\\n\\\nBy Tariq MinhasDec 6, 2024](https://www.firecrawl.dev/blog/how-cargo-empowers-gtm-teams-with-firecrawl)  \n[![Web Scraping Automation: How to Run Scrapers on a Schedule](https://www.firecrawl.dev/images/blog/scheduling-scrapers-images/automated-web-scraping-free-2025.jpg)\\\n**Web Scraping Automation: How to Run Scrapers on a Schedule** \\\nLearn how to automate web scraping in Python using free tools like schedule, asyncio, cron jobs and GitHub Actions. This comprehensive guide covers local and cloud-based scheduling methods to run scrapers reliably in 2025.\\\n\\\nBy Bex TuychievDec 5, 2024](https://www.firecrawl.dev/blog/automated-web-scraping-free-2025)  \n[![How to Generate Sitemaps Using Firecrawl's /map Endpoint: A Complete Guide](https://www.firecrawl.dev/images/blog/generating-sitemaps/how-to-generate-sitemap-using-firecrawl-map-endpoint.jpg)\\\n**How to Generate Sitemaps Using Firecrawl's /map Endpoint: A Complete Guide** \\\nLearn how to generate XML and visual sitemaps using Firecrawl's /map endpoint. Step-by-step guide with Python code examples, performance comparisons, and interactive visualization techniques for effective website mapping.\\\n\\\nBy Bex TuychievNov 29, 2024](https://www.firecrawl.dev/blog/how-to-generate-sitemaps-using-firecrawl-map-endpoint)  \n[![How to Use Firecrawl's Scrape API: Complete Web Scraping Tutorial](https://www.firecrawl.dev/images/blog/scrape-masterclass/mastering-scrape.jpg)\\\n**How to Use Firecrawl's Scrape API: Complete Web Scraping Tutorial** \\\nLearn how to scrape websites using Firecrawl's /scrape endpoint. Master JavaScript rendering, structured data extraction, and batch operations with Python code examples.\\\n\\\nBy Bex TuychievNov 25, 2024](https://www.firecrawl.dev/blog/mastering-firecrawl-scrape-endpoint)  \n[![How to Create an llms.txt File for Any Website](https://www.firecrawl.dev/images/blog/How-to-Create-an-llms-txt-File-for-Any-Website.jpg)\\\n**How to Create an llms.txt File for Any Website** \\\nLearn how to generate an llms.txt file for any website using the llms.txt Generator and Firecrawl.\\\n\\\nBy Eric CiarlaNov 22, 2024](https://www.firecrawl.dev/blog/How-to-Create-an-llms-txt-File-for-Any-Website)  \n[![Mastering Firecrawl's Crawl Endpoint: A Complete Web Scraping Guide](https://www.firecrawl.dev/images/blog/crawl-masterclass/images/mastering-crawl.jpg)\\\n**Mastering Firecrawl's Crawl Endpoint: A Complete Web Scraping Guide** \\\nLearn how to use Firecrawl's /crawl endpoint for efficient web scraping. Master URL control, performance optimization, and integration with LangChain for AI-powered data extraction.\\\n\\\nBy Bex TuychievNov 18, 2024](https://www.firecrawl.dev/blog/mastering-the-crawl-endpoint-in-firecrawl)  \n[![Getting Started with OpenAI's Predicted Outputs for Faster LLM Responses](https://www.firecrawl.dev/images/blog/openai-predicted-outputs.jpg)\\\n**Getting Started with OpenAI's Predicted Outputs for Faster LLM Responses** \\\nA guide to leveraging Predicted Outputs to speed up LLM tasks with GPT-4o models.\\\n\\\nBy Eric CiarlaNov 5, 2024](https://www.firecrawl.dev/blog/getting-started-with-predicted-outputs-openai)  \n[![Launch Week II Recap](https://www.firecrawl.dev/images/blog/launch-week-ii-recap.jpg)\\\n**Launch Week II Recap** \\\nRecapping all the exciting announcements from Firecrawl's second Launch Week.\\\n\\\nBy Eric CiarlaNovember 4, 2024](https://www.firecrawl.dev/blog/launch-week-ii-recap)  \n[![Launch Week II - Day 7: Introducing Faster Markdown Parsing](https://www.firecrawl.dev/images/blog/firecrawl-faster-markdown.jpg)\\\n**Launch Week II - Day 7: Introducing Faster Markdown Parsing** \\\nOur new HTML to Markdown parser is 4x faster, more reliable, and produces cleaner Markdown, built from the ground up for speed and performance.\\\n\\\nBy Eric CiarlaNovember 3, 2024](https://www.firecrawl.dev/blog/launch-week-ii-day-7-introducing-faster-markdown-parsing)  \n[![Launch Week II - Day 6: Introducing Mobile Scraping and Mobile Screenshots](https://www.firecrawl.dev/images/blog/firecrawl-mobile-scraping.jpg)\\\n**Launch Week II - Day 6: Introducing Mobile Scraping and Mobile Screenshots** \\\nInteract with sites as if from a mobile device using Firecrawl's new mobile device emulation.\\\n\\\nBy Eric CiarlaNovember 2, 2024](https://www.firecrawl.dev/blog/launch-week-ii-day-6-introducing-mobile-scraping)  \n[![Launch Week II - Day 5: Introducing New Actions](https://www.firecrawl.dev/images/blog/firecrawl-new-actions.jpg)\\\n**Launch Week II - Day 5: Introducing New Actions** \\\nCapture page content at any point and wait for specific elements with our new Scrape and Wait for Selector actions.\\\n\\\nBy Eric CiarlaNovember 1, 2024](https://www.firecrawl.dev/blog/launch-week-ii-day-5-introducing-two-new-actions)  \n[![Launch Week II - Day 4: Advanced iframe Scraping](https://www.firecrawl.dev/images/blog/firecrawl-iframe-scraping.jpg)\\\n**Launch Week II - Day 4: Advanced iframe Scraping** \\\nWe are thrilled to announce comprehensive iframe scraping support in Firecrawl, enabling seamless handling of nested iframes, dynamically loaded content, and cross-origin frames.\\\n\\\nBy Eric CiarlaOctober 31, 2024](https://www.firecrawl.dev/blog/launch-week-ii-day-4-advanced-iframe-scraping)  \n[![Launch Week II - Day 3: Introducing Credit Packs](https://www.firecrawl.dev/images/blog/firecrawl-credit-packs.jpg)\\\n**Launch Week II - Day 3: Introducing Credit Packs** \\\nEasily top up your plan with Credit Packs to keep your web scraping projects running smoothly. Plus, manage your credits effortlessly with our new Auto Recharge feature.\\\n\\\nBy Eric CiarlaOctober 30, 2024](https://www.firecrawl.dev/blog/launch-week-ii-day-3-introducing-credit-packs)  \n[![Launch Week II - Day 2: Introducing Location and Language Settings](https://www.firecrawl.dev/images/blog/firecrawl-location-language.jpg)\\\n**Launch Week II - Day 2: Introducing Location and Language Settings** \\\nSpecify country and preferred languages to get relevant localized content, enhancing your web scraping results with region-specific data.\\\n\\\nBy Eric CiarlaOctober 29, 2024](https://www.firecrawl.dev/blog/launch-week-ii-day-2-introducing-location-language-settings)  \n[![Launch Week II - Day 1: Introducing the Batch Scrape Endpoint](https://www.firecrawl.dev/images/blog/firecrawl-batch-scrape.jpg)\\\n**Launch Week II - Day 1: Introducing the Batch Scrape Endpoint** \\\nOur new Batch Scrape endpoint lets you scrape multiple URLs simultaneously, making bulk data collection faster and more efficient.\\\n\\\nBy Eric CiarlaOctober 28, 2024](https://www.firecrawl.dev/blog/launch-week-ii-day-1-introducing-batch-scrape-endpoint)  \n[![Getting Started with Grok-2: Setup and Web Crawler Example](https://www.firecrawl.dev/images/blog/grok-2-web-crawler.jpg)\\\n**Getting Started with Grok-2: Setup and Web Crawler Example** \\\nA detailed guide on setting up Grok-2 and building a web crawler using Firecrawl.\\\n\\\nBy Nicolas CamaraOct 21, 2024](https://www.firecrawl.dev/blog/grok-2-setup-and-web-crawler-example)  \n[![OpenAI Swarm Tutorial: Create Marketing Campaigns for Any Website](https://www.firecrawl.dev/images/blog/openai-swarm.png)\\\n**OpenAI Swarm Tutorial: Create Marketing Campaigns for Any Website** \\\nA guide to building a multi-agent system using OpenAI Swarm and Firecrawl for AI-driven marketing strategies\\\n\\\nBy Nicolas CamaraOct 12, 2024](https://www.firecrawl.dev/blog/openai-swarm-agent-tutorial)  \n[![Using OpenAI's Realtime API and Firecrawl to Talk with Any Website](https://www.firecrawl.dev/images/blog/How-to-Talk-with-Any-Website-Using-OpenAIs-Realtime-API-and-Firecrawl.jpg)\\\n**Using OpenAI's Realtime API and Firecrawl to Talk with Any Website** \\\nBuild a real-time conversational agent that interacts with any website using OpenAI's Realtime API and Firecrawl.\\\n\\\nBy Nicolas CamaraOct 11, 2024](https://www.firecrawl.dev/blog/How-to-Talk-with-Any-Website-Using-OpenAIs-Realtime-API-and-Firecrawl)  \n[![Scraping Job Boards Using Firecrawl Actions and OpenAI](https://www.firecrawl.dev/images/blog/firecrawl-openai-job-scraping.jpg)\\\n**Scraping Job Boards Using Firecrawl Actions and OpenAI** \\\nA step-by-step guide to scraping job boards and extracting structured data using Firecrawl and OpenAI.\\\n\\\nBy Eric CiarlaSept 27, 2024](https://www.firecrawl.dev/blog/scrape-job-boards-firecrawl-openai)  \n[![Build a Full-Stack AI Web App in 12 Minutes](https://www.firecrawl.dev/images/blog/Build-a-Full-Stack-AI-Web-App-in-12-Minutes.png)\\\n**Build a Full-Stack AI Web App in 12 Minutes** \\\nBuild a Full-Stack AI Web App in 12 minutes with Cursor, OpenAI o1, V0, Firecrawl & Patched\\\n\\\nBy Dev DigestSep 18, 2024](https://www.firecrawl.dev/blog/Build-a-Full-Stack-AI-Web-App-in-12-Minutes)  \n[![How to Use OpenAI's o1 Reasoning Models in Your Applications](https://www.firecrawl.dev/images/blog/how-to-use-openai-o1-reasoning-models-in-applications.jpg)\\\n**How to Use OpenAI's o1 Reasoning Models in Your Applications** \\\nLearn how to harness OpenAI's latest o1 series models for complex reasoning tasks in your apps.\\\n\\\nBy Eric CiarlaSep 16, 2024](https://www.firecrawl.dev/blog/how-to-use-openai-o1-reasoning-models-in-applications)  \n[![Handling 300k requests per day: an adventure in scaling](https://www.firecrawl.dev/images/blog/an-adventure-in-scaling.jpg)\\\n**Handling 300k requests per day: an adventure in scaling** \\\nPutting out fires was taking up all our time, and we had to scale fast. This is how we did it.\\\n\\\nBy GergÅ‘ MÃ³ricz (mogery)Sep 13, 2024](https://www.firecrawl.dev/blog/an-adventure-in-scaling)  \n[![How Athena Intelligence Empowers Enterprise Analysts with Firecrawl](https://www.firecrawl.dev/images/blog/customer-story-athena-intelligence.jpg)\\\n**How Athena Intelligence Empowers Enterprise Analysts with Firecrawl** \\\nDiscover how Athena Intelligence leverages Firecrawl to fuel its AI-native analytics platform for enterprise analysts.\\\n\\\nBy Ben ReillySep 10, 2024](https://www.firecrawl.dev/blog/how-athena-intelligence-empowers-analysts-with-firecrawl)  \n[![Launch Week I Recap](https://www.firecrawl.dev/images/blog/launch-week-1-recap.png)\\\n**Launch Week I Recap** \\\nA look back at the new features and updates introduced during Firecrawl's inaugural Launch Week.\\\n\\\nBy Eric CiarlaSeptember 2, 2024](https://www.firecrawl.dev/blog/firecrawl-launch-week-1-recap)  \n[![Launch Week I / Day 7: Crawl Webhooks (v1)](https://www.firecrawl.dev/images/blog/webhooks.png)\\\n**Launch Week I / Day 7: Crawl Webhooks (v1)** \\\nNew /crawl webhook support. Send notifications to your apps during a crawl.\\\n\\\nBy Nicolas CamaraSeptember 1, 2024](https://www.firecrawl.dev/blog/launch-week-i-day-7-webhooks)  \n[![Launch Week I / Day 6: LLM Extract (v1)](https://www.firecrawl.dev/images/blog/firecrawl-llm-extract.png)\\\n**Launch Week I / Day 6: LLM Extract (v1)** \\\nExtract structured data from your web pages using the extract format in /scrape.\\\n\\\nBy Nicolas CamaraAugust 31, 2024](https://www.firecrawl.dev/blog/launch-week-i-day-6-llm-extract)  \n[![Launch Week I / Day 5: Real-Time Crawling with WebSockets](https://www.firecrawl.dev/images/blog/firecrawl-websockets.png)\\\n**Launch Week I / Day 5: Real-Time Crawling with WebSockets** \\\nOur new WebSocket-based method for real-time data extraction and monitoring.\\\n\\\nBy Eric CiarlaAugust 30, 2024](https://www.firecrawl.dev/blog/launch-week-i-day-5-real-time-crawling-websockets)  \n[![Launch Week I / Day 4: Introducing Firecrawl /v1](https://www.firecrawl.dev/images/blog/firecrawl-v1-release.png)\\\n**Launch Week I / Day 4: Introducing Firecrawl /v1** \\\nOur biggest release yet - v1, a more reliable and developer-friendly API for seamless web data gathering.\\\n\\\nBy Eric CiarlaAugust 29, 2024](https://www.firecrawl.dev/blog/launch-week-i-day-4-introducing-firecrawl-v1)  \n[![Launch Week I / Day 3: Introducing the Map Endpoint](https://www.firecrawl.dev/images/blog/firecrawl-map-endpoint.png)\\\n**Launch Week I / Day 3: Introducing the Map Endpoint** \\\nOur new Map endpoint enables lightning-fast website mapping for enhanced web scraping projects.\\\n\\\nBy Eric CiarlaAugust 28, 2024](https://www.firecrawl.dev/blog/launch-week-i-day-3-introducing-map-endpoint)  \n[![Launch Week I / Day 2: 2x Rate Limits](https://www.firecrawl.dev/images/blog/firecrawl-rate-limits.png)\\\n**Launch Week I / Day 2: 2x Rate Limits** \\\nFirecrawl doubles rate limits across all plans, supercharging your web scraping capabilities.\\\n\\\nBy Eric CiarlaAugust 27, 2024](https://www.firecrawl.dev/blog/launch-week-i-day-2-doubled-rate-limits)  \n[![Launch Week I / Day 1: Introducing Teams](https://www.firecrawl.dev/images/blog/firecrawl-teams.png)\\\n**Launch Week I / Day 1: Introducing Teams** \\\nOur new Teams feature, enabling seamless collaboration on web scraping projects.\\\n\\\nBy Eric CiarlaAugust 26, 2024](https://www.firecrawl.dev/blog/launch-week-i-day-1-introducing-teams)  \n[![How to Use Prompt Caching and Cache Control with Anthropic Models](https://www.firecrawl.dev/images/blog/anthropic-prompt-caching.png)\\\n**How to Use Prompt Caching and Cache Control with Anthropic Models** \\\nLearn how to cache large context prompts with Anthropic Models like Opus, Sonnet, and Haiku for faster and cheaper chats that analyze website data.\\\n\\\nBy Eric CiarlaAug 14, 2024](https://www.firecrawl.dev/blog/using-prompt-caching-with-anthropic)  \n[![Building Knowledge Graphs from Web Data using CAMEL-AI and Firecrawl](https://www.firecrawl.dev/images/blog/knowledge-graph.jpg)\\\n**Building Knowledge Graphs from Web Data using CAMEL-AI and Firecrawl** \\\nA guide on constructing knowledge graphs from web pages using CAMEL-AI and Firecrawl\\\n\\\nBy Wendong FanAug 13, 2024](https://www.firecrawl.dev/blog/building-knowledge-graphs-from-web-data-camelai-firecrawl)  \n[![How Gamma Supercharges Onboarding with Firecrawl](https://www.firecrawl.dev/images/blog/customer-story-gamma.jpg)\\\n**How Gamma Supercharges Onboarding with Firecrawl** \\\nSee how Gamma uses Firecrawl to instantly generate websites and presentations to 20+ million users.\\\n\\\nBy Jon NoronhaAug 8, 2024](https://www.firecrawl.dev/blog/how-gamma-supercharges-onboarding-with-firecrawl)  \n[![How to Use OpenAI's Structured Outputs and JSON Strict Mode](https://www.firecrawl.dev/images/blog/openai-structured-output.png)\\\n**How to Use OpenAI's Structured Outputs and JSON Strict Mode** \\\nA guide for getting structured data from the latest OpenAI models.\\\n\\\nBy Eric CiarlaAug 7, 2024](https://www.firecrawl.dev/blog/using-structured-output-and-json-strict-mode-openai)  \n[![Introducing Fire Engine for Firecrawl](https://www.firecrawl.dev/images/blog/fire-engine-launch.png)\\\n**Introducing Fire Engine for Firecrawl** \\\nThe most scalable, reliable, and fast way to get web data for Firecrawl.\\\n\\\nBy Eric CiarlaAug 6, 2024](https://www.firecrawl.dev/blog/introducing-fire-engine-for-firecrawl)  \n[![Firecrawl July 2024 Updates](https://www.firecrawl.dev/images/blog/launch-yc-firecrawl.png)\\\n**Firecrawl July 2024 Updates** \\\nDiscover the latest features, integrations, and improvements in Firecrawl for July 2024.\\\n\\\nBy Eric CiarlaJuly 31, 2024](https://www.firecrawl.dev/blog/firecrawl-july-2024-updates)  \n[![Firecrawl June 2024 Updates](https://www.firecrawl.dev/images/blog/dashboard2.png)\\\n**Firecrawl June 2024 Updates** \\\nDiscover the latest features, integrations, and improvements in Firecrawl for June 2024.\\\n\\\nBy Nicolas CamaraJune 30, 2024](https://www.firecrawl.dev/blog/firecrawl-june-2024-updates)  \n[![Scrape and Analyze Airbnb Data with Firecrawl and E2B](https://www.firecrawl.dev/images/blog/firecrawl-e2b-airbnb.png)\\\n**Scrape and Analyze Airbnb Data with Firecrawl and E2B** \\\nLearn how to scrape and analyze Airbnb data using Firecrawl and E2B in a few lines of code.\\\n\\\nBy Nicolas CamaraMay 23, 2024](https://www.firecrawl.dev/blog/scrape-analyze-airbnb-data-with-e2b)  \n[![Build a 'Chat with website' using Groq Llama 3](https://www.firecrawl.dev/images/blog/g4.png)\\\n**Build a 'Chat with website' using Groq Llama 3** \\\nLearn how to use Firecrawl, Groq Llama 3, and Langchain to build a 'Chat with your website' bot.\\\n\\\nBy Nicolas CamaraMay 22, 2024](https://www.firecrawl.dev/blog/chat-with-website)  \n[![Using LLM Extraction for Customer Insights](https://www.firecrawl.dev/images/blog/g3.png)\\\n**Using LLM Extraction for Customer Insights** \\\nUsing LLM Extraction for Insights and Lead Generation using Make and Firecrawl.\\\n\\\nBy Caleb PefferMay 21, 2024](https://www.firecrawl.dev/blog/lead-gen-business-insights-make-firecrawl)  \n[![Extract website data using LLMs](https://www.firecrawl.dev/images/blog/g2.png)\\\n**Extract website data using LLMs** \\\nLearn how to use Firecrawl and Groq to extract structured data from a web page in a few lines of code.\\\n\\\nBy Nicolas CamaraMay 20, 2024](https://www.firecrawl.dev/blog/data-extraction-using-llms)  \n[![Build an agent that checks for website contradictions](https://www.firecrawl.dev/images/blog/g1.png)\\\n**Build an agent that checks for website contradictions** \\\nUsing Firecrawl and Claude to scrape your website's data and look for contradictions.\\\n\\\nBy Eric CiarlaMay 19, 2024](https://www.firecrawl.dev/blog/contradiction-agent)  \n[ðŸ”¥](https://www.firecrawl.dev/)",
      "metadata": {
        "title": "Blog",
        "url": "https://www.firecrawl.dev/blog"
      }
    },
    {
      "id": "4fbd2d81-5af9-4aba-b912-0e477fb91e75",
      "source": "firecrawl/blog/.md",
      "content": "Ready to _Build?_\n\nStart scraping web data for your AI apps today.  \nNo credit card needed.  \nGet Started",
      "metadata": {
        "title": "Blog",
        "url": "https://www.firecrawl.dev/blog"
      }
    },
    {
      "id": "f54c4a40-0514-4145-a8ee-1687d2224841",
      "source": "firecrawl/blog/openai-swarm-agent-tutorial.md",
      "content": "---\ntitle: OpenAI Swarm Tutorial: Create Marketing Campaigns for Any Website\nurl: https://www.firecrawl.dev/blog/openai-swarm-agent-tutorial\n---  \nIntroducing /extract - Get web data with a prompt [Try now](https://www.firecrawl.dev/extract)  \nOct 12, 2024  \nâ€¢  \n[![Nicolas Camara image](https://www.firecrawl.dev/nick-img.jpeg)Nicolas Camara](https://x.com/nickscamara_)  \n# OpenAI Swarm Tutorial: Create Marketing Campaigns for Any Website  \nOpenAI Swarm Tutorial: Create Marketing Campaigns for Any Website with AI - YouTube  \nFirecrawl  \n472 subscribers  \n[OpenAI Swarm Tutorial: Create Marketing Campaigns for Any Website with AI](https://www.youtube.com/watch?v=LaEUGfzDWNo)  \nFirecrawl  \nSearch  \nInfo  \nShopping  \nTap to unmute  \nIf playback doesn't begin shortly, try restarting your device.  \nYou're signed out  \nVideos you watch may be added to the TV's watch history and influence TV recommendations. To avoid this, cancel and sign in to YouTube on your computer.  \nCancelConfirm  \nShare  \nInclude playlist  \nAn error occurred while retrieving sharing information. Please try again later.  \nWatch later  \nShare  \nCopy link  \nWatch on  \n0:00  \n/ â€¢Live  \nâ€¢  \n[Watch on YouTube](https://www.youtube.com/watch?v=LaEUGfzDWNo \"Watch on YouTube\")  \nIn this tutorial, weâ€™ll build a multi-agent system using [OpenAI Swarm](https://github.com/openai/swarm) for AI-powered marketing strategies using [Firecrawl](https://firecrawl.dev/) for web scraping.",
      "metadata": {
        "title": "OpenAI Swarm Tutorial: Create Marketing Campaigns for Any Website",
        "url": "https://www.firecrawl.dev/blog/openai-swarm-agent-tutorial"
      }
    },
    {
      "id": "4ac0738c-f6cd-4fd0-a718-b75f7cf74efa",
      "source": "firecrawl/blog/openai-swarm-agent-tutorial.md",
      "content": "Agents\n\n1. User Interface: Manages user interactions\n2. Website Scraper: Extracts clean LLM-ready content via Firecrawl API\n3. Analyst: Provides marketing insights\n4. Campaign Idea: Generates marketing campaign concepts\n5. Copywriter: Creates compelling marketing copy",
      "metadata": {
        "title": "OpenAI Swarm Tutorial: Create Marketing Campaigns for Any Website",
        "url": "https://www.firecrawl.dev/blog/openai-swarm-agent-tutorial"
      }
    },
    {
      "id": "ce9e8c76-caf7-4230-93fa-460ab2d21de7",
      "source": "firecrawl/blog/openai-swarm-agent-tutorial.md",
      "content": "Requirements\n\n- [Firecrawl](https://firecrawl.dev/) API key\n- [OpenAI](https://platform.openai.com/api-keys) API key",
      "metadata": {
        "title": "OpenAI Swarm Tutorial: Create Marketing Campaigns for Any Website",
        "url": "https://www.firecrawl.dev/blog/openai-swarm-agent-tutorial"
      }
    },
    {
      "id": "3d023190-99bf-4796-858d-d48112e8ac70",
      "source": "firecrawl/blog/openai-swarm-agent-tutorial.md",
      "content": "Setup\n\n1. Install the required packages:  \n```\npip install -r requirements.txt\n\n```  \n2. Set up your environment variables in a `.env` file:  \n```\nOPENAI_API_KEY=your_openai_api_key\nFIRECRAWL_API_KEY=your_firecrawl_api_key\n\n```",
      "metadata": {
        "title": "OpenAI Swarm Tutorial: Create Marketing Campaigns for Any Website",
        "url": "https://www.firecrawl.dev/blog/openai-swarm-agent-tutorial"
      }
    },
    {
      "id": "40aa8b44-e3a0-4a67-8482-1091a91da0e6",
      "source": "firecrawl/blog/openai-swarm-agent-tutorial.md",
      "content": "How it works\n\nOur multi-agent system uses AI to create marketing strategies. Hereâ€™s a breakdown:  \n1. User Interface Agent:\n- Talks to the user\n- Asks for the website URL\n- Can ask follow-up questions if needed\n- Passes the URL to the Website Scraper Agent\n2. Website Scraper Agent:\n- Uses Firecrawl to get content from the website\n- Asks for the content in markdown format\n- Sends the cleaned-up content to the Analyst Agent\n3. Analyst Agent:\n- Looks at the website content\n- Uses GPT-4o-mini to find key marketing insights\n- Figures out things like target audience and business goals\n- Passes these insights to the Campaign Idea Agent\n4. Campaign Idea Agent:\n- Takes the analysis and creates a marketing campaign idea\n- Uses GPT-4o-mini to come up with something creative\n- Considers the target audience and goals from the analysis\n- Sends the campaign idea to the Copywriter Agent\n5. Copywriter Agent:\n- Gets the campaign idea\n- Uses GPT-4o-mini to write catchy marketing copy\n- Creates copy that fits the campaign idea and target audience  \nThe OpenAI Swarm library manages how these agents work together. It makes sure information flows smoothly between agents and each agent does its job when itâ€™s supposed to.  \nThe whole process starts when a user enters a URL. The system then goes through each step, from scraping the website to writing copy. At the end, the user gets a full marketing strategy with analysis, campaign ideas, and copy.  \nArticle updated recently  \n[ðŸ”¥](https://www.firecrawl.dev/)",
      "metadata": {
        "title": "OpenAI Swarm Tutorial: Create Marketing Campaigns for Any Website",
        "url": "https://www.firecrawl.dev/blog/openai-swarm-agent-tutorial"
      }
    },
    {
      "id": "053b3754-3b83-4d7d-b6ca-c0c4ca5a2fab",
      "source": "firecrawl/blog/openai-swarm-agent-tutorial.md",
      "content": "Ready to _Build?_\n\nStart scraping web data for your AI apps today.  \nNo credit card needed.  \nGet Started",
      "metadata": {
        "title": "OpenAI Swarm Tutorial: Create Marketing Campaigns for Any Website",
        "url": "https://www.firecrawl.dev/blog/openai-swarm-agent-tutorial"
      }
    },
    {
      "id": "652a6daf-d955-4f81-9a2f-454c359b5548",
      "source": "firecrawl/blog/openai-swarm-agent-tutorial.md",
      "content": "About the Author\n\n[![Nicolas Camara image](https://www.firecrawl.dev/nick-img.jpeg)\\\nNicolas Camara@nickscamara_](https://x.com/nickscamara_)  \nNicolas Camara is the Chief Technology Officer (CTO) at Firecrawl.\nHe previously built and scaled Mendable, one of the pioneering \"chat with your documents\" apps,\nwhich had major Fortune 500 customers like Snapchat, Coinbase, and MongoDB.\nPrior to that, Nicolas built SideGuide, the first code-learning tool inside VS Code,\nand grew a community of 50,000 users. Nicolas studied Computer Science and has over 10 years of experience in building software.",
      "metadata": {
        "title": "OpenAI Swarm Tutorial: Create Marketing Campaigns for Any Website",
        "url": "https://www.firecrawl.dev/blog/openai-swarm-agent-tutorial"
      }
    },
    {
      "id": "010c962d-bc00-4ea5-9bce-524323be4e6b",
      "source": "firecrawl/blog/openai-swarm-agent-tutorial.md",
      "content": "About the Author > More articles by Nicolas Camara\n\n[Using OpenAI's Realtime API and Firecrawl to Talk with Any Website\\\n\\\nBuild a real-time conversational agent that interacts with any website using OpenAI's Realtime API and Firecrawl.](https://www.firecrawl.dev/blog/How-to-Talk-with-Any-Website-Using-OpenAIs-Realtime-API-and-Firecrawl) [Extract website data using LLMs\\\n\\\nLearn how to use Firecrawl and Groq to extract structured data from a web page in a few lines of code.](https://www.firecrawl.dev/blog/data-extraction-using-llms) [Getting Started with Grok-2: Setup and Web Crawler Example\\\n\\\nA detailed guide on setting up Grok-2 and building a web crawler using Firecrawl.](https://www.firecrawl.dev/blog/grok-2-setup-and-web-crawler-example) [Launch Week I / Day 6: LLM Extract (v1)\\\n\\\nExtract structured data from your web pages using the extract format in /scrape.](https://www.firecrawl.dev/blog/launch-week-i-day-6-llm-extract) [Launch Week I / Day 7: Crawl Webhooks (v1)\\\n\\\nNew /crawl webhook support. Send notifications to your apps during a crawl.](https://www.firecrawl.dev/blog/launch-week-i-day-7-webhooks) [OpenAI Swarm Tutorial: Create Marketing Campaigns for Any Website\\\n\\\nA guide to building a multi-agent system using OpenAI Swarm and Firecrawl for AI-driven marketing strategies](https://www.firecrawl.dev/blog/openai-swarm-agent-tutorial) [Build a 'Chat with website' using Groq Llama 3\\\n\\\nLearn how to use Firecrawl, Groq Llama 3, and Langchain to build a 'Chat with your website' bot.](https://www.firecrawl.dev/blog/chat-with-website) [Scrape and Analyze Airbnb Data with Firecrawl and E2B\\\n\\\nLearn how to scrape and analyze Airbnb data using Firecrawl and E2B in a few lines of code.](https://www.firecrawl.dev/blog/scrape-analyze-airbnb-data-with-e2b)",
      "metadata": {
        "title": "OpenAI Swarm Tutorial: Create Marketing Campaigns for Any Website",
        "url": "https://www.firecrawl.dev/blog/openai-swarm-agent-tutorial"
      }
    },
    {
      "id": "bace58bd-944c-4830-93d6-d7efd253679b",
      "source": "firecrawl/blog/automated-price-tracking-tutorial-python.md",
      "content": "---\ntitle: Building an Automated Price Tracking Tool\nurl: https://www.firecrawl.dev/blog/automated-price-tracking-tutorial-python\n---  \nIntroducing /extract - Get web data with a prompt [Try now](https://www.firecrawl.dev/extract)  \nDec 9, 2024  \nâ€¢  \n[![Bex Tuychiev image](https://www.firecrawl.dev/bex.jpg)Bex Tuychiev](https://x.com/bextuychiev)  \n# Building an Automated Price Tracking Tool  \n![Building an Automated Price Tracking Tool image](https://www.firecrawl.dev/images/blog/price-tracking/price-tracking.jpg)  \nThere is a lot to be said about the psychology of discounts. For example, buying a discounted item we donâ€™t need isnâ€™t saving money at all - itâ€™s falling for one of the oldest sales tactics. However, there are legitimate cases where waiting for a price drop on items you actually need makes perfect sense.  \nThe challenge is that e-commerce websites run flash sales and temporary discounts constantly, but these deals often disappear as quickly as they appear. Missing these brief windows of opportunity can be frustrating.  \nThatâ€™s where automation comes in. In this guide, weâ€™ll build a Python application that monitors product prices across any e-commerce website and instantly notifies you when prices drop on items youâ€™re actually interested in. Here is a sneak peek of the app:  \n![Screenshot of a minimalist price tracking application showing product listings, price history charts, and notification controls for monitoring e-commerce deals using Firecrawl](https://www.firecrawl.dev/images/blog/price-tracking/sneak-peek.png)  \nThe app has a simple appearance but provides complete functionality:  \n- It has a minimalistic UI to add or remove products from the tracker\n- A simple dashboard to display price history for each product\n- Controls for setting the price drop threshold in percentages\n- A notification system that sends Discord alerts when a tracked itemâ€™s price drops\n- A scheduling system that updates the product prices on an interval you specify\n- Runs for free for as long as you want  \nEven though the title says â€œAmazon price trackerâ€ (full disclosure: I was forced to write that for SEO purposes), the app will work for any e-commerce website you can imagine (except Ebay, for some reason).  \nSo, letâ€™s get started building this Amazon price tracker.",
      "metadata": {
        "title": "Building an Automated Price Tracking Tool",
        "url": "https://www.firecrawl.dev/blog/automated-price-tracking-tutorial-python"
      }
    },
    {
      "id": "db69b556-b140-4428-9124-d1045a7f6fab",
      "source": "firecrawl/blog/automated-price-tracking-tutorial-python.md",
      "content": "The Toolstack We Will Use\n\nThe app will be built using Python and these libraries::  \n- [Streamlit](https://www.streamlit.io/) for the UI\n- [Firecrawl](https://www.firecrawl.dev/) for AI-based scraping of e-commerce websites\n- [SQLAlchemy](https://www.sqlalchemy.org/) for database management  \nIn addition to Python, we will use these platforms:  \n- Discord for notifications\n- GitHub for hosting the app\n- GitHub Actions for running the app on a schedule\n- Supabase for hosting a free Postgres database instance",
      "metadata": {
        "title": "Building an Automated Price Tracking Tool",
        "url": "https://www.firecrawl.dev/blog/automated-price-tracking-tutorial-python"
      }
    },
    {
      "id": "1f9a6ac9-6f85-4374-8b0a-fe63ae202db9",
      "source": "firecrawl/blog/automated-price-tracking-tutorial-python.md",
      "content": "Building an Amazon Price Tracker App Step-by-step\n\nSince this project involves multiple components working together, weâ€™ll take a top-down approach rather than building individual pieces first. This approach makes it easier to understand how everything fits together, since weâ€™ll introduce each tool only when itâ€™s needed. The benefits of this strategy will become clear as we progress through the tutorial.",
      "metadata": {
        "title": "Building an Automated Price Tracking Tool",
        "url": "https://www.firecrawl.dev/blog/automated-price-tracking-tutorial-python"
      }
    },
    {
      "id": "17ed3386-d6b1-44bc-a662-1da9de1bbccc",
      "source": "firecrawl/blog/automated-price-tracking-tutorial-python.md",
      "content": "Building an Amazon Price Tracker App Step-by-step > Step 2: Add a sidebar to the UI for product input\n\nLetâ€™s take a look at the final product one more time:  \n![A screenshot of an Amazon price tracker web application showing a sidebar for adding product URLs and a main dashboard displaying tracked products with price history charts. Created with streamlit and firecrawl](https://www.firecrawl.dev/images/blog/price-tracking/sneak-peek.png)  \nIt has two sections: the sidebar and the main dashboard. Since the first thing you do when launching this app is adding products, we will start building the sidebar first. Open `ui.py` and paste the following code:  \n```python\nimport streamlit as st\n\n# Set up sidebar\nwith st.sidebar:\nst.title(\"Add New Product\")\nproduct_url = st.text_input(\"Product URL\")\nadd_button = st.button(\"Add Product\")\n\n# Main content\nst.title(\"Price Tracker Dashboard\")\nst.markdown(\"## Tracked Products\")\n\n```  \nThe code snippet above sets up a basic Streamlit web application with two main sections. In the sidebar, it creates a form for adding new products with a text input field for the product URL and an â€œAdd Productâ€ button. The main content area contains a dashboard title and a section header for tracked products. The code uses Streamlitâ€™s `st.sidebar` context manager to create the sidebar layout and basic Streamlit components like `st.title`, `st.text_input`, and `st.button` to build the user interface elements.  \nTo see how this app looks like, run the following command:  \n```bash\nstreamlit run ui.py\n\n```  \nNow, letâ€™s add a commit to save our progress:  \n```bash\ngit add .\ngit commit -m \"Add a sidebar to the basic UI\"\n\n```",
      "metadata": {
        "title": "Building an Automated Price Tracking Tool",
        "url": "https://www.firecrawl.dev/blog/automated-price-tracking-tutorial-python"
      }
    },
    {
      "id": "58b675ef-329a-45ed-9a0d-201b28dce54c",
      "source": "firecrawl/blog/automated-price-tracking-tutorial-python.md",
      "content": "Building an Amazon Price Tracker App Step-by-step > Step 3: Add a feature to check if input URL is valid\n\nIn the next step, we want to add some restrictions to the input field like checking if the passed URL is valid. For this, create a new file called `utils.py` where we write additional utility functions for our app:  \n```bash\ntouch utils.py\n\n```  \nInside the script, paste following code:  \n```bash\n# utils.py\nfrom urllib.parse import urlparse\nimport re\n\ndef is_valid_url(url: str) -> bool:\ntry:\n# Parse the URL\nresult = urlparse(url)\n\n# Check if scheme and netloc are present\nif not all([result.scheme, result.netloc]):\nreturn False\n\n# Check if scheme is http or https\nif result.scheme not in [\"http\", \"https\"]:\nreturn False\n\n# Basic regex pattern for domain validation\ndomain_pattern = (\nr\"^[a-zA-Z0-9]([a-zA-Z0-9-]{0,61}[a-zA-Z0-9])?(.[a-zA-Z]{2,})+$\"\n)\nif not re.match(domain_pattern, result.netloc):\nreturn False\n\nreturn True\n\nexcept Exception:\nreturn False\n\n```  \nThe above function `is_valid_url()` validates URLs by checking several criteria:  \n1. It verifies the URL has both a scheme ( `http`/ `https`) and domain name\n2. It ensures the scheme is specifically `http` or `https`\n3. It validates the domain name format using regex to check for valid characters and TLD\n4. It returns True only if all checks pass, False otherwise  \nLetâ€™s use this function in our `ui.py` file. Here is the modified code:  \n```python\nimport streamlit as st\nfrom utils import is_valid_url\n\n# Set up sidebar\nwith st.sidebar:\nst.title(\"Add New Product\")\nproduct_url = st.text_input(\"Product URL\")\nadd_button = st.button(\"Add Product\")\n\nif add_button:\nif not product_url:\nst.error(\"Please enter a product URL\")\nelif not is_valid_url(product_url):\nst.error(\"Please enter a valid URL\")\nelse:\nst.success(\"Product is now being tracked!\")\n\n# Main content\n...\n\n```  \nHere is whatâ€™s new:  \n1. We added URL validation using the `is_valid_url()` function from `utils.py`\n2. When the button is clicked, we perform validation:\n- Check if URL is empty\n- Validate URL format using `is_valid_url()`\n3. User feedback is provided through error/success messages:\n- Error shown for empty URL\n- Error shown for invalid URL format\n- Success message when URL passes validation  \nRerun the Streamlit app again and see if our validation works. Then, return to your terminal to commit the changes weâ€™ve made:  \n```bash\ngit add .\ngit commit -m \"Add a feature to check URL validity\"\n\n```",
      "metadata": {
        "title": "Building an Automated Price Tracking Tool",
        "url": "https://www.firecrawl.dev/blog/automated-price-tracking-tutorial-python"
      }
    },
    {
      "id": "ed735800-aae3-42c0-98bb-cdb99ae455b7",
      "source": "firecrawl/blog/automated-price-tracking-tutorial-python.md",
      "content": "Building an Amazon Price Tracker App Step-by-step > Step 4: Scrape the input URL for product details\n\nWhen a valid URL is entered and the add button is clicked, we need to implement product scraping functionality instead of just showing a success message. The system should:  \n1. Immediately scrape the product URL to extract key details:\n- Product name\n- Current price\n- Main product image\n- Brand name\n- Other relevant attributes\n2. Store these details in a database to enable:\n- Regular price monitoring\n- Historical price tracking\n- Price change alerts\n- Product status updates  \nFor the scraper, we will use [Firecrawl](https://www.firecrawl.dev/), an AI-based scraping API for extracting webpage data without HTML parsing. This solution provides several advantages:  \n1. No website HTML code analysis required for element selection\n2. Resilient to HTML structure changes through AI-based element detection\n3. Universal compatibility with product webpages due to structure-agnostic approach\n4. Reliable website blocker bypass via robust API infrastructure  \nFirst, create a new file called `scraper.py`:  \n```bash\ntouch scraper.py\n\n```  \nThen, install these three libraries:  \n```bash\npip install firecrawl-py pydantic python-dotenv\necho \"firecrawl-pynpydanticnpython-dotenvn\" >> requirements.txt # Add them to dependencies\n\n```  \n`firecrawl-py` is the Python SDK for Firecrawl scraping engine, `pydantic` is a data validation library that helps enforce data types and structure through Python class definitions, and `python-dotenv` is a library that loads environment variables from a `.env` file into your Python application.  \nWith that said, head over to the Firecrawl website and [sign up for a free account](https://www.firecrawl.dev/) (the free plan will work fine). You will be given an API key, which you should copy.  \nThen, create a `.env` file in your terminal and add the API key as an environment variable:  \n```bash\ntouch .env\necho \"FIRECRAWL_API_KEY='YOUR-API-KEY-HERE' >> .env\"\necho \".env\" >> .gitignore # Ignore .env files in Git\n\n```  \nThe `.env` file is used to securely store sensitive configuration values like API keys that shouldnâ€™t be committed to version control. By storing the Firecrawl API key in `.env` and adding it to `.gitignore`, we ensure it stays private while still being accessible to our application code. This is a security best practice to avoid exposing credentials in source control.  \nNow, we can start writing the `scraper.py`:  \n```python\nfrom firecrawl import FirecrawlApp\nfrom pydantic import BaseModel, Field\nfrom dotenv import load_dotenv\nfrom datetime import datetime\n\nload_dotenv()\n\napp = FirecrawlApp()\n\n```  \nHere, `load_dotenv()` function reads the `.env` file you have in your working directory and loads the environment variables inside, including the Firecrawl API key. When you create an instance of `FirecrawlApp` class, the API key is automatically detected to establish a connection between your script and the scraping engine in the form of the `app` variable.  \nNow, we create a Pydantic class (usually called a model) that defines the details we want to scrape from each product:  \n```python\nclass Product(BaseModel):\n\"\"\"Schema for creating a new product\"\"\"\n\nurl: str = Field(description=\"The URL of the product\")\nname: str = Field(description=\"The product name/title\")\nprice: float = Field(description=\"The current price of the product\")\ncurrency: str = Field(description=\"Currency code (USD, EUR, etc)\")\nmain_image_url: str = Field(description=\"The URL of the main image of the product\")\n\n```  \nPydantic models may be completely new to you, so letâ€™s break down the `Product` model:  \n- The `url` field stores the product page URL we want to track\n- The `name` field stores the product title/name that will be scraped\n- The `price` field stores the current price as a float number\n- The `currency` field stores the 3-letter currency code (e.g. USD, EUR)\n- The `main_image_url` field stores the URL of the productâ€™s main image  \nEach field is typed and has a description that documents its purpose. The `Field` class from Pydantic allows us to add metadata like descriptions to each field. These descriptions are especially important for Firecrawl since it uses them to automatically locate the relevant HTML elements containing the data we want.  \nNow, letâ€™s create a function to call the engine to scrape URLâ€™s based on the schema above:  \n```python\ndef scrape_product(url: str):\nextracted_data = app.scrape_url(\nurl,\nparams={\n\"formats\": [\"extract\"],\n\"extract\": {\"schema\": Product.model_json_schema()},\n},\n)\n\n# Add the scraping date to the extracted data\nextracted_data[\"extract\"][\"timestamp\"] = datetime.utcnow()\n\nreturn extracted_data[\"extract\"]\n\nif __name__ == \"__main__\":\nproduct = \"https://www.amazon.com/gp/product/B002U21ZZK/\"\n\nprint(scrape_product(product))\n\n```  \nThe code above defines a function called `scrape_product` that takes a URL as input and uses it to scrape product information. Hereâ€™s how it works:  \nThe function calls `app.scrape_url` with two parameters:  \n1. The product URL to scrape\n2. A params dictionary that configures the scraping:\n- It specifies we want to use the â€œextractâ€ format\n- It provides our `Product` Pydantic model schema as the extraction template as a JSON object  \nThe scraper will attempt to find and extract data that matches our Product schema fields - the URL, name, price, currency, and image URL.  \nThe function returns just the â€œextractâ€ portion of the scraped data, which contains the structured product information. `extract` returns a dictionary to which we add the date of the scraping as it will be important later on.  \nLetâ€™s test the script by running it:  \n```bash\npython scraper.py\n\n```  \nYou should get an output like this:  \n```python\n{\n'url': 'https://www.amazon.com/dp/B002U21ZZK',\n'name': 'MOVA Globe Earth with Clouds 4.5\"',\n'price': 212,\n'currency': 'USD',\n'main_image_url': 'https://m.media-amazon.com/images/blog/price-tracking/I/41bQ3Y58y3L._AC_.jpg',\n'timestamp': '2024-12-05 13-20'\n}\n\n```  \nThe output shows that a [MOVA Globe](https://www.amazon.com/dp/B002U21ZZK) costs $212 USD on Amazon at the time of writing this article. You can test the script for any other website that contains the information we are looking (except Ebay):  \n- Price\n- Product name/title\n- Main image URL  \nOne key advantage of using Firecrawl is that it returns data in a consistent dictionary format across all websites. Unlike HTML-based scrapers like BeautifulSoup or Scrapy which require custom code for each site and can break when website layouts change, Firecrawl uses AI to understand and extract the requested data fields regardless of the underlying HTML structure.  \nFinish this step by committing the new changes to Git:  \n```bash\ngit add .\ngit commit -m \"Implement a Firecrawl scraper for products\"\n\n```",
      "metadata": {
        "title": "Building an Automated Price Tracking Tool",
        "url": "https://www.firecrawl.dev/blog/automated-price-tracking-tutorial-python"
      }
    },
    {
      "id": "c88e1e0c-2d6d-462a-8651-363856865bfa",
      "source": "firecrawl/blog/automated-price-tracking-tutorial-python.md",
      "content": "Building an Amazon Price Tracker App Step-by-step > Step 5: Storing new products in a PostgreSQL database\n\nIf we want to check product prices regularly, we need to have an online database. In this case, Postgres is the best option since itâ€™s reliable, scalable, and has great support for storing time-series data like price histories.  \nThere are many platforms for hosting Postgres instances but the one I find the easiest and fastest to set up is Supabase. So, please head over to [the Supabase website](https://supabase.com/) and create your free account. During the sign-up process, you will be given a password, which you should save somewhere safe on your machine.  \nThen, in a few minutes, your free Postgres instance comes online. To connect to this instance, click on Home in the left sidebar and then, â€œConnectâ€:  \n![Screenshot of Supabase dashboard showing database connection settings and credentials for connecting to a PostgreSQL database instance](https://www.firecrawl.dev/images/blog/price-tracking/supabase_connect.png)  \nYou will be shown your database connection string with a placeholder for the password you copied. You should paste this string in your `.env` file with your password added to the `.env` file:  \n```bash\necho POSTGRES_URL=\"THE-SUPABASE-URL-STRING-WITH-YOUR-PASSWORD-ADDED\"\n\n```  \nNow, the easiest way to interact with this database is through SQLAlchemy. Letâ€™s install it:  \n```bash\npip install \"sqlalchemy==2.0.35\" psycopg2-binary\necho \"psycopg2-binarynsqlalchemy==2.0.35n\" >> requirements.txt\n\n```  \n> Note: [SQLAlchemy](https://sqlalchemy.org/) is a Python SQL toolkit and Object-Relational Mapping (ORM) library that lets us interact with databases using Python code instead of raw SQL. For our price tracking project, it provides essential features like database connection management, schema definition through Python classes, and efficient querying capabilities. This makes it much easier to store and retrieve product information and price histories in our Postgres database.  \nAfter the installation, create a new `database.py` file for storing database-related functions:  \n```bash\ntouch database.py\n\n```  \nLetâ€™s populate this script:  \n```python\nfrom sqlalchemy import create_engine, Column, String, Float, DateTime, ForeignKey\nfrom sqlalchemy.orm import sessionmaker, relationship, declarative_base\nfrom datetime import datetime\n\nBase = declarative_base()\n\nclass Product(Base):\n__tablename__ = \"products\"\n\nurl = Column(String, primary_key=True)\nprices = relationship(\n\"PriceHistory\", back_populates=\"product\", cascade=\"all, delete-orphan\"\n)\n\nclass PriceHistory(Base):\n__tablename__ = \"price_histories\"\n\nid = Column(String, primary_key=True)\nproduct_url = Column(String, ForeignKey(\"products.url\"))\nname = Column(String, nullable=False)\nprice = Column(Float, nullable=False)\ncurrency = Column(String, nullable=False)\nmain_image_url = Column(String)\ntimestamp = Column(DateTime, nullable=False)\nproduct = relationship(\"Product\", back_populates=\"prices\")\n\n```  \nThe code above defines two SQLAlchemy models for our price tracking database:  \nThe `Product` model acts as a registry of all items we want to track. Itâ€™s kept simple with just the URL as we donâ€™t want to duplicate data that changes over time.  \nThe `PriceHistory` model stores the actual price data points and product details at specific moments in time. This separation allows us to:  \n- Track how product details (name, price, image) change over time\n- Maintain a clean historical record for each product\n- Efficiently query price trends without loading unnecessary data  \nEach record in `PriceHistory` contains:  \n- A unique ID as primary key\n- The product URL as a foreign key linking to the `Product`\n- The product name\n- The price value and currency\n- The main product image URL\n- A timestamp of when the price was recorded  \nThe relationship between `Product` and `PriceHistory` is bidirectional, allowing easy navigation between related records. The `cascade` setting ensures price histories are deleted when their product is deleted.  \nThese models provide the structure for storing and querying our price tracking data in a PostgreSQL database using SQLAlchemyâ€™s ORM capabilities.  \nNow, we define a `Database` class with a singe `add_product` method:  \n```python\nclass Database:\ndef __init__(self, connection_string):\nself.engine = create_engine(connection_string)\nBase.metadata.create_all(self.engine)\nself.Session = sessionmaker(bind=self.engine)\n\ndef add_product(self, url):\nsession = self.Session()\ntry:\n# Create the product entry\nproduct = Product(url=url)\nsession.merge(product) # merge will update if exists, insert if not\nsession.commit()\nfinally:\nsession.close()\n\n```  \nThe `Database` class above provides core functionality for managing product data in our PostgreSQL database. It takes a connection string in its constructor to establish the database connection using SQLAlchemy.  \nThe `add_product` method allows us to store new product URLs in the database. It uses SQLAlchemyâ€™s `merge` functionality which intelligently handles both inserting new products and updating existing ones, preventing duplicate entries.  \nThe method carefully manages database sessions, ensuring proper resource cleanup by using `try`/ `finally` blocks. This prevents resource leaks and maintains database connection stability.  \nLetâ€™s use this method inside the sidebar of our UI. Switch to `ui.py` and make the following adjustments:  \nFirst, update the imports to load the Database class and initialize it:  \n```python\nimport os\nimport streamlit as st\n\nfrom utils import is_valid_url\nfrom database import Database\nfrom dotenv import load_dotenv\n\nload_dotenv()\n\nwith st.spinner(\"Loading database...\"):\ndb = Database(os.getenv(\"POSTGRES_URL\"))\n\n```  \nThe code integrates the `Database` class into the Streamlit UI by importing required dependencies and establishing a database connection. The database URL is loaded securely from environment variables using `python-dotenv`. The `Database` class creates or updates the tables we specified in `database.py` after being initialized.  \nThe database initialization process is wrapped in a Streamlit spinner component to maintain responsiveness while establishing the connection. This provides visual feedback during the connection setup period, which typically requires a brief initialization time.  \nThen, in the sidebar code, we only need to add a single line of code to add the product to the database if the URL is valid:  \n```python\n# Set up sidebar\nwith st.sidebar:\nst.title(\"Add New Product\")\nproduct_url = st.text_input(\"Product URL\")\nadd_button = st.button(\"Add Product\")\n\nif add_button:\nif not product_url:\nst.error(\"Please enter a product URL\")\nelif not is_valid_url(product_url):\nst.error(\"Please enter a valid URL\")\nelse:\ndb.add_product(product_url) # This is the new line\nst.success(\"Product is now being tracked!\")\n\n```  \nIn the final `else` block that runs when the product URL is valid, we call the `add_product` method to store the product in the database.  \nLetâ€™s commit everything:  \n```bash\ngit add .\ngit commit -m \"Add a Postgres database integration for tracking product URLs\"\n\n```",
      "metadata": {
        "title": "Building an Automated Price Tracking Tool",
        "url": "https://www.firecrawl.dev/blog/automated-price-tracking-tutorial-python"
      }
    },
    {
      "id": "b4696629-83bd-48b8-8153-13d4e98c9e33",
      "source": "firecrawl/blog/automated-price-tracking-tutorial-python.md",
      "content": "Building an Amazon Price Tracker App Step-by-step > Step 6: Storing price histories for new products\n\nNow, after the product is added to the `products` table, we want to add its details and its scraped price to the `price_histories` table.  \nFirst, switch to `database.py` and add a new method for creating entries in the `PriceHistories` table:  \n```python\nclass Database:\n... # the rest of the class\n\ndef add_price(self, product_data):\nsession = self.Session()\ntry:\nprice_history = PriceHistory(\nid=f\"{product_data['url']}_{product_data['timestamp']}\",\nproduct_url=product_data[\"url\"],\nname=product_data[\"name\"],\nprice=product_data[\"price\"],\ncurrency=product_data[\"currency\"],\nmain_image_url=product_data[\"main_image_url\"],\ntimestamp=product_data[\"timestamp\"],\n)\nsession.add(price_history)\nsession.commit()\nfinally:\nsession.close()\n\n```  \nThe `add_price` method takes a dictionary containing product data (which is returned by our scraper) and creates a new entry in the `PriceHistory` table. The entryâ€™s ID is generated by combining the product URL with a timestamp. The method stores essential product information like name, price, currency, image URL, and the timestamp of when the price was recorded. It uses SQLAlchemyâ€™s session management to safely commit the new price history entry to the database.  \nNow, we need to add this functionality to the sidebar as well. In `ui.py`, add a new import statement that loads the `scrape_product` function from `scraper.py`:  \n```python\n... # The rest of the imports\nfrom scraper import scrape_product\n\n```  \nThen, update the `else` block in the sidebar again:  \n```python\nwith st.sidebar:\nst.title(\"Add New Product\")\nproduct_url = st.text_input(\"Product URL\")\nadd_button = st.button(\"Add Product\")\n\nif add_button:\nif not product_url:\nst.error(\"Please enter a product URL\")\nelif not is_valid_url(product_url):\nst.error(\"Please enter a valid URL\")\nelse:\ndb.add_product(product_url)\nwith st.spinner(\"Added product to database. Scraping product data...\"):\nproduct_data = scrape_product(product_url)\ndb.add_price(product_data)\nst.success(\"Product is now being tracked!\")\n\n```  \nNow when a user enters a product URL and clicks the â€œAdd Productâ€ button, several things happen:  \n1. The URL is validated to ensure itâ€™s not empty and is properly formatted.\n2. If valid, the URL is added to the products table via `add_product()`.\n3. The product page is scraped immediately to get current price data.\n4. This initial price data is stored in the price history table via `add_price()`.\n5. The user sees loading spinners and success messages throughout the process.  \nThis gives us a complete workflow for adding new products to track, including capturing their initial price point. The UI provides clear feedback at each step and handles errors gracefully.  \nCheck that everything is working the way we want it and then, commit the new changes:  \n```bash\ngit add .\ngit commit -m \"Add a feature to track product prices after they are added\"\n\n```",
      "metadata": {
        "title": "Building an Automated Price Tracking Tool",
        "url": "https://www.firecrawl.dev/blog/automated-price-tracking-tutorial-python"
      }
    },
    {
      "id": "99f24510-2b10-470d-b2a0-36c8110212a1",
      "source": "firecrawl/blog/automated-price-tracking-tutorial-python.md",
      "content": "Building an Amazon Price Tracker App Step-by-step > Step 7: Displaying each productâ€™s price history in the main dashboard\n\nLetâ€™s take a look at the final product shown in the introduction once again:  \n![Screenshot of a minimalist price tracking dashboard showing product price history charts, add/remove product controls, and notification settings for monitoring e-commerce deals and price drops](https://www.firecrawl.dev/images/blog/price-tracking/sneak-peek.png)  \nApart from the sidebar, the main dashboard shows each productâ€™s price history visualized with a Plotly line plot where the X axis is the timestamp while the Y axis is the prices. Each line plot is wrapped in a Streamlit component that includes buttons for removing the product from the database or visiting its source URL.  \nIn this step, we will implement the plotting feature and leave the two buttons for a later section. First, add a new method to the `Database` class for retrieving the price history for each product:  \n```python\nclass Database:\n... # The rest of the code\n\ndef get_price_history(self, url):\n\"\"\"Get price history for a product\"\"\"\nsession = self.Session()\ntry:\nreturn (\nsession.query(PriceHistory)\n.filter(PriceHistory.product_url == url)\n.order_by(PriceHistory.timestamp.desc())\n.all()\n)\nfinally:\nsession.close()\n\n```  \nThe method queries the price histories table based on product URL, orders the rows in descending order (oldest first) and returns the results.  \nThen, add another method for retrieving all products from the `products` table:  \n```python\nclass Database:\n...\n\ndef get_all_products(self):\nsession = self.Session()\ntry:\nreturn session.query(Product).all()\nfinally:\nsession.close()\n\n```  \nThe idea is that every time our Streamlit app is opened, the main dashboard queries all existing products from the database and render their price histories with line charts in dedicated components.  \nTo create the line charts, we need Plotly and Pandas, so install them in your environment:  \n```bash\npip install pandas plotly\necho \"pandasnplotlyn\" >> requirements.txt\n\n```  \nAfterward, import them at the top of `ui.py` along with other existing imports:  \n```python\nimport pandas as pd\nimport plotly.express as px\n\n```  \nThen, switch to `ui.py` and paste the following snippet of code after the Main content section:  \n```python\n# Main content\nst.title(\"Price Tracker Dashboard\")\nst.markdown(\"## Tracked Products\")\n\n# Get all products\nproducts = db.get_all_products()\n\n```  \nHere, after the page title and subtitle is shown, we are retrieving all products from the database. Letâ€™s loop over them:  \n```python\n# Create a card for each product\nfor product in products:\nprice_history = db.get_price_history(product.url)\nif price_history:\n# Create DataFrame for plotting\ndf = pd.DataFrame(\n[\\\n{\"timestamp\": ph.timestamp, \"price\": ph.price, \"name\": ph.name}\\\nfor ph in price_history\\\n]\n)\n\n```  \nFor each product, we get their price history with `db.get_price_history` and then, convert this data into a dataframe with three columns:  \n- Timestamp\n- Price\n- Product name  \nThis makes plotting easier with Plotly. Next, we create a Streamlit expander component for each product:  \n```python\n# Create a card for each product\nfor product in products:\nprice_history = db.get_price_history(product.url)\nif price_history:\n...\n# Create a card-like container for each product\nwith st.expander(df[\"name\"][0], expanded=False):\nst.markdown(\"---\")\ncol1, col2 = st.columns([1, 3])\n\nwith col1:\nif price_history[0].main_image_url:\nst.image(price_history[0].main_image_url, width=200)\nst.metric(\nlabel=\"Current Price\",\nvalue=f\"{price_history[0].price} {price_history[0].currency}\",\n)\n\n```  \nThe expander shows the product name as its title and contains:  \n1. A divider line\n2. Two columns:\n- Left column: Product image (if available) and current price metric\n- Right column (shown in next section)  \nThe price is displayed using Streamlitâ€™s metric component which shows the current price and currency.  \nHere is the rest of the code:  \n```python\n...\n\nwith col2:\n# Create price history plot\nfig = px.line(\ndf,\nx=\"timestamp\",\ny=\"price\",\ntitle=None,\n)\nfig.update_layout(\nxaxis_title=None,\nyaxis_title=\"Price\",\nshowlegend=False,\nmargin=dict(l=0, r=0, t=0, b=0),\nheight=300,\n)\nfig.update_xaxes(tickformat=\"%Y-%m-%d %H:%M\", tickangle=45)\nfig.update_yaxes(tickprefix=f\"{price_history[0].currency} \", tickformat=\".2f\")\nst.plotly_chart(fig, use_container_width=True)\n\n```  \nIn the right column, we create an interactive line plot using Plotly Express to visualize the price history over time. The plot shows price on the y-axis and timestamp on the x-axis. The layout is customized to remove the title, adjust axis labels and formatting, and optimize the display size. The timestamps are formatted to show date and time, with angled labels for better readability. Prices are displayed with 2 decimal places and a dollar sign prefix. The plot is rendered using Streamlitâ€™s `plotly_chart` component and automatically adjusts its width to fill the container.  \nAfter this step, the UI must be fully functional and ready to track products. For example, here is what mine looks like after adding a couple of products:  \n![Screenshot of a price tracking dashboard showing multiple product listings with price history charts, product images, and current prices for Amazon items](https://www.firecrawl.dev/images/blog/price-tracking/finished.png)  \nBut notice how the price history chart doesnâ€™t show anything. Thatâ€™s because we havenâ€™t populated it by checking the product price in regular intervals. Letâ€™s do that in the next couple of steps. For now, commit the latest changes weâ€™ve made:  \n```bash\ngit add .\ngit commit -m \"Display product price histories for each product in the dashboard\"\n\n```  \n* * *  \nLetâ€™s take a brief moment to summarize the steps we took so far and whatâ€™s next. So far, weâ€™ve built a Streamlit interface that allows users to add product URLs and displays their current prices and basic information. Weâ€™ve implemented the database schema, created functions to scrape product data, and designed a clean UI with price history visualization. The next step is to set up automated price checking to populate our history charts and enable proper price tracking over time.",
      "metadata": {
        "title": "Building an Automated Price Tracking Tool",
        "url": "https://www.firecrawl.dev/blog/automated-price-tracking-tutorial-python"
      }
    },
    {
      "id": "f3314b23-00df-4fd3-8997-6940ae7b6e96",
      "source": "firecrawl/blog/automated-price-tracking-tutorial-python.md",
      "content": "Building an Amazon Price Tracker App Step-by-step > Step 8: Adding new price entries for existing products\n\nNow, we want to write a script that adds new price entries in the `price_histories` table for each product in `products` table. We call this script `check_prices.py`:  \n```python\nimport os\nfrom database import Database\nfrom dotenv import load_dotenv\nfrom firecrawl import FirecrawlApp\nfrom scraper import scrape_product\n\nload_dotenv()\n\ndb = Database(os.getenv(\"POSTGRES_URL\"))\napp = FirecrawlApp()\n\n```  \nAt the top, we are importing the functions and packages and initializing the database and a Firecrawl app. Then, we define a simple `check_prices` function:  \n```python\ndef check_prices():\nproducts = db.get_all_products()\n\nfor product in products:\ntry:\nupdated_product = scrape_product(product.url)\ndb.add_price(updated_product)\nprint(f\"Added new price entry for {updated_product['name']}\")\nexcept Exception as e:\nprint(f\"Error processing {product.url}: {e}\")\n\nif __name__ == \"__main__\":\ncheck_prices()\n\n```  \nIn the function body, we retrieve all products URLs, retrieve their new price data with `scrape_product` function from `scraper.py` and then, add a new price entry for the product with `db.add_price`.  \nIf you run the function once and refresh the Streamlit app, you must see a line chart appear for each product you are tracking:  \n![Screenshot of a price tracking dashboard showing a line chart visualization of product price history over time, with price on the y-axis and dates on the x-axis](https://www.firecrawl.dev/images/blog/price-tracking/linechart.png)  \nLetâ€™s commit the changes in this step:  \n```bash\ngit add .\ngit commit -m \"Add a script for checking prices of existing products\"\n\n```",
      "metadata": {
        "title": "Building an Automated Price Tracking Tool",
        "url": "https://www.firecrawl.dev/blog/automated-price-tracking-tutorial-python"
      }
    },
    {
      "id": "e0d457e3-9ec5-403e-978b-ac127bef2b1e",
      "source": "firecrawl/blog/automated-price-tracking-tutorial-python.md",
      "content": "Building an Amazon Price Tracker App Step-by-step > Step 9: Check prices regularly with GitHub actions\n\nGitHub Actions is a continuous integration and continuous delivery (CI/CD) platform that allows you to automate various software workflows directly from your GitHub repository. In our case, itâ€™s particularly useful because we can set up automated price checks to run the `check_prices.py` script at regular intervals (e.g., daily or hourly) without manual intervention. This ensures we consistently track price changes and maintain an up-to-date database of historical prices for our tracked products.  \nSo, the first step is creating a new GitHub repository for our project and pushing existing code to it:  \n```bash\ngit remote add origin https://github.com/yourusername/price-tracker.git\ngit push origin main\n\n```  \nThen, return to your terminal and create this directory structure:  \n```bash\nmkdir -p .github/workflows\ntouch .github/workflows/check_prices.yml\n\n```  \nThe first command creates a new directory structure `.github/workflows` using the `-p` flag to create parent directories if they donâ€™t exist.  \nThe second command creates an empty YAML file called `check_prices.yml` inside the workflows directory. GitHub Actions looks for workflow files in this specific location - any YAML files in the `.github/workflows` directory will be automatically detected and processed as workflow configurations. These YAML files define when and how your automated tasks should run, what environment they need, and what commands to execute. In our case, this file will contain instructions for GitHub Actions to periodically run our price checking script. Letâ€™s write it:  \n```yaml\nname: Price Check\n\non:\nschedule:\n# Runs every 3 minutes\n- cron: \"*/3 * * * *\"\nworkflow_dispatch: # Allows manual triggering\n\n```  \nLetâ€™s break down this first part of the YAML file:  \nThe `name: Price Check` line gives our workflow a descriptive name that will appear in the GitHub Actions interface.  \nThe `on:` section defines when this workflow should be triggered. Weâ€™ve configured two triggers:  \n1. A schedule using cron syntax `*/3 * * * *` which runs the workflow every 3 minutes. The five asterisks represent minute, hour, day of month, month, and day of week respectively. The `*/3` means â€œevery 3rd minuteâ€. The 3-minute interval is for debugging purposes, we will need to choose a wider interval later on to respect the free limits of GitHub actions.  \n2. `workflow_dispatch` enables manual triggering of the workflow through the GitHub Actions UI, which is useful for testing or running the check on-demand.  \nNow, letâ€™s add the rest:  \n```yaml\njobs:\ncheck-prices:\nruns-on: ubuntu-latest\n\nsteps:\n- name: Checkout code\nuses: actions/checkout@v4\n\n- name: Set up Python\nuses: actions/setup-python@v5\nwith:\npython-version: \"3.10\"\ncache: \"pip\"\n\n- name: Install dependencies\nrun: |\npython -m pip install --upgrade pip\npip install -r requirements.txt\n\n- name: Run price checker\nenv:\nFIRECRAWL_API_KEY: ${{ secrets.FIRECRAWL_API_KEY }}\nPOSTGRES_URL: ${{ secrets.POSTGRES_URL }}\nrun: python check_prices.py\n\n```  \nLetâ€™s break down this second part of the YAML file:  \nThe `jobs:` section defines the actual work to be performed. We have one job named `check-prices` that runs on an Ubuntu virtual machine ( `runs-on: ubuntu-latest`).  \nUnder `steps:`, we define the sequence of actions:  \n1. First, we checkout our repository code using the standard `actions/checkout@v4` action  \n2. Then we set up Python 3.10 using `actions/setup-python@v5`, enabling pip caching to speed up dependency installation  \n3. Next, we install our Python dependencies by upgrading `pip` and installing requirements from our `requirements.txt` file. At this point, it is essential that you were keeping a complete dependency file based on the installs we made in the project.  \n4. Finally, we run our price checker script, providing two environment variables:\n- `FIRECRAWL_API_KEY`: For accessing the web scraping service\n- `POSTGRES_URL`: For connecting to our database  \nBoth variables must be stored in our GitHub repository as secrets for this workflow file to run without errors. So, navigate to the repository youâ€™ve created for the project and open its Settings. Under â€œSecrets and variablesâ€ > â€œActionsâ€, click on â€œNew repository secretâ€ button to add the environment variables we have in the `.env` file one-by-one.  \nThen, return to your terminal, commit the changes and push:  \n```bash\ngit add .\ngit commit -m \"Add a workflow to check prices regularly\"\ngit push origin main\n\n```  \nNext, navigate to your GitHub repository again and click on the â€œActionsâ€ tab:  \n![Screenshot of GitHub Actions interface showing workflow runs and manual trigger button for automated price tracking application](https://www.firecrawl.dev/images/blog/price-tracking/actions.png)  \nFrom there, you can run the workflow manually (click â€œRun workflowâ€ and refresh the page). If it is executed successfully, you can return to the Streamlit app and refresh to see the new price added to the chart.",
      "metadata": {
        "title": "Building an Automated Price Tracking Tool",
        "url": "https://www.firecrawl.dev/blog/automated-price-tracking-tutorial-python"
      }
    },
    {
      "id": "24cf75b5-51d2-480f-b110-9c14f0229b13",
      "source": "firecrawl/blog/automated-price-tracking-tutorial-python.md",
      "content": "Building an Amazon Price Tracker App Step-by-step > Step 10: Setting up Discord for notifications\n\nNow that we know our scheduling workflow works, the first order of business is setting a wider check interval in the workflow file. Even though our first workflow run was manually, the rest happen automatically.  \n```bash\non:\nschedule:\n# Runs every 6 hours\n- cron: \"0 0,6,12,18 * * *\"\nworkflow_dispatch: # Allows manual triggering\n\n```  \nThe cron syntax `0 0,6,12,18 * * *` can be broken down as follows:  \n- First `0`: Run at minute 0\n- `0,6,12,18`: Run at hours 0 (midnight), 6 AM, 12 PM (noon), and 6 PM\n- First `*`: Run every day of the month\n- Second `*`: Run every month\n- Third `*`: Run every day of the week  \nSo this schedule will check prices four times daily: at midnight, 6 AM, noon, and 6 PM (UTC time). This spacing helps stay within GitHub Actionsâ€™ free tier limits while still catching most price changes.  \nNow, commit and push the changes:  \n```bash\ngit add .\ngit commit -m \"Set a wider check interval in the workflow file\"\ngit push origin main\n\n```  \nNow comes the interesting part. Each time the workflow is run, we want to compare the current price of the product to its original price when we started tracking it. If the difference between these two prices exceeds a certain threshold like 5%, this means there is a discount happening for the product and we want to send a notification.  \nThe easiest way to set this up is by using Discord webhooks. So, if you donâ€™t have one already, go to Discord.com and create a new account (optionally, download the desktop app as well). Then, setting up Discord notifications requires a few careful steps:  \n1. **Create a discord server**\n- Click the â€+â€ button in the bottom-left corner of Discord\n- Choose â€œCreate My Ownâ€ â†’ â€œFor me and my friendsâ€\n- Give your server a name (e.g., â€œPrice Alertsâ€)\n2. **Create a channel for alerts**\n- Your server comes with a #general channel by default\n- You can use this or create a new channel called #price-alerts\n- Right-click the channel you want to use\n3. **Set up the webhook**\n- Select â€œEdit Channelâ€ from the right-click menu\n- Go to the â€œIntegrationsâ€ tab\n- Click â€œCreate Webhookâ€\n- Give it a name like â€œPrice Alert Botâ€\n- The webhook URL will be generated automatically\n- Click â€œCopy Webhook URLâ€ - this is your unique notification endpoint\n4. **Secure the webhook URL**\n- Never share or commit your webhook URL directly\n- Add it to your `.env` file as `DISCORD_WEBHOOK_URL`\n- Add it to your GitHub repository secrets\n- The URL should look something like: `https://discord.com/api/webhooks/...`  \nThis webhook will serve as a secure endpoint that our price tracker can use to send notifications directly to your Discord channel.  \nWebhooks are automated messages sent from apps to other apps in real-time. They work like a notification system - when something happens in one app, it automatically sends data to another app through a unique URL. In our case, weâ€™ll use Discord webhooks to automatically notify us when thereâ€™s a price drop. Whenever our price tracking script detects a significant discount, it will send a message to our Discord channel through the webhook URL, ensuring we never miss a good deal.  \nAfter copying the webhook URL, you should save it as environment variable to your `.env` file:  \n```python\necho \"DISCORD_WEBHOOK_URL='THE-URL-YOU-COPIED'\" >> .env\n\n```  \nNow, create a new file called `notifications.py` and paste the following contents:  \n```python\nfrom dotenv import load_dotenv\nimport os\nimport aiohttp\nimport asyncio\n\nload_dotenv()\n\nasync def send_price_alert(\nproduct_name: str, old_price: float, new_price: float, url: str\n):\n\"\"\"Send a price drop alert to Discord\"\"\"\ndrop_percentage = ((old_price - new_price) / old_price) * 100\n\nmessage = {\n\"embeds\": [\\\n{\\\n\"title\": \"Price Drop Alert! ðŸŽ‰\",\\\n\"description\": f\"**{product_name}**nPrice dropped by {drop_percentage:.1f}%!n\"\\\nf\"Old price: ${old_price:.2f}n\"\\\nf\"New price: ${new_price:.2f}n\"\\\nf\"[View Product]({url})\",\\\n\"color\": 3066993,\\\n}\\\n]\n}\n\ntry:\nasync with aiohttp.ClientSession() as session:\nawait session.post(os.getenv(\"DISCORD_WEBHOOK_URL\"), json=message)\nexcept Exception as e:\nprint(f\"Error sending Discord notification: {e}\")\n\n```  \nThe `send_price_alert` function above is responsible for sending price drop notifications to Discord using webhooks. Letâ€™s break down whatâ€™s new:  \n1. The function takes 4 parameters:\n- `product_name`: The name of the product that dropped in price\n- `old_price`: The previous price before the drop\n- `new_price`: The current lower price\n- `url`: Link to view the product\n2. It calculates the percentage drop in price using the formula: `((old_price - new_price) / old_price) * 100`  \n3. The notification is formatted as a Discord embed - a rich message format that includes:\n- A title with a celebration emoji\n- A description showing the product name, price drop percentage, old and new prices\n- A link to view the product\n- A green color (3066993 in decimal)\n4. The message is sent asynchronously using `aiohttp` to post to the Discord webhook URL stored in the environment variables  \n5. Error handling is included to catch and print any issues that occur during the HTTP request  \nThis provides a clean way to notify users through Discord whenever we detect a price drop for tracked products.  \nTo check the notification system works, add this main block to the end of the script:  \n```python\nif __name__ == \"__main__\":\nasyncio.run(send_price_alert(\"Test Product\", 100, 90, \"https://www.google.com\"))\n\n```  \n`asyncio.run()` is used here because `send_price_alert` is an async function that needs to be executed in an event loop. `asyncio.run()` creates and manages this event loop, allowing the async HTTP request to be made properly. Without it, we wouldnâ€™t be able to use the `await` keyword inside `send_price_alert`.  \nTo run the script, install `aiohttp`:  \n```python\npip install aiohttp\necho \"aiohttpn\" >> requirements.txt\npython notifications.py\n\n```  \nIf all is well, you should get a Discord message in your server that looks like this:  \n![Screenshot of a Discord notification showing a price drop alert with product details, original price, new discounted price and percentage savings](https://www.firecrawl.dev/images/blog/price-tracking/alert.png)  \nLetâ€™s commit the changes we have:  \n```bash\ngit add .\ngit commit -m \"Set up Discord alert system\"\n\n```  \nAlso, donâ€™t forget to add the Discord webhook URL to your GitHub repository secrets!",
      "metadata": {
        "title": "Building an Automated Price Tracking Tool",
        "url": "https://www.firecrawl.dev/blog/automated-price-tracking-tutorial-python"
      }
    },
    {
      "id": "32357126-0b3b-4570-a04b-c9413b56350b",
      "source": "firecrawl/blog/automated-price-tracking-tutorial-python.md",
      "content": "Building an Amazon Price Tracker App Step-by-step > Step 11: Sending Discord alerts when prices drop\n\nNow, the only step left is adding a price comparison logic to `check_prices.py`. In other words, we want to use the `send_price_alert` function if the new scraped price is lower than the original. This requires a revamped `check_prices.py` script:  \n```python\nimport os\nimport asyncio\nfrom database import Database\nfrom dotenv import load_dotenv\nfrom firecrawl import FirecrawlApp\nfrom scraper import scrape_product\nfrom notifications import send_price_alert\n\nload_dotenv()\n\ndb = Database(os.getenv(\"POSTGRES_URL\"))\napp = FirecrawlApp()\n\n# Threshold percentage for price drop alerts (e.g., 5% = 0.05)\nPRICE_DROP_THRESHOLD = 0.05\n\nasync def check_prices():\nproducts = db.get_all_products()\nproduct_urls = set(product.url for product in products)\n\nfor product_url in product_urls:\n# Get the price history\nprice_history = db.get_price_history(product_url)\nif not price_history:\ncontinue\n\n# Get the earliest recorded price\nearliest_price = price_history[-1].price\n\n# Retrieve updated product data\nupdated_product = scrape_product(product_url)\ncurrent_price = updated_product[\"price\"]\n\n# Add the price to the database\ndb.add_price(updated_product)\nprint(f\"Added new price entry for {updated_product['name']}\")\n\n# Check if price dropped below threshold\nif earliest_price > 0: # Avoid division by zero\nprice_drop = (earliest_price - current_price) / earliest_price\nif price_drop >= PRICE_DROP_THRESHOLD:\nawait send_price_alert(\nupdated_product[\"name\"], earliest_price, current_price, product_url\n)\n\nif __name__ == \"__main__\":\nasyncio.run(check_prices())\n\n```  \nLetâ€™s examine the key changes in this enhanced version of `check_prices.py`:  \n1. New imports and setup\n- Added `asyncio` for `async`/ `await` support\n- Imported `send_price_alert` from `notifications.py`\n- Defined `PRICE_DROP_THRESHOLD = 0.05` (5% threshold for alerts)\n2. Async function conversion\n- Converted `check_prices()` to async function\n- Gets unique product URLs using set comprehension to avoid duplicates\n3. Price history analysis\n- Retrieves full price history for each product\n- Gets `earliest_price` from `history[-1]` (works because we ordered by timestamp DESC)\n- Skips products with no price history using `continue`\n4. Price drop detection logic\n- Calculates drop percentage: `(earliest_price - current_price) / earliest_price`\n- Checks if drop exceeds 5% threshold\n- Sends Discord alert if threshold exceeded using `await send_price_alert()`\n5. Async main block\n- Uses `asyncio.run()` to execute async `check_prices()` in event loop  \nWhen I tested this new version of the script, I immediately got an alert:  \n![Screenshot of a Discord notification showing a price drop alert for an Amazon product, displaying the original and discounted prices with percentage savings](https://www.firecrawl.dev/images/blog/price-tracking/new-alert.png)  \nBefore we supercharge our workflow with the new notification system, you should add this line of code to your `check_prices.yml` workflow file to read the Discord webhook URL from your GitHub secrets:  \n```python\n...\n- name: Run price checker\nenv:\nFIRECRAWL_API_KEY: ${{ secrets.FIRECRAWL_API_KEY }}\nPOSTGRES_URL: ${{ secrets.POSTGRES_URL }}\nDISCORD_WEBHOOK_URL: ${{ secrets.DISCORD_WEBHOOK_URL }}\nrun: python automated_price_tracking/check_prices.py\n\n```  \nFinally, letâ€™s commit everything and push to GitHub so that our workflow is supercharged with our notification system:  \n```bash\ngit add .\ngit commit -m \"Add notification system to price drops\"\ngit push origin main\n\n```",
      "metadata": {
        "title": "Building an Automated Price Tracking Tool",
        "url": "https://www.firecrawl.dev/blog/automated-price-tracking-tutorial-python"
      }
    },
    {
      "id": "6ae33c5f-9962-4174-b92b-8c37bd2e5829",
      "source": "firecrawl/blog/automated-price-tracking-tutorial-python.md",
      "content": "Limitations of Free Tier Tools Used in the Tutorial\n\nBefore wrapping up, letâ€™s quickly review the limitations of the free tools we used in this tutorial:  \n- GitHub Actions: Limited to 2,000 minutes per month for free accounts. Consider increasing the cron interval to stay within limits.\n- Supabase: Free tier includes 500MB database storage and limited row count. Monitor usage if tracking many products.\n- Firecrawl: Free API tier allows 500 requests per month. This means that at 6 hour intervals, you can track up to four products in the free plan.\n- Streamlit Cloud: Free hosting tier has some memory/compute restrictions and goes to sleep after inactivity.  \nWhile these limitations exist, theyâ€™re quite generous for personal use and learning. The app will work well for tracking a reasonable number of products with daily price checks.",
      "metadata": {
        "title": "Building an Automated Price Tracking Tool",
        "url": "https://www.firecrawl.dev/blog/automated-price-tracking-tutorial-python"
      }
    },
    {
      "id": "733ed51a-4577-4fd6-8131-6a63f56a3037",
      "source": "firecrawl/blog/automated-price-tracking-tutorial-python.md",
      "content": "Conclusion and Next Steps\n\nCongratulations for making it to the end of this extremely long tutorial! Weâ€™ve just covered how to implement an end-to-end Python project you can proudly showcase on your portfolio. We built a complete price tracking system that scrapes product data from e-commerce websites, stores it in a Postgres database, analyzes price histories, and sends automated Discord notifications when prices drop significantly. Along the way, we learned about web scraping with Firecrawl, database management with SQLAlchemy, asynchronous programming with asyncio, building interactive UIs with Streamlit, automating with GitHub actions and integrating external webhooks.  \nHowever, the project is far from perfect. Since we took a top-down approach to building this app, our project code is scattered across multiple files and often doesnâ€™t follow programming best practices. For this reason, Iâ€™ve recreated the same project in a much more sophisticated manner with production-level features. [This new version on GitHub](https://github.com/BexTuychiev/automated-price-tracking) implements proper database session management, faster operations and overall smoother user experience. Also, this version includes buttons for removing products from the database and visiting them through the app.  \nIf you decide to stick with the basic version, you can find the full project code and notebook in the official Firecrawl GitHub repositoryâ€™s example projects. I also recommend that you [deploy your Streamlit app to Streamlit Cloud](https://share.streamlit.io/) so that you have a functional app accessible everywhere you go.  \nHere are some further improvements you might consider for the app:  \n- Improve the price comparison logic: the app compares the current price to the oldest recorded price, which might not be ideal. You may want to compare against recent price trends instead.\n- No handling of currency conversion if products use different currencies.\n- The Discord notification system doesnâ€™t handle rate limits or potential webhook failures gracefully.\n- No error handling for Firecrawl scraper - what happens if the scraping fails?\n- No consistent usage of logging to help track issues in production.\n- No input URL sanitization before scraping.  \nSome of these features are implemented in [the advanced version of the project](https://github.com/BexTuychiev/automated-price-tracking), so definitely check it out!  \nHere are some more guides from our blog if you are interested:  \n- [How to Run Web Scrapers on Schedule](https://www.firecrawl.dev/blog/automated-web-scraping-free-2025)\n- [More about using Firecrawlâ€™s `scrape_url` function](https://www.firecrawl.dev/blog/mastering-firecrawl-scrape-endpoint)\n- [Scraping entire websites with Firecrawl in a single command - the /crawl endpoint](https://www.firecrawl.dev/blog/mastering-the-crawl-endpoint-in-firecrawl)  \nThank you for reading!  \nArticle updated recently  \n[ðŸ”¥](https://www.firecrawl.dev/)",
      "metadata": {
        "title": "Building an Automated Price Tracking Tool",
        "url": "https://www.firecrawl.dev/blog/automated-price-tracking-tutorial-python"
      }
    },
    {
      "id": "0a87dcff-89cb-4b42-9060-b5cef0f68a38",
      "source": "firecrawl/blog/automated-price-tracking-tutorial-python.md",
      "content": "Ready to _Build?_\n\nStart scraping web data for your AI apps today.  \nNo credit card needed.  \nGet Started",
      "metadata": {
        "title": "Building an Automated Price Tracking Tool",
        "url": "https://www.firecrawl.dev/blog/automated-price-tracking-tutorial-python"
      }
    },
    {
      "id": "d3a68eb4-e4f0-4dde-be13-729e207f166b",
      "source": "firecrawl/blog/automated-price-tracking-tutorial-python.md",
      "content": "About the Author\n\n[![Bex Tuychiev image](https://www.firecrawl.dev/bex.jpg)\\\nBex Tuychiev@bextuychiev](https://x.com/bextuychiev)  \nBex is a Top 10 AI writer on Medium and a Kaggle Master with over 15k followers. He loves writing detailed guides, tutorials, and notebooks on complex data science and machine learning topics",
      "metadata": {
        "title": "Building an Automated Price Tracking Tool",
        "url": "https://www.firecrawl.dev/blog/automated-price-tracking-tutorial-python"
      }
    },
    {
      "id": "e8fbc505-16a6-453b-82a9-79015541b04c",
      "source": "firecrawl/blog/automated-price-tracking-tutorial-python.md",
      "content": "About the Author > More articles by Bex Tuychiev\n\n[Building an Automated Price Tracking Tool\\\n\\\nBuild an automated e-commerce price tracker in Python. Learn web scraping, price monitoring, and automated alerts using Firecrawl, Streamlit, PostgreSQL.](https://www.firecrawl.dev/blog/automated-price-tracking-tutorial-python) [Web Scraping Automation: How to Run Scrapers on a Schedule\\\n\\\nLearn how to automate web scraping in Python using free tools like schedule, asyncio, cron jobs and GitHub Actions. This comprehensive guide covers local and cloud-based scheduling methods to run scrapers reliably in 2025.](https://www.firecrawl.dev/blog/automated-web-scraping-free-2025) [Automated Data Collection - A Comprehensive Guide\\\n\\\nLearn how to build robust automated data collection systems using modern tools and best practices. This guide covers everything from selecting the right tools to implementing scalable collection pipelines.](https://www.firecrawl.dev/blog/automated-data-collection-guide) [BeautifulSoup4 vs. Scrapy - A Comprehensive Comparison for Web Scraping in Python\\\n\\\nLearn the key differences between BeautifulSoup4 and Scrapy for web scraping in Python. Compare their features, performance, and use cases to choose the right tool for your web scraping needs.](https://www.firecrawl.dev/blog/beautifulsoup4-vs-scrapy-comparison) [How to Build an Automated Competitor Price Monitoring System with Python\\\n\\\nLearn how to build an automated competitor price monitoring system in Python that tracks prices across e-commerce sites, provides real-time comparisons, and maintains price history using Firecrawl, Streamlit, and GitHub Actions.](https://www.firecrawl.dev/blog/automated-competitor-price-scraping) [Scraping Company Data and Funding Information in Bulk With Firecrawl and Claude\\\n\\\nLearn how to build a web scraper in Python that gathers company details, funding rounds, and investor information from public sources like Crunchbase using Firecrawl and Claude for automated data collection and analysis.](https://www.firecrawl.dev/blog/crunchbase-scraping-with-firecrawl-claude) [Data Enrichment: A Complete Guide to Enhancing Your Data Quality\\\n\\\nLearn how to enrich your data quality with a comprehensive guide covering data enrichment tools, best practices, and real-world examples. Discover how to leverage modern solutions like Firecrawl to automate data collection, validation, and integration for better business insights.](https://www.firecrawl.dev/blog/complete-guide-to-data-enrichment) [Building an Intelligent Code Documentation RAG Assistant with DeepSeek and Firecrawl\\\n\\\nLearn how to build an intelligent documentation assistant powered by DeepSeek and RAG (Retrieval Augmented Generation) that can answer questions about any documentation website by combining language models with efficient information retrieval.](https://www.firecrawl.dev/blog/deepseek-rag-documentation-assistant)",
      "metadata": {
        "title": "Building an Automated Price Tracking Tool",
        "url": "https://www.firecrawl.dev/blog/automated-price-tracking-tutorial-python"
      }
    },
    {
      "id": "408bf2c1-af61-40e1-b10a-1cd18fd62886",
      "source": "firecrawl/blog/lead-gen-business-insights-make-firecrawl.md",
      "content": "---\ntitle: Using LLM Extraction for Customer Insights\nurl: https://www.firecrawl.dev/blog/lead-gen-business-insights-make-firecrawl\n---  \nIntroducing /extract - Get web data with a prompt [Try now](https://www.firecrawl.dev/extract)  \nMay 21, 2024  \nâ€¢  \n[![Caleb Peffer image](https://www.firecrawl.dev/caleb-img.jpeg)Caleb Peffer](https://x.com/CalebPeffer)  \n# Using LLM Extraction for Customer Insights  \n![Using LLM Extraction for Customer Insights image](https://www.firecrawl.dev/images/blog/g3.png)",
      "metadata": {
        "title": "Using LLM Extraction for Customer Insights",
        "url": "https://www.firecrawl.dev/blog/lead-gen-business-insights-make-firecrawl"
      }
    },
    {
      "id": "0a424ac8-9fdc-4562-b30e-51a92d4b2864",
      "source": "firecrawl/blog/lead-gen-business-insights-make-firecrawl.md",
      "content": "Introduction\n\nUnderstanding our customers - not just who they are, but what they doâ€”is crucial to tailoring our products and services effectively. When running a self-serve motion, you have so many customers come in the door with little to no knowledge of them. The process of proactively understanding who these folks are has traditionally been time-intensive, involving manual data collection and analysis to gather actionable insights.  \nHowever, with the power of LLMs and their capacity for advanced data extraction, weâ€™ve automated this process. Using LLM extraction and analysis of customer data, LLM weâ€™ve significantly reduced our workload, allowing us to understand and serve our customer base more effectively than ever before.  \nIf you have limited technical knowledge, you can build an automation that gets targeted information about your customers for the purposes of product direction and lead gen. Hereâ€™s how you can do this yourself with [Make](https://make.com/) and [Firecrawl](https://www.firecrawl.dev/).  \n* * *",
      "metadata": {
        "title": "Using LLM Extraction for Customer Insights",
        "url": "https://www.firecrawl.dev/blog/lead-gen-business-insights-make-firecrawl"
      }
    },
    {
      "id": "4c4de36c-bb7a-4960-a5a7-dee8cd0a9755",
      "source": "firecrawl/blog/lead-gen-business-insights-make-firecrawl.md",
      "content": "Overview of the Tools\n\n**Firecrawl**  \nFirecrawl is a platform for scraping, search, and extraction. It allows you to take data from the web and translate it into LLM-legible markdown or structured data.  \nWhen we want to get information about our customers, we can use Firecrawlâ€™s LLM extraction functionality to specify the specific information we want from their websites.  \n**Make.com (formerly Integromat)**  \nMake is an automation platform that allows users to create customized workflows to connect various apps and services without needing deep technical knowledge. It uses a visual interface where users can drag and drop elements to design their automations.  \nWe can use Make to connect a spreadsheet of user data to Firecrawl, allowing us to do extraction with just a bit of JSON.  \n###Setting Up the Scenario  \n- Step-by-step guide on setting up the data extraction process.\n- **Connecting Google Sheets to Make.com**\n- How user data is initially collected and stored.\n- **Configuring the HTTP Request in Make.com**\n- Description of setting up API requests to Firecrawl.\n- Purpose of these requests (e.g., extracting company information).",
      "metadata": {
        "title": "Using LLM Extraction for Customer Insights",
        "url": "https://www.firecrawl.dev/blog/lead-gen-business-insights-make-firecrawl"
      }
    },
    {
      "id": "5d7d1436-e042-447f-8125-791e65bd9573",
      "source": "firecrawl/blog/lead-gen-business-insights-make-firecrawl.md",
      "content": "Preparing our Data\n\nBefore we get started, we want to make sure we prepare our data for Firecrawl. In this case, I created a simple spreadsheet with imported users from our database. We want to take the email domains of our users and transform them into links using the https:// format:  \n!  \nWe also want to add some attributes that weâ€™d like to know about these companies. For me, I want to understand a bit about the company, their industry, and their customers. Iâ€™ve set these in columns as:\ncompany_description\ncompany_type\nwho_they_serve  \nNow that we have our data prepared, we can start setting up our automation in Make!",
      "metadata": {
        "title": "Using LLM Extraction for Customer Insights",
        "url": "https://www.firecrawl.dev/blog/lead-gen-business-insights-make-firecrawl"
      }
    },
    {
      "id": "1be9ad36-2c33-4a04-9a5d-9a78aa293dec",
      "source": "firecrawl/blog/lead-gen-business-insights-make-firecrawl.md",
      "content": "Setting up our automation\n\nTo get our automation running, we simply need to follow a three step process in Make. Here, we will choose three apps in our scenario:  \nGoogle Sheets - Get range values\nHTTP - Make an API key auth request\nGoogle Sheets - Update a row  \nWeâ€™ll also want to add the ignore flow control tool in case we run into any errors. This will keep the automation going.  \n!  \nThis automation will allow us to extract a set of links from our spreadsheet, send them to Firecrawl for data extraction, then repopulate our spreadsheet with the desired info.  \nLetâ€™s start by configuring our first app. Our goal is to export all of the URLs so that we can send them to Firecrawl for extraction. Here is the configuration for pulling these URLs:  \n!  \n* _Important_ - we want to make sure we start pulling data from the second row. If you include the header, you will eventually run into an error.  \n* * *  \nGreat! Now that we have that configured, we want to prepare to set up our HTTP request. To do this, we will go to [https://firecrawl.dev](https://firecrawl.dev/) to sign up and get our API key (you can get started for free!). Once you sign up, you can go to [https://firecrawl.dev/account](https://firecrawl.dev/account) to see your API key.  \nWe will be using Firecrawlâ€™s Scrape Endpoint. This endpoint will allow us to pull information from a single URL, translate it into clean markdown, and use it to extract the data we need. I will be filling out all the necessary conditions in our Make HTTP request using the API reference in their documentation.  \nNow in Make, I configure the API call using the documentation from Firecrawl. We will be using POST as the HTTP method and have two headers.  \n```\nHeader 1:\nName: Authorization\nValue: Bearer your-API-key\n\nHeader 2:\nName: Content-Type\nValue: application/json\n\n```  \n!\nWe also want to set our body and content types. Here we will do:  \n```\nBody type: Raw\nContent type: Json (application/json)\n\n```  \nWe will also click â€˜yesâ€™ for parsing our response. This will automatically parse our response into JSON.  \nThe request content is the main meat of what we want to achieve. Here is the request content we will use for this use case:  \n```\n{\n\"url\": \"1. url(B)\",\n\n\"pageOptions\": {\n\"onlyMainContent\": true\n},\n\"extractorOptions\": {\n\"mode\": \"llm-extraction\",\n\"extractionPrompt\": \"Extract the company description (in one sentence explain what the company does), company industry (software, services, AI, etc.) - this really should just be a tag with a couple keywords, and who they serve (who are their customers). If there is no clear information to answer the question, write 'no info'.\",\n\"extractionSchema\": {\n\"type\": \"object\",\n\"properties\": {\n\"company_description\": {\n\"type\": \"string\"\n},\n\"company_industry\": {\n\"type\": \"string\"\n},\n\"who_they_serve\": {\n\"type\": \"string\"\n}\n},\n\"required\": [\\\n\"company_description\",\\\n\"company_industry\",\\\n\"who_they_serve\"\\\n]\n}\n}\n}\n\n```  \n!  \n* _Note_ the green field in the screenshot is a dynamic item that you can choose in the Make UI. Instead of `url (B)`, the block may be the first URL in your data.  \n!  \nFantastic! Now we have configured our HTTP request. Letâ€™s test it to make sure everything is working as it should be. Click â€˜run onceâ€™ in Make and we should be getting data back.  \n!  \nWhen we run, letâ€™s check our first operation. In the output, we should be getting a `status code: 200`, meaning that our API request was successful. In the output, click on data to make sure we got the data we needed.  \n!  \nOur output looks successful! In the llm_extraction we are seeing the three attributes of data that we wanted from the website.  \n* _Note_ if you are getting a `500` error on your first operation and `200` responses on the subsequent ones, this may be because the operation is trying to be performed on the first row of your data (the header row). This will cause issues importing the data back into sheets! Make sure you start from the second row as mentioned before.  \nNow that we know the HTTP request is working correctly, all thatâ€™s left is to take the outputted JSON from Firecrawl and put it back into our spreadsheet.  \n* * *  \nNow we need to take our extracted data and put it back into our spreadsheet. To do this, we will take the outputted JSON from our HTTP request and export the text into the relevant tables.  \nLetâ€™s start by connecting the same google sheet and specifying the Row Number criteria. Here we will just use the Make UI to choose â€˜row numberâ€™  \n!  \nAll thatâ€™s left is to specify which LLM extracted data goes into which column. Here, we can simply use the UI in Make to set this up.  \n!  \nThatâ€™s it, now itâ€™s time to test our automation!  \n* * *  \nLetâ€™s click `run once` on the Make UI and make sure everything is running smoothly. The automation should start iterating through link-by-link and populating our spreadsheet in real time.  \n!  \nWe have success! Using Make and Firecrawl, we have been able to extract specific information about our customers without the need of manually going to each of their websites.  \nLooking at the data, we are starting to get a better understanding of our customers. However, we are not limited to these specific characteristics. If we want, we can customize our JSON and Extraction Prompt to find out other information about these companies.",
      "metadata": {
        "title": "Using LLM Extraction for Customer Insights",
        "url": "https://www.firecrawl.dev/blog/lead-gen-business-insights-make-firecrawl"
      }
    },
    {
      "id": "039dea93-65bc-4ad6-96ba-9556ae9af8ec",
      "source": "firecrawl/blog/lead-gen-business-insights-make-firecrawl.md",
      "content": "Setting up our automation > Use Cases\n\nLLM extraction allows us to quickly get specific information from the web thatâ€™s relevant to our business. We can use these automations to do a variety of tasks.  \n**Product:**\nEspecially for self-serve companies, we can understand the trends in industries using our product. What are the top 2-3 industries using our tech and what are they using it for? This will allow us to make better product decisions by prioritizing the right customers to focus on.  \n**Business Development:**\nBy understanding who our users are, we can look for similar companies who could benefit from our product as well. By doing a similar automation, we can extract positive indicators from prospects that would benefit from our product.  \nWe can also use this data to generate better outreach emails that are more specific to the individual prospect.  \n**Market Research:**\nMarket research firms spend tons of time doing secondary research, especially in niche sectors. We can streamline data collection by automating the extraction and organization of data from diverse sources. This automation helps boost efficiency and scales with growing data needs, making it a valuable tool for strategic decision-making in fast-evolving industries.",
      "metadata": {
        "title": "Using LLM Extraction for Customer Insights",
        "url": "https://www.firecrawl.dev/blog/lead-gen-business-insights-make-firecrawl"
      }
    },
    {
      "id": "c29c777e-b635-4150-9ebf-9797ea4028b3",
      "source": "firecrawl/blog/lead-gen-business-insights-make-firecrawl.md",
      "content": "Setting up our automation > Going a step further\n\nThis was just a simple example of how we can use LLMs to extract relevant data from websites using a static spreadsheet. You can always make this more advanced by connecting this dynamically to your sign ups. Additionally, you could connect this to other tools to further accelerate your productivity. For example, using the extracted content to generate more personalized copy for prospecting.  \nIf you found this useful, feel free to let me know! Iâ€™d love to hear your feedback or learn about what youâ€™re building. You can reach me at [garrett@mendable.ai](mailto:garrett@mendable.ai). Good luck and happy building!  \nArticle updated recently  \n[ðŸ”¥](https://www.firecrawl.dev/)",
      "metadata": {
        "title": "Using LLM Extraction for Customer Insights",
        "url": "https://www.firecrawl.dev/blog/lead-gen-business-insights-make-firecrawl"
      }
    },
    {
      "id": "5e3eb355-abe5-4e5e-8e9a-48e908513b79",
      "source": "firecrawl/blog/lead-gen-business-insights-make-firecrawl.md",
      "content": "Ready to _Build?_\n\nStart scraping web data for your AI apps today.  \nNo credit card needed.  \nGet Started",
      "metadata": {
        "title": "Using LLM Extraction for Customer Insights",
        "url": "https://www.firecrawl.dev/blog/lead-gen-business-insights-make-firecrawl"
      }
    },
    {
      "id": "f2f4415d-1873-497f-ad6f-04cf4fca3591",
      "source": "firecrawl/blog/lead-gen-business-insights-make-firecrawl.md",
      "content": "About the Author\n\n[![Caleb Peffer image](https://www.firecrawl.dev/caleb-img.jpeg)\\\nCaleb Peffer@CalebPeffer](https://x.com/CalebPeffer)  \nCaleb Peffer is the Chief Executive Officer (CEO) of Firecrawl. Previously, built and scaled Mendable, an innovative \"chat with your documents\" application,\nand sold it to major customers like Snapchat, Coinbase, and MongoDB. Also co-founded SideGuide, a tool for learning code within VS Code with 50,000 users.\nCaleb has a passion for building products that help people do their best work. Caleb studied Computer Science and has over 10 years of experience in software engineering.",
      "metadata": {
        "title": "Using LLM Extraction for Customer Insights",
        "url": "https://www.firecrawl.dev/blog/lead-gen-business-insights-make-firecrawl"
      }
    },
    {
      "id": "d3a5338e-1ae2-43c9-8533-508d8a3a5a9f",
      "source": "firecrawl/blog/lead-gen-business-insights-make-firecrawl.md",
      "content": "About the Author > More articles by Caleb Peffer\n\n[Using LLM Extraction for Customer Insights\\\n\\\nUsing LLM Extraction for Insights and Lead Generation using Make and Firecrawl.](https://www.firecrawl.dev/blog/lead-gen-business-insights-make-firecrawl)",
      "metadata": {
        "title": "Using LLM Extraction for Customer Insights",
        "url": "https://www.firecrawl.dev/blog/lead-gen-business-insights-make-firecrawl"
      }
    },
    {
      "id": "94f2eecc-66b0-474b-be35-da25f72d6e2d",
      "source": "firecrawl/blog/category-tips-and-resources.md",
      "content": "---\ntitle: Blog\nurl: https://www.firecrawl.dev/blog/category/tips-and-resources\n---  \nIntroducing /extract - Get web data with a prompt [Try now](https://www.firecrawl.dev/extract)  \n# Tips and Resources  \n[**How to easily install requests with pip and python** \\\nA tutorial on installing the requests library in Python using various methods, with usage examples and troubleshooting tips\\\n\\\nBy Eric CiarlaAug 9, 2024](https://www.firecrawl.dev/blog/how-to-easily-install-requests-with-pip-and-python)  \n[**How to quickly install BeautifulSoup with Python** \\\nA guide on installing the BeautifulSoup library in Python using various methods, with usage examples and troubleshooting tips\\\n\\\nBy Eric CiarlaAug 9, 2024](https://www.firecrawl.dev/blog/how-to-quickly-install-beautifulsoup-with-python)  \n[**Cloudflare Error 1015: How to solve it?** \\\nCloudflare Error 1015 is a rate limiting error that occurs when Cloudflare detects that you are exceeding the request limit set by the website owner.\\\n\\\nBy Eric CiarlaAug 6, 2024](https://www.firecrawl.dev/blog/cloudflare-error-1015-how-to-solve-it)  \n[**Your IP got blocked or banned? Here is how to unblock it** \\\nTo unblock your IP, you can use the following methods: Modify MAC Address, Utilize VPN, Clear Digital Footprint, and Uninstall and Reinstall.\\\n\\\nBy Eric CiarlaAug 6, 2024](https://www.firecrawl.dev/blog/your-ip-has-been-temporarily-blocked-or-banned)  \n[ðŸ”¥](https://www.firecrawl.dev/)",
      "metadata": {
        "title": "Blog",
        "url": "https://www.firecrawl.dev/blog/category/tips-and-resources"
      }
    },
    {
      "id": "6506b7f0-4d3e-4a5f-898e-d16d5c0439dd",
      "source": "firecrawl/blog/category-tips-and-resources.md",
      "content": "Ready to _Build?_\n\nStart scraping web data for your AI apps today.  \nNo credit card needed.  \nGet Started",
      "metadata": {
        "title": "Blog",
        "url": "https://www.firecrawl.dev/blog/category/tips-and-resources"
      }
    },
    {
      "id": "70406abf-f907-4d4d-a376-32b3e6336ad6",
      "source": "firecrawl/blog/launch-week-i-day-5-real-time-crawling-websockets.md",
      "content": "---\ntitle: Launch Week I / Day 5: Real-Time Crawling with WebSockets\nurl: https://www.firecrawl.dev/blog/launch-week-i-day-5-real-time-crawling-websockets\n---  \nIntroducing /extract - Get web data with a prompt [Try now](https://www.firecrawl.dev/extract)  \nAugust 30, 2024  \nâ€¢  \n[![Eric Ciarla image](https://www.firecrawl.dev/eric-img.jpeg)Eric Ciarla](https://x.com/ericciarla)  \n# Launch Week I / Day 5: Real-Time Crawling with WebSockets  \n![Launch Week I / Day 5: Real-Time Crawling with WebSockets image](https://www.firecrawl.dev/images/blog/firecrawl-websockets.png)  \nWelcome to Day 5 of Firecrawlâ€™s Launch Week! Weâ€™re excited to introduce an exciting new feature that will bring your web scraping projects to the next level: Real-Time Crawling with WebSockets.  \n**Introducing Crawl URL and Watch**  \nWeâ€™re thrilled to announce our new WebSocket-based method, `Crawl URL and Watch`. This powerful feature enables real-time data extraction and monitoring, opening up new possibilities for immediate data processing.  \n**How It Works**  \nThe `Crawl URL and Watch` method initiates a crawl job and returns a watcher object. You can then add event listeners for various events like â€œdocumentâ€ (when a new page is crawled), â€œerrorâ€ (if an error occurs), and â€œdoneâ€ (when the crawl is complete).  \nThis approach allows you to process data in real-time, react to errors immediately, and know exactly when your crawl is finished.  \nWeâ€™re excited to see how youâ€™ll use this new feature to enhance your web scraping projects and create more dynamic, responsive applications.  \nCheck out our documentation for a detailed guide on how to implement `Crawl URL and Watch` in your projects: [Firecrawl WebSocket Documentation](https://docs.firecrawl.dev/features/crawl#crawl-websocket)  \nArticle updated recently  \n[ðŸ”¥](https://www.firecrawl.dev/)",
      "metadata": {
        "title": "Launch Week I / Day 5: Real-Time Crawling with WebSockets",
        "url": "https://www.firecrawl.dev/blog/launch-week-i-day-5-real-time-crawling-websockets"
      }
    },
    {
      "id": "19f540bd-2b97-4565-817a-18852ba1c28b",
      "source": "firecrawl/blog/launch-week-i-day-5-real-time-crawling-websockets.md",
      "content": "Ready to _Build?_\n\nStart scraping web data for your AI apps today.  \nNo credit card needed.  \nGet Started",
      "metadata": {
        "title": "Launch Week I / Day 5: Real-Time Crawling with WebSockets",
        "url": "https://www.firecrawl.dev/blog/launch-week-i-day-5-real-time-crawling-websockets"
      }
    },
    {
      "id": "1ebad6a1-98de-41aa-b373-6993dfd6155c",
      "source": "firecrawl/blog/launch-week-i-day-5-real-time-crawling-websockets.md",
      "content": "About the Author\n\n[![Eric Ciarla image](https://www.firecrawl.dev/eric-img.jpeg)\\\nEric Ciarla@ericciarla](https://x.com/ericciarla)  \nEric Ciarla is the Chief Operating Officer (COO) of Firecrawl and leads marketing. He also worked on Mendable.ai and sold it to companies like Snapchat, Coinbase, and MongoDB.\nPreviously worked at Ford and Fracta as a Data Scientist. Eric also co-founded SideGuide, a tool for learning code within VS Code with 50,000 users.",
      "metadata": {
        "title": "Launch Week I / Day 5: Real-Time Crawling with WebSockets",
        "url": "https://www.firecrawl.dev/blog/launch-week-i-day-5-real-time-crawling-websockets"
      }
    },
    {
      "id": "3bb380a4-bf2f-4091-a3ed-2e04965ce76c",
      "source": "firecrawl/blog/launch-week-i-day-5-real-time-crawling-websockets.md",
      "content": "About the Author > More articles by Eric Ciarla\n\n[How to Create an llms.txt File for Any Website\\\n\\\nLearn how to generate an llms.txt file for any website using the llms.txt Generator and Firecrawl.](https://www.firecrawl.dev/blog/How-to-Create-an-llms-txt-File-for-Any-Website) [Cloudflare Error 1015: How to solve it?\\\n\\\nCloudflare Error 1015 is a rate limiting error that occurs when Cloudflare detects that you are exceeding the request limit set by the website owner.](https://www.firecrawl.dev/blog/cloudflare-error-1015-how-to-solve-it) [Build an agent that checks for website contradictions\\\n\\\nUsing Firecrawl and Claude to scrape your website's data and look for contradictions.](https://www.firecrawl.dev/blog/contradiction-agent) [Why Companies Need a Data Strategy for Generative AI\\\n\\\nLearn why a well-defined data strategy is essential for building robust, production-ready generative AI systems, and discover practical steps for curation, maintenance, and integration.](https://www.firecrawl.dev/blog/why-companies-need-a-data-strategy-for-generative-ai) [Getting Started with OpenAI's Predicted Outputs for Faster LLM Responses\\\n\\\nA guide to leveraging Predicted Outputs to speed up LLM tasks with GPT-4o models.](https://www.firecrawl.dev/blog/getting-started-with-predicted-outputs-openai) [How to easily install requests with pip and python\\\n\\\nA tutorial on installing the requests library in Python using various methods, with usage examples and troubleshooting tips](https://www.firecrawl.dev/blog/how-to-easily-install-requests-with-pip-and-python) [How to quickly install BeautifulSoup with Python\\\n\\\nA guide on installing the BeautifulSoup library in Python using various methods, with usage examples and troubleshooting tips](https://www.firecrawl.dev/blog/how-to-quickly-install-beautifulsoup-with-python) [How to Use OpenAI's o1 Reasoning Models in Your Applications\\\n\\\nLearn how to harness OpenAI's latest o1 series models for complex reasoning tasks in your apps.](https://www.firecrawl.dev/blog/how-to-use-openai-o1-reasoning-models-in-applications)",
      "metadata": {
        "title": "Launch Week I / Day 5: Real-Time Crawling with WebSockets",
        "url": "https://www.firecrawl.dev/blog/launch-week-i-day-5-real-time-crawling-websockets"
      }
    },
    {
      "id": "ae2e3c51-e0be-4f95-b8f4-6f80b060a57c",
      "source": "firecrawl/blog/automated-web-scraping-free-2025.md",
      "content": "---\ntitle: Web Scraping Automation: How to Run Scrapers on a Schedule\nurl: https://www.firecrawl.dev/blog/automated-web-scraping-free-2025\n---  \nIntroducing /extract - Get web data with a prompt [Try now](https://www.firecrawl.dev/extract)  \nDec 5, 2024  \nâ€¢  \n[![Bex Tuychiev image](https://www.firecrawl.dev/bex.jpg)Bex Tuychiev](https://x.com/bextuychiev)  \n# Web Scraping Automation: How to Run Scrapers on a Schedule  \n![Web Scraping Automation: How to Run Scrapers on a Schedule image](https://www.firecrawl.dev/images/blog/scheduling-scrapers-images/automated-web-scraping-free-2025.jpg)",
      "metadata": {
        "title": "Web Scraping Automation: How to Run Scrapers on a Schedule",
        "url": "https://www.firecrawl.dev/blog/automated-web-scraping-free-2025"
      }
    },
    {
      "id": "710a59a2-7299-4a9b-8079-a6926839deff",
      "source": "firecrawl/blog/automated-web-scraping-free-2025.md",
      "content": "Introduction\n\nWeb scraping is an essential skill for programmers in this data-driven world. Whether youâ€™re tracking prices, monitoring competitors, or gathering research data, automated web scraping can save you countless hours of manual work. In this comprehensive guide, youâ€™ll learn how to schedule and automate your Python web scrapers using completely free tools and services.",
      "metadata": {
        "title": "Web Scraping Automation: How to Run Scrapers on a Schedule",
        "url": "https://www.firecrawl.dev/blog/automated-web-scraping-free-2025"
      }
    },
    {
      "id": "436516bb-0fa1-40eb-90ae-cb2c360d512c",
      "source": "firecrawl/blog/automated-web-scraping-free-2025.md",
      "content": "Introduction > Why automate your web scraper?\n\nManual or one-off web scraping can be time-consuming and error-prone. You need to repeatedly run scripts, update data frequently, and sometimes work during off-hours just to gather information.  \nScheduling your web scrapers automates this entire process. It collects data at optimal times without manual intervention, ensures consistency, and frees up your valuable time for actual data analysis rather than repetitive data gathering.",
      "metadata": {
        "title": "Web Scraping Automation: How to Run Scrapers on a Schedule",
        "url": "https://www.firecrawl.dev/blog/automated-web-scraping-free-2025"
      }
    },
    {
      "id": "735129e4-a960-461d-969a-b1642683d841",
      "source": "firecrawl/blog/automated-web-scraping-free-2025.md",
      "content": "Introduction > What tools can you use to automate web scrapers in Python?\n\nIf you are building scrapers in Python, you have many completely free options to schedule them.  \nIn terms of local scraping, Pythonâ€™s `schedule` library is very intuitive and fastest to set up. There is also the built-in `asyncio` for concurrent scraping. There are also system-level automation tools like cron jobs for macOS/Linux and the Task Scheduler for Windows.  \nThere are also many cloud-based solutions like GitHub Actions (completely free), PythonAnywhere (free tier), Google Cloud Functions (free tier), and Heroku (free tier with limitations).  \nIn this guide, we will start with basic local scheduling and progress to using GitHub Actions, all while following best practices and ethical scraping guidelines.  \nLetâ€™s begin by setting up your development environment and writing your first scheduled web scraper.",
      "metadata": {
        "title": "Web Scraping Automation: How to Run Scrapers on a Schedule",
        "url": "https://www.firecrawl.dev/blog/automated-web-scraping-free-2025"
      }
    },
    {
      "id": "11500450-e431-4a1c-a803-b01e10d196a9",
      "source": "firecrawl/blog/automated-web-scraping-free-2025.md",
      "content": "Introduction > Common challenges in automating web scrapers\n\nScheduling web scrapers is an easy process, as you will discover through this tutorial. The real challenge lies in ensuring that the scrapers donâ€™t break the day after they are put on schedule. Common issues include websites changing their HTML structure, implementing new anti-bot measures, or modifying their robots.txt policies. Additionally, network issues, rate limiting, and IP blocking can cause scheduled scrapers to fail.  \nFor these reasons, it is almost impossible to build long-lasting scrapers written in Python frameworks. But the web scraping landscape is changing as more AI-based tools are emerging, like [Firecrawl](https://www.firecrawl.dev/).  \nFirecrawl provides an AI-powered web scraping API that can identify and extract data from HTML elements based on semantic descriptions in Python classes. While traditional scrapers rely on specific HTML selectors that can break when websites change, Firecrawlâ€™s AI approach helps maintain scraper reliability over time.  \nFor demonstration purposes, weâ€™ll implement examples using Firecrawl, though the scheduling techniques covered in this tutorial can be applied to any Python web scraper built with common libraries like BeautifulSoup, Scrapy, Selenium, or `lxml`. If you want to follow along with a scraper of your own, make sure to have it in a script and ready to go.",
      "metadata": {
        "title": "Web Scraping Automation: How to Run Scrapers on a Schedule",
        "url": "https://www.firecrawl.dev/blog/automated-web-scraping-free-2025"
      }
    },
    {
      "id": "ea8168dc-7a64-4886-97fc-d972a02fbb6d",
      "source": "firecrawl/blog/automated-web-scraping-free-2025.md",
      "content": "Prerequisites\n\nThis article assumes that you are already comfortable with web scraping and its related concepts like HTML structure, CSS selectors, HTTP requests, and handling rate limits. If you need a refresher on web scraping basics, check out [this introductory guide to web scraping with Python](https://realpython.com/python-web-scraping-practical-introduction/).  \nOtherwise, letâ€™s jump in by setting up the tools we will use for the tutorial.",
      "metadata": {
        "title": "Web Scraping Automation: How to Run Scrapers on a Schedule",
        "url": "https://www.firecrawl.dev/blog/automated-web-scraping-free-2025"
      }
    },
    {
      "id": "7f54b4d3-fce0-4486-b645-0f3a0da248bd",
      "source": "firecrawl/blog/automated-web-scraping-free-2025.md",
      "content": "Prerequisites > Environment setup\n\nWe will mainly use Firecrawl in this article, so, please make sure that you sign up at [firecrawl.dev](https://www.firecrawl.dev/), choose the free plan and get an API token.  \nThen, create a new working directory on your machine to follow along in this tutorial:  \n```bash\nmkdir learn-scheduling\ncd learn-scheduling\n\n```  \nIt is always a best practice to create a new virtual environment for projects. In this tutorial, we will use Pythonâ€™s `virtualenv`:  \n```bash\npython -m venv venv\nsource venv/bin/activate # For Unix/macOS\nvenvScriptsactivate # For Windows\n\n```  \nNow, letâ€™s install the libraries we will use:  \n```bash\npip install requests beautifulsoup4 firecrawl-py python-dotenv\n\n```  \nWe will touch on what each library does as we use them.",
      "metadata": {
        "title": "Web Scraping Automation: How to Run Scrapers on a Schedule",
        "url": "https://www.firecrawl.dev/blog/automated-web-scraping-free-2025"
      }
    },
    {
      "id": "970db597-1105-4bf1-8210-820b420eb6de",
      "source": "firecrawl/blog/automated-web-scraping-free-2025.md",
      "content": "Prerequisites > Firecrawl API key setup\n\nSince we will push our code to a GitHub repository later, you will need to save your Firecrawl API key securely by using a `.env` file:  \n```bash\ntouch .env\necho \"FIRECRAWL_API_KEY='your-key-here'\" >> .env\n\n```  \nAlso, create a `.gitignore` file and add the `.env` to it so that it isnâ€™t pushed to GitHub:  \n```bash\ntouch .gitignore\necho \".env\" >> .gitignore\n\n```",
      "metadata": {
        "title": "Web Scraping Automation: How to Run Scrapers on a Schedule",
        "url": "https://www.firecrawl.dev/blog/automated-web-scraping-free-2025"
      }
    },
    {
      "id": "727d87ac-4f64-47ac-b4e0-00e3a5574172",
      "source": "firecrawl/blog/automated-web-scraping-free-2025.md",
      "content": "Writing a Basic Web Scraper\n\nIn this tutorial, we will build a scraper for [the Hacker News homepage](https://news.ycombinator.com/) that extracts post title, URL, author, rank, number of upvotes and date.  \n![Hacker News Homepage showing posts to be scraped using automated scheduling with Firecrawl](https://www.firecrawl.dev/images/blog/scheduling-scrapers-images/hacker_news_homepage.png)  \nLike we mentioned, we will build the scraper in Firecrawl but I have also prepared an [identical scraper written in BeautifulSoup](https://github.com/mendableai/firecrawl/tree/main/examples/blog-articles/scheduling_scrapers) if you want a more traditional approach.  \nIn your working directory, create a new `firecrawl_scraper.py` script and import the following packages:  \n```python\n# firecrawl_scraper.py\nimport json\nfrom firecrawl import FirecrawlApp\nfrom dotenv import load_dotenv\nfrom pydantic import BaseModel, Field\nfrom typing import List\nfrom datetime import datetime\n\nload_dotenv()\nBASE_URL = \"https://news.ycombinator.com/\"\n\n```  \nAfter the imports, we are calling `load_dotenv()` so that our Firecrawl API key is loaded from the `.env` file. Then, we are defining a new variable containing the URL we will scrape.  \nNext, we create a Pydantic model to specify the information we want to scrape from each Hacker News post:  \n```python\nclass NewsItem(BaseModel):\ntitle: str = Field(description=\"The title of the news item\")\nsource_url: str = Field(description=\"The URL of the news item\")\nauthor: str = Field(\ndescription=\"The URL of the post author's profile concatenated with the base URL.\"\n)\nrank: str = Field(description=\"The rank of the news item\")\nupvotes: str = Field(description=\"The number of upvotes of the news item\")\ndate: str = Field(description=\"The date of the news item.\")\n\n```  \nPydantic models are Python classes that provide data validation and serialization capabilities. They allow you to define the structure and types of your data using Python type hints, while automatically handling validation, serialization, and documentation.  \nIn the context of our Firecrawl scraper, the `NewsItem` model defines the exact structure of data we want to extract from each Hacker News post. Each field in the model ( `title`, `source_url`, `author`, etc.) specifies what data should be scraped and includes a description of what that field represents.  \nThis model is crucial for Firecrawl because it uses the modelâ€™s schema to understand exactly what data to extract from the webpage. When we pass this model to Firecrawl, it will automatically attempt to find and extract data matching these field definitions from the HTML structure of Hacker News.  \nFor example, when Firecrawl sees we want a â€œtitleâ€ field, it will look for elements on the page that are likely to contain post titles based on their HTML structure and content. The `Field` descriptions help provide additional context about what each piece of data represents.  \nNext, we create another model called `NewsData` that contains a list of `NewsItem` objects. This model will serve as a container for all the news items we scrape from Hacker News. The `news_items` field is defined as a List of `NewsItem` objects, which means it can store multiple news items in a single data structure.  \n```python\nclass NewsData(BaseModel):\nnews_items: List[NewsItem]\n\n```  \nWithout this second model, our scraper will return not one but all news items.  \nNow, we define a new function that will run Firecrawl based on the scraping schema we just defined:  \n```python\ndef get_firecrawl_news_data():\napp = FirecrawlApp()\n\ndata = app.scrape_url(\nBASE_URL,\nparams={\n\"formats\": [\"extract\"],\n\"extract\": {\"schema\": NewsData.model_json_schema()},\n},\n)\n\nreturn data\n\n```  \nThis function initializes a FirecrawlApp instance and uses it to scrape data from Hacker News. It passes the `BASE_URL` and parameters specifying that we want to extract data according to our `NewsData` schema. The schema tells Firecrawl exactly what fields to look for and extract from each news item on the page. The function returns the scraped data which will contain a list of news items matching our defined structure.  \nLetâ€™s quickly test it:  \n```python\ndata = get_firecrawl_news_data()\n\nprint(type(data))\n\n```  \n```python\n<class 'dict'>\n\n```  \nFirecrawl always returns the scraped data in a dictionary. Letâ€™s look at the keys:  \n```python\ndata['metadata']\n\n```  \n```python\n{\n'title': 'Hacker News',\n'language': 'en',\n'ogLocaleAlternate': [],\n'referrer': 'origin',\n'viewport': 'width=device-width, initial-scale=1.0',\n'sourceURL': 'https://news.ycombinator.com/',\n'url': 'https://news.ycombinator.com/',\n'statusCode': 200\n}\n\n```  \nThe first key is the metadata field containing basic page information. We are interested in the `extract` field which contains the data scraped by the engine:  \n```python\ndata['extract']['news_items'][0]\n\n```  \n```python\n{\n'title': \"Send someone you appreciate an official 'Continue and Persist' Letter\",\n'source_url': 'https://ContinueAndPersist.org',\n'author': 'https://news.ycombinator.com/user?id=adnanaga',\n'rank': '1',\n'upvotes': '1122',\n'date': '17 hours ago'\n}\n\n```  \nThe `extract` field contains a dictionary with a list of scraped news items. We can see above that when printing the first item, it includes all the fields we defined in our `NewsItem` Pydantic model, including title, source URL, author, rank, upvotes and date.  \n```python\nlen(data['extract']['news_items'])\n\n```  \n```python\n30\n\n```  \nThe output shows 30 news items, confirming that our scraper successfully extracted all posts from the first page of Hacker News. This matches the siteâ€™s standard layout which displays exactly 30 posts per page.  \nNow, letâ€™s create a new function that saves this data to a JSON file:  \n```python\ndef save_firecrawl_news_data():\n# Get the data\ndata = get_firecrawl_news_data()\n# Format current date for filename\ndate_str = datetime.now().strftime(\"%Y_%m_%d_%H_%M\")\nfilename = f\"firecrawl_hacker_news_data_{date_str}.json\"\n\n# Save the news items to JSON file\nwith open(filename, \"w\") as f:\njson.dump(data[\"extract\"][\"news_items\"], f, indent=4)\n\nreturn filename\n\n```  \n`save_firecrawl_news_data()` handles saving the scraped Hacker News data to a JSON file. It first calls `get_firecrawl_news_data()` to fetch the latest data from Hacker News. Then, it generates a filename using the current timestamp in the format `YYYY_MM_DD_HH_MM`. The data is saved to this timestamped JSON file with proper indentation, and the filename is returned. This allows us to maintain a historical record of the scraped data with clear timestamps indicating when each scrape occurred.  \nFinally, add a `__main__` block to the `firecrawl_scraper.py` script to allow running the scraper directly from the command line:  \n```python\nif __name__ == \"__main__\":\nsave_firecrawl_news_data()\n\n```  \nThe complete scraper script is available in [our GitHub repository](https://github.com/mendableai/firecrawl/blob/main/examples/hacker_news_scraper/firecrawl_scraper.py). For reference, we also provide [a BeautifulSoup implementation of the same scraper](https://github.com/mendableai/firecrawl/blob/main/examples/hacker_news_scraper/bs4_scraper.py).",
      "metadata": {
        "title": "Web Scraping Automation: How to Run Scrapers on a Schedule",
        "url": "https://www.firecrawl.dev/blog/automated-web-scraping-free-2025"
      }
    },
    {
      "id": "b8d70b2b-71f3-40e9-9ec3-c0f0576566d6",
      "source": "firecrawl/blog/automated-web-scraping-free-2025.md",
      "content": "Local Web Scraping Automation Methods\n\nIn this section, we will explore how to run the scraper from the previous section on schedule using local tools like the Python `schedule` library and cron.",
      "metadata": {
        "title": "Web Scraping Automation: How to Run Scrapers on a Schedule",
        "url": "https://www.firecrawl.dev/blog/automated-web-scraping-free-2025"
      }
    },
    {
      "id": "e333bf5d-2953-457a-af26-08bb24dddb6f",
      "source": "firecrawl/blog/automated-web-scraping-free-2025.md",
      "content": "Local Web Scraping Automation Methods > The basics of the Python `schedule` library\n\n`schedule` is a job scheduling library written for humans ( [from the documentation](https://schedule.readthedocs.io/en/stable/index.html)). It runs any Python function or callable periodically using intuitive syntax.  \nTo get started, please install it with `pip`:  \n```bash\npip install schedule\n\n```  \nThen, scheduling a Python function is as easy as shown in the codeblock below:  \n```python\nimport schedule\nimport time\n\ndef job():\ncurrent_time = time.strftime(\"%H:%M:%S\")\nprint(f\"{current_time}: I'm working...\")\n\n# Schedule it\nschedule.every(3).seconds.do(job)\n\nwhile True:\nschedule.run_pending()\ntime.sleep(1)\n\n```  \n```out\n14:58:23: I'm working...\n14:58:26: I'm working...\n14:58:29: I'm working...\n14:58:32: I'm working...\n14:58:35: I'm working...\n...\n\n```  \nTo implement scheduling, first convert your task into a function (which weâ€™ve already completed). Next, apply scheduling logic using the `.every(n).period.do` syntax. Below are several examples demonstrating different scheduling patterns:  \n```python\nschedule.every(10).minutes.do(job)\nschedule.every().hour.do(job)\nschedule.every().day.at(\"10:30\").do(job)\nschedule.every().monday.do(job)\nschedule.every().wednesday.at(\"13:15\").do(job)\nschedule.every().day.at(\"12:42\", \"Europe/Amsterdam\").do(job)\nschedule.every().minute.at(\":17\").do(job) # 17th second of a minute\n\n```  \nFinally, you need to run an infinite loop that checks for pending scheduled jobs and executes them. The loop below runs continuously, checking if any scheduled tasks are due to run:  \n```python\nwhile True:\nschedule.run_pending()\ntime.sleep(1)\n\n```  \nThere is much more to the `schedule` library than what we just covered (you should check out [the examples from the documentation](https://schedule.readthedocs.io/en/stable/examples.html)) but they are enough for the purposes of this article.",
      "metadata": {
        "title": "Web Scraping Automation: How to Run Scrapers on a Schedule",
        "url": "https://www.firecrawl.dev/blog/automated-web-scraping-free-2025"
      }
    },
    {
      "id": "21ccaaac-e40b-40f8-b3d0-aa95593cd421",
      "source": "firecrawl/blog/automated-web-scraping-free-2025.md",
      "content": "Local Web Scraping Automation Methods > Using Pythonâ€™s `schedule` library to schedule web scrapers\n\nNow that we know the basics of `schedule`, letâ€™s use it for our Firecrawl scraper. Start by creating a new `scrape_scheduler.py` script and making the necessary imports:  \n```python\nimport schedule\nimport time\nfrom firecrawl_scraper import save_firecrawl_news_data\n\n```  \nHere, we import the `schedule` module itself and the `save_firecrawl_news_data()` function from `firecrawl_scraper.py` that downloads the top 30 posts of Hacker News.  \nThen, to run this function on schedule, like every hour, we only need to add a few lines of code:  \n```python\n# Schedule the scraper to run every hour\nschedule.every().hour.do(save_firecrawl_news_data)\n\nwhile True:\nschedule.run_pending()\ntime.sleep(1)\n\n```  \nYou can start the schedule with:  \n```bash\npython scrape_scheduler.py\n\n```  \n> **Tip**: For debugging purposes, start with a shorter interval like 60 seconds before implementing the hourly schedule.  \nThe scheduler will continue running until you terminate the main terminal process executing the `scrape_scheduler.py` script. Thanks to Firecrawlâ€™s AI-powered HTML parsing and layout adaptation capabilities, the scraper is quite resilient to website changes and has a low probability of breaking.  \nNevertheless, web scraping can be unpredictable, so itâ€™s recommended to review [the exception handling](https://schedule.readthedocs.io/en/stable/exception-handling.html) section of the `schedule` documentation to handle potential errors gracefully.  \n* * *  \nThe `schedule` library provides a simple and intuitive way to run periodic tasks like web scrapers. While it lacks some advanced features of other scheduling methods, itâ€™s a great choice for basic scheduling needs and getting started with automated scraping. Just remember to implement proper error handling for production use.",
      "metadata": {
        "title": "Web Scraping Automation: How to Run Scrapers on a Schedule",
        "url": "https://www.firecrawl.dev/blog/automated-web-scraping-free-2025"
      }
    },
    {
      "id": "e9b36780-4233-47f2-848c-f50a4dc082c9",
      "source": "firecrawl/blog/automated-web-scraping-free-2025.md",
      "content": "Local Web Scraping Automation Methods > Using Pythonâ€™s built-in tools to automate web scrapers\n\nIn this section, we will explore a few other local scheduling methods that have the advantage of being built into Python or the operating system, making them more reliable and robust than third-party libraries. These methods also provide better error handling, logging capabilities, and system-level control over the scheduling process.",
      "metadata": {
        "title": "Web Scraping Automation: How to Run Scrapers on a Schedule",
        "url": "https://www.firecrawl.dev/blog/automated-web-scraping-free-2025"
      }
    },
    {
      "id": "9a2d77e4-0dd6-4699-a657-16673b25572f",
      "source": "firecrawl/blog/automated-web-scraping-free-2025.md",
      "content": "Local Web Scraping Automation Methods > Using Pythonâ€™s built-in tools to automate web scrapers > How to automate a web scraper with `asyncio`?\n\n`asyncio` is a Python library for running code concurrently - executing multiple tasks at the same time by switching between them when one is waiting. Itâ€™s built into Python and helps schedule tasks efficiently. Hereâ€™s why itâ€™s great for web scrapers:  \n1. It can do other work while waiting for web requests to complete.\n2. You can run multiple scrapers at the same time with precise timing control.\n3. It uses less computer resources than regular multitasking.\n4. It handles errors well with `try/except` blocks.  \nLetâ€™s see how to use `asyncio` for scheduling scrapers:  \n```python\nimport asyncio\nimport time\nfrom firecrawl_scraper import save_firecrawl_news_data\n\nasync def schedule_scraper(interval_hours: float = 1):\nwhile True:\ntry:\nprint(f\"Starting scrape at {time.strftime('%Y-%m-%d %H:%M:%S')}\")\n# Run the scraper\nfilename = save_firecrawl_news_data()\nprint(f\"Data saved to {filename}\")\n\nexcept Exception as e:\nprint(f\"Error during scraping: {e}\")\n\n# Wait for the specified interval\nawait asyncio.sleep(interval_hours * 3600) # Convert hours to seconds\n\nasync def main():\n# Create tasks for different scheduling intervals\ntasks = [\\\nschedule_scraper(interval_hours=1), # Run every hour\\\n# Add more tasks with different intervals if needed\\\n# schedule_scraper(interval_hours=0.5), # Run every 30 minutes\\\n# schedule_scraper(interval_hours=2), # Run every 2 hours\\\n]\n\n# Run all tasks concurrently\nawait asyncio.gather(*tasks)\n\nif __name__ == \"__main__\":\n# Run the async scheduler\nasyncio.run(main())\n\n```  \nLetâ€™s break down whatâ€™s happening above:  \nThe `schedule_scraper()` function is an `async` function that runs indefinitely in a loop. For each iteration, it:  \n1. Runs the scraper and saves the data\n2. Handles any errors that occur during scraping\n3. Waits for the specified interval using `asyncio.sleep()`  \nThe `main()` function sets up concurrent execution of multiple scraper tasks with different intervals. This allows running multiple scrapers simultaneously without blocking each other.  \nThis asyncio-based approach has several advantages over the `schedule` library:  \n1. True concurrency: Multiple scrapers can run simultaneously without blocking each other, unlike `schedule` which runs tasks sequentially.  \n2. Precise Timing: `asyncio.sleep()` provides more accurate timing control compared to scheduleâ€™s `run_pending()` approach.  \n3. Resource Efficiency: `asyncio` uses cooperative multitasking which requires less system resources than `schedule`â€™s threading-based approach.  \n4. Better Error Handling: `Async/await` makes it easier to implement proper error handling and recovery  \n5. Flexibility: You can easily add or remove scraper tasks and modify their intervals without affecting other tasks  \nThe code structure also makes it simple to extend functionality by adding more concurrent tasks or implementing additional error handling logic.",
      "metadata": {
        "title": "Web Scraping Automation: How to Run Scrapers on a Schedule",
        "url": "https://www.firecrawl.dev/blog/automated-web-scraping-free-2025"
      }
    },
    {
      "id": "26b326d3-5d54-49fd-91f9-df53596f89d9",
      "source": "firecrawl/blog/automated-web-scraping-free-2025.md",
      "content": "Local Web Scraping Automation Methods > Using Pythonâ€™s built-in tools to automate web scrapers > How to automate a web scraper with cron jobs?\n\nCron is a time-based job scheduler in Unix-like operating systems (Linux, macOS). Think of it as a digital scheduler or calendar that can automatically run programs at specified times. A cron job is simply a task that you schedule to run at specific intervals.  \nFor web scraping, cron jobs are incredibly useful because they let you automate your scraper to run at predetermined times. For example, you could set up a cron job to:  \n- Run your scraper every hour to collect real-time data\n- Execute scraping tasks during off-peak hours (like 2 AM) to minimize server load\n- Collect data at specific times when websites update their content  \nThe scheduling format uses five time fields: minute, hour, day of month, month, and day of week. For instance:  \n- `0 * * * *` means â€œrun every hour at minute 0â€\n- `0 0 * * *` means â€œrun at midnight every dayâ€\n- `*/15 * * * *` means â€œrun every 15 minutesâ€  \nCron jobs are especially reliable for web scraping because theyâ€™re built into the operating system, use minimal resources, and continue running even after system reboots.  \nSo, letâ€™s run the `save_firecrawl_news_data()` function on a schedule using cron. First, we will create a dedicated script for the cron job named `cron_scraper.py`:  \n```python\n# cron_scraper.py\nimport sys\nimport logging\nfrom datetime import datetime\nfrom pathlib import Path\nfrom firecrawl_scraper import save_firecrawl_news_data\n\n# Set up logging\nlog_dir = Path(\"logs\")\nlog_dir.mkdir(exist_ok=True)\nlog_file = log_dir / f\"scraper_{datetime.now().strftime('%Y_%m')}.log\"\n\nlogging.basicConfig(\nlevel=logging.INFO,\nformat=\"%(asctime)s - %(levelname)s - %(message)s\",\nhandlers=[logging.FileHandler(log_file), logging.StreamHandler(sys.stdout)],\n)\n\ndef main():\ntry:\nlogging.info(\"Starting scraping job\")\nfilename = save_firecrawl_news_data() # Actual scraping function\nlogging.info(f\"Successfully saved data to {filename}\")\nexcept Exception as e:\nlogging.error(f\"Scraping failed: {str(e)}\", exc_info=True)\n\nif __name__ == \"__main__\":\nmain()\n\n```  \nThe script implements a production-ready web scraper with logging and error handling. It creates a logs directory, configures detailed logging to both files and console, and wraps the scraping operation in error handling. When run, it executes our scraper function while tracking its progress and any potential issues, making it ideal for automated scheduling through cron jobs.  \nNow, to schedule this script using cron, you will need to make it executable:  \n```bash\nchmod +x cron_scraper.py\n\n```  \nThen, open your crontab file (which is usually empty) with Nano:  \n```bash\nEDITOR=nano crontab -e\n\n```  \nThen, add one or more entries specifying the frequency with which the scraper must run:  \n```python\n# Run every minute\n*/1 * * * * cd /absolute/path/to/project && /absolute/path/to/.venv/bin/python cron_scraper.py >> ~/cron.log 2>&1\n\n# Run every hour\n*/1 * * * * cd /absolute/path/to/project && /absolute/path/to/.venv/bin/python cron_scraper.py >> ~/cron.log 2>&1\n\n```  \nThe above cron job syntax consists of several parts:  \nThe timing pattern â€œ _/1_ ** *â€ breaks down as follows:  \n- First `*/1`: Specifies every minute\n- First `*`: Represents any hour\n- Second `*`: Represents any day of the month\n- Third `*`: Represents any month\n- Fourth `*`: Represents any day of the week  \nAfter the timing pattern:  \n- `cd /absolute/path/to/project`: Changes to the project directory of your scraper\n- `&&`: Chains commands, executing the next only if previous succeeds\n- `/absolute/path/to/.venv/bin/python`: Specifies the Python interpreter path\n- `cron_scraper.py`: The script to execute\n- `>> ~/cron.log 2>&1`: Redirects both standard output (>>) and errors ( `2>&1`) to `cron.log`  \nFor hourly execution, the same pattern applies but with `0 * * * *` timing to run at the start of each hour instead of every minute.  \nAs soon as you save your crontab file with these commands, the schedule starts and you should see a `logs` directory in the same folder as your `cron_scraper.py`. It must look like this if you have been following along:  \n![\"Logs directory showing successful automated web scraping schedule execution with cron jobs and Firecrawl\"](https://www.firecrawl.dev/images/blog/scheduling-scrapers-images/output.png)  \nYou can always check the status of your cron jobs with the following command as well:  \n```bash\ntail -f ~/cron.log\n\n```  \nTo cancel a cron job, simply open your crontab file again and remove the line corresponding to the job.",
      "metadata": {
        "title": "Web Scraping Automation: How to Run Scrapers on a Schedule",
        "url": "https://www.firecrawl.dev/blog/automated-web-scraping-free-2025"
      }
    },
    {
      "id": "7457635c-e953-482e-8988-38b965ae4856",
      "source": "firecrawl/blog/automated-web-scraping-free-2025.md",
      "content": "Local Web Scraping Automation Methods > Using Pythonâ€™s built-in tools to automate web scrapers > How to automate a web scraper using Windows Task Scheduler?\n\nWindows Task Scheduler is a built-in Windows tool that can automate running programs or scripts at specified times. Itâ€™s a robust alternative to cron jobs for Windows users. Letâ€™s set up our scraper to run automatically.  \nFirst, create a batch file ( `run_scraper.bat`) to run our Python script:  \n```python\n@echo off\ncd /d \"C:pathtoyourproject\"\ncall venvScriptsactivate\npython cron_scraper.py\ndeactivate\n\n```  \nThen, to set up the task in Windows Task Scheduler:  \n- Open Task Scheduler (search â€œTask Schedulerâ€ in Windows search)\n- Click â€œCreate Basic Taskâ€ in the right panel\n- Follow the wizard:\n- Name: â€œHacker News Scraperâ€\n- Description: â€œScrapes Hacker News hourlyâ€\n- Trigger: Choose when to run (e.g., â€œDailyâ€)\n- Action: â€œStart a programâ€\n- Program/script: Browse to your `run_scraper.bat`\n- Start in: Your project directory  \nFor more control over the task, you can modify its properties after creation:  \n- Double-click the task\n- In the â€œTriggersâ€ tab, click â€œEditâ€ to set custom schedules\n- Common scheduling options:\n- Run every hour\n- Run at specific times\n- Run on system startup\n- In the â€œSettingsâ€ tab, useful options include:\n- â€œRun task as soon as possible after a scheduled start is missed\"\n- \"Stop the task if it runs longer than X hoursâ€  \nThe Task Scheduler provides several advantages:  \n- Runs even when user is logged out\n- Detailed history and logging\n- Ability to run with elevated privileges\n- Options for network conditions\n- Retry logic for failed tasks  \nYou can monitor your scheduled task through the Task Scheduler interface or check the logs we set up in `cron_scraper.py`.",
      "metadata": {
        "title": "Web Scraping Automation: How to Run Scrapers on a Schedule",
        "url": "https://www.firecrawl.dev/blog/automated-web-scraping-free-2025"
      }
    },
    {
      "id": "ac2e6117-d234-4fc9-9016-21caef8a2e30",
      "source": "firecrawl/blog/automated-web-scraping-free-2025.md",
      "content": "Automating Web Scrapers With GitHub Actions\n\nOne disadvantage all local scheduling methods have is that they rely on your local machine being powered on and connected to the internet. If your computer is turned off, loses power, or loses internet connectivity, your scheduled scraping tasks wonâ€™t run. This is where cloud-based solutions like GitHub Actions can provide more reliability and uptime for your web scraping workflows.",
      "metadata": {
        "title": "Web Scraping Automation: How to Run Scrapers on a Schedule",
        "url": "https://www.firecrawl.dev/blog/automated-web-scraping-free-2025"
      }
    },
    {
      "id": "30dff1ed-2a40-4330-9c34-9aecb48e9288",
      "source": "firecrawl/blog/automated-web-scraping-free-2025.md",
      "content": "Automating Web Scrapers With GitHub Actions > What is GitHub Actions?\n\nGitHub Actions is a continuous integration and deployment (CI/CD) platform provided by GitHub that allows you to automate various workflows directly from your [GitHub](https://github.com/) repository.  \nFor web scraping, GitHub Actions provides a reliable way to schedule and run your scraping scripts in the cloud. You can define workflows using YAML files that specify when and how your scraper should run, such as on a regular schedule using cron syntax. This means your scraping jobs will continue running even when your local machine is off, as they execute on GitHubâ€™s servers.",
      "metadata": {
        "title": "Web Scraping Automation: How to Run Scrapers on a Schedule",
        "url": "https://www.firecrawl.dev/blog/automated-web-scraping-free-2025"
      }
    },
    {
      "id": "d7bb5b8f-4bc3-4dd3-884d-b2a70ebe423e",
      "source": "firecrawl/blog/automated-web-scraping-free-2025.md",
      "content": "Automating Web Scrapers With GitHub Actions > Step 1: Setting Up a GitHub repository\n\nTo schedule our scrapers with GitHub actions, we first need a GitHub repository. Start by initializing Git in your current workspace:  \n```bash\n# Initialize git in your project directory\ngit init\ngit add .\ngit commit -m \"Initial commit\"\n\n```  \nNext, create a new public or private GitHub repository and add it as the remote:  \n```bash\n# Create a new repo on GitHub.com, then:\ngit remote add origin https://github.com/YOUR_USERNAME/YOUR_REPO_NAME.git\ngit branch -M main\ngit push -u origin main\n\n```  \nThen, create the following directory structure:  \n```bash\nmkdir -p .github/workflows\n\n```  \nThis directory will contain our GitHub Actions workflow files with YAML format. These files define how and when our scraping scripts should run. The workflows can be scheduled using cron syntax, triggered by events like pushes or pull requests, and can include multiple steps like installing dependencies and running scripts.",
      "metadata": {
        "title": "Web Scraping Automation: How to Run Scrapers on a Schedule",
        "url": "https://www.firecrawl.dev/blog/automated-web-scraping-free-2025"
      }
    },
    {
      "id": "06ae978e-babe-4dba-baa2-6de72b68e1a5",
      "source": "firecrawl/blog/automated-web-scraping-free-2025.md",
      "content": "Automating Web Scrapers With GitHub Actions > Step 2: Creating a Workflow file\n\nAt this stage, create a new `scraper.yml` file inside `.github/workflows` and paste the following contents:  \n```yaml\nname: Run Firecrawl Scraper\n\non:\nschedule:\n- cron: \"0/5 * * * *\" # Runs every five minute\nworkflow_dispatch: # Allows manual trigger\n\njobs:\nscrape:\nruns-on: ubuntu-latest\n\nsteps:\n- uses: actions/checkout@v3\n\n- name: Set up Python\nuses: actions/setup-python@v4\nwith:\npython-version: \"3.9\"\n\n- name: Install dependencies\nrun: |\npython -m pip install --upgrade pip\npip install pydantic firecrawl-py\n\n- name: Run scraper\nrun: python firecrawl_scraper.py\nenv:\n# Add any environment variables your scraper needs\nFIRECRAWL_API_KEY: ${{ secrets.FIRECRAWL_API_KEY }}\n\n- name: Commit and push if changes\nrun: |\ngit config --global user.name 'GitHub Actions Bot'\ngit config --global user.email 'actions@github.com'\ngit add .\ngit commit -m \"Update scraped data\" || exit 0\ngit push\n\n```  \nLetâ€™s break down the key components of this GitHub Actions workflow file:  \nWorkflow name:\nThe workflow is named â€œRun Firecrawl Scraperâ€ which helps identify it in the GitHub Actions interface.  \nTriggers:  \n- Scheduled to run every 5 minutes using cron syntax `0/5 * * * *` (5 minutes is for debugging purposes, please change to hourly later)\n- Can be manually triggered using `workflow_dispatch`  \nJob configuration:  \n- Runs on latest Ubuntu virtual environment\n- Contains multiple sequential steps  \nStep details:  \n1. Checkout:\n- Uses `actions/checkout@v3` to get repository code\n2. Python Setup:\n- Uses `actions/setup-python@v4`\n- Configures Python 3.9 environment\n3. Dependencies:\n- Upgrades `pip`\n- Installs required packages: `pydantic` and `firecrawl-py`\n4. Scraper Execution:\n- Runs `firecrawl_scraper.py`\n- Uses `FIRECRAWL_API_KEY` from repository secrets\n5. Committing the changes:\n- Creates a commit persisting the downloaded data using GitHub Actions bot.  \nTo run this action successfully, youâ€™ll need to store your Firecrawl API key in GitHub secrets. Navigate to your repositoryâ€™s Settings, then Secrets and variables â†’ Actions. Click the â€œNew repository secretâ€ button and add your API key, making sure to use the exact key name specified in `scraper.yml`.  \nAfter ensuring that everything is set up correctly, commit and push the latest changes to GitHub:  \n```bash\ngit add .\ngit commit -m \"Add a workflow to scrape on a schedule\"\n\n```  \nOnce you do, the workflow must show up in the Actions tab of your repository like below:  \n![\"GitHub Actions workflow showing successful execution of scheduled web scraping with Firecrawl\"](https://www.firecrawl.dev/images/blog/scheduling-scrapers-images/actions.png)  \nClick on the workflow name and press the â€œRun workflowâ€ button. This launches the action manually and starts the schedule. If you check in after some time, you should see more automatic runs and the results persisted in your repository:  \n![\"GitHub Actions workflow showing multiple successful executions of scheduled web scraping with Firecrawl over a 24 hour period\"](https://www.firecrawl.dev/images/blog/scheduling-scrapers-images/finished_actions.png)  \nCaution: I left the workflow running overnight (at five minute intervals) and was nastily surprised by 96 workflow runs the next day. Thankfully, GitHub actions are free (up to 2000 min/month) unlike AWS instances.  \nNow, unless you disable the workflow manually by clicking the three dots in the upper-right corner, the scraper continues running on the schedule you specified.",
      "metadata": {
        "title": "Web Scraping Automation: How to Run Scrapers on a Schedule",
        "url": "https://www.firecrawl.dev/blog/automated-web-scraping-free-2025"
      }
    },
    {
      "id": "ffb744d4-9d6a-467a-975a-6bd7132b3a3b",
      "source": "firecrawl/blog/automated-web-scraping-free-2025.md",
      "content": "Best Practices and Optimization\n\nWhen scheduling web scrapers, following best practices ensures reliability, efficiency, and ethical behavior. Here are the key areas to consider:",
      "metadata": {
        "title": "Web Scraping Automation: How to Run Scrapers on a Schedule",
        "url": "https://www.firecrawl.dev/blog/automated-web-scraping-free-2025"
      }
    },
    {
      "id": "e2eb7675-71eb-42c5-866e-0bb8e2c3a5d7",
      "source": "firecrawl/blog/automated-web-scraping-free-2025.md",
      "content": "Best Practices and Optimization > 1. Rate limiting and delays\n\nScraping engines like Firecrawl usually come with built-in rate limiting. However, if you are using custom scrapers written with Python libraries, you must always respect website servers by implementing proper rate limiting and delay strategies. For example, the below example shows adding random delays between your requests in-between requests to respect server load and avoid getting your IP blocked:  \n```python\nimport time\nimport random\n\ndef scrape_with_delays(urls):\nfor url in urls:\ntry:\n# Random delay between 2-5 seconds\ndelay = random.uniform(2, 5)\ntime.sleep(delay)\n\n# Your scraping code here\nresponse = requests.get(url)\n\nexcept requests.RequestException as e:\nlogging.error(f\"Error scraping {url}: {e}\")\n\n```  \nBest practices for rate limiting:  \n- Add random delays between requests (2-5 seconds minimum)\n- Respect `robots.txt` directives\n- Implement exponential backoff for retries\n- Stay under 1 request per second for most sites\n- Monitor response headers for rate limit information",
      "metadata": {
        "title": "Web Scraping Automation: How to Run Scrapers on a Schedule",
        "url": "https://www.firecrawl.dev/blog/automated-web-scraping-free-2025"
      }
    },
    {
      "id": "a994b3cf-589e-440e-ace2-6107d3ffbd2f",
      "source": "firecrawl/blog/automated-web-scraping-free-2025.md",
      "content": "Best Practices and Optimization > 2. Proxy Rotation\n\nThis best practice is related to using custom web scrapers. Proxy rotation involves cycling through different IP addresses when making requests to avoid getting blocked. By distributing requests across multiple IPs, you can maintain access to websites that might otherwise flag high-volume traffic from a single source.  \n```python\nfrom itertools import cycle\n\ndef get_proxy_pool():\nproxies = [\\\n'http://proxy1:port',\\\n'http://proxy2:port',\\\n'http://proxy3:port'\\\n]\nreturn cycle(proxies)\n\ndef scrape_with_proxies(url, proxy_pool):\nfor _ in range(3): # Max 3 retries\ntry:\nproxy = next(proxy_pool)\nresponse = requests.get(\nurl,\nproxies={'http': proxy, 'https': proxy},\ntimeout=10\n)\nreturn response\nexcept requests.RequestException:\ncontinue\nreturn None\n\n```  \nProxy best practices:  \n- Rotate IPs regularly\n- Use high-quality proxy services\n- Implement timeout handling\n- Monitor proxy health\n- Keep backup proxies ready",
      "metadata": {
        "title": "Web Scraping Automation: How to Run Scrapers on a Schedule",
        "url": "https://www.firecrawl.dev/blog/automated-web-scraping-free-2025"
      }
    },
    {
      "id": "834cd03e-8977-46e8-b5f5-d4681fb3af75",
      "source": "firecrawl/blog/automated-web-scraping-free-2025.md",
      "content": "Best Practices and Optimization > 3. Data Storage Strategies\n\nThe example scrapers we have built today saved the data to a JSON file, which is a simple method. Based on your needs, you may consider building a full data storage solution that saves the scraped data to various formats like as CSV files or to an SQL database. The storage type always depends on the kind of information scraped. Below is an example class that can save lists of dictionaries (like we scraped today) to a CSV file:  \n```python\nfrom datetime import datetime\nimport json\nimport pandas as pd\n\nclass DataManager:\ndef __init__(self, base_path='data'):\nself.base_path = Path(base_path)\nself.base_path.mkdir(exist_ok=True)\n\ndef save_data(self, data, format='csv'):\ntimestamp = datetime.now().strftime('%Y%m%d_%H%M%S')\n\nif format == 'csv':\nfilename = self.base_path / f'data_{timestamp}.csv'\npd.DataFrame(data).to_csv(filename, index=False)\nelif format == 'json':\nfilename = self.base_path / f'data_{timestamp}.json'\nwith open(filename, 'w') as f:\njson.dump(data, f, indent=2)\n\n# Cleanup old files (keep last 7 days)\nself._cleanup_old_files(days=7)\n\nreturn filename\n\ndef _cleanup_old_files(self, days):\n# Implementation for cleaning up old files\npass\n\n```  \nStorage recommendations:  \n- Use appropriate file formats (CSV/JSON/Database)\n- Implement data versioning\n- Regular cleanup of old data\n- Compress historical data\n- Consider using a database for large datasets",
      "metadata": {
        "title": "Web Scraping Automation: How to Run Scrapers on a Schedule",
        "url": "https://www.firecrawl.dev/blog/automated-web-scraping-free-2025"
      }
    },
    {
      "id": "17542167-aa39-4e07-a495-1fee13665851",
      "source": "firecrawl/blog/automated-web-scraping-free-2025.md",
      "content": "Best Practices and Optimization > 4. Error Logging and Monitoring\n\nWeb scraping usually requires more sophisticated logging than plain old print statements. Save the following or bookmark it on your browser as it properly sets up a logger using the built-in `logging` module of Python:  \n```python\nimport logging\nfrom pathlib import Path\n\ndef setup_logging():\nlog_dir = Path('logs')\nlog_dir.mkdir(exist_ok=True)\n\n# File handler for detailed logs\nfile_handler = logging.FileHandler(\nlog_dir / f'scraper_{datetime.now().strftime(\"%Y%m%d\")}.log'\n)\nfile_handler.setLevel(logging.DEBUG)\n\n# Console handler for important messages\nconsole_handler = logging.StreamHandler()\nconsole_handler.setLevel(logging.INFO)\n\n# Configure logging\nlogging.basicConfig(\nlevel=logging.DEBUG,\nformat='%(asctime)s - %(name)s - %(levelname)s - %(message)s',\nhandlers=[file_handler, console_handler]\n)\n\n```  \nMonitoring best practices:  \n- Implement comprehensive logging\n- Set up alerts for critical failures\n- Monitor memory usage\n- Track success rates\n- Log response times",
      "metadata": {
        "title": "Web Scraping Automation: How to Run Scrapers on a Schedule",
        "url": "https://www.firecrawl.dev/blog/automated-web-scraping-free-2025"
      }
    },
    {
      "id": "7b3bf79b-56ac-4b64-8f53-77a6c30bae39",
      "source": "firecrawl/blog/automated-web-scraping-free-2025.md",
      "content": "Best Practices and Optimization > 5. Maintaining Your Scraper\n\nMaintaining your web scraper is crucial for ensuring reliable and continuous data collection. Web scraping targets are dynamic - websites frequently update their structure, implement new security measures, or change their content organization. Regular maintenance helps catch these changes early, prevents scraping failures, and ensures your data pipeline remains robust. A well-maintained scraper also helps manage resources efficiently, keeps code quality high, and adapts to evolving requirements.  \nMaintenance guidelines:  \n- Regular code updates\n- Monitor site changes\n- Update user agents periodically\n- Check for library updates\n- Implement health checks\n- Document maintenance procedures",
      "metadata": {
        "title": "Web Scraping Automation: How to Run Scrapers on a Schedule",
        "url": "https://www.firecrawl.dev/blog/automated-web-scraping-free-2025"
      }
    },
    {
      "id": "19f22b65-996b-4a65-9a73-18d230385a73",
      "source": "firecrawl/blog/automated-web-scraping-free-2025.md",
      "content": "Conclusion\n\nThroughout this guide, weâ€™ve explored how to effectively schedule web scrapers using local Python and operation system tools as well as GitHub actions. From basic setup to advanced optimization techniques, weâ€™ve covered the essential components needed to build reliable, automated data collection pipelines. The workflow weâ€™ve created not only handles the technical aspects of scheduling but also incorporates best practices for rate limiting, error handling, and data storage - crucial elements for any production-grade scraping system.  \nFor those looking to enhance their web scraping capabilities further, I recommend exploring Firecrawlâ€™s comprehensive features through their [/crawl endpoint guide](https://www.firecrawl.dev/blog/mastering-the-crawl-endpoint-in-firecrawl) and [/scrape endpoint tutorial](https://www.firecrawl.dev/blog/mastering-firecrawl-scrape-endpoint). These resources, along with the [official documentation](https://docs.firecrawl.dev/), provide deeper insights into advanced topics like JavaScript rendering, structured data extraction, and batch operations that can significantly improve your web scraping workflows. Whether youâ€™re building training datasets for AI models or monitoring websites for changes, combining scheduled scraping with these powerful tools can help you build more sophisticated and efficient data collection systems.  \nArticle updated recently  \n[ðŸ”¥](https://www.firecrawl.dev/)",
      "metadata": {
        "title": "Web Scraping Automation: How to Run Scrapers on a Schedule",
        "url": "https://www.firecrawl.dev/blog/automated-web-scraping-free-2025"
      }
    },
    {
      "id": "27936a7b-63bb-4df1-a426-a0f1e21b21b4",
      "source": "firecrawl/blog/automated-web-scraping-free-2025.md",
      "content": "Ready to _Build?_\n\nStart scraping web data for your AI apps today.  \nNo credit card needed.  \nGet Started",
      "metadata": {
        "title": "Web Scraping Automation: How to Run Scrapers on a Schedule",
        "url": "https://www.firecrawl.dev/blog/automated-web-scraping-free-2025"
      }
    },
    {
      "id": "eb4d5e1b-8dea-4be7-97be-734691d575df",
      "source": "firecrawl/blog/automated-web-scraping-free-2025.md",
      "content": "About the Author\n\n[![Bex Tuychiev image](https://www.firecrawl.dev/bex.jpg)\\\nBex Tuychiev@bextuychiev](https://x.com/bextuychiev)  \nBex is a Top 10 AI writer on Medium and a Kaggle Master with over 15k followers. He loves writing detailed guides, tutorials, and notebooks on complex data science and machine learning topics",
      "metadata": {
        "title": "Web Scraping Automation: How to Run Scrapers on a Schedule",
        "url": "https://www.firecrawl.dev/blog/automated-web-scraping-free-2025"
      }
    },
    {
      "id": "918c706c-6cbb-4a32-a256-fa088c37ca53",
      "source": "firecrawl/blog/automated-web-scraping-free-2025.md",
      "content": "About the Author > More articles by Bex Tuychiev\n\n[Building an Automated Price Tracking Tool\\\n\\\nBuild an automated e-commerce price tracker in Python. Learn web scraping, price monitoring, and automated alerts using Firecrawl, Streamlit, PostgreSQL.](https://www.firecrawl.dev/blog/automated-price-tracking-tutorial-python) [Web Scraping Automation: How to Run Scrapers on a Schedule\\\n\\\nLearn how to automate web scraping in Python using free tools like schedule, asyncio, cron jobs and GitHub Actions. This comprehensive guide covers local and cloud-based scheduling methods to run scrapers reliably in 2025.](https://www.firecrawl.dev/blog/automated-web-scraping-free-2025) [Automated Data Collection - A Comprehensive Guide\\\n\\\nLearn how to build robust automated data collection systems using modern tools and best practices. This guide covers everything from selecting the right tools to implementing scalable collection pipelines.](https://www.firecrawl.dev/blog/automated-data-collection-guide) [BeautifulSoup4 vs. Scrapy - A Comprehensive Comparison for Web Scraping in Python\\\n\\\nLearn the key differences between BeautifulSoup4 and Scrapy for web scraping in Python. Compare their features, performance, and use cases to choose the right tool for your web scraping needs.](https://www.firecrawl.dev/blog/beautifulsoup4-vs-scrapy-comparison) [How to Build an Automated Competitor Price Monitoring System with Python\\\n\\\nLearn how to build an automated competitor price monitoring system in Python that tracks prices across e-commerce sites, provides real-time comparisons, and maintains price history using Firecrawl, Streamlit, and GitHub Actions.](https://www.firecrawl.dev/blog/automated-competitor-price-scraping) [Scraping Company Data and Funding Information in Bulk With Firecrawl and Claude\\\n\\\nLearn how to build a web scraper in Python that gathers company details, funding rounds, and investor information from public sources like Crunchbase using Firecrawl and Claude for automated data collection and analysis.](https://www.firecrawl.dev/blog/crunchbase-scraping-with-firecrawl-claude) [Data Enrichment: A Complete Guide to Enhancing Your Data Quality\\\n\\\nLearn how to enrich your data quality with a comprehensive guide covering data enrichment tools, best practices, and real-world examples. Discover how to leverage modern solutions like Firecrawl to automate data collection, validation, and integration for better business insights.](https://www.firecrawl.dev/blog/complete-guide-to-data-enrichment) [Building an Intelligent Code Documentation RAG Assistant with DeepSeek and Firecrawl\\\n\\\nLearn how to build an intelligent documentation assistant powered by DeepSeek and RAG (Retrieval Augmented Generation) that can answer questions about any documentation website by combining language models with efficient information retrieval.](https://www.firecrawl.dev/blog/deepseek-rag-documentation-assistant)",
      "metadata": {
        "title": "Web Scraping Automation: How to Run Scrapers on a Schedule",
        "url": "https://www.firecrawl.dev/blog/automated-web-scraping-free-2025"
      }
    },
    {
      "id": "b1b437b8-8dfa-4f17-a681-59313874e1db",
      "source": "firecrawl/blog/ai-resume-parser-job-matcher-python.md",
      "content": "---\ntitle: Building an AI Resume Job Matching App With Firecrawl And Claude\nurl: https://www.firecrawl.dev/blog/ai-resume-parser-job-matcher-python\n---  \nIntroducing /extract - Get web data with a prompt [Try now](https://www.firecrawl.dev/extract)  \nFeb 1, 2025  \nâ€¢  \n[![Bex Tuychiev image](https://www.firecrawl.dev/bex.jpg)Bex Tuychiev](https://x.com/bextuychiev)  \n# Building an AI Resume Job Matching App With Firecrawl And Claude  \n![Building an AI Resume Job Matching App With Firecrawl And Claude image](https://www.firecrawl.dev/images/blog/resume_parser/ai-resume-parser.jpg)",
      "metadata": {
        "title": "Building an AI Resume Job Matching App With Firecrawl And Claude",
        "url": "https://www.firecrawl.dev/blog/ai-resume-parser-job-matcher-python"
      }
    },
    {
      "id": "23b75ca8-a0e6-42fc-974f-413cfe33c113",
      "source": "firecrawl/blog/ai-resume-parser-job-matcher-python.md",
      "content": "Introduction\n\nFinding the perfect job can feel like searching for a needle in a haystack. As a developer, you might spend hours scanning through job boards, trying to determine if each position matches your skills and experience. What if we could automate this process using AI?  \nIn this tutorial, weâ€™ll build a sophisticated job matching system that combines several powerful technologies:  \n- **Firecrawl** for intelligent web scraping of job postings and resume parsing\n- **Claude 3.5 Sonnet** for job matching analysis\n- **Supabase** for managing job sources and tracking\n- **Discord** for when there is a matching job\n- **Streamlit** for a user-friendly web interface  \nOur application will:  \n1. Automatically scrape user-provided job boards at regular intervals\n2. Parse your resume from a PDF\n3. Use AI to evaluate each job posting against your qualifications\n4. Send notifications to Discord when strong matches are found\n5. Provide a web interface for managing job sources and viewing results  \n![Screenshot of the AI resume job matching app showing the main interface with job sources sidebar and resume upload section](https://www.firecrawl.dev/blog/images/demo.png)  \nBy the end of this tutorial, youâ€™ll have a fully automated job search assistant that runs in the cloud and helps you focus on the opportunities that matter most. Whether youâ€™re actively job hunting or just keeping an eye on the market, this tool will save you countless hours of manual searching and evaluation.  \nIf this project sounds interesting, you can start using it straight away by cloning [its GitHub repository](https://github.com/BexTuychiev/ai-resume-job-matching). The local setup instructions are provided in the README.  \nOn the other hand, if you want to understand how the different parts of the project work together, continue reading!",
      "metadata": {
        "title": "Building an AI Resume Job Matching App With Firecrawl And Claude",
        "url": "https://www.firecrawl.dev/blog/ai-resume-parser-job-matcher-python"
      }
    },
    {
      "id": "08b425ad-eba4-4a2d-acaa-9cd6da80d560",
      "source": "firecrawl/blog/ai-resume-parser-job-matcher-python.md",
      "content": "Overview of the App\n\nBefore diving into the technical details, letâ€™s walk through a typical user journey to understand how the app works.  \nThe process starts with the user adding web pages with job listings. Here are examples of acceptable pages:  \n- `https://datacamp.com/jobs`\n- `https://openai.com/careers/search/`\n- `https://apply.workable.com/huggingface`  \nAs you can probably tell from the example URLs, the app doesnâ€™t work with popular job platforms like Indeed or Glassdoor. This is because these platforms already have sophisticated job matching functionality built into their systems. Instead, this app focuses on company career pages and job boards that donâ€™t offer automated matching - places where youâ€™d normally have to manually review each posting. This allows you to apply the same intelligent matching to opportunities that might otherwise slip through the cracks.  \nEach job listings source is added to a Supabase database under the hood for persistence and displayed in the sidebar (you have the option to delete them). After the user inputs all job sources, they can add their PDF link in the main section of the app.  \nThe app uses [Firecrawl](https://firecrawl.dev/), an AI-powered scraping engine that extracts structured data from webpages and PDF documents. To parse resumes, Firecrawl requires a direct file link to the PDF.  \nAfter parsing the resume, the app crawls all job sources using Firecrawl to gather job listings. Each listing is then analyzed against the resume by Claude to determine compatibility. The UI clearly shows whether a candidate is qualified for each position, along with Claudeâ€™s reasoning. For matching jobs, the app automatically sends notifications to the userâ€™s Discord account via a webhook.  \n![Screenshot of a Discord notification showing a job match alert with details about the position and match analysis](https://www.firecrawl.dev/blog/images/discord_alert.png)  \nThe app automatically rechecks all job sources weekly to ensure you never miss a great opportunity.",
      "metadata": {
        "title": "Building an AI Resume Job Matching App With Firecrawl And Claude",
        "url": "https://www.firecrawl.dev/blog/ai-resume-parser-job-matcher-python"
      }
    },
    {
      "id": "a08cd984-0788-40eb-9dce-e6d2bf320f17",
      "source": "firecrawl/blog/ai-resume-parser-job-matcher-python.md",
      "content": "The Tech Stack Used in the App > 1. [Firecrawl](https://firecrawl.ai/) for AI-powered web scraping\n\nAt the heart of our job discovery system is Firecrawl, an AI-powered web scraping engine. Unlike traditional scraping libraries that rely on brittle HTML selectors, Firecrawl uses natural language understanding to identify and extract content. This makes it ideal for our use case because:  \n- It can handle diverse job board layouts without custom code for each site\n- Maintains reliability even when websites update their structure\n- Automatically bypasses common anti-bot measures\n- Handles JavaScript-rendered content out of the box\n- Provides clean, structured data through [Pydantic](https://pydantic.dev/) schemas",
      "metadata": {
        "title": "Building an AI Resume Job Matching App With Firecrawl And Claude",
        "url": "https://www.firecrawl.dev/blog/ai-resume-parser-job-matcher-python"
      }
    },
    {
      "id": "5a0e6f70-04e6-4930-a25b-800ffb0b1cf7",
      "source": "firecrawl/blog/ai-resume-parser-job-matcher-python.md",
      "content": "The Tech Stack Used in the App > 3. [Supabase](https://supabase.com/) for data management\n\nTo manage job sources and tracking, we use Supabase as our database backend. This modern database platform offers:  \n- PostgreSQL database with a generous free tier\n- Real-time capabilities for future features\n- Simple REST API for database operations\n- Built-in authentication system\n- Excellent developer experience with their Python SDK",
      "metadata": {
        "title": "Building an AI Resume Job Matching App With Firecrawl And Claude",
        "url": "https://www.firecrawl.dev/blog/ai-resume-parser-job-matcher-python"
      }
    },
    {
      "id": "d626dfb6-3a36-4210-9e46-57403c26c611",
      "source": "firecrawl/blog/ai-resume-parser-job-matcher-python.md",
      "content": "The Tech Stack Used in the App > 4. [Discord](https://discord.com/) for Notifications\n\nWhen a matching job is found, our system sends notifications through Discord webhooks. This might seem like an unusual choice, but Discord offers several advantages:  \n- Free and widely adopted\n- Rich message formatting with embeds\n- Simple webhook integration\n- Mobile notifications\n- Supports dedicated channels for job alerts\n- Threading for discussions about specific opportunities",
      "metadata": {
        "title": "Building an AI Resume Job Matching App With Firecrawl And Claude",
        "url": "https://www.firecrawl.dev/blog/ai-resume-parser-job-matcher-python"
      }
    },
    {
      "id": "49d68255-4565-42f2-b243-02fd6129a827",
      "source": "firecrawl/blog/ai-resume-parser-job-matcher-python.md",
      "content": "The Tech Stack Used in the App > 5. [Streamlit](https://streamlit.io/) for user interface\n\nThe web interface is built with Streamlit, a Python framework for data applications. We chose Streamlit because:  \n- It enables rapid development of data-focused interfaces\n- Provides built-in components for common UI patterns\n- Handles async operations smoothly\n- Offers automatic hot-reloading during development\n- Requires no JavaScript knowledge\n- Makes deployment straightforward",
      "metadata": {
        "title": "Building an AI Resume Job Matching App With Firecrawl And Claude",
        "url": "https://www.firecrawl.dev/blog/ai-resume-parser-job-matcher-python"
      }
    },
    {
      "id": "39a591a6-e657-4cae-9779-29eca54c1533",
      "source": "firecrawl/blog/ai-resume-parser-job-matcher-python.md",
      "content": "The Tech Stack Used in the App > 6. [GitHub Actions](https://github.com/features/actions) for automation\n\nTo ensure regular job checking, we use GitHub Actions for scheduling. This service provides:  \n- Free scheduling for public repositories\n- Built-in secret management\n- Reliable cron scheduling\n- Easy maintenance and modifications\n- Integrated version control\n- Comprehensive logging and monitoring  \nThis carefully selected stack provides a robust foundation while keeping costs minimal through generous free tiers. The combination of AI-powered tools (Firecrawl and Claude) with modern infrastructure (Supabase, Discord, GitHub Actions) creates a reliable and scalable job matching system that can grow with your needs.  \nMost importantly, this stack minimizes maintenance overhead - a crucial factor for any automated system. The AI-powered components adapt to changes automatically, while the infrastructure services are managed by their respective providers, letting you focus on finding your next great opportunity rather than maintaining the system.",
      "metadata": {
        "title": "Building an AI Resume Job Matching App With Firecrawl And Claude",
        "url": "https://www.firecrawl.dev/blog/ai-resume-parser-job-matcher-python"
      }
    },
    {
      "id": "bb23dd2e-c69b-4c37-b49a-0a0eeabc1acf",
      "source": "firecrawl/blog/ai-resume-parser-job-matcher-python.md",
      "content": "Breaking Down the App Components\n\nWhen you look at [the GitHub repository](https://github.com/BexTuychiev/ai-resume-job-matching) of the app, you will see the following file structure:  \n![GitHub repository file structure showing key files like models.py, scraper.py, and app.py](https://www.firecrawl.dev/blog/images/github_snapshot.png)  \nSeveral files in the repository serve common purposes that most developers will recognize:  \n- `.gitignore`: Specifies which files Git should ignore when tracking changes\n- `README.md`: Documentation explaining what the project does and how to use it\n- `requirements.txt`: Lists all Python package dependencies needed to run the project  \nLetâ€™s examine the remaining Python scripts and understand how they work together to power the application. The explanations will be in a logical order building from foundational elements to higher-level functionality.",
      "metadata": {
        "title": "Building an AI Resume Job Matching App With Firecrawl And Claude",
        "url": "https://www.firecrawl.dev/blog/ai-resume-parser-job-matcher-python"
      }
    },
    {
      "id": "14dab87b-693c-4c42-aa8c-08509b2ba278",
      "source": "firecrawl/blog/ai-resume-parser-job-matcher-python.md",
      "content": "Breaking Down the App Components > 1. Core data structures - `src/models.py`\n\nAt the heart of our job matching system are three Pydantic models that define the core data structures used throughout the application. These models not only provide type safety and validation but also serve as schemas that guide Firecrawlâ€™s AI in extracting structured data from web pages.  \n```python\nclass Job(BaseModel):\ntitle: str = Field(description=\"Job title\")\nurl: str = Field(description=\"URL of the job posting\")\ncompany: str = Field(description=\"Company name\")\n\n```  \nThe `Job` model represents an individual job posting with three essential fields:  \n- `title`: The positionâ€™s name\n- `url`: Direct link to the job posting\n- `company`: Name of the hiring organization  \nThis model is used by both the scraper to extract job listings and the Discord notifier to format job match notifications. The `Field` descriptions guide the Firecrawlâ€™s AI to better locate the HTML/CSS components containing the relevant information.  \n```python\nclass JobSource(BaseModel):\nurl: str = Field(description=\"URL of the job board\")\nlast_checked: Optional[datetime] = Field(description=\"Last check timestamp\")\n\n```  \nThe `JobSource` model tracks job board URLs and when they were last checked:  \n- `url`: The job boardâ€™s web address\n- `last_checked`: Optional timestamp of the last scraping attempt  \nThis model is primarily used by the database component to manage job sources and the scheduler to track when sources need to be rechecked.  \n```python\nclass JobListings(BaseModel):\njobs: List[Job] = Field(description=\"List of job postings\")\n\n```  \nFinally, the `JobListings` model serves as a container for multiple `Job` objects. This model is crucial for the scraper component, as it tells Firecrawl to extract all job postings from a page rather than just the first one it finds.  \nThese models form the foundation of our applicationâ€™s data flow:  \n1. The scraper uses them to extract structured data from web pages\n2. The database uses them to store and retrieve job sources\n3. The matcher uses them to process job details\n4. The Discord notifier uses them to format notifications  \nBy defining these data structures upfront, we ensure consistency throughout the application and make it easier to modify the data model in the future if needed.",
      "metadata": {
        "title": "Building an AI Resume Job Matching App With Firecrawl And Claude",
        "url": "https://www.firecrawl.dev/blog/ai-resume-parser-job-matcher-python"
      }
    },
    {
      "id": "668e64c1-6ccb-423d-a6dd-b3826b7ed012",
      "source": "firecrawl/blog/ai-resume-parser-job-matcher-python.md",
      "content": "Breaking Down the App Components > 2. Database operations - `src/database.py`\n\nThe database component handles persistence of job sources using Supabase, a PostgreSQL-based backend service. This module provides essential CRUD (Create, Read, Update, Delete) operations for managing job board URLs and their check history.  \n```python\nclass Database:\ndef __init__(self):\nurl = os.getenv(\"SUPABASE_URL\")\nkey = os.getenv(\"SUPABASE_KEY\")\nself.client = create_client(url, key)\n\ndef save_job_source(self, url: str) -> None:\n\"\"\"Save a job source to the database\"\"\"\nself.client.table(\"job_sources\").upsert(\n{\"url\": url, \"last_checked\": None}\n).execute()\n\n```  \nThe `Database` class initializes a connection to Supabase using environment variables and provides four key methods:  \n1. `save_job_source`: Adds or updates a job board URL in the database. The `upsert` operation ensures no duplicate entries are created.  \n2. `delete_job_source`: Removes a job source from tracking:  \n```python\ndef delete_job_source(self, url: str) -> None:\nself.client.table(\"job_sources\").delete().eq(\"url\", url).execute()\n\n```  \n3. `get_job_sources`: Retrieves all tracked job sources:  \n```python\ndef get_job_sources(self) -> List[JobSource]:\nresponse = self.client.table(\"job_sources\").select(\"*\").execute()\nreturn [JobSource(**source) for source in response.data]\n\n```  \n4. `update_last_checked`: Updates the timestamp when a source was last checked:  \n```python\ndef update_last_checked(self, url: str) -> None:\nself.client.table(\"job_sources\").update({\"last_checked\": \"now()\"}).eq(\n\"url\", url\n).execute()\n\n```  \nThis database component is used by:  \n- The Streamlit interface ( `app.py`) for managing job sources through the sidebar\n- The scheduler ( `scheduler.py`) for tracking when sources were last checked\n- The automated GitHub Action workflow for persistent storage between runs  \nBy using Supabase, we get a reliable, scalable database with minimal setup and maintenance requirements. The `JobSource` model we defined earlier ensures type safety when working with the database records throughout the application.",
      "metadata": {
        "title": "Building an AI Resume Job Matching App With Firecrawl And Claude",
        "url": "https://www.firecrawl.dev/blog/ai-resume-parser-job-matcher-python"
      }
    },
    {
      "id": "533358ab-1bd2-4666-948d-4c11e7a3f9ea",
      "source": "firecrawl/blog/ai-resume-parser-job-matcher-python.md",
      "content": "Breaking Down the App Components > 3. Scraping with Firecrawl - `src/scraper.py`\n\nThe scraper component handles all web scraping operations using Firecrawl, an AI-powered scraping engine. This module is responsible for parsing resumes and extracting job listings from various sources.  \n```python\n@st.cache_data(show_spinner=False)\ndef _cached_parse_resume(pdf_link: str) -> str:\n\"\"\"Cached version of resume parsing\"\"\"\napp = FirecrawlApp()\nresponse = app.scrape_url(url=pdf_link)\nreturn response[\"markdown\"]\n\nclass JobScraper:\ndef __init__(self):\nself.app = FirecrawlApp()\n\n```  \nThe `JobScraper` class initializes a Firecrawl connection and provides three main methods:  \n1. `parse_resume`: Extracts text content from a PDF resume. Uses Streamlitâ€™s caching to avoid re-parsing the same resume:  \n```python\nasync def parse_resume(self, pdf_link: str) -> str:\n\"\"\"Parse a resume from a PDF link.\"\"\"\nreturn _cached_parse_resume(pdf_link)\n\n```  \n2. `scrape_job_postings`: Batch scrapes multiple job board URLs using the `JobListings` schema to guide Firecrawlâ€™s extraction:  \n```python\nasync def scrape_job_postings(self, source_urls: list[str]) -> list[Job]:\nresponse = self.app.batch_scrape_urls(\nurls=source_urls,\nparams={\n\"formats\": [\"extract\"],\n\"extract\": {\n\"schema\": JobListings.model_json_schema(),\n\"prompt\": \"Extract information based on the schema provided\",\n},\n},\n)\n\njobs = []\nfor job in response[\"data\"]:\njobs.extend(job[\"extract\"][\"jobs\"])\n\nreturn [Job(**job) for job in jobs]\n\n```  \nIf you want to understand Firecrawlâ€™s syntax better, refer to our [separate guide on its `/scrape` endpoint](https://www.firecrawl.dev/blog/mastering-firecrawl-scrape-endpoint).  \n3. `scrape_job_content`: Retrieves the full content of a specific job posting for detailed analysis:  \n```python\nasync def scrape_job_content(self, job_url: str) -> str:\n\"\"\"Scrape the content of a specific job posting.\"\"\"\nresponse = self.app.scrape_url(url=job_url)\nreturn response[\"markdown\"]\n\n```  \nThis entire scraper component is used by:  \n- The Streamlit interface ( `app.py`) for initial resume parsing and job discovery\n- The scheduler ( `scheduler.py`) for automated periodic job checks\n- The matcher component for detailed job content analysis  \nThe use of Firecrawlâ€™s AI capabilities allows the scraper to handle diverse webpage layouts without custom selectors, while Streamlitâ€™s caching helps optimize performance by avoiding redundant resume parsing.",
      "metadata": {
        "title": "Building an AI Resume Job Matching App With Firecrawl And Claude",
        "url": "https://www.firecrawl.dev/blog/ai-resume-parser-job-matcher-python"
      }
    },
    {
      "id": "c9757a97-9dcc-403b-a6a9-d690bfd4b7e4",
      "source": "firecrawl/blog/ai-resume-parser-job-matcher-python.md",
      "content": "Breaking Down the App Components > 5. Sending notifications with Discord - `src/discord.py`\n\nThe Discord component handles sending notifications when matching jobs are found. It uses Discordâ€™s webhook functionality to deliver rich, formatted messages about job matches.  \n```python\nclass DiscordNotifier:\ndef __init__(self):\nself.webhook_url = os.getenv(\"DISCORD_WEBHOOK_URL\")\n\n```  \nFirst, we initialize the notifier with a Discord webhook URL from environment variables. This URL is where all notifications will be sent.  \n```python\nasync def send_match(self, job: Job, match_reason: str):\n\"\"\"Send a job match notification to Discord\"\"\"\nif not self.webhook_url:\nreturn\n\nwebhook = DiscordWebhook(url=self.webhook_url)\nembed = DiscordEmbed(\ntitle=f\"ðŸŽ¯ New Job Match Found!\",\ndescription=f\"**{job.title}** at **{job.company}**nn{match_reason}\",\ncolor=\"5865F2\", # Discord's blue color scheme\n)\n\n```  \nThe `send_match` method creates the notification:  \n- Takes a `Job` object and the AIâ€™s matching reason as input\n- Creates a webhook connection to Discord\n- Builds an embed message with:\n- An eye-catching title with emoji\n- Job title and company in bold\n- The AIâ€™s explanation of why this job matches  \n```python\n# Add fields with job details\nembed.add_embed_field(name=\"ðŸ¢ Company\", value=job.company, inline=True)\nembed.add_embed_field(\nname=\"ðŸ”— Job URL\", value=f\"[Apply Here]({job.url})\", inline=True\n)\n\nwebhook.add_embed(embed)\nwebhook.execute()\n\n```  \nFinally, the method:  \n- Adds structured fields for company and job URL\n- Uses emojis for visual appeal\n- Creates a clickable â€œApply Hereâ€ link\n- Sends the formatted message to Discord  \nThis component is used by:  \n- The matcher component when a job match is found\n- The scheduler for automated notifications\n- The Streamlit interface for real-time match alerts  \nThe use of Discord embeds provides a clean, professional look for notifications while making it easy for users to access job details and apply links directly from the message.",
      "metadata": {
        "title": "Building an AI Resume Job Matching App With Firecrawl And Claude",
        "url": "https://www.firecrawl.dev/blog/ai-resume-parser-job-matcher-python"
      }
    },
    {
      "id": "4d0d4068-f6d8-40db-aba6-7f6d1bd240bc",
      "source": "firecrawl/blog/ai-resume-parser-job-matcher-python.md",
      "content": "Breaking Down the App Components > 6. Automated source checking script - `src/scheduler.py`\n\nThe scheduler component handles automated periodic checking of job sources, coordinating between all other components to continuously monitor for new matching positions.  \n```python\nclass JobScheduler:\ndef __init__(self):\nself.scraper = JobScraper()\nself.matcher = JobMatcher()\nself.notifier = DiscordNotifier()\nself.db = Database()\nself.resume_url = os.getenv(\"RESUME_URL\")\nself.check_interval = int(os.getenv(\"CHECK_INTERVAL_MINUTES\", \"15\"))\nself.processed_jobs = set()\nlogger.info(f\"Initialized scheduler with {self.check_interval} minute interval\")\n\n```  \nThe `JobScheduler` class initializes with:  \n- All necessary components (scraper, matcher, notifier, database)\n- Resume URL from environment variables\n- Configurable check interval (defaults to 15 minutes)\n- A set to track processed jobs and avoid duplicates\n- Logging setup for monitoring operations  \n```python\nasync def process_source(self, source):\n\"\"\"Process a single job source\"\"\"\ntry:\nlogger.info(f\"Processing source: {source.url}\")\n\n# Parse resume\nresume_content = await self.scraper.parse_resume(self.resume_url)\n\n# Get jobs from source\njobs = await self.scraper.scrape_job_postings([source.url])\nlogger.info(f\"Found {len(jobs)} jobs from {source.url}\")\n\n```  \nThe `process_source` method starts by:  \n- Logging the current operation\n- Parsing the userâ€™s resume\n- Scraping all jobs from the given source  \n```python\n# Process new jobs\nfor job in jobs:\nif job.url in self.processed_jobs:\nlogger.debug(f\"Skipping already processed job: {job.url}\")\ncontinue\n\njob_content = await self.scraper.scrape_job_content(job.url)\nresult = await self.matcher.evaluate_match(resume_content, job_content)\n\nif result[\"is_match\"]:\nlogger.info(f\"Found match: {job.title} at {job.company}\")\nawait self.notifier.send_match(job, result[\"reason\"])\n\nself.processed_jobs.add(job.url)\n\n```  \nFor each job found, it:  \n- Skips if already processed\n- Scrapes the full job description\n- Evaluates the match against the resume\n- Sends a Discord notification if itâ€™s a match\n- Marks the job as processed  \n```python\nasync def run(self):\n\"\"\"Main scheduling loop\"\"\"\nlogger.info(\"Starting job scheduler...\")\n\nwhile True:\ntry:\nsources = self.db.get_job_sources()\nlogger.info(f\"Found {len(sources)} job sources\")\n\n```  \nThe `run` method starts the main loop by:  \n- Getting all job sources from the database\n- Logging the number of sources found  \n```python\nfor source in sources:\nif not source.last_checked or (\ndatetime.utcnow() - source.last_checked\n> timedelta(minutes=self.check_interval)\n):\nawait self.process_source(source)\nelse:\nlogger.debug(\nf\"Skipping {source.url}, next check in \"\nf\"{(source.last_checked + timedelta(minutes=self.check_interval) - datetime.utcnow()).total_seconds() / 60:.1f} minutes\"\n)\n\nawait asyncio.sleep(60) # Check every minute\n\n```  \nFor each source, it:  \n- Checks if it needs processing (never checked or interval elapsed)\n- Processes the source if needed\n- Logs skipped sources with time until next check\n- Waits a minute before the next iteration  \n```python\nexcept Exception as e:\nlogger.error(f\"Scheduler error: {str(e)}\")\nawait asyncio.sleep(60)\n\n```  \nError handling:  \n- Catches and logs any exceptions\n- Waits a minute before retrying\n- Ensures the scheduler keeps running despite errors  \nThis component is used by:  \n- The GitHub Actions workflow for automated checks\n- The command-line interface for manual checks\n- The logging system for monitoring and debugging  \nThe extensive logging helps track operations and diagnose issues, while the modular design allows for easy maintenance and updates.",
      "metadata": {
        "title": "Building an AI Resume Job Matching App With Firecrawl And Claude",
        "url": "https://www.firecrawl.dev/blog/ai-resume-parser-job-matcher-python"
      }
    },
    {
      "id": "5eeecfcb-b140-41a2-b837-f658c55f53bc",
      "source": "firecrawl/blog/ai-resume-parser-job-matcher-python.md",
      "content": "Breaking Down the App Components > 7. User interface with Streamlit - `app.py`\n\nThe Streamlit interface provides a user-friendly way to manage job sources and run manual job matching checks. Letâ€™s break down each component:  \n1. First, we set up the necessary imports and helper functions:  \n```python\nimport streamlit as st\nimport asyncio\nfrom dotenv import load_dotenv\nfrom src.scraper import JobScraper\nfrom src.matcher import JobMatcher\nfrom src.discord import DiscordNotifier\nfrom src.database import Database\n\nload_dotenv()\n\nasync def process_job(scraper, matcher, notifier, job, resume_content):\n\"\"\"Process a single job posting\"\"\"\njob_content = await scraper.scrape_job_content(job.url)\nresult = await matcher.evaluate_match(resume_content, job_content)\n\nif result[\"is_match\"]:\nawait notifier.send_match(job, result[\"reason\"])\n\nreturn job, result\n\n```  \nThe `process_job` function handles the core job matching logic for a single posting:  \n1. Scrapes the full job content using the provided URL  \n2. Evaluates if the resume matches the job requirements  \n3. Sends a notification if thereâ€™s a match  \n4. Returns both the job and match result for further processing  \n5. The main application setup and sidebar for managing job sources:  \n```python\nasync def main():\nst.title(\"Resume Parser and Job Matcher\")\n\n# Initialize services\nscraper = JobScraper()\nmatcher = JobMatcher()\nnotifier = DiscordNotifier()\ndb = Database()\n\n# Sidebar for managing job sources\nwith st.sidebar:\nst.header(\"Manage Job Sources\")\nnew_source = st.text_input(\"Add Job Source URL\")\n\nif st.button(\"Add Source\"):\ndb.save_job_source(new_source)\nst.success(\"Job source added!\")\n\n```  \nThe `main()` function sets up the core Streamlit application interface:  \n1. Creates a title for the app\n2. Initializes the key services (scraper, matcher, notifier, database)\n3. Adds a sidebar with controls for managing job source URLs\n4. Provides a text input and button to add new job sources\n5. Saves valid sources to the database  \nThe sidebar allows users to maintain a list of job boards and company career pages to monitor for new postings.  \n3. The source management interface:  \n```python\n# List and delete existing sources\nst.subheader(\"Current Sources\")\nfor source in db.get_job_sources():\ncol1, col2 = st.columns([3, 1])\nwith col1:\nst.text(source.url)\nwith col2:\nif st.button(\"Delete\", key=source.url):\ndb.delete_job_source(source.url)\nst.rerun()\n\n```  \nThis section displays the list of current job sources and provides delete functionality:  \n1. Shows a â€œCurrent Sourcesâ€ subheader\n2. Iterates through all sources from the database\n3. Creates a two-column layout for each source\n4. First column shows the source URL\n5. Second column has a delete button\n6. When delete is clicked, removes the source and refreshes the page  \nThe delete functionality helps users maintain their source list by removing outdated or unwanted job boards. The `rerun()` call ensures the UI updates immediately after deletion.  \n4. The main content area with instructions and resume input:  \n```python\nst.markdown(\n\"\"\"\nThis app helps you find matching jobs by:\n- Analyzing your resume from a PDF URL\n- Scraping job postings from your saved job sources\n- Using AI to evaluate if you're a good fit for each position\n\nSimply paste your resume URL below to get started!\n\"\"\"\n)\n\nresume_url = st.text_input(\n\"**Enter Resume PDF URL**\",\nplaceholder=\"https://www.website.com/resume.pdf\",\n)\n\n```  \n5. The job analysis workflow:  \n```python\nif st.button(\"Analyze\") and resume_url:\nwith st.spinner(\"Parsing resume...\"):\nresume_content = await scraper.parse_resume(resume_url)\n\nsources = db.get_job_sources()\nif not sources:\nst.warning(\"No job sources configured. Add some in the sidebar!\")\nreturn\n\nwith st.spinner(\"Scraping job postings...\"):\njobs = await scraper.scrape_job_postings([s.url for s in sources])\n\n```  \n6. Parallel job processing and results display:  \n```python\nwith st.spinner(f\"Analyzing {len(jobs)} jobs...\"):\ntasks = []\nfor job in jobs:\ntask = process_job(scraper, matcher, notifier, job, resume_content)\ntasks.append(task)\n\nfor coro in asyncio.as_completed(tasks):\njob, result = await coro\nst.subheader(f\"Job: {job.title}\")\nst.write(f\"URL: {job.url}\")\nst.write(f\"Match: {'âœ…' if result['is_match'] else 'âŒ'}\")\nst.write(f\"Reason: {result['reason']}\")\nst.divider()\n\nst.success(f\"Analysis complete! Processed {len(jobs)} jobs.\")\n\n```  \nThis section creates tasks to analyze multiple jobs simultaneously by comparing them against the userâ€™s resume. As each analysis completes, it displays the results including job title, URL, match status and reasoning. The parallel approach makes the processing more efficient than analyzing jobs one at a time.  \nThe interface provides:  \n- A sidebar for managing job sources\n- Clear instructions for users\n- Real-time feedback during processing\n- Visual indicators for matches (âœ…) and non-matches (âŒ)\n- Detailed explanations for each job evaluation\n- Parallel processing for better performance  \nThis component ties together all the backend services into a user-friendly interface that makes it easy to manage job sources and run manual checks.",
      "metadata": {
        "title": "Building an AI Resume Job Matching App With Firecrawl And Claude",
        "url": "https://www.firecrawl.dev/blog/ai-resume-parser-job-matcher-python"
      }
    },
    {
      "id": "890e6c90-ac63-47c7-b0f1-e41007f0761d",
      "source": "firecrawl/blog/ai-resume-parser-job-matcher-python.md",
      "content": "Breaking Down the App Components > 8. GitHub Actions workflow - `.github/workflows/scheduler.yml`\n\nThe GitHub Actions workflow automates the job checking process by running the scheduler at regular intervals. Letâ€™s break down the configuration:  \n1. First, we define the workflow name and triggers:  \n```yaml\nname: Job Matcher Scheduler\n\non:\npush:\nbranches: [main]\nschedule:\n- cron: \"0 0 * * 1\" # Run every Monday at midnight\n\n```  \nThis configuration:  \n- Names the workflow â€œJob Matcher Schedulerâ€\n- Triggers on pushes to the main branch (for testing)\n- Runs automatically every Monday at midnight using cron syntax\n- 0: Minute (0)\n- 0: Hour (0 = midnight)\n- *: Day of month (any)\n- *: Month (any)\n- 1: Day of week (1 = Monday)  \n2. Define the job and its environment:  \n```yaml\njobs:\ncheck-jobs:\nruns-on: ubuntu-latest\n\nsteps:\n- uses: actions/checkout@v2\n\n- name: Set up Python\nuses: actions/setup-python@v2\nwith:\npython-version: \"3.10\"\n\n```  \nThis section:  \n- Creates a job named â€œcheck-jobsâ€\n- Uses the latest Ubuntu runner\n- Checks out the repository code\n- Sets up Python 3.10  \n3. Install dependencies:  \n```yaml\n- name: Install dependencies\nrun: |\npython -m pip install --upgrade pip\npip install -r requirements.txt\n\n```  \n4. Set up environment variables and run the scheduler:  \n```yml\n- name: Run job checker\nenv:\nFIRECRAWL_API_KEY: ${{ secrets.FIRECRAWL_API_KEY }}\nANTHROPIC_API_KEY: ${{ secrets.ANTHROPIC_API_KEY }}\nDISCORD_WEBHOOK_URL: ${{ secrets.DISCORD_WEBHOOK_URL }}\nRESUME_URL: ${{ secrets.RESUME_URL }}\nSUPABASE_URL: ${{ secrets.SUPABASE_URL }}\nSUPABASE_KEY: ${{ secrets.SUPABASE_KEY }}\nCHECK_INTERVAL_MINUTES: 15\nrun: |\npython -m src.scheduler\n\n```  \nThis final step:  \n- Sets up all necessary environment variables from GitHub Secrets\n- Configures the check interval\n- Runs the scheduler script  \nThe workflow provides:  \n- Automated weekly job checks\n- Secure handling of sensitive credentials\n- Consistent environment for running checks\n- Detailed logs of each run\n- Easy modification of the schedule  \nTo use this workflow, you need to:  \n1. Add all required secrets to your GitHub repository\n2. Ensure your repository is public (for free GitHub Actions minutes)\n3. Verify the workflow is enabled in your Actions tab  \nThe weekly schedule helps stay within GitHubâ€™s free tier limits while still regularly checking for new opportunities.",
      "metadata": {
        "title": "Building an AI Resume Job Matching App With Firecrawl And Claude",
        "url": "https://www.firecrawl.dev/blog/ai-resume-parser-job-matcher-python"
      }
    },
    {
      "id": "1e9e8592-33da-4775-85a9-471e596621d0",
      "source": "firecrawl/blog/ai-resume-parser-job-matcher-python.md",
      "content": "Conclusion\n\nWeâ€™ve built a powerful automated job matching system that combines several modern technologies into a cohesive solution. By integrating Firecrawl for web scraping, Claude AI for intelligent matching, Discord for notifications, GitHub Actions for scheduling, and Supabase for storage, weâ€™ve created a practical tool that automates the tedious parts of job searching. This allows job seekers to focus their energy on more important tasks like preparing for interviews and improving their skills.",
      "metadata": {
        "title": "Building an AI Resume Job Matching App With Firecrawl And Claude",
        "url": "https://www.firecrawl.dev/blog/ai-resume-parser-job-matcher-python"
      }
    },
    {
      "id": "cb58605b-9b7c-4d16-99a4-65325558d242",
      "source": "firecrawl/blog/ai-resume-parser-job-matcher-python.md",
      "content": "Conclusion > Next Steps\n\nThe modular design of this system opens up many possibilities for future enhancements. You could expand support to additional job boards, implement more sophisticated matching algorithms, or add alternative notification methods like email. Consider building a mobile interface or adding analytics to track your application success rates. The foundation weâ€™ve built makes it easy to adapt and enhance the system as your needs evolve. Feel free to fork the repository and customize it to match your specific job search requirements.  \nArticle updated recently  \n[ðŸ”¥](https://www.firecrawl.dev/)",
      "metadata": {
        "title": "Building an AI Resume Job Matching App With Firecrawl And Claude",
        "url": "https://www.firecrawl.dev/blog/ai-resume-parser-job-matcher-python"
      }
    },
    {
      "id": "3aec90ba-a640-4af1-8fdf-d476d2036166",
      "source": "firecrawl/blog/ai-resume-parser-job-matcher-python.md",
      "content": "Ready to _Build?_\n\nStart scraping web data for your AI apps today.  \nNo credit card needed.  \nGet Started",
      "metadata": {
        "title": "Building an AI Resume Job Matching App With Firecrawl And Claude",
        "url": "https://www.firecrawl.dev/blog/ai-resume-parser-job-matcher-python"
      }
    },
    {
      "id": "c0589f99-3e30-47ae-b08b-7a9c0ab8a469",
      "source": "firecrawl/blog/ai-resume-parser-job-matcher-python.md",
      "content": "About the Author\n\n[![Bex Tuychiev image](https://www.firecrawl.dev/bex.jpg)\\\nBex Tuychiev@bextuychiev](https://x.com/bextuychiev)  \nBex is a Top 10 AI writer on Medium and a Kaggle Master with over 15k followers. He loves writing detailed guides, tutorials, and notebooks on complex data science and machine learning topics",
      "metadata": {
        "title": "Building an AI Resume Job Matching App With Firecrawl And Claude",
        "url": "https://www.firecrawl.dev/blog/ai-resume-parser-job-matcher-python"
      }
    },
    {
      "id": "24ae2fcf-1489-4acf-8301-2354595f99c5",
      "source": "firecrawl/blog/ai-resume-parser-job-matcher-python.md",
      "content": "About the Author > More articles by Bex Tuychiev\n\n[Building an Automated Price Tracking Tool\\\n\\\nBuild an automated e-commerce price tracker in Python. Learn web scraping, price monitoring, and automated alerts using Firecrawl, Streamlit, PostgreSQL.](https://www.firecrawl.dev/blog/automated-price-tracking-tutorial-python) [Web Scraping Automation: How to Run Scrapers on a Schedule\\\n\\\nLearn how to automate web scraping in Python using free tools like schedule, asyncio, cron jobs and GitHub Actions. This comprehensive guide covers local and cloud-based scheduling methods to run scrapers reliably in 2025.](https://www.firecrawl.dev/blog/automated-web-scraping-free-2025) [Automated Data Collection - A Comprehensive Guide\\\n\\\nLearn how to build robust automated data collection systems using modern tools and best practices. This guide covers everything from selecting the right tools to implementing scalable collection pipelines.](https://www.firecrawl.dev/blog/automated-data-collection-guide) [BeautifulSoup4 vs. Scrapy - A Comprehensive Comparison for Web Scraping in Python\\\n\\\nLearn the key differences between BeautifulSoup4 and Scrapy for web scraping in Python. Compare their features, performance, and use cases to choose the right tool for your web scraping needs.](https://www.firecrawl.dev/blog/beautifulsoup4-vs-scrapy-comparison) [How to Build an Automated Competitor Price Monitoring System with Python\\\n\\\nLearn how to build an automated competitor price monitoring system in Python that tracks prices across e-commerce sites, provides real-time comparisons, and maintains price history using Firecrawl, Streamlit, and GitHub Actions.](https://www.firecrawl.dev/blog/automated-competitor-price-scraping) [Scraping Company Data and Funding Information in Bulk With Firecrawl and Claude\\\n\\\nLearn how to build a web scraper in Python that gathers company details, funding rounds, and investor information from public sources like Crunchbase using Firecrawl and Claude for automated data collection and analysis.](https://www.firecrawl.dev/blog/crunchbase-scraping-with-firecrawl-claude) [Data Enrichment: A Complete Guide to Enhancing Your Data Quality\\\n\\\nLearn how to enrich your data quality with a comprehensive guide covering data enrichment tools, best practices, and real-world examples. Discover how to leverage modern solutions like Firecrawl to automate data collection, validation, and integration for better business insights.](https://www.firecrawl.dev/blog/complete-guide-to-data-enrichment) [Building an Intelligent Code Documentation RAG Assistant with DeepSeek and Firecrawl\\\n\\\nLearn how to build an intelligent documentation assistant powered by DeepSeek and RAG (Retrieval Augmented Generation) that can answer questions about any documentation website by combining language models with efficient information retrieval.](https://www.firecrawl.dev/blog/deepseek-rag-documentation-assistant)",
      "metadata": {
        "title": "Building an AI Resume Job Matching App With Firecrawl And Claude",
        "url": "https://www.firecrawl.dev/blog/ai-resume-parser-job-matcher-python"
      }
    },
    {
      "id": "78b769c2-f142-4bdc-a3fa-d957b26782f1",
      "source": "firecrawl/blog/how-gamma-supercharges-onboarding-with-firecrawl.md",
      "content": "---\ntitle: How Gamma Supercharges Onboarding with Firecrawl\nurl: https://www.firecrawl.dev/blog/how-gamma-supercharges-onboarding-with-firecrawl\n---  \nIntroducing /extract - Get web data with a prompt [Try now](https://www.firecrawl.dev/extract)  \nAug 8, 2024  \nâ€¢  \n[![Jon Noronha image](https://www.firecrawl.dev/customers/jon-noronha.jpg)Jon Noronha](https://x.com/thatsjonsense)  \n# How Gamma Supercharges Onboarding with Firecrawl  \n![How Gamma Supercharges Onboarding with Firecrawl image](https://www.firecrawl.dev/images/blog/customer-story-gamma.jpg)  \nAt [Gamma](https://gamma.app/), we recently launched Gamma Sites, which allows anyone to build a website as easily as writing a doc. To showcase the power of our platform, we wanted to transform existing sites into the Gamma format. Thatâ€™s where Firecrawl came in. Not only did Firecrawl enable us to import existing web pages, but it also unlocked a new input for our AI presentation generator. Now, users can pull in a blog post, Notion page, or other online document and convert it into a presentation effortlessly.  \nIntegrating Firecrawl into our production environment was a breeze. We already use markdown internally, so it was just a matter of plugging in the Firecrawl API, feeding it a URL, and getting clean markdown in return. The simplicity of scraping out all the extraneous content and retrieving just the text and images is what we would miss the most if we had to stop using Firecrawl. Throughout the integration process, the support from the Firecrawl team was outstanding. They were quick to respond to our feature requests and ensured a smooth experience.  \nArticle updated recently",
      "metadata": {
        "title": "How Gamma Supercharges Onboarding with Firecrawl",
        "url": "https://www.firecrawl.dev/blog/how-gamma-supercharges-onboarding-with-firecrawl"
      }
    },
    {
      "id": "399419d3-b342-49c5-ac4b-82b7972a0167",
      "source": "firecrawl/blog/how-gamma-supercharges-onboarding-with-firecrawl.md",
      "content": "About the Author\n\n[![Jon Noronha image](https://www.firecrawl.dev/customers/jon-noronha.jpg)\\\nJon Noronha@thatsjonsense](https://x.com/thatsjonsense)  \nJon Noronha is the founder of Gamma, building the anti-Powerpoint. He is also the Ex VP of Product at Optimizely.  \n[ðŸ”¥](https://www.firecrawl.dev/)",
      "metadata": {
        "title": "How Gamma Supercharges Onboarding with Firecrawl",
        "url": "https://www.firecrawl.dev/blog/how-gamma-supercharges-onboarding-with-firecrawl"
      }
    },
    {
      "id": "fbe2d8ea-1611-4be1-a338-d7e58625c6dc",
      "source": "firecrawl/blog/how-gamma-supercharges-onboarding-with-firecrawl.md",
      "content": "Ready to _Build?_\n\nStart scraping web data for your AI apps today.  \nNo credit card needed.  \nGet Started",
      "metadata": {
        "title": "How Gamma Supercharges Onboarding with Firecrawl",
        "url": "https://www.firecrawl.dev/blog/how-gamma-supercharges-onboarding-with-firecrawl"
      }
    },
    {
      "id": "5a750b4f-3c98-4786-a167-c87847fbc2d8",
      "source": "firecrawl/blog/using-structured-output-and-json-strict-mode-openai.md",
      "content": "---\ntitle: How to Use OpenAI's Structured Outputs and JSON Strict Mode\nurl: https://www.firecrawl.dev/blog/using-structured-output-and-json-strict-mode-openai\n---  \nIntroducing /extract - Get web data with a prompt [Try now](https://www.firecrawl.dev/extract)  \nAug 7, 2024  \nâ€¢  \n[![Eric Ciarla image](https://www.firecrawl.dev/eric-img.jpeg)Eric Ciarla](https://x.com/ericciarla)  \n# How to Use OpenAI's Structured Outputs and JSON Strict Mode  \n![How to Use OpenAI's Structured Outputs and JSON Strict Mode image](https://www.firecrawl.dev/images/blog/openai-structured-output.png)  \nGetting structured data from LLMs is super useful for developers integrating AI into their applications, enabling more reliable parsing and processing of model outputs.  \nOpenAI just released new versions of gpt-4o and gpt-4o-mini which include huge improvements for developers looking to get structured data from LLMs. With the introduction of Structured Outputs and JSON Strict Mode developers can now guarantee a JSON output 100% of the time when setting strict to true.  \n![Structured Outputs Evaluation Scores from OpenAI's latest models](https://www.firecrawl.dev/images/blog/structured-data-eval-chart.png)**Figure 1:** Structured Output Evaluation Scores from OpenAIâ€™s latest models  \nWithout further ado, letâ€™s dig into how to use these latest models with and get reliable structured data from them.",
      "metadata": {
        "title": "How to Use OpenAI's Structured Outputs and JSON Strict Mode",
        "url": "https://www.firecrawl.dev/blog/using-structured-output-and-json-strict-mode-openai"
      }
    },
    {
      "id": "7c65099d-466f-484e-be61-09c74bc9c44c",
      "source": "firecrawl/blog/using-structured-output-and-json-strict-mode-openai.md",
      "content": "How to use Structured Outputs with JSON Strict Mode\n\nTo demonstrate the power of these models, we can use JSON Strict mode to extract structured data from a web page. [See the code on Github.](https://github.com/mendableai/openai-structured-outputs-with-firecrawl)",
      "metadata": {
        "title": "How to Use OpenAI's Structured Outputs and JSON Strict Mode",
        "url": "https://www.firecrawl.dev/blog/using-structured-output-and-json-strict-mode-openai"
      }
    },
    {
      "id": "f583de34-63dc-4fc1-ae56-9478353b4838",
      "source": "firecrawl/blog/using-structured-output-and-json-strict-mode-openai.md",
      "content": "How to use Structured Outputs with JSON Strict Mode > Prerequisites\n\nInstall the required libraries:  \n```bash\n!pip install firecrawl-py openai\n\n```",
      "metadata": {
        "title": "How to Use OpenAI's Structured Outputs and JSON Strict Mode",
        "url": "https://www.firecrawl.dev/blog/using-structured-output-and-json-strict-mode-openai"
      }
    },
    {
      "id": "3dd855df-4f26-49ea-80db-311551c67d5a",
      "source": "firecrawl/blog/using-structured-output-and-json-strict-mode-openai.md",
      "content": "How to use Structured Outputs with JSON Strict Mode > Step 1: Initialize the FirecrawlApp and OpenAI Client\n\n```python\nfrom firecrawl import FirecrawlApp\nfrom openai import OpenAI\n\nfirecrawl_app = FirecrawlApp(api_key='FIRECRAWL_API_KEY')\nclient = OpenAI(api_key='OPENAI_API_KEY')\n\n```",
      "metadata": {
        "title": "How to Use OpenAI's Structured Outputs and JSON Strict Mode",
        "url": "https://www.firecrawl.dev/blog/using-structured-output-and-json-strict-mode-openai"
      }
    },
    {
      "id": "96f4c2ac-add2-4136-bc81-40c6aca5f96b",
      "source": "firecrawl/blog/using-structured-output-and-json-strict-mode-openai.md",
      "content": "How to use Structured Outputs with JSON Strict Mode > Step 2: Scrape Data from a Web Page\n\n```python\nurl = 'https://mendable.ai'\nscraped_data = firecrawl_app.scrape_url(url)\n\n```",
      "metadata": {
        "title": "How to Use OpenAI's Structured Outputs and JSON Strict Mode",
        "url": "https://www.firecrawl.dev/blog/using-structured-output-and-json-strict-mode-openai"
      }
    },
    {
      "id": "a559a954-7c22-46e4-8cfd-dfcffb202f5f",
      "source": "firecrawl/blog/using-structured-output-and-json-strict-mode-openai.md",
      "content": "How to use Structured Outputs with JSON Strict Mode > Step 3: Define the OpenAI API Request\n\n```python\nmessages = [\\\n{\\\n\"role\": \"system\",\\\n\"content\": \"You are a helpful assistant that extracts structured data from web pages.\"\\\n},\\\n{\\\n\"role\": \"user\",\\\n\"content\": f\"Extract the headline and description from the following HTML content: {scraped_data['content']}\"\\\n}\\\n]\n\nresponse_format = {\n\"type\": \"json_schema\",\n\"json_schema\": {\n\"name\": \"extracted_data\",\n\"strict\": True,\n\"schema\": {\n\"type\": \"object\",\n\"properties\": {\n\"headline\": {\n\"type\": \"string\"\n},\n\"description\": {\n\"type\": \"string\"\n}\n},\n\"required\": [\"headline\", \"description\"],\n\"additionalProperties\": False\n}\n}\n}\n\n```",
      "metadata": {
        "title": "How to Use OpenAI's Structured Outputs and JSON Strict Mode",
        "url": "https://www.firecrawl.dev/blog/using-structured-output-and-json-strict-mode-openai"
      }
    },
    {
      "id": "621ef211-f9d6-451c-948d-31ee8af92eb5",
      "source": "firecrawl/blog/using-structured-output-and-json-strict-mode-openai.md",
      "content": "How to use Structured Outputs with JSON Strict Mode > Step 4: Call the OpenAI API and Extract Structured Data\n\nIf you are wondering which models you can use with OpenAIâ€™s structued output and JSON Strict mode it is both gpt-4o-2024-08-06 and gpt-4o-mini-2024-07-18.  \n```python\nchat_completion = client.chat.completions.create(\nmodel=\"gpt-4o-2024-08-06\",\nmessages=messages,\nresponse_format=response_format\n)\n\nextracted_data = chat_completion.choices[0].message.content\n\nprint(extracted_data)\n\n```  \nBy following these steps, you can reliably extract structured data from web pages using OpenAIâ€™s latest models with JSON Strict Mode.  \nThatâ€™s about it! In this article, we showed you how to use Structured Output with scraped web data, but the skyâ€™s the limit when it comes to what you can build with reliable structured output from LLMs!",
      "metadata": {
        "title": "How to Use OpenAI's Structured Outputs and JSON Strict Mode",
        "url": "https://www.firecrawl.dev/blog/using-structured-output-and-json-strict-mode-openai"
      }
    },
    {
      "id": "0c467d24-6c8f-4ee6-9c52-095458751381",
      "source": "firecrawl/blog/using-structured-output-and-json-strict-mode-openai.md",
      "content": "References\n\n- [Introducing Structured Outputs in the API](https://openai.com/index/introducing-structured-outputs-in-the-api/)  \nArticle updated recently  \n[ðŸ”¥](https://www.firecrawl.dev/)",
      "metadata": {
        "title": "How to Use OpenAI's Structured Outputs and JSON Strict Mode",
        "url": "https://www.firecrawl.dev/blog/using-structured-output-and-json-strict-mode-openai"
      }
    },
    {
      "id": "5eca96ac-ba90-4863-bcea-eaa2150b4594",
      "source": "firecrawl/blog/using-structured-output-and-json-strict-mode-openai.md",
      "content": "Ready to _Build?_\n\nStart scraping web data for your AI apps today.  \nNo credit card needed.  \nGet Started",
      "metadata": {
        "title": "How to Use OpenAI's Structured Outputs and JSON Strict Mode",
        "url": "https://www.firecrawl.dev/blog/using-structured-output-and-json-strict-mode-openai"
      }
    },
    {
      "id": "a73e5a0e-2b7c-479d-8ff5-0961dbb59ead",
      "source": "firecrawl/blog/using-structured-output-and-json-strict-mode-openai.md",
      "content": "About the Author\n\n[![Eric Ciarla image](https://www.firecrawl.dev/eric-img.jpeg)\\\nEric Ciarla@ericciarla](https://x.com/ericciarla)  \nEric Ciarla is the Chief Operating Officer (COO) of Firecrawl and leads marketing. He also worked on Mendable.ai and sold it to companies like Snapchat, Coinbase, and MongoDB.\nPreviously worked at Ford and Fracta as a Data Scientist. Eric also co-founded SideGuide, a tool for learning code within VS Code with 50,000 users.",
      "metadata": {
        "title": "How to Use OpenAI's Structured Outputs and JSON Strict Mode",
        "url": "https://www.firecrawl.dev/blog/using-structured-output-and-json-strict-mode-openai"
      }
    },
    {
      "id": "e79bf568-5973-4d72-9f81-16fcb3dc6fcf",
      "source": "firecrawl/blog/using-structured-output-and-json-strict-mode-openai.md",
      "content": "About the Author > More articles by Eric Ciarla\n\n[How to Create an llms.txt File for Any Website\\\n\\\nLearn how to generate an llms.txt file for any website using the llms.txt Generator and Firecrawl.](https://www.firecrawl.dev/blog/How-to-Create-an-llms-txt-File-for-Any-Website) [Cloudflare Error 1015: How to solve it?\\\n\\\nCloudflare Error 1015 is a rate limiting error that occurs when Cloudflare detects that you are exceeding the request limit set by the website owner.](https://www.firecrawl.dev/blog/cloudflare-error-1015-how-to-solve-it) [Build an agent that checks for website contradictions\\\n\\\nUsing Firecrawl and Claude to scrape your website's data and look for contradictions.](https://www.firecrawl.dev/blog/contradiction-agent) [Why Companies Need a Data Strategy for Generative AI\\\n\\\nLearn why a well-defined data strategy is essential for building robust, production-ready generative AI systems, and discover practical steps for curation, maintenance, and integration.](https://www.firecrawl.dev/blog/why-companies-need-a-data-strategy-for-generative-ai) [Getting Started with OpenAI's Predicted Outputs for Faster LLM Responses\\\n\\\nA guide to leveraging Predicted Outputs to speed up LLM tasks with GPT-4o models.](https://www.firecrawl.dev/blog/getting-started-with-predicted-outputs-openai) [How to easily install requests with pip and python\\\n\\\nA tutorial on installing the requests library in Python using various methods, with usage examples and troubleshooting tips](https://www.firecrawl.dev/blog/how-to-easily-install-requests-with-pip-and-python) [How to quickly install BeautifulSoup with Python\\\n\\\nA guide on installing the BeautifulSoup library in Python using various methods, with usage examples and troubleshooting tips](https://www.firecrawl.dev/blog/how-to-quickly-install-beautifulsoup-with-python) [How to Use OpenAI's o1 Reasoning Models in Your Applications\\\n\\\nLearn how to harness OpenAI's latest o1 series models for complex reasoning tasks in your apps.](https://www.firecrawl.dev/blog/how-to-use-openai-o1-reasoning-models-in-applications)",
      "metadata": {
        "title": "How to Use OpenAI's Structured Outputs and JSON Strict Mode",
        "url": "https://www.firecrawl.dev/blog/using-structured-output-and-json-strict-mode-openai"
      }
    },
    {
      "id": "18e52266-b0ee-4aec-9589-eb45890012f0",
      "source": "firecrawl/blog/building-knowledge-graphs-from-web-data-camelai-firecrawl.md",
      "content": "---\ntitle: Building Knowledge Graphs from Web Data using CAMEL-AI and Firecrawl\nurl: https://www.firecrawl.dev/blog/building-knowledge-graphs-from-web-data-camelai-firecrawl\n---  \nIntroducing /extract - Get web data with a prompt [Try now](https://www.firecrawl.dev/extract)  \nAug 13, 2024  \nâ€¢  \n[![Wendong Fan image](https://www.firecrawl.dev/customers/wendong-fan.png)Wendong Fan](https://x.com/ttokzzzzz)  \n# Building Knowledge Graphs from Web Data using CAMEL-AI and Firecrawl  \n![Building Knowledge Graphs from Web Data using CAMEL-AI and Firecrawl image](https://www.firecrawl.dev/images/blog/knowledge-graph.jpg)  \nThis post explores techniques for building knowledge graphs by extracting data from web pages using [CAMEL-AI](https://www.camel-ai.org/) and Firecrawl.  \nWeâ€™ll cover:  \n- Multi-agent role-playing task setup\n- Web scraping implementation\n- Knowledge graph construction\n- Agent monitoring techniques  \nTo demonstrate these concepts, weâ€™ll build a knowledge graph to analyze Yusuf Dikecâ€™s performance in the 2024 Paris Olympics. [The notebook version is here](https://colab.research.google.com/drive/1Rhl5U3av0-tBZzhh45P0FuuJ7r62L4mw?authuser=1#scrollTo=dD7p_9CkyUmK).  \n![Yusuf Dikec at the Paris Olympics](https://www.firecrawl.dev/images/blog/yusuf.jpg)",
      "metadata": {
        "title": "Building Knowledge Graphs from Web Data using CAMEL-AI and Firecrawl",
        "url": "https://www.firecrawl.dev/blog/building-knowledge-graphs-from-web-data-camelai-firecrawl"
      }
    },
    {
      "id": "ce953c08-6c28-49e8-bc72-14c68080f388",
      "source": "firecrawl/blog/building-knowledge-graphs-from-web-data-camelai-firecrawl.md",
      "content": "ðŸ« Setting Up CAMEL and Firecrawl\n\nTo get started, install the CAMEL package with all its dependencies:  \n```\npip install camel-ai[all]==0.1.6.3\n\n```  \nNext, set up your API keys for Firecrawl and OpenAI to enable interaction with external services.",
      "metadata": {
        "title": "Building Knowledge Graphs from Web Data using CAMEL-AI and Firecrawl",
        "url": "https://www.firecrawl.dev/blog/building-knowledge-graphs-from-web-data-camelai-firecrawl"
      }
    },
    {
      "id": "8dc58d62-d916-47de-883b-a1796d63abde",
      "source": "firecrawl/blog/building-knowledge-graphs-from-web-data-camelai-firecrawl.md",
      "content": "API Keys\n\nYouâ€™ll need to set up your API keys for both Firecrawl and OpenAI. This ensures that the tools can interact with external services securely.  \nYour can go to here to get free API Key from Firecrawl  \n```\nimport os\nfrom getpass import getpass\n\n# Prompt for the Firecrawl API key securely\nfirecrawl_api_key = getpass('Enter your API key: ')\nos.environ[\"FIRECRAWL_API_KEY\"] = firecrawl_api_key\n\nopenai_api_key = getpass('Enter your API key: ')\nos.environ[\"OPENAI_API_KEY\"] = openai_api_key\n\n```",
      "metadata": {
        "title": "Building Knowledge Graphs from Web Data using CAMEL-AI and Firecrawl",
        "url": "https://www.firecrawl.dev/blog/building-knowledge-graphs-from-web-data-camelai-firecrawl"
      }
    },
    {
      "id": "4cdc7c93-e4dd-4ba6-8f5d-bd619fef1319",
      "source": "firecrawl/blog/building-knowledge-graphs-from-web-data-camelai-firecrawl.md",
      "content": "ðŸŒ Effortless Web Scraping with Firecrawl\n\nFirecrawl simplifies web scraping and cleaning content from web pages. Hereâ€™s an example of scraping content from a specific post on the CAMEL AI website:  \n```python\nfrom camel.loaders import Firecrawl\n\nfirecrawl = Firecrawl()\n\nresponse = firecrawl.tidy_scrape(\nurl=\"https://www.camel-ai.org/post/crab\"\n)\n\nprint(response)\n\n```",
      "metadata": {
        "title": "Building Knowledge Graphs from Web Data using CAMEL-AI and Firecrawl",
        "url": "https://www.firecrawl.dev/blog/building-knowledge-graphs-from-web-data-camelai-firecrawl"
      }
    },
    {
      "id": "49760e50-b327-4cf9-b7ea-15780db73659",
      "source": "firecrawl/blog/building-knowledge-graphs-from-web-data-camelai-firecrawl.md",
      "content": "ðŸ› ï¸ Web Information Retrieval using CAMELâ€™s RAG and Firecrawl\n\nLetâ€™s retrieve relevant information from a list of URLs using CAMELâ€™s RAG model. Weâ€™ll define a function that uses Firecrawl for web scraping and CAMELâ€™s AutoRetriever for retrieving the most relevant information based on a query:  \n```python\nfrom camel.configs import ChatGPTConfig\nfrom camel.models import ModelFactory\nfrom camel.retrievers import AutoRetriever\nfrom camel.toolkits import OpenAIFunction, SearchToolkit\nfrom camel.types import ModelPlatformType, ModelType, StorageType\n\ndef retrieve_information_from_urls(urls: list[str], query: str) -> str:\nr\"\"\"Retrieves relevant information from a list of URLs based on a given\nquery.\n\nThis function uses the `Firecrawl` tool to scrape content from the\nprovided URLs and then uses the `AutoRetriever` from CAMEL to retrieve the\nmost relevant information based on the query from the scraped content.\n\nArgs:\nurls (list[str]): A list of URLs to scrape content from.\nquery (str): The query string to search for relevant information.\n\nReturns:\nstr: The most relevant information retrieved based on the query.\n\nExample:\n>>> urls = [\"https://example.com/article1\", \"https://example.com/\\\narticle2\"]\n>>> query = \"latest advancements in AI\"\n>>> result = retrieve_information_from_urls(urls, query)\n\"\"\"\naggregated_content = ''\n\n# Scrape and aggregate content from each URL\nfor url in urls:\nscraped_content = Firecrawl().tidy_scrape(url)\naggregated_content += scraped_content\n\n# Initialize the AutoRetriever for retrieving relevant content\nauto_retriever = AutoRetriever(\nvector_storage_local_path=\"local_data\", storage_type=StorageType.QDRANT\n)\n\n# Retrieve the most relevant information based on the query\n# You can adjust the top_k and similarity_threshold value based on your needs\nretrieved_info = auto_retriever.run_vector_retriever(\nquery=query,\ncontents=aggregated_content,\ntop_k=3,\nsimilarity_threshold=0.5,\n)\n\nreturn retrieved_info\n\n```  \nLetâ€™s put the retrieval function to the test by gathering some information about the 2024 Olympics. The first run may take about 50 seconds as it needs to build a local vector database  \n```python\nretrieved_info = retrieve_information_from_urls(\nquery=\"Which country won the most golden prize in 2024 Olympics?\",\nurls=[\\\n\"https://en.wikipedia.org/wiki/2024_Summer_Olympics\",\\\n\"https://olympics.com/en/paris-2024\",\\\n],\n)\n\nprint(retrieved_info)\n\n```  \nðŸŽ‰ Thanks to CAMELâ€™s RAG pipeline and Firecrawlâ€™s tidy scraping capabilities, this function effectively retrieves relevant information from the specified URLs! You can now integrate this function into CAMELâ€™s Agents to automate the retrieval process further.",
      "metadata": {
        "title": "Building Knowledge Graphs from Web Data using CAMEL-AI and Firecrawl",
        "url": "https://www.firecrawl.dev/blog/building-knowledge-graphs-from-web-data-camelai-firecrawl"
      }
    },
    {
      "id": "e6df2839-7f50-4659-8ceb-2bccd51ce243",
      "source": "firecrawl/blog/building-knowledge-graphs-from-web-data-camelai-firecrawl.md",
      "content": "ðŸ“¹ Monitoring AI Agents with AgentOps\n\nAgentOps is a powerful tool for tracking and analyzing the execution of CAMEL agents. To set up AgentOps, obtain an API key and configure it in your environment:  \n```python\nimport os\nfrom getpass import getpass\n\nagentops_api_key = getpass('Enter your API key: ')\nos.environ[\"AGENTOPS_API_KEY\"] = agentops_api_key\n\nimport agentops\nagentops.init(default_tags=[\"CAMEL\"])\n\n```  \nWith AgentOps set up, you can monitor and analyze the execution of your CAMEL agents, gaining valuable insights into their performance and behavior.",
      "metadata": {
        "title": "Building Knowledge Graphs from Web Data using CAMEL-AI and Firecrawl",
        "url": "https://www.firecrawl.dev/blog/building-knowledge-graphs-from-web-data-camelai-firecrawl"
      }
    },
    {
      "id": "fe39cda7-eae9-49bd-be7d-1750c1242d4f",
      "source": "firecrawl/blog/building-knowledge-graphs-from-web-data-camelai-firecrawl.md",
      "content": "ðŸ§  Constructing Knowledge Graphs\n\nCAMEL can build and store knowledge graphs from text data, enabling advanced analysis and visualization of relationships. Hereâ€™s how to set up a Neo4j instance and define a function to create a knowledge graph:  \n```python\nfrom camel.storages import Neo4jGraph\nfrom camel.loaders import UnstructuredIO\nfrom camel.agents import KnowledgeGraphAgent\n\nfrom camel.storages import Neo4jGraph\nfrom camel.loaders import UnstructuredIO\nfrom camel.agents import KnowledgeGraphAgent\n\ndef knowledge_graph_builder(text_input: str) -> None:\nr\"\"\"Build and store a knowledge graph from the provided text.\n\nThis function processes the input text to create and extract nodes and relationships,\nwhich are then added to a Neo4j database as a knowledge graph.\n\nArgs:\ntext_input (str): The input text from which the knowledge graph is to be constructed.\n\nReturns:\ngraph_elements: The generated graph element from knowlegde graph agent.\n\"\"\"\n\n# Set Neo4j instance\nn4j = Neo4jGraph(\nurl=\"Your_URI\",\nusername=\"Your_Username\",\npassword=\"Your_Password\",\n)\n\n# Initialize instances\nuio = UnstructuredIO()\nkg_agent = KnowledgeGraphAgent()\n\n# Create an element from the provided text\nelement_example = uio.create_element_from_text(text_input, element_id=\"001\")\n\n# Extract nodes and relationships using the Knowledge Graph Agent\ngraph_elements = kg_agent.run(element_example, parse_graph_elements=True)\n\n# Add the extracted graph elements to the Neo4j database\nn4j.add_graph_elements(graph_elements=[graph_elements])\n\nreturn graph_elements\n\n```",
      "metadata": {
        "title": "Building Knowledge Graphs from Web Data using CAMEL-AI and Firecrawl",
        "url": "https://www.firecrawl.dev/blog/building-knowledge-graphs-from-web-data-camelai-firecrawl"
      }
    },
    {
      "id": "533105d5-f281-4f07-b5fc-40defd4c7e1a",
      "source": "firecrawl/blog/building-knowledge-graphs-from-web-data-camelai-firecrawl.md",
      "content": "ðŸ¤–ðŸ¤– Multi-Agent Role-Playing with CAMEL\n\nCAMEL enables role-playing sessions where AI agents interact to accomplish tasks using various tools. Letâ€™s guide an assistant agent to perform a comprehensive study of the Turkish shooter in the 2024 Paris Olympics:  \n1. Define the task prompt.\n2. Configure the assistant agent with tools for web information retrieval and knowledge graph building.\n3. Initialize the role-playing session.\n4. Start the interaction between agents.  \n```python\nfrom typing import List\n\nfrom colorama import Fore\n\nfrom camel.agents.chat_agent import FunctionCallingRecord\nfrom camel.societies import RolePlaying\nfrom camel.utils import print_text_animated\nfrom camel.societies import RolePlaying\n\ntask_prompt = \"\"\"Do a comprehensive study of the Turkish shooter in 2024 paris\nolympics, write a report for me, then create a knowledge graph for the report.\nYou should use search tool to get related urls first, then use retrieval tool\nto get the retrieved content back, finally use tool to create the\nknowledge graph to finish the task.\"\"\"\n\nretrieval_tool = OpenAIFunction(retrieve_information_from_urls)\nsearch_tool = OpenAIFunction(SearchToolkit().search_duckduckgo)\nknowledge_graph_tool = OpenAIFunction(knowledge_graph_builder)\n\ntool_list = [\\\nretrieval_tool,\\\nsearch_tool,\\\nknowledge_graph_tool,\\\n]\n\nassistant_model_config = ChatGPTConfig(\ntools=tool_list,\ntemperature=0.0,\n)\n\nrole_play_session = RolePlaying(\nassistant_role_name=\"CAMEL Assistant\",\nuser_role_name=\"CAMEL User\",\nassistant_agent_kwargs=dict(\nmodel=ModelFactory.create(\nmodel_platform=ModelPlatformType.OPENAI,\nmodel_type=ModelType.GPT_4O,\nmodel_config_dict=assistant_model_config.as_dict(),\n),\ntools=tool_list,\n),\nuser_agent_kwargs=dict(),\ntask_prompt=task_prompt,\nwith_task_specify=False,\n)\n\ninput_msg = role_play_session.init_chat()\nwhile n < 10:\nn += 1\nassistant_response, user_response = role_play_session.step(input_msg)\n\nif \"CAMEL_TASK_DONE\" in user_response.msg.content:\nbreak\n\ninput_msg = assistant_response.msg\n\n```  \nNow we can set up the role playing session with this:  \n```python\n# Initialize the role-playing session\nrole_play_session = RolePlaying(\nassistant_role_name=\"CAMEL Assistant\",\nuser_role_name=\"CAMEL User\",\nassistant_agent_kwargs=dict(\nmodel=ModelFactory.create(\nmodel_platform=ModelPlatformType.OPENAI,\nmodel_type=ModelType.GPT_4O_MINI,\nmodel_config_dict=assistant_model_config.as_dict(),\n),\ntools=tool_list,\n),\nuser_agent_kwargs=dict(),\ntask_prompt=task_prompt,\nwith_task_specify=False,\n)\n\n```  \nPrint the system message and task prompt like this:  \n```python\nprint(\nFore.GREEN\n+ f\"AI Assistant sys message:n{role_play_session.assistant_sys_msg}n\"\n)\nprint(Fore.BLUE + f\"AI User sys message:n{role_play_session.user_sys_msg}n\")\n\nprint(Fore.YELLOW + f\"Original task prompt:n{task_prompt}n\")\nprint(\nFore.CYAN\n+ \"Specified task prompt:\"\n+ f\"n{role_play_session.specified_task_prompt}n\"\n)\nprint(Fore.RED + f\"Final task prompt:n{role_play_session.task_prompt}n\")\n\n```  \nSet the termination rule and start the interaction between agents:  \nNOTE: This session will take approximately 5 minutes and will consume around $0.02 in tokens by using GPT4o-mini.  \n```python\nn = 0\ninput_msg = role_play_session.init_chat()\nwhile n < 10: # Limit the chat to 10 turns\nn += 1\nassistant_response, user_response = role_play_session.step(input_msg)\n\nif assistant_response.terminated:\nprint(\nFore.GREEN\n+ (\n\"AI Assistant terminated. Reason: \"\nf\"{assistant_response.info['termination_reasons']}.\"\n)\n)\nbreak\nif user_response.terminated:\nprint(\nFore.GREEN\n+ (\n\"AI User terminated. \"\nf\"Reason: {user_response.info['termination_reasons']}.\"\n)\n)\nbreak\n# Print output from the user\nprint_text_animated(\nFore.BLUE + f\"AI User:nn{user_response.msg.content}n\",\n0.01\n)\n\n# Print output from the assistant, including any function\n# execution information\nprint_text_animated(Fore.GREEN + \"AI Assistant:\", 0.01)\ntool_calls: List[FunctionCallingRecord] = [\\\nFunctionCallingRecord(**call.as_dict())\\\nfor call in assistant_response.info['tool_calls']\\\n]\nfor func_record in tool_calls:\nprint_text_animated(f\"{func_record}\", 0.01)\nprint_text_animated(f\"{assistant_response.msg.content}n\", 0.01)\n\nif \"CAMEL_TASK_DONE\" in user_response.msg.content:\nbreak\n\ninput_msg = assistant_response.msg\n\n```  \nEnd the AgentOps Session like so:  \n```python\n# End the AgentOps session\nagentops.end_session(\"Success\")\n\n```",
      "metadata": {
        "title": "Building Knowledge Graphs from Web Data using CAMEL-AI and Firecrawl",
        "url": "https://www.firecrawl.dev/blog/building-knowledge-graphs-from-web-data-camelai-firecrawl"
      }
    },
    {
      "id": "b060b89e-8dc5-49fb-b79b-08937f8fe8bb",
      "source": "firecrawl/blog/building-knowledge-graphs-from-web-data-camelai-firecrawl.md",
      "content": "ðŸŒŸ Highlights\n\nThis blog demonstrates the power of CAMEL and Firecrawl for Advanced RAG with Knowledge Graphs. Key tools utilized include:  \n- **CAMEL**: A multi-agent framework for Retrieval-Augmented Generation and role-playing scenarios.\n- **Firecrawl**: A web scraping tool for extracting and cleaning content from web pages.\n- **AgentOps**: A monitoring and analysis tool for tracking CAMEL agent execution.\n- **Qdrant**: A vector storage system used with CAMELâ€™s AutoRetriever.\n- **Neo4j**: A graph database for constructing and storing knowledge graphs.\n- **DuckDuckGo Search**: Utilized within the SearchToolkit to gather relevant URLs.\n- **OpenAI**: Provides state-of-the-art language models for tool-calling and embeddings.  \nWe hope this blog post has inspired you to harness the power of CAMEL and Firecrawl for your own projects. Happy researching and building! If you want to run this blog post as a notebook, [click here](https://colab.research.google.com/drive/1Rhl5U3av0-tBZzhh45P0FuuJ7r62L4mw?authuser=1#scrollTo=dD7p_9CkyUmK)!  \nArticle updated recently",
      "metadata": {
        "title": "Building Knowledge Graphs from Web Data using CAMEL-AI and Firecrawl",
        "url": "https://www.firecrawl.dev/blog/building-knowledge-graphs-from-web-data-camelai-firecrawl"
      }
    },
    {
      "id": "ed27715b-98e9-434d-a500-68860a4c728b",
      "source": "firecrawl/blog/building-knowledge-graphs-from-web-data-camelai-firecrawl.md",
      "content": "About the Author\n\n[![Wendong Fan image](https://www.firecrawl.dev/customers/wendong-fan.png)\\\nWendong Fan@ttokzzzzz](https://x.com/ttokzzzzz)  \nWendong Fan is an AI Engineer at Eigent AI.  \n[ðŸ”¥](https://www.firecrawl.dev/)",
      "metadata": {
        "title": "Building Knowledge Graphs from Web Data using CAMEL-AI and Firecrawl",
        "url": "https://www.firecrawl.dev/blog/building-knowledge-graphs-from-web-data-camelai-firecrawl"
      }
    },
    {
      "id": "59c97de4-0359-4ec4-98fe-4c4e934d03b8",
      "source": "firecrawl/blog/building-knowledge-graphs-from-web-data-camelai-firecrawl.md",
      "content": "Ready to _Build?_\n\nStart scraping web data for your AI apps today.  \nNo credit card needed.  \nGet Started",
      "metadata": {
        "title": "Building Knowledge Graphs from Web Data using CAMEL-AI and Firecrawl",
        "url": "https://www.firecrawl.dev/blog/building-knowledge-graphs-from-web-data-camelai-firecrawl"
      }
    },
    {
      "id": "a7ea73e8-e71b-4784-94ee-098b08f8522d",
      "source": "firecrawl/blog/using-prompt-caching-with-anthropic.md",
      "content": "---\ntitle: How to Use Prompt Caching and Cache Control with Anthropic Models\nurl: https://www.firecrawl.dev/blog/using-prompt-caching-with-anthropic\n---  \nIntroducing /extract - Get web data with a prompt [Try now](https://www.firecrawl.dev/extract)  \nAug 14, 2024  \nâ€¢  \n[![Eric Ciarla image](https://www.firecrawl.dev/eric-img.jpeg)Eric Ciarla](https://x.com/ericciarla)  \n# How to Use Prompt Caching and Cache Control with Anthropic Models  \n![How to Use Prompt Caching and Cache Control with Anthropic Models image](https://www.firecrawl.dev/images/blog/anthropic-prompt-caching.png)  \nAnthropic recently launched prompt caching and cache control in beta, allowing you to cache large context prompts up to 200k tokens and chat with them faster and cheaper than ever before. This is a game changer for Retrieval Augmented Generation (RAG) applications that analyze large amounts of data. Currently caching is only avialable for Sonnet and Haiku but it is coming soon to Opus.  \nTo showcase the power of prompt caching, letâ€™s walk through an example of crawling a website with Firecrawl, caching the contents with Anthropic, and having an AI assistant analyze the copy to provide suggestions for improvement. [See the code on Github.](https://github.com/ericciarla/prompt_caching_websites_anthropic)",
      "metadata": {
        "title": "How to Use Prompt Caching and Cache Control with Anthropic Models",
        "url": "https://www.firecrawl.dev/blog/using-prompt-caching-with-anthropic"
      }
    },
    {
      "id": "0d40deaf-0431-41cc-8f09-370f67c2fcb0",
      "source": "firecrawl/blog/using-prompt-caching-with-anthropic.md",
      "content": "Setup\n\nFirst, make sure you have API keys for both Anthropic and Firecrawl. Store them securely in a `.env` file:  \n```\nANTHROPIC_API_KEY=your_anthropic_key\nFIRECRAWL_API_KEY=your_firecrawl_key\n\n```  \nInstall the required Python packages:  \n```\npip install python-dotenv anthropic firecrawl requests\n\n```",
      "metadata": {
        "title": "How to Use Prompt Caching and Cache Control with Anthropic Models",
        "url": "https://www.firecrawl.dev/blog/using-prompt-caching-with-anthropic"
      }
    },
    {
      "id": "835a8c19-507c-413c-996d-e22e29da5d74",
      "source": "firecrawl/blog/using-prompt-caching-with-anthropic.md",
      "content": "Crawling a Website with Firecrawl\n\nInitialize the Firecrawl app with your API key:  \n```python\napp = FirecrawlApp(api_key=firecrawl_api_key)\n\n```  \nCrawl a website, limiting the results to 10 pages:  \n```python\ncrawl_url = 'https://dify.ai/'\nparams = {\n'crawlOptions': {\n'limit': 10\n}\n}\ncrawl_result = app.crawl_url(crawl_url, params=params)\n\n```  \nClean up the crawl results by removing the `content` field from each entry and save it to a file:  \n```python\ncleaned_crawl_result = [{k: v for k, v in entry.items() if k != 'content'} for entry in crawl_result]\n\nwith open('crawl_result.txt', 'w') as file:\nfile.write(json.dumps(cleaned_crawl_result, indent=4))\n\n```",
      "metadata": {
        "title": "How to Use Prompt Caching and Cache Control with Anthropic Models",
        "url": "https://www.firecrawl.dev/blog/using-prompt-caching-with-anthropic"
      }
    },
    {
      "id": "080a71f1-96c4-4ba6-b4b7-065af6a9e46c",
      "source": "firecrawl/blog/using-prompt-caching-with-anthropic.md",
      "content": "Caching the Crawl Data with Anthropic\n\nLoad the crawl data into a string:  \n```python\nwebsite_dump = open('crawl_result.txt', 'r').read()\n\n```  \nSet up the headers for the Anthropic API request, including the `anthropic-beta` header to enable prompt caching:  \n```python\nheaders = {\n\"content-type\": \"application/json\",\n\"x-api-key\": anthropic_api_key,\n\"anthropic-version\": \"2023-06-01\",\n\"anthropic-beta\": \"prompt-caching-2024-07-31\"\n}\n\n```  \nConstruct the API request data, adding the `website_dump` as an ephemeral cached text:  \n```python\ndata = {\n\"model\": \"claude-3-5-sonnet-20240620\",\n\"max_tokens\": 1024,\n\"system\": [\\\n{\\\n\"type\": \"text\",\\\n\"text\": \"You are an AI assistant tasked with analyzing literary works. Your goal is to provide insightful commentary on themes, characters, and writing style.n\"\\\n},\\\n{\\\n\"type\": \"text\",\\\n\"text\": website_dump,\\\n\"cache_control\": {\"type\": \"ephemeral\"}\\\n}\\\n],\n\"messages\": [\\\n{\\\n\"role\": \"user\",\\\n\"content\": \"How can I improve the copy on this website?\"\\\n}\\\n]\n}\n\n```  \nMake the API request and print the response:  \n```python\nresponse = requests.post(\n\"https://api.anthropic.com/v1/messages\",\nheaders=headers,\ndata=json.dumps(data)\n)\n\nprint(response.json())\n\n```  \nThe key parts here are:  \n1. Including the `anthropic-beta` header to enable prompt caching\n2. Adding the large `website_dump` text as a cached ephemeral text in the `system` messages\n3. Asking the assistant to analyze the cached text and provide suggestions",
      "metadata": {
        "title": "How to Use Prompt Caching and Cache Control with Anthropic Models",
        "url": "https://www.firecrawl.dev/blog/using-prompt-caching-with-anthropic"
      }
    },
    {
      "id": "c7ab8970-3b7d-4f12-a912-86859679a80b",
      "source": "firecrawl/blog/using-prompt-caching-with-anthropic.md",
      "content": "Benefits of Prompt Caching\n\nBy caching the large `website_dump` text, subsequent API calls can reference that data without needing to resend it each time. This makes conversations much faster and cheaper.  \nImagine expanding this to cache an entire knowledge base with up to 200k tokens of data. You can then have highly contextual conversations drawing from that knowledge base in a very efficient manner. The possibilities are endless!  \nAnthropicâ€™s prompt caching is a powerful tool for building AI applications that can process and chat about large datasets. Give it a try and see how it can enhance your projects!  \nArticle updated recently  \n[ðŸ”¥](https://www.firecrawl.dev/)",
      "metadata": {
        "title": "How to Use Prompt Caching and Cache Control with Anthropic Models",
        "url": "https://www.firecrawl.dev/blog/using-prompt-caching-with-anthropic"
      }
    },
    {
      "id": "8b18563f-d89a-4db9-b7c1-cae9d2e74532",
      "source": "firecrawl/blog/using-prompt-caching-with-anthropic.md",
      "content": "Ready to _Build?_\n\nStart scraping web data for your AI apps today.  \nNo credit card needed.  \nGet Started",
      "metadata": {
        "title": "How to Use Prompt Caching and Cache Control with Anthropic Models",
        "url": "https://www.firecrawl.dev/blog/using-prompt-caching-with-anthropic"
      }
    },
    {
      "id": "aeebffc2-77d4-4d86-b241-fcbe6da8cd41",
      "source": "firecrawl/blog/using-prompt-caching-with-anthropic.md",
      "content": "About the Author\n\n[![Eric Ciarla image](https://www.firecrawl.dev/eric-img.jpeg)\\\nEric Ciarla@ericciarla](https://x.com/ericciarla)  \nEric Ciarla is the Chief Operating Officer (COO) of Firecrawl and leads marketing. He also worked on Mendable.ai and sold it to companies like Snapchat, Coinbase, and MongoDB.\nPreviously worked at Ford and Fracta as a Data Scientist. Eric also co-founded SideGuide, a tool for learning code within VS Code with 50,000 users.",
      "metadata": {
        "title": "How to Use Prompt Caching and Cache Control with Anthropic Models",
        "url": "https://www.firecrawl.dev/blog/using-prompt-caching-with-anthropic"
      }
    },
    {
      "id": "3241783f-a944-417f-b340-44ecb77155d3",
      "source": "firecrawl/blog/using-prompt-caching-with-anthropic.md",
      "content": "About the Author > More articles by Eric Ciarla\n\n[How to Create an llms.txt File for Any Website\\\n\\\nLearn how to generate an llms.txt file for any website using the llms.txt Generator and Firecrawl.](https://www.firecrawl.dev/blog/How-to-Create-an-llms-txt-File-for-Any-Website) [Cloudflare Error 1015: How to solve it?\\\n\\\nCloudflare Error 1015 is a rate limiting error that occurs when Cloudflare detects that you are exceeding the request limit set by the website owner.](https://www.firecrawl.dev/blog/cloudflare-error-1015-how-to-solve-it) [Build an agent that checks for website contradictions\\\n\\\nUsing Firecrawl and Claude to scrape your website's data and look for contradictions.](https://www.firecrawl.dev/blog/contradiction-agent) [Why Companies Need a Data Strategy for Generative AI\\\n\\\nLearn why a well-defined data strategy is essential for building robust, production-ready generative AI systems, and discover practical steps for curation, maintenance, and integration.](https://www.firecrawl.dev/blog/why-companies-need-a-data-strategy-for-generative-ai) [Getting Started with OpenAI's Predicted Outputs for Faster LLM Responses\\\n\\\nA guide to leveraging Predicted Outputs to speed up LLM tasks with GPT-4o models.](https://www.firecrawl.dev/blog/getting-started-with-predicted-outputs-openai) [How to easily install requests with pip and python\\\n\\\nA tutorial on installing the requests library in Python using various methods, with usage examples and troubleshooting tips](https://www.firecrawl.dev/blog/how-to-easily-install-requests-with-pip-and-python) [How to quickly install BeautifulSoup with Python\\\n\\\nA guide on installing the BeautifulSoup library in Python using various methods, with usage examples and troubleshooting tips](https://www.firecrawl.dev/blog/how-to-quickly-install-beautifulsoup-with-python) [How to Use OpenAI's o1 Reasoning Models in Your Applications\\\n\\\nLearn how to harness OpenAI's latest o1 series models for complex reasoning tasks in your apps.](https://www.firecrawl.dev/blog/how-to-use-openai-o1-reasoning-models-in-applications)",
      "metadata": {
        "title": "How to Use Prompt Caching and Cache Control with Anthropic Models",
        "url": "https://www.firecrawl.dev/blog/using-prompt-caching-with-anthropic"
      }
    }
  ]
}